program start:
num_rounds= 3
node_emb_dim= 16

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.453125
train loss:  0.7417659163475037
train gradient:  5.759207403173435
iteration : 1
train acc:  0.421875
train loss:  0.7423382997512817
train gradient:  3.203348276098382
iteration : 2
train acc:  0.4609375
train loss:  0.7430390119552612
train gradient:  3.0817639858462105
iteration : 3
train acc:  0.4375
train loss:  0.7170336246490479
train gradient:  1.7749220605071854
iteration : 4
train acc:  0.5859375
train loss:  0.6903507113456726
train gradient:  2.037053692209317
iteration : 5
train acc:  0.53125
train loss:  0.6608185172080994
train gradient:  0.4947137394953891
iteration : 6
train acc:  0.5546875
train loss:  0.6915952563285828
train gradient:  1.0343237814452968
iteration : 7
train acc:  0.4921875
train loss:  0.7027513980865479
train gradient:  1.3885148619124552
iteration : 8
train acc:  0.625
train loss:  0.6598551273345947
train gradient:  1.2954837824575434
iteration : 9
train acc:  0.546875
train loss:  0.6783871650695801
train gradient:  1.3594120530999318
iteration : 10
train acc:  0.578125
train loss:  0.6554359197616577
train gradient:  0.3625059537230802
iteration : 11
train acc:  0.609375
train loss:  0.650712788105011
train gradient:  1.6028531789439717
iteration : 12
train acc:  0.6015625
train loss:  0.6859248876571655
train gradient:  0.693810882555404
iteration : 13
train acc:  0.4921875
train loss:  0.690251350402832
train gradient:  1.1935972233682817
iteration : 14
train acc:  0.5625
train loss:  0.6784303188323975
train gradient:  1.0367954082336732
iteration : 15
train acc:  0.609375
train loss:  0.7112915515899658
train gradient:  1.226846054082271
iteration : 16
train acc:  0.5703125
train loss:  0.6997827291488647
train gradient:  2.1493343149188093
iteration : 17
train acc:  0.546875
train loss:  0.7299118638038635
train gradient:  2.1036596528804563
iteration : 18
train acc:  0.671875
train loss:  0.6498145461082458
train gradient:  0.5365061880824274
iteration : 19
train acc:  0.6328125
train loss:  0.6390963196754456
train gradient:  0.6184442660520991
iteration : 20
train acc:  0.640625
train loss:  0.6757447123527527
train gradient:  0.5162425391649801
iteration : 21
train acc:  0.65625
train loss:  0.6691669821739197
train gradient:  0.7613123637459653
iteration : 22
train acc:  0.5703125
train loss:  0.6612308025360107
train gradient:  0.2848488039543384
iteration : 23
train acc:  0.6171875
train loss:  0.6748365759849548
train gradient:  0.4859687081503533
iteration : 24
train acc:  0.65625
train loss:  0.7181456089019775
train gradient:  1.5638784116329516
iteration : 25
train acc:  0.609375
train loss:  0.7385035157203674
train gradient:  1.106293808624249
iteration : 26
train acc:  0.6171875
train loss:  0.6640156507492065
train gradient:  0.5063077935918431
iteration : 27
train acc:  0.578125
train loss:  0.6648536920547485
train gradient:  0.3379768088779735
iteration : 28
train acc:  0.6171875
train loss:  0.7167598009109497
train gradient:  0.922424307067212
iteration : 29
train acc:  0.59375
train loss:  0.6779104471206665
train gradient:  1.1984646834690422
iteration : 30
train acc:  0.53125
train loss:  0.6976401805877686
train gradient:  0.9829003823922144
iteration : 31
train acc:  0.59375
train loss:  0.6834031343460083
train gradient:  0.6427334414880885
iteration : 32
train acc:  0.6015625
train loss:  0.6551917195320129
train gradient:  0.44475578250068953
iteration : 33
train acc:  0.5859375
train loss:  0.6620391607284546
train gradient:  0.5488056455433238
iteration : 34
train acc:  0.625
train loss:  0.6506292819976807
train gradient:  0.4380831417779605
iteration : 35
train acc:  0.5859375
train loss:  0.6600712537765503
train gradient:  0.3086208513628305
iteration : 36
train acc:  0.6328125
train loss:  0.6567098498344421
train gradient:  0.4603468273988009
iteration : 37
train acc:  0.59375
train loss:  0.724041223526001
train gradient:  0.8726075092890001
iteration : 38
train acc:  0.609375
train loss:  0.6484935283660889
train gradient:  0.44900071394561525
iteration : 39
train acc:  0.625
train loss:  0.6712279915809631
train gradient:  0.3286193980009096
iteration : 40
train acc:  0.4765625
train loss:  0.7195930480957031
train gradient:  0.41342918137289475
iteration : 41
train acc:  0.671875
train loss:  0.6453701257705688
train gradient:  0.3563070530858897
iteration : 42
train acc:  0.6171875
train loss:  0.6357616186141968
train gradient:  0.32066322073531806
iteration : 43
train acc:  0.65625
train loss:  0.6329970359802246
train gradient:  0.18010365279818247
iteration : 44
train acc:  0.59375
train loss:  0.6685320138931274
train gradient:  0.5571680215886794
iteration : 45
train acc:  0.6015625
train loss:  0.6506545543670654
train gradient:  0.290853439351807
iteration : 46
train acc:  0.578125
train loss:  0.6837472915649414
train gradient:  0.5292877713734918
iteration : 47
train acc:  0.6171875
train loss:  0.7039477825164795
train gradient:  0.747821561158503
iteration : 48
train acc:  0.6015625
train loss:  0.6560021638870239
train gradient:  0.18187872803607213
iteration : 49
train acc:  0.59375
train loss:  0.656485378742218
train gradient:  0.12289683370993605
iteration : 50
train acc:  0.6328125
train loss:  0.6810359954833984
train gradient:  0.7922543115659083
iteration : 51
train acc:  0.59375
train loss:  0.639927089214325
train gradient:  0.27867806509996385
iteration : 52
train acc:  0.640625
train loss:  0.6488375663757324
train gradient:  0.3233695662136508
iteration : 53
train acc:  0.609375
train loss:  0.6501044034957886
train gradient:  0.33783583194059535
iteration : 54
train acc:  0.625
train loss:  0.6664050221443176
train gradient:  0.5719041451063629
iteration : 55
train acc:  0.6015625
train loss:  0.6791005730628967
train gradient:  0.5604462419112994
iteration : 56
train acc:  0.6484375
train loss:  0.6252735257148743
train gradient:  0.2381753458290981
iteration : 57
train acc:  0.5546875
train loss:  0.6708938479423523
train gradient:  0.29868912809342263
iteration : 58
train acc:  0.5390625
train loss:  0.6813338398933411
train gradient:  0.4907093518018934
iteration : 59
train acc:  0.625
train loss:  0.652777910232544
train gradient:  0.4605972118131506
iteration : 60
train acc:  0.65625
train loss:  0.663825273513794
train gradient:  0.5012608023355289
iteration : 61
train acc:  0.609375
train loss:  0.6459366083145142
train gradient:  0.3139528184072748
iteration : 62
train acc:  0.59375
train loss:  0.6731948256492615
train gradient:  0.3534010295429035
iteration : 63
train acc:  0.6328125
train loss:  0.6386475563049316
train gradient:  0.39298165670276913
iteration : 64
train acc:  0.671875
train loss:  0.6219697594642639
train gradient:  0.332732628296997
iteration : 65
train acc:  0.6796875
train loss:  0.6289389133453369
train gradient:  0.2641868461779721
iteration : 66
train acc:  0.6328125
train loss:  0.6564830541610718
train gradient:  0.3447638313734342
iteration : 67
train acc:  0.6484375
train loss:  0.6377671957015991
train gradient:  0.29340635358895367
iteration : 68
train acc:  0.578125
train loss:  0.6590145826339722
train gradient:  0.24402902774706214
iteration : 69
train acc:  0.6953125
train loss:  0.6332733035087585
train gradient:  0.8428754951814631
iteration : 70
train acc:  0.6484375
train loss:  0.6584113836288452
train gradient:  0.4051271591362823
iteration : 71
train acc:  0.5859375
train loss:  0.6469830274581909
train gradient:  0.28349429194233594
iteration : 72
train acc:  0.5859375
train loss:  0.6661803722381592
train gradient:  0.1938782073937547
iteration : 73
train acc:  0.6796875
train loss:  0.6258571743965149
train gradient:  0.3606146352384624
iteration : 74
train acc:  0.578125
train loss:  0.6839097738265991
train gradient:  0.6117684941059301
iteration : 75
train acc:  0.625
train loss:  0.6666523218154907
train gradient:  0.4103861734264649
iteration : 76
train acc:  0.5703125
train loss:  0.6539269685745239
train gradient:  0.26461267172179376
iteration : 77
train acc:  0.6171875
train loss:  0.6455843448638916
train gradient:  0.15219069360117182
iteration : 78
train acc:  0.640625
train loss:  0.6470665335655212
train gradient:  0.16677812540121065
iteration : 79
train acc:  0.671875
train loss:  0.6124093532562256
train gradient:  0.33231000164230023
iteration : 80
train acc:  0.578125
train loss:  0.6773747801780701
train gradient:  0.5866483827126955
iteration : 81
train acc:  0.5625
train loss:  0.6578826904296875
train gradient:  0.3697193006603029
iteration : 82
train acc:  0.6484375
train loss:  0.6375643610954285
train gradient:  0.4338271008008381
iteration : 83
train acc:  0.6484375
train loss:  0.6318029761314392
train gradient:  0.19838951311327602
iteration : 84
train acc:  0.65625
train loss:  0.6098986268043518
train gradient:  0.4905236346924168
iteration : 85
train acc:  0.6484375
train loss:  0.6027162075042725
train gradient:  0.44727565129648406
iteration : 86
train acc:  0.671875
train loss:  0.6183637380599976
train gradient:  0.42091112282882354
iteration : 87
train acc:  0.578125
train loss:  0.6668201684951782
train gradient:  0.32952269741921464
iteration : 88
train acc:  0.5625
train loss:  0.6840424537658691
train gradient:  0.28966785792926547
iteration : 89
train acc:  0.609375
train loss:  0.6599727869033813
train gradient:  0.6222117977893222
iteration : 90
train acc:  0.6484375
train loss:  0.6375399231910706
train gradient:  0.40532443885291114
iteration : 91
train acc:  0.6640625
train loss:  0.6272726058959961
train gradient:  0.8718672695694221
iteration : 92
train acc:  0.5859375
train loss:  0.6473088264465332
train gradient:  0.3591962968642431
iteration : 93
train acc:  0.671875
train loss:  0.60779869556427
train gradient:  0.3483149846899568
iteration : 94
train acc:  0.4921875
train loss:  0.6973341703414917
train gradient:  0.5260159773094044
iteration : 95
train acc:  0.703125
train loss:  0.622410774230957
train gradient:  0.4325011393730317
iteration : 96
train acc:  0.6796875
train loss:  0.6106090545654297
train gradient:  0.17656334040572075
iteration : 97
train acc:  0.640625
train loss:  0.6654864549636841
train gradient:  0.6037327593433319
iteration : 98
train acc:  0.6015625
train loss:  0.6762815713882446
train gradient:  0.46303740322154907
iteration : 99
train acc:  0.6640625
train loss:  0.6212277412414551
train gradient:  0.30716516296197177
iteration : 100
train acc:  0.6328125
train loss:  0.6600028276443481
train gradient:  0.7253792576777912
iteration : 101
train acc:  0.5546875
train loss:  0.6964400410652161
train gradient:  0.395571193948209
iteration : 102
train acc:  0.6484375
train loss:  0.6844818592071533
train gradient:  0.46423686194659936
iteration : 103
train acc:  0.640625
train loss:  0.6287044882774353
train gradient:  0.39346090561593616
iteration : 104
train acc:  0.6171875
train loss:  0.6219577789306641
train gradient:  0.44695089128952326
iteration : 105
train acc:  0.5859375
train loss:  0.6529700756072998
train gradient:  0.47108927282883234
iteration : 106
train acc:  0.7265625
train loss:  0.6239620447158813
train gradient:  1.533401565115606
iteration : 107
train acc:  0.640625
train loss:  0.6323072910308838
train gradient:  0.2490093593198823
iteration : 108
train acc:  0.5703125
train loss:  0.6767886877059937
train gradient:  0.6690064845069501
iteration : 109
train acc:  0.6171875
train loss:  0.6394110918045044
train gradient:  0.40240235182554057
iteration : 110
train acc:  0.6171875
train loss:  0.6267284750938416
train gradient:  0.3876066523114128
iteration : 111
train acc:  0.6640625
train loss:  0.6247280836105347
train gradient:  0.52057290689031
iteration : 112
train acc:  0.6796875
train loss:  0.605107843875885
train gradient:  0.217452604275664
iteration : 113
train acc:  0.6171875
train loss:  0.6201592087745667
train gradient:  0.33270412367538765
iteration : 114
train acc:  0.5859375
train loss:  0.696586549282074
train gradient:  1.2911805369445748
iteration : 115
train acc:  0.5703125
train loss:  0.6731732487678528
train gradient:  0.562769728622934
iteration : 116
train acc:  0.625
train loss:  0.6463258862495422
train gradient:  0.6256184583425768
iteration : 117
train acc:  0.6171875
train loss:  0.6420257091522217
train gradient:  0.33588437011869327
iteration : 118
train acc:  0.6796875
train loss:  0.640414834022522
train gradient:  0.6194714332325608
iteration : 119
train acc:  0.65625
train loss:  0.6183936595916748
train gradient:  0.43635042111682665
iteration : 120
train acc:  0.6328125
train loss:  0.6671396493911743
train gradient:  0.5066194570833757
iteration : 121
train acc:  0.53125
train loss:  0.7026548385620117
train gradient:  0.3480444124016568
iteration : 122
train acc:  0.578125
train loss:  0.6555107831954956
train gradient:  0.4227202837718895
iteration : 123
train acc:  0.5
train loss:  0.6973891258239746
train gradient:  0.6862701011535554
iteration : 124
train acc:  0.578125
train loss:  0.6639231443405151
train gradient:  0.531256967106516
iteration : 125
train acc:  0.65625
train loss:  0.6058597564697266
train gradient:  0.30446169825507635
iteration : 126
train acc:  0.671875
train loss:  0.6338977813720703
train gradient:  0.4605437308453205
iteration : 127
train acc:  0.5859375
train loss:  0.6535699367523193
train gradient:  0.4213598151919163
iteration : 128
train acc:  0.609375
train loss:  0.6299436092376709
train gradient:  0.4848399105248045
iteration : 129
train acc:  0.65625
train loss:  0.637901246547699
train gradient:  0.47669142712692897
iteration : 130
train acc:  0.6015625
train loss:  0.6528704762458801
train gradient:  0.47914544280953625
iteration : 131
train acc:  0.65625
train loss:  0.6343569755554199
train gradient:  0.18803461257100917
iteration : 132
train acc:  0.6640625
train loss:  0.6050286889076233
train gradient:  0.3668694587219623
iteration : 133
train acc:  0.6328125
train loss:  0.63686203956604
train gradient:  0.6158708095199943
iteration : 134
train acc:  0.6640625
train loss:  0.6589946746826172
train gradient:  0.7204944529274985
iteration : 135
train acc:  0.5859375
train loss:  0.6574596166610718
train gradient:  0.38234596091273354
iteration : 136
train acc:  0.5546875
train loss:  0.6735023856163025
train gradient:  0.2957856152570434
iteration : 137
train acc:  0.6875
train loss:  0.5979812145233154
train gradient:  0.26807492489000895
iteration : 138
train acc:  0.65625
train loss:  0.6278700828552246
train gradient:  0.13745909296850062
iteration : 139
train acc:  0.53125
train loss:  0.6947880983352661
train gradient:  0.4208493058119545
iteration : 140
train acc:  0.671875
train loss:  0.6227803826332092
train gradient:  0.16199343030269972
iteration : 141
train acc:  0.65625
train loss:  0.6157770156860352
train gradient:  0.48352177973274746
iteration : 142
train acc:  0.703125
train loss:  0.593929648399353
train gradient:  0.2187089132449055
iteration : 143
train acc:  0.734375
train loss:  0.5872459411621094
train gradient:  0.3534497428706185
iteration : 144
train acc:  0.59375
train loss:  0.6676548719406128
train gradient:  0.2033340800725487
iteration : 145
train acc:  0.6796875
train loss:  0.6377130746841431
train gradient:  0.30164941208737395
iteration : 146
train acc:  0.6171875
train loss:  0.6395854353904724
train gradient:  0.2999841546821819
iteration : 147
train acc:  0.625
train loss:  0.6295504570007324
train gradient:  0.24811447265565711
iteration : 148
train acc:  0.625
train loss:  0.6429029703140259
train gradient:  0.37409301896917335
iteration : 149
train acc:  0.703125
train loss:  0.5737367272377014
train gradient:  0.27678501882602013
iteration : 150
train acc:  0.71875
train loss:  0.5660998225212097
train gradient:  0.28319988158313747
iteration : 151
train acc:  0.59375
train loss:  0.6178318858146667
train gradient:  0.3086915000053662
iteration : 152
train acc:  0.6953125
train loss:  0.5592305064201355
train gradient:  0.2757655476184649
iteration : 153
train acc:  0.5859375
train loss:  0.6379407644271851
train gradient:  0.3250772598289809
iteration : 154
train acc:  0.671875
train loss:  0.6230261325836182
train gradient:  0.42600628901361315
iteration : 155
train acc:  0.6953125
train loss:  0.5846771001815796
train gradient:  0.28516139275221186
iteration : 156
train acc:  0.5859375
train loss:  0.6411136388778687
train gradient:  0.48951628880462844
iteration : 157
train acc:  0.7109375
train loss:  0.5896948575973511
train gradient:  0.2042839877177919
iteration : 158
train acc:  0.6640625
train loss:  0.5871984362602234
train gradient:  0.2856554068445051
iteration : 159
train acc:  0.6640625
train loss:  0.688267707824707
train gradient:  0.7696051960687238
iteration : 160
train acc:  0.640625
train loss:  0.6414965391159058
train gradient:  0.35942925944771825
iteration : 161
train acc:  0.6171875
train loss:  0.6767346858978271
train gradient:  0.6712750244974467
iteration : 162
train acc:  0.6796875
train loss:  0.6190340518951416
train gradient:  0.5654059550911692
iteration : 163
train acc:  0.6171875
train loss:  0.6579986810684204
train gradient:  0.6086598679926666
iteration : 164
train acc:  0.7109375
train loss:  0.61273193359375
train gradient:  0.5004808872853271
iteration : 165
train acc:  0.6015625
train loss:  0.63758784532547
train gradient:  0.332370367849787
iteration : 166
train acc:  0.6953125
train loss:  0.5520108938217163
train gradient:  0.24836431099442882
iteration : 167
train acc:  0.6640625
train loss:  0.5909578800201416
train gradient:  0.27718521028602744
iteration : 168
train acc:  0.640625
train loss:  0.616554856300354
train gradient:  0.2608007338991489
iteration : 169
train acc:  0.6875
train loss:  0.6071391105651855
train gradient:  0.24483183068479358
iteration : 170
train acc:  0.6015625
train loss:  0.6711810231208801
train gradient:  0.38374727206216236
iteration : 171
train acc:  0.6796875
train loss:  0.596733570098877
train gradient:  0.38433938937337736
iteration : 172
train acc:  0.71875
train loss:  0.5669103860855103
train gradient:  0.33470006457389045
iteration : 173
train acc:  0.546875
train loss:  0.6844714879989624
train gradient:  0.2161670917712043
iteration : 174
train acc:  0.6015625
train loss:  0.6155223846435547
train gradient:  0.22206764420502617
iteration : 175
train acc:  0.71875
train loss:  0.5500577688217163
train gradient:  0.5210852826367979
iteration : 176
train acc:  0.65625
train loss:  0.5385353565216064
train gradient:  0.3275932815190752
iteration : 177
train acc:  0.625
train loss:  0.6324206590652466
train gradient:  0.24291616600394295
iteration : 178
train acc:  0.5859375
train loss:  0.6393014192581177
train gradient:  0.5786265291307715
iteration : 179
train acc:  0.703125
train loss:  0.5592126846313477
train gradient:  0.2553274456464774
iteration : 180
train acc:  0.6796875
train loss:  0.6314983367919922
train gradient:  0.4890376539351448
iteration : 181
train acc:  0.6015625
train loss:  0.6860077381134033
train gradient:  0.7329225984261671
iteration : 182
train acc:  0.6953125
train loss:  0.5759441256523132
train gradient:  0.4484574392800569
iteration : 183
train acc:  0.671875
train loss:  0.6118447780609131
train gradient:  0.4547999708600826
iteration : 184
train acc:  0.6875
train loss:  0.6320446729660034
train gradient:  0.207051356956403
iteration : 185
train acc:  0.6875
train loss:  0.5893335342407227
train gradient:  0.7506171392052774
iteration : 186
train acc:  0.6171875
train loss:  0.6691678762435913
train gradient:  0.6280650059158954
iteration : 187
train acc:  0.6796875
train loss:  0.5948485136032104
train gradient:  0.3398993706997555
iteration : 188
train acc:  0.671875
train loss:  0.6114978790283203
train gradient:  0.6570498872439193
iteration : 189
train acc:  0.6640625
train loss:  0.6466344594955444
train gradient:  0.7427310918583563
iteration : 190
train acc:  0.6171875
train loss:  0.640151858329773
train gradient:  0.3884915741759955
iteration : 191
train acc:  0.6875
train loss:  0.6215230226516724
train gradient:  0.6821496380532152
iteration : 192
train acc:  0.6640625
train loss:  0.5882771015167236
train gradient:  0.31534238901331013
iteration : 193
train acc:  0.515625
train loss:  0.677034854888916
train gradient:  0.6245751531881133
iteration : 194
train acc:  0.5234375
train loss:  0.6714234948158264
train gradient:  0.8138602779724239
iteration : 195
train acc:  0.625
train loss:  0.6256829500198364
train gradient:  0.36332322876128476
iteration : 196
train acc:  0.6875
train loss:  0.5938634872436523
train gradient:  0.677774722036959
iteration : 197
train acc:  0.671875
train loss:  0.5888983011245728
train gradient:  0.5635100652584294
iteration : 198
train acc:  0.59375
train loss:  0.6578547954559326
train gradient:  0.4813815377726808
iteration : 199
train acc:  0.578125
train loss:  0.6801434755325317
train gradient:  0.3560755716194676
iteration : 200
train acc:  0.6875
train loss:  0.6255592107772827
train gradient:  0.2876418820477879
iteration : 201
train acc:  0.6796875
train loss:  0.6153045296669006
train gradient:  0.6133021780643677
iteration : 202
train acc:  0.6171875
train loss:  0.6443828344345093
train gradient:  0.34418278329046686
iteration : 203
train acc:  0.6328125
train loss:  0.6151297688484192
train gradient:  0.5228417750840086
iteration : 204
train acc:  0.5859375
train loss:  0.6615131497383118
train gradient:  0.3212759670612387
iteration : 205
train acc:  0.609375
train loss:  0.6123186349868774
train gradient:  0.4603724583675229
iteration : 206
train acc:  0.65625
train loss:  0.6264767646789551
train gradient:  0.4105674018123147
iteration : 207
train acc:  0.640625
train loss:  0.6339589357376099
train gradient:  0.40214979382145505
iteration : 208
train acc:  0.6015625
train loss:  0.6517757177352905
train gradient:  0.34130096418417516
iteration : 209
train acc:  0.6953125
train loss:  0.5843431949615479
train gradient:  0.2362567696043381
iteration : 210
train acc:  0.6953125
train loss:  0.6077268719673157
train gradient:  0.3249380808005787
iteration : 211
train acc:  0.65625
train loss:  0.6103904247283936
train gradient:  0.54703561208662
iteration : 212
train acc:  0.6484375
train loss:  0.6276566982269287
train gradient:  0.42498859515572834
iteration : 213
train acc:  0.65625
train loss:  0.610994279384613
train gradient:  0.4612279250764349
iteration : 214
train acc:  0.578125
train loss:  0.6463233232498169
train gradient:  0.35613757026133863
iteration : 215
train acc:  0.671875
train loss:  0.5902587175369263
train gradient:  0.23721618293566155
iteration : 216
train acc:  0.6640625
train loss:  0.6269843578338623
train gradient:  0.17886178508672948
iteration : 217
train acc:  0.640625
train loss:  0.6130196452140808
train gradient:  0.41892472695432525
iteration : 218
train acc:  0.6796875
train loss:  0.5644559264183044
train gradient:  0.3077472595663964
iteration : 219
train acc:  0.703125
train loss:  0.5949679017066956
train gradient:  0.4152034505943512
iteration : 220
train acc:  0.6015625
train loss:  0.6520222425460815
train gradient:  0.3511874037913572
iteration : 221
train acc:  0.6015625
train loss:  0.6597857475280762
train gradient:  0.549226410686015
iteration : 222
train acc:  0.7109375
train loss:  0.5668240785598755
train gradient:  0.17781188518680863
iteration : 223
train acc:  0.6171875
train loss:  0.6403696537017822
train gradient:  0.2238594203511613
iteration : 224
train acc:  0.5859375
train loss:  0.7008428573608398
train gradient:  0.7784029201903362
iteration : 225
train acc:  0.7109375
train loss:  0.5651828050613403
train gradient:  0.24027422114042984
iteration : 226
train acc:  0.7109375
train loss:  0.6420447826385498
train gradient:  0.40160499907993774
iteration : 227
train acc:  0.5859375
train loss:  0.6651170253753662
train gradient:  0.3197785440006846
iteration : 228
train acc:  0.5859375
train loss:  0.6410919427871704
train gradient:  0.4920481202514051
iteration : 229
train acc:  0.6171875
train loss:  0.6399834156036377
train gradient:  0.6756814822500066
iteration : 230
train acc:  0.6484375
train loss:  0.6456425189971924
train gradient:  0.72972095854334
iteration : 231
train acc:  0.7109375
train loss:  0.5840021371841431
train gradient:  0.27533995029422703
iteration : 232
train acc:  0.6328125
train loss:  0.6218780279159546
train gradient:  0.2690902900653467
iteration : 233
train acc:  0.6328125
train loss:  0.6575378775596619
train gradient:  0.3150719535195475
iteration : 234
train acc:  0.640625
train loss:  0.5802513360977173
train gradient:  0.24894722701426347
iteration : 235
train acc:  0.609375
train loss:  0.613550066947937
train gradient:  0.3503565476907396
iteration : 236
train acc:  0.6875
train loss:  0.5945478081703186
train gradient:  0.21092511681723697
iteration : 237
train acc:  0.7421875
train loss:  0.5511919260025024
train gradient:  0.2706025297222832
iteration : 238
train acc:  0.6640625
train loss:  0.6096965074539185
train gradient:  0.3173958024829714
iteration : 239
train acc:  0.6484375
train loss:  0.625518798828125
train gradient:  0.27028263257610985
iteration : 240
train acc:  0.640625
train loss:  0.6093989610671997
train gradient:  0.2850739337953133
iteration : 241
train acc:  0.6640625
train loss:  0.6118446588516235
train gradient:  0.2632278450965446
iteration : 242
train acc:  0.734375
train loss:  0.5520297288894653
train gradient:  0.1812871978730633
iteration : 243
train acc:  0.6953125
train loss:  0.5664554834365845
train gradient:  0.2330123333050107
iteration : 244
train acc:  0.671875
train loss:  0.611335813999176
train gradient:  0.21370904071668498
iteration : 245
train acc:  0.65625
train loss:  0.6279817819595337
train gradient:  0.22314900787312242
iteration : 246
train acc:  0.65625
train loss:  0.6304033398628235
train gradient:  0.27533193908808934
iteration : 247
train acc:  0.7109375
train loss:  0.5647585988044739
train gradient:  0.23372009592948673
iteration : 248
train acc:  0.640625
train loss:  0.6303733587265015
train gradient:  0.30180118866758965
iteration : 249
train acc:  0.640625
train loss:  0.6015702486038208
train gradient:  0.14514423624313583
iteration : 250
train acc:  0.6953125
train loss:  0.5820883512496948
train gradient:  0.16943578915952337
iteration : 251
train acc:  0.671875
train loss:  0.6153305768966675
train gradient:  0.2563727766506203
iteration : 252
train acc:  0.65625
train loss:  0.615834653377533
train gradient:  0.519249500854498
iteration : 253
train acc:  0.75
train loss:  0.5628392696380615
train gradient:  0.45661491542118365
iteration : 254
train acc:  0.625
train loss:  0.6640723347663879
train gradient:  0.4929903265696655
iteration : 255
train acc:  0.6796875
train loss:  0.5811872482299805
train gradient:  0.27104884315449396
iteration : 256
train acc:  0.640625
train loss:  0.5869417190551758
train gradient:  0.1640121647122643
iteration : 257
train acc:  0.6875
train loss:  0.5965511798858643
train gradient:  0.28449885634276195
iteration : 258
train acc:  0.6328125
train loss:  0.6468634605407715
train gradient:  0.5907635668149246
iteration : 259
train acc:  0.578125
train loss:  0.6978205442428589
train gradient:  0.5114512109936897
iteration : 260
train acc:  0.75
train loss:  0.5405457019805908
train gradient:  0.21012800588399372
iteration : 261
train acc:  0.6171875
train loss:  0.6969047784805298
train gradient:  0.638279732269078
iteration : 262
train acc:  0.65625
train loss:  0.587126612663269
train gradient:  0.22688120380069993
iteration : 263
train acc:  0.6328125
train loss:  0.5966663360595703
train gradient:  0.34408819293774195
iteration : 264
train acc:  0.671875
train loss:  0.6205442547798157
train gradient:  0.24468643535467707
iteration : 265
train acc:  0.703125
train loss:  0.5907847881317139
train gradient:  0.27460805553291046
iteration : 266
train acc:  0.640625
train loss:  0.5895113348960876
train gradient:  0.18678486994233734
iteration : 267
train acc:  0.6875
train loss:  0.5619454383850098
train gradient:  0.23245918156307777
iteration : 268
train acc:  0.671875
train loss:  0.5806961059570312
train gradient:  0.2724598179836245
iteration : 269
train acc:  0.6875
train loss:  0.5746374130249023
train gradient:  0.4545203129468883
iteration : 270
train acc:  0.6171875
train loss:  0.6963390111923218
train gradient:  0.8889936910390454
iteration : 271
train acc:  0.6328125
train loss:  0.6567913889884949
train gradient:  1.0479385981665719
iteration : 272
train acc:  0.6015625
train loss:  0.6374969482421875
train gradient:  0.511559711600325
iteration : 273
train acc:  0.6328125
train loss:  0.6421411633491516
train gradient:  0.40630116458184196
iteration : 274
train acc:  0.6171875
train loss:  0.6490418314933777
train gradient:  0.5769982472917587
iteration : 275
train acc:  0.6640625
train loss:  0.6018193960189819
train gradient:  0.2916888052517633
iteration : 276
train acc:  0.703125
train loss:  0.567535936832428
train gradient:  0.38071898276517924
iteration : 277
train acc:  0.625
train loss:  0.6061868667602539
train gradient:  0.39074042338660825
iteration : 278
train acc:  0.71875
train loss:  0.5491661429405212
train gradient:  0.37491680959092605
iteration : 279
train acc:  0.5703125
train loss:  0.6727204918861389
train gradient:  0.3642596313648521
iteration : 280
train acc:  0.6796875
train loss:  0.6059615612030029
train gradient:  0.350710469667115
iteration : 281
train acc:  0.6640625
train loss:  0.606619119644165
train gradient:  0.4296045552993717
iteration : 282
train acc:  0.703125
train loss:  0.6160184144973755
train gradient:  0.37220979755779443
iteration : 283
train acc:  0.6171875
train loss:  0.6252368092536926
train gradient:  0.4476575953768368
iteration : 284
train acc:  0.6328125
train loss:  0.6030625104904175
train gradient:  0.22775617579050228
iteration : 285
train acc:  0.65625
train loss:  0.5909745693206787
train gradient:  0.401958895692555
iteration : 286
train acc:  0.7421875
train loss:  0.5725291967391968
train gradient:  0.30982774411058006
iteration : 287
train acc:  0.65625
train loss:  0.6119298934936523
train gradient:  0.337874589787752
iteration : 288
train acc:  0.6953125
train loss:  0.60052490234375
train gradient:  0.35589643595296355
iteration : 289
train acc:  0.703125
train loss:  0.6005263328552246
train gradient:  0.30034679021822724
iteration : 290
train acc:  0.671875
train loss:  0.6327145099639893
train gradient:  0.4332155280017069
iteration : 291
train acc:  0.6171875
train loss:  0.6090914607048035
train gradient:  0.2925648470755124
iteration : 292
train acc:  0.625
train loss:  0.6545438766479492
train gradient:  0.32990577235854074
iteration : 293
train acc:  0.640625
train loss:  0.6100761890411377
train gradient:  0.43398337342201604
iteration : 294
train acc:  0.671875
train loss:  0.6540600657463074
train gradient:  0.4837045309349629
iteration : 295
train acc:  0.6640625
train loss:  0.6068040132522583
train gradient:  0.34300376410529276
iteration : 296
train acc:  0.625
train loss:  0.6176596283912659
train gradient:  0.344177721590409
iteration : 297
train acc:  0.6796875
train loss:  0.583797812461853
train gradient:  0.2733591711991984
iteration : 298
train acc:  0.625
train loss:  0.6190750598907471
train gradient:  0.3003859196827753
iteration : 299
train acc:  0.7265625
train loss:  0.5601520538330078
train gradient:  0.33718618384714355
iteration : 300
train acc:  0.6875
train loss:  0.6121664047241211
train gradient:  0.44321890144677917
iteration : 301
train acc:  0.640625
train loss:  0.6284542679786682
train gradient:  0.4087165026609877
iteration : 302
train acc:  0.6484375
train loss:  0.6219066381454468
train gradient:  0.38329687860759404
iteration : 303
train acc:  0.6328125
train loss:  0.6600805521011353
train gradient:  0.4279945353297895
iteration : 304
train acc:  0.6796875
train loss:  0.5920488834381104
train gradient:  0.2197163564805752
iteration : 305
train acc:  0.7421875
train loss:  0.5421189069747925
train gradient:  0.2730285705319356
iteration : 306
train acc:  0.6953125
train loss:  0.6045593023300171
train gradient:  0.3917685183338117
iteration : 307
train acc:  0.625
train loss:  0.6213833093643188
train gradient:  0.24493438190077038
iteration : 308
train acc:  0.6484375
train loss:  0.6271804571151733
train gradient:  0.2573997469112229
iteration : 309
train acc:  0.6796875
train loss:  0.6075770258903503
train gradient:  0.3070711851081089
iteration : 310
train acc:  0.6875
train loss:  0.5834007859230042
train gradient:  0.21659545029012206
iteration : 311
train acc:  0.6015625
train loss:  0.6371238231658936
train gradient:  0.30007912612588566
iteration : 312
train acc:  0.71875
train loss:  0.5876338481903076
train gradient:  0.27041980607410104
iteration : 313
train acc:  0.703125
train loss:  0.5769609808921814
train gradient:  0.2365442958543041
iteration : 314
train acc:  0.6796875
train loss:  0.5918935537338257
train gradient:  0.18737240020847906
iteration : 315
train acc:  0.671875
train loss:  0.5740953087806702
train gradient:  0.29087407578977814
iteration : 316
train acc:  0.578125
train loss:  0.6675915718078613
train gradient:  0.323561924921936
iteration : 317
train acc:  0.640625
train loss:  0.634523868560791
train gradient:  0.30374304802873586
iteration : 318
train acc:  0.6171875
train loss:  0.6254085302352905
train gradient:  0.2529563059550999
iteration : 319
train acc:  0.6796875
train loss:  0.5925964117050171
train gradient:  0.20658256370699468
iteration : 320
train acc:  0.6640625
train loss:  0.6023106575012207
train gradient:  0.3091862266632108
iteration : 321
train acc:  0.6328125
train loss:  0.6050604581832886
train gradient:  0.32550034302000663
iteration : 322
train acc:  0.75
train loss:  0.5558604001998901
train gradient:  0.22410099357488134
iteration : 323
train acc:  0.7109375
train loss:  0.6167475581169128
train gradient:  0.4665792698660217
iteration : 324
train acc:  0.71875
train loss:  0.591362714767456
train gradient:  0.35458917816538044
iteration : 325
train acc:  0.6953125
train loss:  0.6194748282432556
train gradient:  0.34921501097400687
iteration : 326
train acc:  0.734375
train loss:  0.5590952634811401
train gradient:  0.24896566042738347
iteration : 327
train acc:  0.6875
train loss:  0.6190897226333618
train gradient:  0.4395122591429637
iteration : 328
train acc:  0.671875
train loss:  0.5971016883850098
train gradient:  0.23384794128701336
iteration : 329
train acc:  0.734375
train loss:  0.5589488744735718
train gradient:  0.37100260190140827
iteration : 330
train acc:  0.671875
train loss:  0.6062496900558472
train gradient:  0.3097940053902366
iteration : 331
train acc:  0.65625
train loss:  0.6059800982475281
train gradient:  0.39389196033584256
iteration : 332
train acc:  0.6953125
train loss:  0.5850531458854675
train gradient:  0.29150609093594065
iteration : 333
train acc:  0.671875
train loss:  0.5903764963150024
train gradient:  0.3007250814883839
iteration : 334
train acc:  0.6328125
train loss:  0.5842435359954834
train gradient:  0.30063252632158144
iteration : 335
train acc:  0.7265625
train loss:  0.5645376443862915
train gradient:  0.3817435670312493
iteration : 336
train acc:  0.6015625
train loss:  0.6668670177459717
train gradient:  0.49352330110421855
iteration : 337
train acc:  0.6875
train loss:  0.5896847248077393
train gradient:  0.41725074754529434
iteration : 338
train acc:  0.6640625
train loss:  0.5685784816741943
train gradient:  0.3466709468328242
iteration : 339
train acc:  0.7109375
train loss:  0.5554338097572327
train gradient:  0.4076830326475443
iteration : 340
train acc:  0.7265625
train loss:  0.580542802810669
train gradient:  0.41583194503521986
iteration : 341
train acc:  0.6328125
train loss:  0.6166481375694275
train gradient:  0.3933552799563393
iteration : 342
train acc:  0.6640625
train loss:  0.5993300676345825
train gradient:  0.4526019206329482
iteration : 343
train acc:  0.6484375
train loss:  0.6116446256637573
train gradient:  0.42214004351916046
iteration : 344
train acc:  0.6484375
train loss:  0.6284789443016052
train gradient:  0.4094950223907928
iteration : 345
train acc:  0.625
train loss:  0.6100013256072998
train gradient:  0.3517108837615395
iteration : 346
train acc:  0.6484375
train loss:  0.6240309476852417
train gradient:  0.3890482918779605
iteration : 347
train acc:  0.703125
train loss:  0.5803009271621704
train gradient:  0.37623039802855257
iteration : 348
train acc:  0.6953125
train loss:  0.5788743495941162
train gradient:  0.33515854770926995
iteration : 349
train acc:  0.6640625
train loss:  0.5853670835494995
train gradient:  0.28496017554162173
iteration : 350
train acc:  0.65625
train loss:  0.6049461364746094
train gradient:  0.49386088683166285
iteration : 351
train acc:  0.7265625
train loss:  0.521691083908081
train gradient:  0.26506242421143017
iteration : 352
train acc:  0.75
train loss:  0.606805682182312
train gradient:  0.554945280596767
iteration : 353
train acc:  0.6796875
train loss:  0.5893202424049377
train gradient:  0.3783151536603256
iteration : 354
train acc:  0.671875
train loss:  0.5862317085266113
train gradient:  0.22385246043604537
iteration : 355
train acc:  0.7109375
train loss:  0.5246087312698364
train gradient:  0.27075495145367934
iteration : 356
train acc:  0.640625
train loss:  0.5947694778442383
train gradient:  0.5182507078141918
iteration : 357
train acc:  0.6796875
train loss:  0.6378580331802368
train gradient:  0.45004318243777935
iteration : 358
train acc:  0.6796875
train loss:  0.5974646806716919
train gradient:  0.5253328999132583
iteration : 359
train acc:  0.6640625
train loss:  0.6190364360809326
train gradient:  0.30192967991969594
iteration : 360
train acc:  0.6875
train loss:  0.5790807604789734
train gradient:  0.41936003969929425
iteration : 361
train acc:  0.6796875
train loss:  0.6194872856140137
train gradient:  0.36926038652709053
iteration : 362
train acc:  0.6171875
train loss:  0.6168749928474426
train gradient:  0.35912980334764943
iteration : 363
train acc:  0.671875
train loss:  0.6034138202667236
train gradient:  0.27928928644830847
iteration : 364
train acc:  0.703125
train loss:  0.5554846525192261
train gradient:  0.24070633735935465
iteration : 365
train acc:  0.6875
train loss:  0.5836763978004456
train gradient:  0.19371220900713731
iteration : 366
train acc:  0.734375
train loss:  0.5841356515884399
train gradient:  0.531286352450692
iteration : 367
train acc:  0.71875
train loss:  0.5592765212059021
train gradient:  0.21406137445114382
iteration : 368
train acc:  0.7265625
train loss:  0.520685613155365
train gradient:  0.2530613158125022
iteration : 369
train acc:  0.6640625
train loss:  0.6092827916145325
train gradient:  0.2763150834461145
iteration : 370
train acc:  0.703125
train loss:  0.5542575716972351
train gradient:  0.3525030813918188
iteration : 371
train acc:  0.6796875
train loss:  0.5830713510513306
train gradient:  0.5356545953219021
iteration : 372
train acc:  0.6796875
train loss:  0.583630383014679
train gradient:  0.407383825847861
iteration : 373
train acc:  0.6328125
train loss:  0.6200236082077026
train gradient:  0.3385167980879378
iteration : 374
train acc:  0.6875
train loss:  0.6001116633415222
train gradient:  0.36439195344244135
iteration : 375
train acc:  0.7109375
train loss:  0.6155245900154114
train gradient:  0.2852834497573758
iteration : 376
train acc:  0.6640625
train loss:  0.5864394307136536
train gradient:  0.3391514155968592
iteration : 377
train acc:  0.65625
train loss:  0.6326716542243958
train gradient:  0.5638519143620977
iteration : 378
train acc:  0.65625
train loss:  0.6112417578697205
train gradient:  0.3414299554141718
iteration : 379
train acc:  0.703125
train loss:  0.561417818069458
train gradient:  0.35744874629838236
iteration : 380
train acc:  0.6640625
train loss:  0.6060407161712646
train gradient:  0.30717432878513434
iteration : 381
train acc:  0.75
train loss:  0.5384913682937622
train gradient:  0.3835897467262263
iteration : 382
train acc:  0.6953125
train loss:  0.5778070688247681
train gradient:  0.32603778115530185
iteration : 383
train acc:  0.6875
train loss:  0.660415768623352
train gradient:  0.5508799483656187
iteration : 384
train acc:  0.6953125
train loss:  0.5775676965713501
train gradient:  0.2097045623085584
iteration : 385
train acc:  0.7265625
train loss:  0.5407298803329468
train gradient:  0.4399311131599352
iteration : 386
train acc:  0.7265625
train loss:  0.5409433245658875
train gradient:  0.36619379611525155
iteration : 387
train acc:  0.7578125
train loss:  0.5429822206497192
train gradient:  0.3651722957957081
iteration : 388
train acc:  0.65625
train loss:  0.6336462497711182
train gradient:  0.6745109627295846
iteration : 389
train acc:  0.671875
train loss:  0.5932104587554932
train gradient:  0.43315703079483137
iteration : 390
train acc:  0.6953125
train loss:  0.5864886045455933
train gradient:  0.6553146935087949
iteration : 391
train acc:  0.734375
train loss:  0.5791109204292297
train gradient:  0.43393413333592323
iteration : 392
train acc:  0.609375
train loss:  0.6274290084838867
train gradient:  0.4332772074125997
iteration : 393
train acc:  0.640625
train loss:  0.63057541847229
train gradient:  0.5026380377942363
iteration : 394
train acc:  0.6640625
train loss:  0.6063811779022217
train gradient:  0.3392963331218112
iteration : 395
train acc:  0.609375
train loss:  0.6178285479545593
train gradient:  0.4415455378335939
iteration : 396
train acc:  0.703125
train loss:  0.5642266273498535
train gradient:  0.35807923961125177
iteration : 397
train acc:  0.71875
train loss:  0.5846196413040161
train gradient:  0.44467652463205126
iteration : 398
train acc:  0.765625
train loss:  0.5005183815956116
train gradient:  0.2643027658076092
iteration : 399
train acc:  0.65625
train loss:  0.6315757036209106
train gradient:  0.41578109499396765
iteration : 400
train acc:  0.6328125
train loss:  0.6477714776992798
train gradient:  0.7560499150386324
iteration : 401
train acc:  0.6484375
train loss:  0.6041387319564819
train gradient:  0.4418985407657765
iteration : 402
train acc:  0.6953125
train loss:  0.5721584558486938
train gradient:  0.41480015011015736
iteration : 403
train acc:  0.671875
train loss:  0.6141988635063171
train gradient:  0.4996087263702904
iteration : 404
train acc:  0.75
train loss:  0.5254338979721069
train gradient:  0.31208405340504325
iteration : 405
train acc:  0.65625
train loss:  0.5746934413909912
train gradient:  0.3278640333050724
iteration : 406
train acc:  0.640625
train loss:  0.618935227394104
train gradient:  0.5607398043428263
iteration : 407
train acc:  0.6796875
train loss:  0.6103806495666504
train gradient:  0.39422113991605057
iteration : 408
train acc:  0.6484375
train loss:  0.5853000283241272
train gradient:  0.25625633984676227
iteration : 409
train acc:  0.6640625
train loss:  0.5916441679000854
train gradient:  0.4790691139748143
iteration : 410
train acc:  0.6875
train loss:  0.5775874853134155
train gradient:  0.4039675069797978
iteration : 411
train acc:  0.640625
train loss:  0.6104769706726074
train gradient:  0.47332484139332826
iteration : 412
train acc:  0.640625
train loss:  0.604694128036499
train gradient:  0.48789617179890715
iteration : 413
train acc:  0.7109375
train loss:  0.5799278616905212
train gradient:  0.42405558997390513
iteration : 414
train acc:  0.65625
train loss:  0.6437832117080688
train gradient:  0.49360098447920653
iteration : 415
train acc:  0.71875
train loss:  0.5988569855690002
train gradient:  0.38364449458467387
iteration : 416
train acc:  0.703125
train loss:  0.558875322341919
train gradient:  0.3407340994251017
iteration : 417
train acc:  0.734375
train loss:  0.5614063739776611
train gradient:  0.33406149136323254
iteration : 418
train acc:  0.6640625
train loss:  0.6048152446746826
train gradient:  0.40653942269412563
iteration : 419
train acc:  0.7265625
train loss:  0.5578471422195435
train gradient:  0.44229854539610197
iteration : 420
train acc:  0.75
train loss:  0.5640628337860107
train gradient:  0.2776044504176972
iteration : 421
train acc:  0.734375
train loss:  0.5660141706466675
train gradient:  0.3186090639306479
iteration : 422
train acc:  0.7421875
train loss:  0.5343553423881531
train gradient:  0.2692185935568091
iteration : 423
train acc:  0.6640625
train loss:  0.5723688006401062
train gradient:  0.42856677660561215
iteration : 424
train acc:  0.6953125
train loss:  0.5493613481521606
train gradient:  0.41371120610060563
iteration : 425
train acc:  0.765625
train loss:  0.527866005897522
train gradient:  0.39672601285812575
iteration : 426
train acc:  0.734375
train loss:  0.5620928406715393
train gradient:  0.2740788730549921
iteration : 427
train acc:  0.6640625
train loss:  0.5998068451881409
train gradient:  0.4464546970461533
iteration : 428
train acc:  0.75
train loss:  0.53534996509552
train gradient:  0.2788021852504761
iteration : 429
train acc:  0.671875
train loss:  0.5733787417411804
train gradient:  0.3395392074178537
iteration : 430
train acc:  0.6875
train loss:  0.6055185794830322
train gradient:  0.38796431137649734
iteration : 431
train acc:  0.7265625
train loss:  0.5669267773628235
train gradient:  0.264193312283348
iteration : 432
train acc:  0.7109375
train loss:  0.5293801426887512
train gradient:  0.46393418481537935
iteration : 433
train acc:  0.6953125
train loss:  0.6105051040649414
train gradient:  0.424659451710716
iteration : 434
train acc:  0.734375
train loss:  0.5486589074134827
train gradient:  0.38580870818529317
iteration : 435
train acc:  0.734375
train loss:  0.5400530099868774
train gradient:  0.4002243261314352
iteration : 436
train acc:  0.8046875
train loss:  0.48040303587913513
train gradient:  0.322437548583335
iteration : 437
train acc:  0.703125
train loss:  0.5437420010566711
train gradient:  0.40092616798355424
iteration : 438
train acc:  0.6484375
train loss:  0.6159552335739136
train gradient:  0.3941990641229236
iteration : 439
train acc:  0.6953125
train loss:  0.540568470954895
train gradient:  0.35904783247238575
iteration : 440
train acc:  0.734375
train loss:  0.5137391090393066
train gradient:  0.38556632418412673
iteration : 441
train acc:  0.671875
train loss:  0.5985526442527771
train gradient:  0.5668267630705712
iteration : 442
train acc:  0.7890625
train loss:  0.49164390563964844
train gradient:  0.49592370069141195
iteration : 443
train acc:  0.671875
train loss:  0.5572955012321472
train gradient:  0.48360553257511196
iteration : 444
train acc:  0.6953125
train loss:  0.6266466379165649
train gradient:  0.6639259543444955
iteration : 445
train acc:  0.65625
train loss:  0.5992804765701294
train gradient:  0.7068290298001786
iteration : 446
train acc:  0.6953125
train loss:  0.5706623196601868
train gradient:  0.6572703799001938
iteration : 447
train acc:  0.671875
train loss:  0.5874684453010559
train gradient:  0.5302012194638749
iteration : 448
train acc:  0.6875
train loss:  0.5721954107284546
train gradient:  0.4742284589616152
iteration : 449
train acc:  0.65625
train loss:  0.6137839555740356
train gradient:  0.4752267242965501
iteration : 450
train acc:  0.7109375
train loss:  0.5606738328933716
train gradient:  0.5584651428459034
iteration : 451
train acc:  0.703125
train loss:  0.5582714676856995
train gradient:  0.4635487439526606
iteration : 452
train acc:  0.703125
train loss:  0.5327842235565186
train gradient:  0.4618575459673107
iteration : 453
train acc:  0.71875
train loss:  0.5347614884376526
train gradient:  0.6512817693676122
iteration : 454
train acc:  0.6875
train loss:  0.58786940574646
train gradient:  0.5440967264933769
iteration : 455
train acc:  0.7421875
train loss:  0.5239835381507874
train gradient:  0.29925641846473616
iteration : 456
train acc:  0.7578125
train loss:  0.49557870626449585
train gradient:  0.2531373186470723
iteration : 457
train acc:  0.765625
train loss:  0.5326346755027771
train gradient:  0.30442821000034115
iteration : 458
train acc:  0.703125
train loss:  0.5277237892150879
train gradient:  0.355926760490649
iteration : 459
train acc:  0.75
train loss:  0.5573387145996094
train gradient:  0.389169396031536
iteration : 460
train acc:  0.765625
train loss:  0.5174008011817932
train gradient:  0.43693273583681996
iteration : 461
train acc:  0.640625
train loss:  0.6091440916061401
train gradient:  0.8086503815830755
iteration : 462
train acc:  0.75
train loss:  0.5192290544509888
train gradient:  0.37601831413086617
iteration : 463
train acc:  0.71875
train loss:  0.551355242729187
train gradient:  0.4234210074313124
iteration : 464
train acc:  0.7109375
train loss:  0.5052218437194824
train gradient:  0.2976408668099273
iteration : 465
train acc:  0.734375
train loss:  0.5470970869064331
train gradient:  0.7582029764130828
iteration : 466
train acc:  0.765625
train loss:  0.4990748167037964
train gradient:  0.44898717570113106
iteration : 467
train acc:  0.7109375
train loss:  0.5610899925231934
train gradient:  0.5554335278555348
iteration : 468
train acc:  0.7109375
train loss:  0.5940061211585999
train gradient:  0.7809746325190742
iteration : 469
train acc:  0.671875
train loss:  0.630814790725708
train gradient:  0.6986556131936091
iteration : 470
train acc:  0.7265625
train loss:  0.5311903357505798
train gradient:  0.5257448546968883
iteration : 471
train acc:  0.671875
train loss:  0.5918056964874268
train gradient:  0.6349274765633517
iteration : 472
train acc:  0.78125
train loss:  0.4825133681297302
train gradient:  0.4139376007928716
iteration : 473
train acc:  0.703125
train loss:  0.5550268292427063
train gradient:  0.48647562295647456
iteration : 474
train acc:  0.7265625
train loss:  0.5269337892532349
train gradient:  0.4193216746035184
iteration : 475
train acc:  0.71875
train loss:  0.5286215543746948
train gradient:  0.34563128107906305
iteration : 476
train acc:  0.6953125
train loss:  0.5397050380706787
train gradient:  0.3826814855677991
iteration : 477
train acc:  0.75
train loss:  0.5504952669143677
train gradient:  0.43306927179194654
iteration : 478
train acc:  0.6953125
train loss:  0.5707687139511108
train gradient:  0.4060830110113946
iteration : 479
train acc:  0.71875
train loss:  0.5491559505462646
train gradient:  0.44649571830560286
iteration : 480
train acc:  0.7109375
train loss:  0.574555516242981
train gradient:  0.4931476834499337
iteration : 481
train acc:  0.703125
train loss:  0.5855183601379395
train gradient:  0.5581144900660009
iteration : 482
train acc:  0.7578125
train loss:  0.5178549885749817
train gradient:  0.31155941217304367
iteration : 483
train acc:  0.765625
train loss:  0.5692209005355835
train gradient:  0.8126168656832489
iteration : 484
train acc:  0.703125
train loss:  0.5570396184921265
train gradient:  0.4348231745365186
iteration : 485
train acc:  0.7734375
train loss:  0.48215627670288086
train gradient:  0.3163820582029476
iteration : 486
train acc:  0.75
train loss:  0.5488179922103882
train gradient:  0.5359279503427834
iteration : 487
train acc:  0.7265625
train loss:  0.5369546413421631
train gradient:  0.5913650672143053
iteration : 488
train acc:  0.7109375
train loss:  0.5771968364715576
train gradient:  0.3892329319491588
iteration : 489
train acc:  0.7421875
train loss:  0.5181468725204468
train gradient:  0.4194086908648073
iteration : 490
train acc:  0.75
train loss:  0.5201224684715271
train gradient:  0.44043936189639354
iteration : 491
train acc:  0.7421875
train loss:  0.48333442211151123
train gradient:  0.37773446376113534
iteration : 492
train acc:  0.703125
train loss:  0.6093989014625549
train gradient:  0.43907143385681724
iteration : 493
train acc:  0.6875
train loss:  0.6203433871269226
train gradient:  0.39909491292856625
iteration : 494
train acc:  0.703125
train loss:  0.5375173687934875
train gradient:  0.6684299349040099
iteration : 495
train acc:  0.7734375
train loss:  0.5236019492149353
train gradient:  0.511189213557909
iteration : 496
train acc:  0.71875
train loss:  0.5542449355125427
train gradient:  0.6124657773657429
iteration : 497
train acc:  0.7578125
train loss:  0.5238131284713745
train gradient:  0.5879699965269054
iteration : 498
train acc:  0.6875
train loss:  0.5634883642196655
train gradient:  0.3792710979131101
iteration : 499
train acc:  0.7578125
train loss:  0.5213432908058167
train gradient:  0.4322361393700161
iteration : 500
train acc:  0.7109375
train loss:  0.507110595703125
train gradient:  0.5026314192699606
iteration : 501
train acc:  0.625
train loss:  0.6553161144256592
train gradient:  0.7857392459135735
iteration : 502
train acc:  0.6796875
train loss:  0.5466686487197876
train gradient:  0.7400212399996491
iteration : 503
train acc:  0.6484375
train loss:  0.6192632913589478
train gradient:  0.6007078598491387
iteration : 504
train acc:  0.6484375
train loss:  0.6496610641479492
train gradient:  0.7144383082927871
iteration : 505
train acc:  0.7421875
train loss:  0.5259618759155273
train gradient:  0.5976041063057356
iteration : 506
train acc:  0.7578125
train loss:  0.44977977871894836
train gradient:  0.3904251642327103
iteration : 507
train acc:  0.734375
train loss:  0.5396000146865845
train gradient:  0.3529636139405558
iteration : 508
train acc:  0.6328125
train loss:  0.6352954506874084
train gradient:  0.6947655265855284
iteration : 509
train acc:  0.703125
train loss:  0.5467881560325623
train gradient:  0.4455607465872733
iteration : 510
train acc:  0.7265625
train loss:  0.5019457936286926
train gradient:  0.5298234336883798
iteration : 511
train acc:  0.75
train loss:  0.5075870752334595
train gradient:  1.0052077379578344
iteration : 512
train acc:  0.6953125
train loss:  0.5233681797981262
train gradient:  0.4588073824715944
iteration : 513
train acc:  0.6953125
train loss:  0.535306990146637
train gradient:  0.2954790181930553
iteration : 514
train acc:  0.71875
train loss:  0.5582377910614014
train gradient:  0.4675685198113892
iteration : 515
train acc:  0.7890625
train loss:  0.45790743827819824
train gradient:  0.470313715035069
iteration : 516
train acc:  0.78125
train loss:  0.4947448670864105
train gradient:  0.60080187117121
iteration : 517
train acc:  0.703125
train loss:  0.5484389066696167
train gradient:  0.43297937338297715
iteration : 518
train acc:  0.7109375
train loss:  0.5651087760925293
train gradient:  0.5312542693866497
iteration : 519
train acc:  0.734375
train loss:  0.5092357993125916
train gradient:  0.3352330453446329
iteration : 520
train acc:  0.7109375
train loss:  0.5664504170417786
train gradient:  0.7449921196107935
iteration : 521
train acc:  0.671875
train loss:  0.6324738264083862
train gradient:  0.6408614364545453
iteration : 522
train acc:  0.78125
train loss:  0.5041264891624451
train gradient:  0.5858640957863014
iteration : 523
train acc:  0.734375
train loss:  0.5397064089775085
train gradient:  0.4044840230583116
iteration : 524
train acc:  0.6875
train loss:  0.5584537982940674
train gradient:  0.4901394530847766
iteration : 525
train acc:  0.703125
train loss:  0.534174919128418
train gradient:  0.41320041392716317
iteration : 526
train acc:  0.8203125
train loss:  0.4942744970321655
train gradient:  0.4715030436027584
iteration : 527
train acc:  0.75
train loss:  0.5144302248954773
train gradient:  0.4394388556256211
iteration : 528
train acc:  0.75
train loss:  0.5525362491607666
train gradient:  0.4678230424349695
iteration : 529
train acc:  0.734375
train loss:  0.48774200677871704
train gradient:  0.47454227419440215
iteration : 530
train acc:  0.6640625
train loss:  0.5750783681869507
train gradient:  0.8083972215588435
iteration : 531
train acc:  0.7421875
train loss:  0.5520951747894287
train gradient:  0.47321052499108773
iteration : 532
train acc:  0.7578125
train loss:  0.5040512681007385
train gradient:  0.40151034405816666
iteration : 533
train acc:  0.6875
train loss:  0.5741272568702698
train gradient:  0.6850499858615373
iteration : 534
train acc:  0.71875
train loss:  0.546067476272583
train gradient:  0.4714249460897538
iteration : 535
train acc:  0.7578125
train loss:  0.5503244400024414
train gradient:  0.47112319926552243
iteration : 536
train acc:  0.7109375
train loss:  0.4886528551578522
train gradient:  0.40879545568686865
iteration : 537
train acc:  0.6640625
train loss:  0.5885037183761597
train gradient:  0.5162507102606322
iteration : 538
train acc:  0.6875
train loss:  0.5704168081283569
train gradient:  0.5076970732420698
iteration : 539
train acc:  0.7421875
train loss:  0.5032466650009155
train gradient:  0.40602491535794966
iteration : 540
train acc:  0.640625
train loss:  0.5989491939544678
train gradient:  0.7465715302460055
iteration : 541
train acc:  0.7578125
train loss:  0.5384999513626099
train gradient:  0.5629042591175398
iteration : 542
train acc:  0.7890625
train loss:  0.45801669359207153
train gradient:  0.44317422847948096
iteration : 543
train acc:  0.7109375
train loss:  0.6042958498001099
train gradient:  0.7823948174742784
iteration : 544
train acc:  0.734375
train loss:  0.5253292322158813
train gradient:  0.5286208572425954
iteration : 545
train acc:  0.71875
train loss:  0.5409759283065796
train gradient:  0.7057062940181001
iteration : 546
train acc:  0.7734375
train loss:  0.5135197639465332
train gradient:  0.35243747058090497
iteration : 547
train acc:  0.703125
train loss:  0.5329509973526001
train gradient:  0.469148432550668
iteration : 548
train acc:  0.7109375
train loss:  0.558010458946228
train gradient:  0.6007801081074902
iteration : 549
train acc:  0.7734375
train loss:  0.5526739358901978
train gradient:  0.5236592870301018
iteration : 550
train acc:  0.75
train loss:  0.5282751321792603
train gradient:  0.48857774653127783
iteration : 551
train acc:  0.6796875
train loss:  0.6247497797012329
train gradient:  0.6269834915895112
iteration : 552
train acc:  0.6953125
train loss:  0.5784910321235657
train gradient:  0.5110725365906776
iteration : 553
train acc:  0.6953125
train loss:  0.5623826384544373
train gradient:  0.45734355500223917
iteration : 554
train acc:  0.71875
train loss:  0.533611536026001
train gradient:  0.4219603120334053
iteration : 555
train acc:  0.75
train loss:  0.5364922285079956
train gradient:  0.4603884758054009
iteration : 556
train acc:  0.796875
train loss:  0.4695853888988495
train gradient:  0.49885774457441295
iteration : 557
train acc:  0.7421875
train loss:  0.545615017414093
train gradient:  0.5553713872819293
iteration : 558
train acc:  0.765625
train loss:  0.4684939384460449
train gradient:  0.4204295402721986
iteration : 559
train acc:  0.703125
train loss:  0.5574157238006592
train gradient:  0.7432239685797519
iteration : 560
train acc:  0.7421875
train loss:  0.5518841743469238
train gradient:  0.6354499305677106
iteration : 561
train acc:  0.75
train loss:  0.5251694321632385
train gradient:  0.7138977185640945
iteration : 562
train acc:  0.7265625
train loss:  0.4979268014431
train gradient:  0.5224171821639672
iteration : 563
train acc:  0.734375
train loss:  0.5045729875564575
train gradient:  0.3625755931586365
iteration : 564
train acc:  0.734375
train loss:  0.5347989797592163
train gradient:  0.4926305992379895
iteration : 565
train acc:  0.734375
train loss:  0.4939350187778473
train gradient:  0.5081622826393777
iteration : 566
train acc:  0.765625
train loss:  0.504362940788269
train gradient:  0.39014551961396743
iteration : 567
train acc:  0.7109375
train loss:  0.5540064573287964
train gradient:  0.5925280929302845
iteration : 568
train acc:  0.7109375
train loss:  0.5223334431648254
train gradient:  0.42393915301697244
iteration : 569
train acc:  0.734375
train loss:  0.5355803966522217
train gradient:  0.49400916436650116
iteration : 570
train acc:  0.65625
train loss:  0.6296255588531494
train gradient:  0.6543683983827716
iteration : 571
train acc:  0.6875
train loss:  0.5588247776031494
train gradient:  0.7488647125217187
iteration : 572
train acc:  0.7890625
train loss:  0.4456145167350769
train gradient:  0.40586786036228945
iteration : 573
train acc:  0.671875
train loss:  0.6189389824867249
train gradient:  0.6500620812672117
iteration : 574
train acc:  0.703125
train loss:  0.5451329946517944
train gradient:  0.7807226777447733
iteration : 575
train acc:  0.75
train loss:  0.4817728102207184
train gradient:  0.4314818269607433
iteration : 576
train acc:  0.7890625
train loss:  0.49599355459213257
train gradient:  0.4669334006354307
iteration : 577
train acc:  0.734375
train loss:  0.5360356569290161
train gradient:  0.4828815786424262
iteration : 578
train acc:  0.8046875
train loss:  0.45953264832496643
train gradient:  0.3058357641452664
iteration : 579
train acc:  0.7109375
train loss:  0.6388363838195801
train gradient:  0.6937882622914997
iteration : 580
train acc:  0.8046875
train loss:  0.4847753047943115
train gradient:  0.4354605519913639
iteration : 581
train acc:  0.734375
train loss:  0.5498594045639038
train gradient:  0.5913273521481799
iteration : 582
train acc:  0.7734375
train loss:  0.5268329381942749
train gradient:  0.507389715792564
iteration : 583
train acc:  0.7421875
train loss:  0.46946170926094055
train gradient:  0.5590657992194639
iteration : 584
train acc:  0.75
train loss:  0.4860082268714905
train gradient:  0.4049601484292112
iteration : 585
train acc:  0.7734375
train loss:  0.4885943531990051
train gradient:  0.43007862994439827
iteration : 586
train acc:  0.84375
train loss:  0.44879448413848877
train gradient:  0.4233279714050768
iteration : 587
train acc:  0.765625
train loss:  0.5212355256080627
train gradient:  0.43228723921563367
iteration : 588
train acc:  0.7734375
train loss:  0.5023983716964722
train gradient:  0.3522741364212597
iteration : 589
train acc:  0.734375
train loss:  0.5338661670684814
train gradient:  0.4299785824425664
iteration : 590
train acc:  0.7578125
train loss:  0.49701228737831116
train gradient:  0.39745287451185884
iteration : 591
train acc:  0.65625
train loss:  0.5333410501480103
train gradient:  0.49048912444827214
iteration : 592
train acc:  0.7421875
train loss:  0.5596383213996887
train gradient:  0.572817995375706
iteration : 593
train acc:  0.734375
train loss:  0.4755763113498688
train gradient:  0.4127110757565082
iteration : 594
train acc:  0.734375
train loss:  0.5626112818717957
train gradient:  0.5534664261273753
iteration : 595
train acc:  0.6875
train loss:  0.5523470640182495
train gradient:  0.6940883757632031
iteration : 596
train acc:  0.7734375
train loss:  0.5247834920883179
train gradient:  0.6857267256909316
iteration : 597
train acc:  0.7578125
train loss:  0.49594956636428833
train gradient:  0.6184111104303174
iteration : 598
train acc:  0.7109375
train loss:  0.5917561054229736
train gradient:  0.5860353038478059
iteration : 599
train acc:  0.765625
train loss:  0.4890151023864746
train gradient:  0.5723583892255728
iteration : 600
train acc:  0.6640625
train loss:  0.5703325271606445
train gradient:  0.4439660505505357
iteration : 601
train acc:  0.75
train loss:  0.5194005966186523
train gradient:  0.7401724279386117
iteration : 602
train acc:  0.75
train loss:  0.546033501625061
train gradient:  0.5261212324013331
iteration : 603
train acc:  0.78125
train loss:  0.49819105863571167
train gradient:  0.6046244433048038
iteration : 604
train acc:  0.7734375
train loss:  0.5055636167526245
train gradient:  0.6380725982101356
iteration : 605
train acc:  0.7109375
train loss:  0.5213977098464966
train gradient:  0.6803064902222754
iteration : 606
train acc:  0.7421875
train loss:  0.54222571849823
train gradient:  0.6257080009743454
iteration : 607
train acc:  0.7265625
train loss:  0.555862307548523
train gradient:  0.6012476547297557
iteration : 608
train acc:  0.7578125
train loss:  0.5365488529205322
train gradient:  0.6716268935320224
iteration : 609
train acc:  0.75
train loss:  0.5056525468826294
train gradient:  0.5878682728766806
iteration : 610
train acc:  0.765625
train loss:  0.5100959539413452
train gradient:  0.5182612701551519
iteration : 611
train acc:  0.7421875
train loss:  0.5600075721740723
train gradient:  0.5989203759354971
iteration : 612
train acc:  0.7265625
train loss:  0.5419048070907593
train gradient:  0.7596787150289825
iteration : 613
train acc:  0.71875
train loss:  0.510211169719696
train gradient:  0.9095461503866136
iteration : 614
train acc:  0.78125
train loss:  0.4685392379760742
train gradient:  0.4532181758963326
iteration : 615
train acc:  0.6640625
train loss:  0.5735862851142883
train gradient:  0.702579813391
iteration : 616
train acc:  0.65625
train loss:  0.5885961651802063
train gradient:  0.5752023113447254
iteration : 617
train acc:  0.7421875
train loss:  0.5099886059761047
train gradient:  0.4903940038288042
iteration : 618
train acc:  0.765625
train loss:  0.5260244607925415
train gradient:  0.5363527790252762
iteration : 619
train acc:  0.71875
train loss:  0.5377099514007568
train gradient:  0.4834008249436293
iteration : 620
train acc:  0.671875
train loss:  0.5979781746864319
train gradient:  0.6649332034387071
iteration : 621
train acc:  0.6875
train loss:  0.6016559600830078
train gradient:  0.48327840545785267
iteration : 622
train acc:  0.7578125
train loss:  0.520843505859375
train gradient:  0.5667424237088998
iteration : 623
train acc:  0.7890625
train loss:  0.4951668679714203
train gradient:  0.4019944584632154
iteration : 624
train acc:  0.7578125
train loss:  0.5581190586090088
train gradient:  0.555034510534979
iteration : 625
train acc:  0.703125
train loss:  0.5615870356559753
train gradient:  0.529622897608788
iteration : 626
train acc:  0.7421875
train loss:  0.5377147197723389
train gradient:  0.7200664969630414
iteration : 627
train acc:  0.765625
train loss:  0.5109075307846069
train gradient:  0.30782096041915796
iteration : 628
train acc:  0.8203125
train loss:  0.46956291794776917
train gradient:  0.29809004953274526
iteration : 629
train acc:  0.703125
train loss:  0.5750132203102112
train gradient:  0.638103451572833
iteration : 630
train acc:  0.7890625
train loss:  0.463750958442688
train gradient:  0.31084528143378265
iteration : 631
train acc:  0.71875
train loss:  0.5861583948135376
train gradient:  0.45679043043315226
iteration : 632
train acc:  0.7578125
train loss:  0.47935205698013306
train gradient:  0.46469730255437436
iteration : 633
train acc:  0.796875
train loss:  0.45906370878219604
train gradient:  0.41114185735192954
iteration : 634
train acc:  0.7734375
train loss:  0.48519688844680786
train gradient:  0.4035598647293459
iteration : 635
train acc:  0.75
train loss:  0.5380924344062805
train gradient:  0.4840719164920918
iteration : 636
train acc:  0.78125
train loss:  0.5252630710601807
train gradient:  0.36577987345018415
iteration : 637
train acc:  0.7578125
train loss:  0.5207750201225281
train gradient:  0.47289615042448074
iteration : 638
train acc:  0.75
train loss:  0.4957915246486664
train gradient:  0.45457880682290047
iteration : 639
train acc:  0.7578125
train loss:  0.49021774530410767
train gradient:  0.428538191459661
iteration : 640
train acc:  0.75
train loss:  0.5062781572341919
train gradient:  0.4287369446443389
iteration : 641
train acc:  0.7421875
train loss:  0.5764583349227905
train gradient:  0.6679374094388562
iteration : 642
train acc:  0.6953125
train loss:  0.5266076326370239
train gradient:  0.539090998590434
iteration : 643
train acc:  0.7578125
train loss:  0.48982328176498413
train gradient:  0.3164655194803382
iteration : 644
train acc:  0.765625
train loss:  0.4917002320289612
train gradient:  0.4728752096192754
iteration : 645
train acc:  0.8046875
train loss:  0.4374071955680847
train gradient:  0.3220434838746272
iteration : 646
train acc:  0.6953125
train loss:  0.543709397315979
train gradient:  0.4978030383037677
iteration : 647
train acc:  0.71875
train loss:  0.5267605781555176
train gradient:  0.5084488966394598
iteration : 648
train acc:  0.75
train loss:  0.49641749262809753
train gradient:  0.387751831126512
iteration : 649
train acc:  0.7578125
train loss:  0.5235610008239746
train gradient:  0.6579417710648641
iteration : 650
train acc:  0.828125
train loss:  0.46948856115341187
train gradient:  0.37046653764128923
iteration : 651
train acc:  0.71875
train loss:  0.5551307201385498
train gradient:  0.42296519770184104
iteration : 652
train acc:  0.796875
train loss:  0.4466541111469269
train gradient:  0.3252396310374338
iteration : 653
train acc:  0.671875
train loss:  0.5757864713668823
train gradient:  0.46173147751546095
iteration : 654
train acc:  0.7421875
train loss:  0.49206364154815674
train gradient:  0.34274674464815497
iteration : 655
train acc:  0.7265625
train loss:  0.5766247510910034
train gradient:  0.4938645151819885
iteration : 656
train acc:  0.765625
train loss:  0.49757876992225647
train gradient:  0.38091341494926373
iteration : 657
train acc:  0.7109375
train loss:  0.5747247338294983
train gradient:  0.5858540495264606
iteration : 658
train acc:  0.6953125
train loss:  0.5413094758987427
train gradient:  0.4328798672222667
iteration : 659
train acc:  0.75
train loss:  0.5177796483039856
train gradient:  0.5856978736896586
iteration : 660
train acc:  0.7109375
train loss:  0.5676734447479248
train gradient:  0.535962184720109
iteration : 661
train acc:  0.7421875
train loss:  0.5431039333343506
train gradient:  0.5161724932026139
iteration : 662
train acc:  0.703125
train loss:  0.5454191565513611
train gradient:  0.5173940963486873
iteration : 663
train acc:  0.7734375
train loss:  0.48610448837280273
train gradient:  0.40292912538642783
iteration : 664
train acc:  0.78125
train loss:  0.4908598065376282
train gradient:  0.48724260414627935
iteration : 665
train acc:  0.75
train loss:  0.5146362781524658
train gradient:  0.5204938682812441
iteration : 666
train acc:  0.7578125
train loss:  0.4869518578052521
train gradient:  0.34585476157420825
iteration : 667
train acc:  0.7421875
train loss:  0.5422048568725586
train gradient:  0.5744959879965915
iteration : 668
train acc:  0.7578125
train loss:  0.4898396134376526
train gradient:  0.4001148352209431
iteration : 669
train acc:  0.75
train loss:  0.507635772228241
train gradient:  0.48819233193491424
iteration : 670
train acc:  0.8046875
train loss:  0.4443274140357971
train gradient:  0.36916818223519865
iteration : 671
train acc:  0.734375
train loss:  0.5000624656677246
train gradient:  0.4806154516350295
iteration : 672
train acc:  0.78125
train loss:  0.4854823350906372
train gradient:  0.4784030412861925
iteration : 673
train acc:  0.765625
train loss:  0.49532175064086914
train gradient:  0.5020922477449901
iteration : 674
train acc:  0.765625
train loss:  0.49089550971984863
train gradient:  0.44248088120772644
iteration : 675
train acc:  0.7109375
train loss:  0.5831143856048584
train gradient:  0.5037048034773689
iteration : 676
train acc:  0.7578125
train loss:  0.5101783275604248
train gradient:  0.5275216702926684
iteration : 677
train acc:  0.734375
train loss:  0.5372042655944824
train gradient:  0.8494561492073558
iteration : 678
train acc:  0.75
train loss:  0.4854320287704468
train gradient:  0.5028369071122081
iteration : 679
train acc:  0.7421875
train loss:  0.4800238609313965
train gradient:  0.4547534703906961
iteration : 680
train acc:  0.75
train loss:  0.5077183246612549
train gradient:  0.5874750657120591
iteration : 681
train acc:  0.78125
train loss:  0.48312631249427795
train gradient:  0.4345462603564312
iteration : 682
train acc:  0.765625
train loss:  0.46236342191696167
train gradient:  0.3294601136902621
iteration : 683
train acc:  0.796875
train loss:  0.4984550476074219
train gradient:  0.7512078845312954
iteration : 684
train acc:  0.8046875
train loss:  0.4739030599594116
train gradient:  0.34814109318806724
iteration : 685
train acc:  0.75
train loss:  0.4913003444671631
train gradient:  0.3822005556559573
iteration : 686
train acc:  0.640625
train loss:  0.5958057641983032
train gradient:  0.8275839643817632
iteration : 687
train acc:  0.7734375
train loss:  0.504149317741394
train gradient:  0.4158120724445967
iteration : 688
train acc:  0.703125
train loss:  0.5272901058197021
train gradient:  0.44305039731330514
iteration : 689
train acc:  0.8515625
train loss:  0.4377344846725464
train gradient:  0.4070503471760426
iteration : 690
train acc:  0.7578125
train loss:  0.4505767822265625
train gradient:  0.3309946152702978
iteration : 691
train acc:  0.75
train loss:  0.5117220878601074
train gradient:  0.42186284258536183
iteration : 692
train acc:  0.78125
train loss:  0.4634271264076233
train gradient:  0.3059252196678061
iteration : 693
train acc:  0.78125
train loss:  0.4777153730392456
train gradient:  0.4314734246769284
iteration : 694
train acc:  0.71875
train loss:  0.5682165622711182
train gradient:  0.4624259564316564
iteration : 695
train acc:  0.7421875
train loss:  0.5067211985588074
train gradient:  0.37759944758950853
iteration : 696
train acc:  0.765625
train loss:  0.48123249411582947
train gradient:  0.4914724479737243
iteration : 697
train acc:  0.703125
train loss:  0.5726985931396484
train gradient:  0.7333587860414293
iteration : 698
train acc:  0.7734375
train loss:  0.45385509729385376
train gradient:  0.3017228888920055
iteration : 699
train acc:  0.7890625
train loss:  0.4482830762863159
train gradient:  0.35785151156602923
iteration : 700
train acc:  0.75
train loss:  0.496296763420105
train gradient:  0.436148711874103
iteration : 701
train acc:  0.765625
train loss:  0.48846983909606934
train gradient:  0.36060713952579265
iteration : 702
train acc:  0.7734375
train loss:  0.44864290952682495
train gradient:  0.3567705044385173
iteration : 703
train acc:  0.7109375
train loss:  0.5426912307739258
train gradient:  0.5666977223125009
iteration : 704
train acc:  0.703125
train loss:  0.5764440298080444
train gradient:  0.6788307966201534
iteration : 705
train acc:  0.6875
train loss:  0.5209464430809021
train gradient:  0.48046263527461663
iteration : 706
train acc:  0.6875
train loss:  0.6222766637802124
train gradient:  0.7401501564355228
iteration : 707
train acc:  0.75
train loss:  0.5285272002220154
train gradient:  0.5971276925204617
iteration : 708
train acc:  0.7734375
train loss:  0.4445093274116516
train gradient:  0.5136465724585455
iteration : 709
train acc:  0.8046875
train loss:  0.4509671926498413
train gradient:  0.3055172801170297
iteration : 710
train acc:  0.7265625
train loss:  0.5889185070991516
train gradient:  0.8087812361516509
iteration : 711
train acc:  0.78125
train loss:  0.48807764053344727
train gradient:  0.5247305712820084
iteration : 712
train acc:  0.796875
train loss:  0.4831116795539856
train gradient:  0.5075019443052822
iteration : 713
train acc:  0.75
train loss:  0.5233738422393799
train gradient:  0.5593718438248474
iteration : 714
train acc:  0.8203125
train loss:  0.4355356693267822
train gradient:  0.3763354512410146
iteration : 715
train acc:  0.75
train loss:  0.5601707696914673
train gradient:  0.41870256381878845
iteration : 716
train acc:  0.75
train loss:  0.4869348406791687
train gradient:  0.45423887015481273
iteration : 717
train acc:  0.7109375
train loss:  0.5258687734603882
train gradient:  0.4388635486874562
iteration : 718
train acc:  0.7734375
train loss:  0.5123929381370544
train gradient:  0.3216169815024765
iteration : 719
train acc:  0.78125
train loss:  0.4711192846298218
train gradient:  0.3458378177206154
iteration : 720
train acc:  0.6953125
train loss:  0.5331672430038452
train gradient:  0.5103924237912125
iteration : 721
train acc:  0.7421875
train loss:  0.5078268051147461
train gradient:  0.6227742993904013
iteration : 722
train acc:  0.8359375
train loss:  0.4332919716835022
train gradient:  0.3540102412311735
iteration : 723
train acc:  0.703125
train loss:  0.5648579597473145
train gradient:  0.5381347140727767
iteration : 724
train acc:  0.7890625
train loss:  0.46341466903686523
train gradient:  0.3060858430737037
iteration : 725
train acc:  0.7890625
train loss:  0.49648550152778625
train gradient:  0.41443032023359255
iteration : 726
train acc:  0.703125
train loss:  0.5635532140731812
train gradient:  0.5929159078680866
iteration : 727
train acc:  0.7265625
train loss:  0.5543795228004456
train gradient:  0.4730858025245239
iteration : 728
train acc:  0.75
train loss:  0.47461822628974915
train gradient:  0.3875578695730265
iteration : 729
train acc:  0.7265625
train loss:  0.535237729549408
train gradient:  0.6618638510753599
iteration : 730
train acc:  0.796875
train loss:  0.46129196882247925
train gradient:  0.4597024078921476
iteration : 731
train acc:  0.71875
train loss:  0.5404729843139648
train gradient:  0.5032206240962089
iteration : 732
train acc:  0.8046875
train loss:  0.4472607672214508
train gradient:  0.40186839101882366
iteration : 733
train acc:  0.71875
train loss:  0.5120705366134644
train gradient:  0.46472100141276296
iteration : 734
train acc:  0.7734375
train loss:  0.47639918327331543
train gradient:  0.6005893886625591
iteration : 735
train acc:  0.78125
train loss:  0.4612220525741577
train gradient:  0.34182898234802367
iteration : 736
train acc:  0.7578125
train loss:  0.5262757539749146
train gradient:  0.6315217292232502
iteration : 737
train acc:  0.671875
train loss:  0.576823890209198
train gradient:  0.5678514093042615
iteration : 738
train acc:  0.78125
train loss:  0.48245635628700256
train gradient:  0.5786533947406777
iteration : 739
train acc:  0.7578125
train loss:  0.4941266179084778
train gradient:  0.36105401070506016
iteration : 740
train acc:  0.71875
train loss:  0.5700408220291138
train gradient:  0.640570559551052
iteration : 741
train acc:  0.7890625
train loss:  0.4583055377006531
train gradient:  0.3678901160045876
iteration : 742
train acc:  0.8046875
train loss:  0.4324345290660858
train gradient:  0.29618127235054037
iteration : 743
train acc:  0.7109375
train loss:  0.5093490481376648
train gradient:  0.43853391688972165
iteration : 744
train acc:  0.671875
train loss:  0.5505343675613403
train gradient:  0.4765754237747399
iteration : 745
train acc:  0.828125
train loss:  0.4393484592437744
train gradient:  0.38650460703544814
iteration : 746
train acc:  0.8046875
train loss:  0.4442451000213623
train gradient:  0.34931916118968265
iteration : 747
train acc:  0.796875
train loss:  0.46012383699417114
train gradient:  0.5510764205593193
iteration : 748
train acc:  0.7421875
train loss:  0.5273510217666626
train gradient:  0.5351791736982252
iteration : 749
train acc:  0.78125
train loss:  0.4760168194770813
train gradient:  0.39782941025966395
iteration : 750
train acc:  0.765625
train loss:  0.4940588176250458
train gradient:  0.44041192348960356
iteration : 751
train acc:  0.78125
train loss:  0.4743775725364685
train gradient:  0.4293692018228101
iteration : 752
train acc:  0.78125
train loss:  0.45375412702560425
train gradient:  0.3830119049113814
iteration : 753
train acc:  0.671875
train loss:  0.5782319903373718
train gradient:  0.7195771756407356
iteration : 754
train acc:  0.6796875
train loss:  0.5524352788925171
train gradient:  0.8110177020127671
iteration : 755
train acc:  0.75
train loss:  0.4921203851699829
train gradient:  0.46399296081900065
iteration : 756
train acc:  0.8046875
train loss:  0.4459327459335327
train gradient:  0.36651334299095034
iteration : 757
train acc:  0.7578125
train loss:  0.4643228054046631
train gradient:  0.4575127522407914
iteration : 758
train acc:  0.6953125
train loss:  0.5784941911697388
train gradient:  0.7366577767349598
iteration : 759
train acc:  0.7890625
train loss:  0.45090940594673157
train gradient:  0.35108562135016824
iteration : 760
train acc:  0.7734375
train loss:  0.4404239058494568
train gradient:  0.5082853195818271
iteration : 761
train acc:  0.796875
train loss:  0.4895758330821991
train gradient:  0.38450968236739075
iteration : 762
train acc:  0.734375
train loss:  0.5697097182273865
train gradient:  0.5473243377044933
iteration : 763
train acc:  0.8125
train loss:  0.4255364239215851
train gradient:  0.4158587202059519
iteration : 764
train acc:  0.796875
train loss:  0.44291749596595764
train gradient:  0.45273771900869436
iteration : 765
train acc:  0.765625
train loss:  0.49571436643600464
train gradient:  0.6069105216215425
iteration : 766
train acc:  0.7578125
train loss:  0.5126583576202393
train gradient:  0.6275366257464801
iteration : 767
train acc:  0.7421875
train loss:  0.5200786590576172
train gradient:  0.5770331690722788
iteration : 768
train acc:  0.765625
train loss:  0.4467238783836365
train gradient:  0.47100435199382945
iteration : 769
train acc:  0.7734375
train loss:  0.4641302824020386
train gradient:  0.4912868974300748
iteration : 770
train acc:  0.796875
train loss:  0.42685556411743164
train gradient:  0.35266544283886875
iteration : 771
train acc:  0.8203125
train loss:  0.4377012252807617
train gradient:  0.37417338945907946
iteration : 772
train acc:  0.703125
train loss:  0.5617734789848328
train gradient:  0.6086584015083123
iteration : 773
train acc:  0.7734375
train loss:  0.5324448943138123
train gradient:  0.3874780166574113
iteration : 774
train acc:  0.7265625
train loss:  0.5187084674835205
train gradient:  0.5016015559633205
iteration : 775
train acc:  0.7734375
train loss:  0.5106906890869141
train gradient:  0.5406567652853463
iteration : 776
train acc:  0.8046875
train loss:  0.4171087145805359
train gradient:  0.4094078842196319
iteration : 777
train acc:  0.6953125
train loss:  0.5374926328659058
train gradient:  0.6453033324916796
iteration : 778
train acc:  0.71875
train loss:  0.5294320583343506
train gradient:  0.6430789203846731
iteration : 779
train acc:  0.765625
train loss:  0.46234798431396484
train gradient:  0.5479679925356598
iteration : 780
train acc:  0.8046875
train loss:  0.4240179657936096
train gradient:  0.3578748468858206
iteration : 781
train acc:  0.7734375
train loss:  0.46142470836639404
train gradient:  0.4327866575641983
iteration : 782
train acc:  0.7421875
train loss:  0.5084460377693176
train gradient:  0.5784362301756578
iteration : 783
train acc:  0.7109375
train loss:  0.5348237752914429
train gradient:  0.6845115880127752
iteration : 784
train acc:  0.8125
train loss:  0.4491267800331116
train gradient:  0.5168529626883763
iteration : 785
train acc:  0.75
train loss:  0.4646105170249939
train gradient:  0.5071993795630053
iteration : 786
train acc:  0.71875
train loss:  0.553465723991394
train gradient:  0.5890000381809464
iteration : 787
train acc:  0.703125
train loss:  0.5147609114646912
train gradient:  0.6528440394566937
iteration : 788
train acc:  0.84375
train loss:  0.4023303985595703
train gradient:  0.34970612577233506
iteration : 789
train acc:  0.7734375
train loss:  0.5113800764083862
train gradient:  0.690131864062185
iteration : 790
train acc:  0.8359375
train loss:  0.3820785880088806
train gradient:  0.3744243754786922
iteration : 791
train acc:  0.703125
train loss:  0.5488567352294922
train gradient:  0.7187743413536605
iteration : 792
train acc:  0.7421875
train loss:  0.4737599790096283
train gradient:  0.47551387266851636
iteration : 793
train acc:  0.765625
train loss:  0.4920499622821808
train gradient:  0.48499933452112204
iteration : 794
train acc:  0.78125
train loss:  0.4969586133956909
train gradient:  0.4204067268663524
iteration : 795
train acc:  0.7734375
train loss:  0.5013383030891418
train gradient:  0.6298673283211711
iteration : 796
train acc:  0.78125
train loss:  0.4581012725830078
train gradient:  0.44179175125679215
iteration : 797
train acc:  0.765625
train loss:  0.48799023032188416
train gradient:  0.5072370624032496
iteration : 798
train acc:  0.78125
train loss:  0.48108765482902527
train gradient:  0.40424195142585956
iteration : 799
train acc:  0.7265625
train loss:  0.5123443603515625
train gradient:  0.5579711991703361
iteration : 800
train acc:  0.65625
train loss:  0.540181040763855
train gradient:  0.672422898371822
iteration : 801
train acc:  0.75
train loss:  0.5328179001808167
train gradient:  0.5850774904629465
iteration : 802
train acc:  0.765625
train loss:  0.4977685213088989
train gradient:  0.5219356305590659
iteration : 803
train acc:  0.7578125
train loss:  0.47860610485076904
train gradient:  0.42846921547472955
iteration : 804
train acc:  0.75
train loss:  0.5428162813186646
train gradient:  0.4941728670528008
iteration : 805
train acc:  0.7734375
train loss:  0.4615558385848999
train gradient:  0.35524149911153247
iteration : 806
train acc:  0.7421875
train loss:  0.5192052721977234
train gradient:  0.6628570185631524
iteration : 807
train acc:  0.75
train loss:  0.546725869178772
train gradient:  0.4945064518945424
iteration : 808
train acc:  0.7734375
train loss:  0.45061618089675903
train gradient:  0.40725657917523234
iteration : 809
train acc:  0.7890625
train loss:  0.4966222941875458
train gradient:  0.39709010555058255
iteration : 810
train acc:  0.7265625
train loss:  0.5549514293670654
train gradient:  0.7499669989332927
iteration : 811
train acc:  0.71875
train loss:  0.5569382905960083
train gradient:  0.675366324265738
iteration : 812
train acc:  0.7109375
train loss:  0.5334029197692871
train gradient:  0.543504119685228
iteration : 813
train acc:  0.734375
train loss:  0.5177664756774902
train gradient:  0.6286948263083346
iteration : 814
train acc:  0.7109375
train loss:  0.5093668699264526
train gradient:  0.5779034864698409
iteration : 815
train acc:  0.7578125
train loss:  0.5060391426086426
train gradient:  0.651359903098689
iteration : 816
train acc:  0.7421875
train loss:  0.5351998209953308
train gradient:  0.6753483482986663
iteration : 817
train acc:  0.734375
train loss:  0.5275295376777649
train gradient:  0.517786280557617
iteration : 818
train acc:  0.7734375
train loss:  0.4669235944747925
train gradient:  0.4275063256724737
iteration : 819
train acc:  0.8125
train loss:  0.38873475790023804
train gradient:  0.2933659992068105
iteration : 820
train acc:  0.7265625
train loss:  0.5243821144104004
train gradient:  0.6097749120470572
iteration : 821
train acc:  0.6640625
train loss:  0.5484094619750977
train gradient:  0.6758071250910807
iteration : 822
train acc:  0.7265625
train loss:  0.6090812683105469
train gradient:  0.8876269472505716
iteration : 823
train acc:  0.7578125
train loss:  0.47203072905540466
train gradient:  0.3452932129021454
iteration : 824
train acc:  0.7734375
train loss:  0.47519248723983765
train gradient:  0.40453334382302913
iteration : 825
train acc:  0.7734375
train loss:  0.4481844902038574
train gradient:  0.2837122444571328
iteration : 826
train acc:  0.7890625
train loss:  0.49439889192581177
train gradient:  0.4133422733977754
iteration : 827
train acc:  0.7578125
train loss:  0.5073941349983215
train gradient:  0.6967301527896868
iteration : 828
train acc:  0.84375
train loss:  0.4084038734436035
train gradient:  0.27298734229667343
iteration : 829
train acc:  0.78125
train loss:  0.45831722021102905
train gradient:  0.44729384265601735
iteration : 830
train acc:  0.7265625
train loss:  0.5510377883911133
train gradient:  0.5261707922602856
iteration : 831
train acc:  0.7421875
train loss:  0.49082326889038086
train gradient:  0.4451051146360731
iteration : 832
train acc:  0.71875
train loss:  0.5281392335891724
train gradient:  0.8780016498301438
iteration : 833
train acc:  0.8203125
train loss:  0.3952144980430603
train gradient:  0.453284307243324
iteration : 834
train acc:  0.8046875
train loss:  0.4160408079624176
train gradient:  0.43634143642845186
iteration : 835
train acc:  0.7890625
train loss:  0.432549387216568
train gradient:  0.372278906934319
iteration : 836
train acc:  0.7578125
train loss:  0.5541406273841858
train gradient:  0.6906278186494309
iteration : 837
train acc:  0.7578125
train loss:  0.49777668714523315
train gradient:  0.3929586188519239
iteration : 838
train acc:  0.7734375
train loss:  0.5165181756019592
train gradient:  0.6788171764135061
iteration : 839
train acc:  0.8125
train loss:  0.45717525482177734
train gradient:  0.6732624962507628
iteration : 840
train acc:  0.8515625
train loss:  0.4465237557888031
train gradient:  0.4508802465637377
iteration : 841
train acc:  0.7890625
train loss:  0.4599115252494812
train gradient:  0.5689869093244124
iteration : 842
train acc:  0.765625
train loss:  0.5445787906646729
train gradient:  0.630575736287522
iteration : 843
train acc:  0.7421875
train loss:  0.554087221622467
train gradient:  0.5238394957338033
iteration : 844
train acc:  0.7265625
train loss:  0.5012305378913879
train gradient:  0.5427718132361987
iteration : 845
train acc:  0.7578125
train loss:  0.4536030888557434
train gradient:  0.48749277315940315
iteration : 846
train acc:  0.734375
train loss:  0.47231751680374146
train gradient:  0.4881772294559283
iteration : 847
train acc:  0.7890625
train loss:  0.4431837797164917
train gradient:  0.6037535440890146
iteration : 848
train acc:  0.78125
train loss:  0.44391930103302
train gradient:  0.4555343858386982
iteration : 849
train acc:  0.7265625
train loss:  0.5493811368942261
train gradient:  0.5584487150830708
iteration : 850
train acc:  0.78125
train loss:  0.4755098819732666
train gradient:  0.5301604858785764
iteration : 851
train acc:  0.7734375
train loss:  0.45350152254104614
train gradient:  0.392267476350322
iteration : 852
train acc:  0.78125
train loss:  0.4624941349029541
train gradient:  0.49339557216615176
iteration : 853
train acc:  0.734375
train loss:  0.5505526065826416
train gradient:  0.5733158082226952
iteration : 854
train acc:  0.8359375
train loss:  0.3744781017303467
train gradient:  0.3463522649423406
iteration : 855
train acc:  0.7421875
train loss:  0.5662631988525391
train gradient:  0.590266626054952
iteration : 856
train acc:  0.7265625
train loss:  0.5801685452461243
train gradient:  0.5290462122369428
iteration : 857
train acc:  0.734375
train loss:  0.48145440220832825
train gradient:  0.41831548422179854
iteration : 858
train acc:  0.8046875
train loss:  0.4653403162956238
train gradient:  0.40818344086798786
iteration : 859
train acc:  0.75
train loss:  0.5108434557914734
train gradient:  0.45085705137126864
iteration : 860
train acc:  0.796875
train loss:  0.4590030908584595
train gradient:  0.3618692508245224
iteration : 861
train acc:  0.765625
train loss:  0.5075597167015076
train gradient:  0.4492674836468124
iteration : 862
train acc:  0.75
train loss:  0.4702438712120056
train gradient:  0.31947434868323593
iteration : 863
train acc:  0.75
train loss:  0.4758782386779785
train gradient:  0.38259371658067204
iteration : 864
train acc:  0.7890625
train loss:  0.4515032172203064
train gradient:  0.3162369952383494
iteration : 865
train acc:  0.78125
train loss:  0.4359920620918274
train gradient:  0.36839599660032585
iteration : 866
train acc:  0.7109375
train loss:  0.5128042101860046
train gradient:  0.49726584929109363
iteration : 867
train acc:  0.8203125
train loss:  0.4742709696292877
train gradient:  0.5037322495241636
iteration : 868
train acc:  0.7265625
train loss:  0.488339364528656
train gradient:  0.430576011647524
iteration : 869
train acc:  0.765625
train loss:  0.45981988310813904
train gradient:  0.4588892066403493
iteration : 870
train acc:  0.7265625
train loss:  0.5237137079238892
train gradient:  0.4011471701182463
iteration : 871
train acc:  0.7734375
train loss:  0.45159292221069336
train gradient:  0.4282519309925789
iteration : 872
train acc:  0.7578125
train loss:  0.4353902339935303
train gradient:  0.4187436474085356
iteration : 873
train acc:  0.7734375
train loss:  0.46073323488235474
train gradient:  0.4426253347026668
iteration : 874
train acc:  0.7578125
train loss:  0.4668124318122864
train gradient:  0.47300900492066933
iteration : 875
train acc:  0.7734375
train loss:  0.47402799129486084
train gradient:  0.3170481219431799
iteration : 876
train acc:  0.796875
train loss:  0.44924235343933105
train gradient:  0.4794191113183789
iteration : 877
train acc:  0.796875
train loss:  0.443306565284729
train gradient:  0.4196079848873749
iteration : 878
train acc:  0.75
train loss:  0.5023243427276611
train gradient:  0.6163290288787027
iteration : 879
train acc:  0.78125
train loss:  0.45317748188972473
train gradient:  0.4409834435442132
iteration : 880
train acc:  0.7890625
train loss:  0.47785642743110657
train gradient:  0.3581347356206033
iteration : 881
train acc:  0.828125
train loss:  0.4025498330593109
train gradient:  0.383612530357302
iteration : 882
train acc:  0.765625
train loss:  0.46326035261154175
train gradient:  0.3652303467698546
iteration : 883
train acc:  0.8515625
train loss:  0.39422523975372314
train gradient:  0.31171032894567907
iteration : 884
train acc:  0.765625
train loss:  0.4703115224838257
train gradient:  0.4796052147805645
iteration : 885
train acc:  0.8203125
train loss:  0.41429170966148376
train gradient:  0.3103473174697108
iteration : 886
train acc:  0.8125
train loss:  0.43471038341522217
train gradient:  0.33375649351483894
iteration : 887
train acc:  0.8125
train loss:  0.42315346002578735
train gradient:  0.3608395871207385
iteration : 888
train acc:  0.6953125
train loss:  0.55331951379776
train gradient:  0.6288219871525149
iteration : 889
train acc:  0.765625
train loss:  0.4687323272228241
train gradient:  0.607675091563487
iteration : 890
train acc:  0.6953125
train loss:  0.5532823204994202
train gradient:  0.5673546743296208
iteration : 891
train acc:  0.796875
train loss:  0.514801025390625
train gradient:  0.5851963576129867
iteration : 892
train acc:  0.8046875
train loss:  0.3976142406463623
train gradient:  0.4407707825440172
iteration : 893
train acc:  0.7578125
train loss:  0.47260257601737976
train gradient:  0.5332635750119039
iteration : 894
train acc:  0.7421875
train loss:  0.5157485604286194
train gradient:  0.5672620321766755
iteration : 895
train acc:  0.8125
train loss:  0.41908127069473267
train gradient:  0.382892735723884
iteration : 896
train acc:  0.765625
train loss:  0.4833623170852661
train gradient:  0.47499470514788195
iteration : 897
train acc:  0.75
train loss:  0.525700569152832
train gradient:  0.5859159849436469
iteration : 898
train acc:  0.75
train loss:  0.44785746932029724
train gradient:  0.43345249663187563
iteration : 899
train acc:  0.7890625
train loss:  0.4844590127468109
train gradient:  0.3848324971685642
iteration : 900
train acc:  0.7421875
train loss:  0.48245149850845337
train gradient:  0.5209201621257193
iteration : 901
train acc:  0.8046875
train loss:  0.46399861574172974
train gradient:  0.48152520735524
iteration : 902
train acc:  0.859375
train loss:  0.36986833810806274
train gradient:  0.4270215885743753
iteration : 903
train acc:  0.828125
train loss:  0.39507803320884705
train gradient:  0.37644745466977875
iteration : 904
train acc:  0.765625
train loss:  0.4730836749076843
train gradient:  0.7483515328308996
iteration : 905
train acc:  0.828125
train loss:  0.4330398440361023
train gradient:  0.40938051370467743
iteration : 906
train acc:  0.796875
train loss:  0.42061948776245117
train gradient:  0.5267619984848909
iteration : 907
train acc:  0.78125
train loss:  0.5219632983207703
train gradient:  0.7161840849189812
iteration : 908
train acc:  0.8515625
train loss:  0.4493506848812103
train gradient:  0.41986281918570484
iteration : 909
train acc:  0.75
train loss:  0.48910820484161377
train gradient:  0.764522209023507
iteration : 910
train acc:  0.7421875
train loss:  0.5124398469924927
train gradient:  0.562392326909305
iteration : 911
train acc:  0.8515625
train loss:  0.40030816197395325
train gradient:  0.3071775396661537
iteration : 912
train acc:  0.8125
train loss:  0.4231269359588623
train gradient:  0.4350864800855379
iteration : 913
train acc:  0.796875
train loss:  0.453724205493927
train gradient:  0.5335340220373394
iteration : 914
train acc:  0.8203125
train loss:  0.43077734112739563
train gradient:  0.3454351510572401
iteration : 915
train acc:  0.765625
train loss:  0.4612007439136505
train gradient:  0.4753526567111167
iteration : 916
train acc:  0.7578125
train loss:  0.4994537830352783
train gradient:  0.5813232162452853
iteration : 917
train acc:  0.765625
train loss:  0.5214369893074036
train gradient:  0.5792397155787763
iteration : 918
train acc:  0.765625
train loss:  0.5600889921188354
train gradient:  0.6392684077264382
iteration : 919
train acc:  0.8359375
train loss:  0.39835771918296814
train gradient:  0.4635643809258235
iteration : 920
train acc:  0.7734375
train loss:  0.44611769914627075
train gradient:  0.5705354099462254
iteration : 921
train acc:  0.7421875
train loss:  0.5033125877380371
train gradient:  0.5603159211644473
iteration : 922
train acc:  0.8203125
train loss:  0.41816502809524536
train gradient:  0.5072862863867605
iteration : 923
train acc:  0.8515625
train loss:  0.3909880518913269
train gradient:  0.3341047438160963
iteration : 924
train acc:  0.6640625
train loss:  0.6040191650390625
train gradient:  0.8970587789278799
iteration : 925
train acc:  0.78125
train loss:  0.4425903260707855
train gradient:  0.4976275450202887
iteration : 926
train acc:  0.8515625
train loss:  0.3781512677669525
train gradient:  0.38413434612300207
iteration : 927
train acc:  0.8203125
train loss:  0.36802560091018677
train gradient:  0.3847509834947157
iteration : 928
train acc:  0.796875
train loss:  0.42278003692626953
train gradient:  0.43007879973885843
iteration : 929
train acc:  0.75
train loss:  0.49239271879196167
train gradient:  0.5925343174294994
iteration : 930
train acc:  0.734375
train loss:  0.5126033425331116
train gradient:  0.5573055867585844
iteration : 931
train acc:  0.8125
train loss:  0.49167442321777344
train gradient:  0.5105880912259061
iteration : 932
train acc:  0.7890625
train loss:  0.49265313148498535
train gradient:  0.5820477840361941
iteration : 933
train acc:  0.7109375
train loss:  0.5431638956069946
train gradient:  1.0624142265103331
iteration : 934
train acc:  0.75
train loss:  0.4931381642818451
train gradient:  0.5188452310608194
iteration : 935
train acc:  0.8046875
train loss:  0.4257277846336365
train gradient:  0.32696595703928416
iteration : 936
train acc:  0.7734375
train loss:  0.45107197761535645
train gradient:  0.43470037262210887
iteration : 937
train acc:  0.703125
train loss:  0.5063740015029907
train gradient:  0.4743111660678134
iteration : 938
train acc:  0.796875
train loss:  0.47002655267715454
train gradient:  0.4729937757742787
iteration : 939
train acc:  0.765625
train loss:  0.4471777081489563
train gradient:  0.48198821544440146
iteration : 940
train acc:  0.8125
train loss:  0.4852963387966156
train gradient:  0.32642959615114114
iteration : 941
train acc:  0.7890625
train loss:  0.4769628643989563
train gradient:  0.4453670814936178
iteration : 942
train acc:  0.765625
train loss:  0.470681369304657
train gradient:  0.4364010503536094
iteration : 943
train acc:  0.859375
train loss:  0.3852829039096832
train gradient:  0.30475788853565317
iteration : 944
train acc:  0.796875
train loss:  0.5027313232421875
train gradient:  0.4832676580509453
iteration : 945
train acc:  0.7890625
train loss:  0.4420129656791687
train gradient:  0.42333577228283786
iteration : 946
train acc:  0.8203125
train loss:  0.4068993926048279
train gradient:  0.4663327435039029
iteration : 947
train acc:  0.7109375
train loss:  0.5132584571838379
train gradient:  0.6351629006426531
iteration : 948
train acc:  0.7890625
train loss:  0.49204733967781067
train gradient:  0.3969420337863556
iteration : 949
train acc:  0.78125
train loss:  0.5232678055763245
train gradient:  0.5017606176049583
iteration : 950
train acc:  0.7578125
train loss:  0.476053386926651
train gradient:  0.46733293180168733
iteration : 951
train acc:  0.8046875
train loss:  0.40997037291526794
train gradient:  0.3751071344373176
iteration : 952
train acc:  0.8359375
train loss:  0.4193874001502991
train gradient:  0.32604305661508304
iteration : 953
train acc:  0.7578125
train loss:  0.5259500741958618
train gradient:  0.5739092076279156
iteration : 954
train acc:  0.734375
train loss:  0.4939593970775604
train gradient:  0.7695762271541325
iteration : 955
train acc:  0.7734375
train loss:  0.46082764863967896
train gradient:  0.5539733673791134
iteration : 956
train acc:  0.84375
train loss:  0.42299121618270874
train gradient:  0.38401162846978165
iteration : 957
train acc:  0.7890625
train loss:  0.4428360164165497
train gradient:  0.4222417836782498
iteration : 958
train acc:  0.7734375
train loss:  0.43348342180252075
train gradient:  0.39454989793897227
iteration : 959
train acc:  0.734375
train loss:  0.5164331197738647
train gradient:  0.5002802265449088
iteration : 960
train acc:  0.7109375
train loss:  0.5390984416007996
train gradient:  0.5988411162751242
iteration : 961
train acc:  0.7421875
train loss:  0.5157643556594849
train gradient:  0.4971772451523847
iteration : 962
train acc:  0.7890625
train loss:  0.40212392807006836
train gradient:  0.4504910753461366
iteration : 963
train acc:  0.71875
train loss:  0.5026745796203613
train gradient:  0.6254386289544815
iteration : 964
train acc:  0.796875
train loss:  0.3978731632232666
train gradient:  0.40509222362396663
iteration : 965
train acc:  0.8359375
train loss:  0.4071621894836426
train gradient:  0.7080000934393053
iteration : 966
train acc:  0.8125
train loss:  0.42768827080726624
train gradient:  0.46902524579500093
iteration : 967
train acc:  0.7578125
train loss:  0.49411875009536743
train gradient:  0.6726468787524011
iteration : 968
train acc:  0.71875
train loss:  0.534932017326355
train gradient:  0.7354464145948705
iteration : 969
train acc:  0.8046875
train loss:  0.4113157093524933
train gradient:  0.2928926624848808
iteration : 970
train acc:  0.7578125
train loss:  0.5039343237876892
train gradient:  0.596826490236827
iteration : 971
train acc:  0.6953125
train loss:  0.5676440000534058
train gradient:  0.9302304253418221
iteration : 972
train acc:  0.8125
train loss:  0.4192048907279968
train gradient:  0.3808556485013672
iteration : 973
train acc:  0.765625
train loss:  0.43667298555374146
train gradient:  0.41928923400719675
iteration : 974
train acc:  0.7265625
train loss:  0.544615626335144
train gradient:  0.7479261302567962
iteration : 975
train acc:  0.8046875
train loss:  0.43678274750709534
train gradient:  0.43054918834327033
iteration : 976
train acc:  0.8046875
train loss:  0.4362824559211731
train gradient:  0.37154484736965754
iteration : 977
train acc:  0.78125
train loss:  0.42619872093200684
train gradient:  0.47031511064929193
iteration : 978
train acc:  0.765625
train loss:  0.46272435784339905
train gradient:  0.4156670957628078
iteration : 979
train acc:  0.796875
train loss:  0.47629696130752563
train gradient:  0.4678117989767612
iteration : 980
train acc:  0.7109375
train loss:  0.551598310470581
train gradient:  0.7776240119578282
iteration : 981
train acc:  0.8359375
train loss:  0.3885003924369812
train gradient:  0.40212298661659207
iteration : 982
train acc:  0.734375
train loss:  0.5421751737594604
train gradient:  0.45163017484776674
iteration : 983
train acc:  0.84375
train loss:  0.40971893072128296
train gradient:  0.3629905899868305
iteration : 984
train acc:  0.78125
train loss:  0.4281730353832245
train gradient:  0.2872791236157803
iteration : 985
train acc:  0.84375
train loss:  0.3503187596797943
train gradient:  0.3543369536642221
iteration : 986
train acc:  0.75
train loss:  0.47248247265815735
train gradient:  0.3879236489030284
iteration : 987
train acc:  0.8046875
train loss:  0.4152398705482483
train gradient:  0.4630833296149102
iteration : 988
train acc:  0.8046875
train loss:  0.46776825189590454
train gradient:  0.6371609683421335
iteration : 989
train acc:  0.796875
train loss:  0.44902169704437256
train gradient:  0.3385736665774578
iteration : 990
train acc:  0.765625
train loss:  0.48844245076179504
train gradient:  0.621436636033491
iteration : 991
train acc:  0.75
train loss:  0.5280200839042664
train gradient:  0.5971469122700916
iteration : 992
train acc:  0.8515625
train loss:  0.39238041639328003
train gradient:  0.40807891543747943
iteration : 993
train acc:  0.78125
train loss:  0.42683982849121094
train gradient:  0.3953410993796007
iteration : 994
train acc:  0.8125
train loss:  0.40834635496139526
train gradient:  0.3803339174280958
iteration : 995
train acc:  0.78125
train loss:  0.4981886148452759
train gradient:  0.46986161517351915
iteration : 996
train acc:  0.7578125
train loss:  0.451491117477417
train gradient:  0.45720535748647134
iteration : 997
train acc:  0.78125
train loss:  0.4420522451400757
train gradient:  0.3781488622420748
iteration : 998
train acc:  0.7890625
train loss:  0.4392163157463074
train gradient:  0.550066666388776
iteration : 999
train acc:  0.8203125
train loss:  0.40934115648269653
train gradient:  0.36029460691646376
iteration : 1000
train acc:  0.7890625
train loss:  0.41105222702026367
train gradient:  0.38938243932395433
iteration : 1001
train acc:  0.75
train loss:  0.4392167925834656
train gradient:  0.35867833847559233
iteration : 1002
train acc:  0.8359375
train loss:  0.42144447565078735
train gradient:  0.3372666415868927
iteration : 1003
train acc:  0.8359375
train loss:  0.38567018508911133
train gradient:  0.4943979760793817
iteration : 1004
train acc:  0.703125
train loss:  0.5697969198226929
train gradient:  0.6503071740005935
iteration : 1005
train acc:  0.7734375
train loss:  0.48476988077163696
train gradient:  0.5696689829885567
iteration : 1006
train acc:  0.7265625
train loss:  0.5046514272689819
train gradient:  0.5124701795158175
iteration : 1007
train acc:  0.765625
train loss:  0.4531748294830322
train gradient:  0.7008530730439199
iteration : 1008
train acc:  0.78125
train loss:  0.45881956815719604
train gradient:  0.4745183152882457
iteration : 1009
train acc:  0.84375
train loss:  0.3655703067779541
train gradient:  0.4353895190208786
iteration : 1010
train acc:  0.859375
train loss:  0.3755060136318207
train gradient:  0.40416991872480007
iteration : 1011
train acc:  0.7890625
train loss:  0.44440650939941406
train gradient:  0.34096566375221515
iteration : 1012
train acc:  0.796875
train loss:  0.43265748023986816
train gradient:  0.3864481163620843
iteration : 1013
train acc:  0.734375
train loss:  0.4979429841041565
train gradient:  0.49326837420404357
iteration : 1014
train acc:  0.7578125
train loss:  0.5354444980621338
train gradient:  0.6431033263807366
iteration : 1015
train acc:  0.78125
train loss:  0.4536914527416229
train gradient:  0.5010842848882007
iteration : 1016
train acc:  0.65625
train loss:  0.5794244408607483
train gradient:  0.8786212913191281
iteration : 1017
train acc:  0.78125
train loss:  0.4747675359249115
train gradient:  0.6403253407184012
iteration : 1018
train acc:  0.796875
train loss:  0.4483741521835327
train gradient:  0.49796904723102997
iteration : 1019
train acc:  0.7578125
train loss:  0.5037081837654114
train gradient:  0.6225266525251361
iteration : 1020
train acc:  0.7421875
train loss:  0.45550215244293213
train gradient:  0.46941266424735384
iteration : 1021
train acc:  0.71875
train loss:  0.4747341275215149
train gradient:  0.6036633781697528
iteration : 1022
train acc:  0.796875
train loss:  0.47828370332717896
train gradient:  0.8159082635715795
iteration : 1023
train acc:  0.7734375
train loss:  0.5185192823410034
train gradient:  0.5177752327101515
iteration : 1024
train acc:  0.796875
train loss:  0.4785056710243225
train gradient:  0.4968817019211198
iteration : 1025
train acc:  0.828125
train loss:  0.383739709854126
train gradient:  0.38192948826625434
iteration : 1026
train acc:  0.7265625
train loss:  0.5332241654396057
train gradient:  0.49070874662391656
iteration : 1027
train acc:  0.8046875
train loss:  0.4383949637413025
train gradient:  0.5798606728375602
iteration : 1028
train acc:  0.78125
train loss:  0.4641810655593872
train gradient:  0.4986481225989706
iteration : 1029
train acc:  0.796875
train loss:  0.4599723219871521
train gradient:  0.4160598859479927
iteration : 1030
train acc:  0.8203125
train loss:  0.43061596155166626
train gradient:  0.4156347547194122
iteration : 1031
train acc:  0.75
train loss:  0.49603211879730225
train gradient:  0.5444654257917875
iteration : 1032
train acc:  0.7109375
train loss:  0.5289347171783447
train gradient:  0.6039478509332658
iteration : 1033
train acc:  0.7578125
train loss:  0.47343507409095764
train gradient:  0.4981686641720442
iteration : 1034
train acc:  0.765625
train loss:  0.4983195662498474
train gradient:  0.5744143035183564
iteration : 1035
train acc:  0.7578125
train loss:  0.5104452967643738
train gradient:  0.5159749089101018
iteration : 1036
train acc:  0.7578125
train loss:  0.4851338267326355
train gradient:  0.4075128493473697
iteration : 1037
train acc:  0.828125
train loss:  0.3918888568878174
train gradient:  0.35648680444597675
iteration : 1038
train acc:  0.7734375
train loss:  0.43814000487327576
train gradient:  0.4711124797131201
iteration : 1039
train acc:  0.8203125
train loss:  0.3883481025695801
train gradient:  0.2958173461947871
iteration : 1040
train acc:  0.78125
train loss:  0.4347304403781891
train gradient:  0.4065699432845048
iteration : 1041
train acc:  0.78125
train loss:  0.4672865867614746
train gradient:  0.5360568378914147
iteration : 1042
train acc:  0.7265625
train loss:  0.541077733039856
train gradient:  0.44211842946217617
iteration : 1043
train acc:  0.796875
train loss:  0.4689062833786011
train gradient:  0.4933167069254385
iteration : 1044
train acc:  0.78125
train loss:  0.4665099084377289
train gradient:  0.4964266280406335
iteration : 1045
train acc:  0.8203125
train loss:  0.3992806077003479
train gradient:  0.5402634047152908
iteration : 1046
train acc:  0.765625
train loss:  0.4200195074081421
train gradient:  0.3181541652670547
iteration : 1047
train acc:  0.796875
train loss:  0.46076899766921997
train gradient:  0.4908975103217379
iteration : 1048
train acc:  0.8125
train loss:  0.4614255130290985
train gradient:  1.1571377385929678
iteration : 1049
train acc:  0.796875
train loss:  0.415357768535614
train gradient:  0.34653472790547596
iteration : 1050
train acc:  0.8125
train loss:  0.40530920028686523
train gradient:  0.3315483843012794
iteration : 1051
train acc:  0.8515625
train loss:  0.39001592993736267
train gradient:  0.357156528539002
iteration : 1052
train acc:  0.7109375
train loss:  0.5378931760787964
train gradient:  0.5296600654298039
iteration : 1053
train acc:  0.796875
train loss:  0.4566498398780823
train gradient:  0.5439206541406446
iteration : 1054
train acc:  0.7578125
train loss:  0.532660961151123
train gradient:  0.5496190776799688
iteration : 1055
train acc:  0.75
train loss:  0.45228391885757446
train gradient:  0.38320222419898015
iteration : 1056
train acc:  0.7890625
train loss:  0.4538532793521881
train gradient:  0.4512380373614003
iteration : 1057
train acc:  0.71875
train loss:  0.5424982309341431
train gradient:  0.7111403230132373
iteration : 1058
train acc:  0.7578125
train loss:  0.5253764986991882
train gradient:  0.5265374422100657
iteration : 1059
train acc:  0.7578125
train loss:  0.44743114709854126
train gradient:  0.4241410455298719
iteration : 1060
train acc:  0.796875
train loss:  0.4418918788433075
train gradient:  0.4023820164594555
iteration : 1061
train acc:  0.765625
train loss:  0.4868907034397125
train gradient:  0.6413129437568396
iteration : 1062
train acc:  0.7890625
train loss:  0.5028746724128723
train gradient:  0.71568034933274
iteration : 1063
train acc:  0.78125
train loss:  0.5057503581047058
train gradient:  0.44400580632915504
iteration : 1064
train acc:  0.7578125
train loss:  0.4976176917552948
train gradient:  0.5294019110306248
iteration : 1065
train acc:  0.8125
train loss:  0.42745935916900635
train gradient:  0.42171872538807825
iteration : 1066
train acc:  0.75
train loss:  0.4973205029964447
train gradient:  0.4476484466965322
iteration : 1067
train acc:  0.75
train loss:  0.5167973041534424
train gradient:  0.5642303233636446
iteration : 1068
train acc:  0.796875
train loss:  0.3990669846534729
train gradient:  0.5329587421247778
iteration : 1069
train acc:  0.734375
train loss:  0.5062991976737976
train gradient:  0.4788707380530274
iteration : 1070
train acc:  0.7578125
train loss:  0.47625812888145447
train gradient:  0.36770081449604824
iteration : 1071
train acc:  0.78125
train loss:  0.45375776290893555
train gradient:  0.5398690519435225
iteration : 1072
train acc:  0.734375
train loss:  0.548151433467865
train gradient:  0.7099517610635699
iteration : 1073
train acc:  0.7890625
train loss:  0.4209654629230499
train gradient:  0.39867789744560056
iteration : 1074
train acc:  0.7890625
train loss:  0.43326833844184875
train gradient:  0.3891333017695909
iteration : 1075
train acc:  0.78125
train loss:  0.48155921697616577
train gradient:  0.44869852822925305
iteration : 1076
train acc:  0.8125
train loss:  0.439127653837204
train gradient:  0.4987341987748306
iteration : 1077
train acc:  0.78125
train loss:  0.4609682559967041
train gradient:  0.485824624073473
iteration : 1078
train acc:  0.796875
train loss:  0.4513067603111267
train gradient:  0.42007196203423836
iteration : 1079
train acc:  0.7421875
train loss:  0.48621153831481934
train gradient:  0.48927244431599265
iteration : 1080
train acc:  0.796875
train loss:  0.41437479853630066
train gradient:  0.46839571384419065
iteration : 1081
train acc:  0.734375
train loss:  0.49450838565826416
train gradient:  0.4427690340228791
iteration : 1082
train acc:  0.8125
train loss:  0.4168086349964142
train gradient:  0.39990045462229423
iteration : 1083
train acc:  0.7578125
train loss:  0.462660551071167
train gradient:  0.4494177497188998
iteration : 1084
train acc:  0.8046875
train loss:  0.45437246561050415
train gradient:  0.4700393590539019
iteration : 1085
train acc:  0.7890625
train loss:  0.4543212652206421
train gradient:  0.5157016872975726
iteration : 1086
train acc:  0.7734375
train loss:  0.4892593324184418
train gradient:  0.3622171722638348
iteration : 1087
train acc:  0.7734375
train loss:  0.5051374435424805
train gradient:  0.4863881733333858
iteration : 1088
train acc:  0.75
train loss:  0.48402896523475647
train gradient:  0.4626690992188401
iteration : 1089
train acc:  0.7578125
train loss:  0.47800105810165405
train gradient:  0.43136880610732076
iteration : 1090
train acc:  0.796875
train loss:  0.40895015001296997
train gradient:  0.38616801264812917
iteration : 1091
train acc:  0.7734375
train loss:  0.4509826600551605
train gradient:  0.4920430475889026
iteration : 1092
train acc:  0.75
train loss:  0.5787318348884583
train gradient:  0.6592869145355404
iteration : 1093
train acc:  0.7890625
train loss:  0.4364747405052185
train gradient:  0.4214018109658731
iteration : 1094
train acc:  0.75
train loss:  0.5008203983306885
train gradient:  0.6061069018103796
iteration : 1095
train acc:  0.7421875
train loss:  0.5199549198150635
train gradient:  0.5291655245508436
iteration : 1096
train acc:  0.796875
train loss:  0.40648436546325684
train gradient:  0.33809651256977863
iteration : 1097
train acc:  0.78125
train loss:  0.43394920229911804
train gradient:  0.35191527722585164
iteration : 1098
train acc:  0.796875
train loss:  0.4767218232154846
train gradient:  0.629605607178448
iteration : 1099
train acc:  0.8203125
train loss:  0.46693068742752075
train gradient:  0.4714442485426312
iteration : 1100
train acc:  0.8359375
train loss:  0.3864021897315979
train gradient:  0.44946181441833843
iteration : 1101
train acc:  0.7734375
train loss:  0.4374595880508423
train gradient:  0.49619662192281383
iteration : 1102
train acc:  0.7265625
train loss:  0.5065479278564453
train gradient:  0.5387708799319887
iteration : 1103
train acc:  0.7265625
train loss:  0.5179121494293213
train gradient:  0.579607543892773
iteration : 1104
train acc:  0.7734375
train loss:  0.45896247029304504
train gradient:  0.4421236389591289
iteration : 1105
train acc:  0.765625
train loss:  0.44820278882980347
train gradient:  0.33915192400694233
iteration : 1106
train acc:  0.671875
train loss:  0.5545307993888855
train gradient:  0.7780042970213643
iteration : 1107
train acc:  0.8125
train loss:  0.4004824161529541
train gradient:  0.4386102768802129
iteration : 1108
train acc:  0.75
train loss:  0.4720010757446289
train gradient:  0.4175404454451885
iteration : 1109
train acc:  0.71875
train loss:  0.5257569551467896
train gradient:  0.7416806759763392
iteration : 1110
train acc:  0.734375
train loss:  0.46543553471565247
train gradient:  0.3937065722584685
iteration : 1111
train acc:  0.828125
train loss:  0.42479056119918823
train gradient:  0.4552891223259456
iteration : 1112
train acc:  0.7734375
train loss:  0.44317901134490967
train gradient:  0.42095671141404095
iteration : 1113
train acc:  0.671875
train loss:  0.5849759578704834
train gradient:  0.5936550023401904
iteration : 1114
train acc:  0.765625
train loss:  0.5171751976013184
train gradient:  0.4892363205931126
iteration : 1115
train acc:  0.7890625
train loss:  0.4589768350124359
train gradient:  0.5253945955525198
iteration : 1116
train acc:  0.859375
train loss:  0.40318652987480164
train gradient:  0.3350268896864674
iteration : 1117
train acc:  0.8125
train loss:  0.37731724977493286
train gradient:  0.30431451439809687
iteration : 1118
train acc:  0.7109375
train loss:  0.5275943279266357
train gradient:  0.5808275940521608
iteration : 1119
train acc:  0.765625
train loss:  0.4565429091453552
train gradient:  0.38553650703009623
iteration : 1120
train acc:  0.796875
train loss:  0.4221515655517578
train gradient:  0.4077756779750983
iteration : 1121
train acc:  0.78125
train loss:  0.4318583309650421
train gradient:  0.3802801285802212
iteration : 1122
train acc:  0.8359375
train loss:  0.401897132396698
train gradient:  0.47674477941512505
iteration : 1123
train acc:  0.7890625
train loss:  0.4772350788116455
train gradient:  0.6095714057160471
iteration : 1124
train acc:  0.8359375
train loss:  0.43409809470176697
train gradient:  0.392382129499746
iteration : 1125
train acc:  0.7734375
train loss:  0.44516268372535706
train gradient:  0.5821967007351854
iteration : 1126
train acc:  0.765625
train loss:  0.47298288345336914
train gradient:  0.31535605150884816
iteration : 1127
train acc:  0.75
train loss:  0.5044775009155273
train gradient:  0.5945531126209755
iteration : 1128
train acc:  0.765625
train loss:  0.48160022497177124
train gradient:  0.3945733158922275
iteration : 1129
train acc:  0.8671875
train loss:  0.3760904371738434
train gradient:  0.3786298468369638
iteration : 1130
train acc:  0.765625
train loss:  0.5146471261978149
train gradient:  0.6681890130111948
iteration : 1131
train acc:  0.7890625
train loss:  0.467894971370697
train gradient:  0.47661366356973534
iteration : 1132
train acc:  0.8046875
train loss:  0.4324526786804199
train gradient:  0.478631127018168
iteration : 1133
train acc:  0.703125
train loss:  0.5578585863113403
train gradient:  0.6405307511305276
iteration : 1134
train acc:  0.78125
train loss:  0.4764299690723419
train gradient:  0.5111085887046442
iteration : 1135
train acc:  0.75
train loss:  0.4994851052761078
train gradient:  0.5408235485902009
iteration : 1136
train acc:  0.796875
train loss:  0.46315211057662964
train gradient:  0.34072642869707676
iteration : 1137
train acc:  0.734375
train loss:  0.44922128319740295
train gradient:  0.5749058098956212
iteration : 1138
train acc:  0.796875
train loss:  0.44042888283729553
train gradient:  0.6127999396203234
iteration : 1139
train acc:  0.7890625
train loss:  0.46786728501319885
train gradient:  0.6601325111148021
iteration : 1140
train acc:  0.8515625
train loss:  0.3708382844924927
train gradient:  0.3319876178168289
iteration : 1141
train acc:  0.734375
train loss:  0.5106903314590454
train gradient:  0.5668470077730555
iteration : 1142
train acc:  0.828125
train loss:  0.4398694634437561
train gradient:  0.5221808925361509
iteration : 1143
train acc:  0.828125
train loss:  0.4024331271648407
train gradient:  0.3273187992354903
iteration : 1144
train acc:  0.8203125
train loss:  0.42725688219070435
train gradient:  0.3885335312399238
iteration : 1145
train acc:  0.796875
train loss:  0.48267149925231934
train gradient:  0.5055441802727685
iteration : 1146
train acc:  0.8203125
train loss:  0.44196319580078125
train gradient:  0.4560259662194129
iteration : 1147
train acc:  0.7578125
train loss:  0.47298502922058105
train gradient:  0.4957697295261675
iteration : 1148
train acc:  0.8203125
train loss:  0.3957214951515198
train gradient:  0.31668954968875374
iteration : 1149
train acc:  0.796875
train loss:  0.4347330927848816
train gradient:  0.3751096744474749
iteration : 1150
train acc:  0.8203125
train loss:  0.3721524477005005
train gradient:  0.3535648539241161
iteration : 1151
train acc:  0.8125
train loss:  0.39356133341789246
train gradient:  0.4309157290352618
iteration : 1152
train acc:  0.8046875
train loss:  0.42945125699043274
train gradient:  0.3813478657203798
iteration : 1153
train acc:  0.84375
train loss:  0.38848844170570374
train gradient:  0.372231639115855
iteration : 1154
train acc:  0.8515625
train loss:  0.39211195707321167
train gradient:  0.3979807157058595
iteration : 1155
train acc:  0.859375
train loss:  0.43958595395088196
train gradient:  0.4570334666687145
iteration : 1156
train acc:  0.75
train loss:  0.4795646667480469
train gradient:  0.48086733241605484
iteration : 1157
train acc:  0.7578125
train loss:  0.4171332120895386
train gradient:  0.43120615583313043
iteration : 1158
train acc:  0.7578125
train loss:  0.47330740094184875
train gradient:  0.5081555165834527
iteration : 1159
train acc:  0.84375
train loss:  0.3569546937942505
train gradient:  0.29674355845557715
iteration : 1160
train acc:  0.8359375
train loss:  0.4067342281341553
train gradient:  0.5296041963887961
iteration : 1161
train acc:  0.78125
train loss:  0.48532915115356445
train gradient:  0.5771290335739263
iteration : 1162
train acc:  0.8046875
train loss:  0.44216394424438477
train gradient:  0.46376094809619295
iteration : 1163
train acc:  0.796875
train loss:  0.4337957203388214
train gradient:  0.42198904258487985
iteration : 1164
train acc:  0.8125
train loss:  0.4182892441749573
train gradient:  0.35296731633714873
iteration : 1165
train acc:  0.7890625
train loss:  0.4381259083747864
train gradient:  0.4023209090874307
iteration : 1166
train acc:  0.7578125
train loss:  0.43410924077033997
train gradient:  0.40604135487687365
iteration : 1167
train acc:  0.8125
train loss:  0.38316377997398376
train gradient:  0.41675754430132406
iteration : 1168
train acc:  0.8203125
train loss:  0.40326952934265137
train gradient:  0.4871676339520343
iteration : 1169
train acc:  0.7578125
train loss:  0.45741698145866394
train gradient:  0.5932788536592287
iteration : 1170
train acc:  0.765625
train loss:  0.5399273633956909
train gradient:  0.7462573432438487
iteration : 1171
train acc:  0.7890625
train loss:  0.4822693169116974
train gradient:  0.5708987207967959
iteration : 1172
train acc:  0.8046875
train loss:  0.38304799795150757
train gradient:  0.36103969569732086
iteration : 1173
train acc:  0.7578125
train loss:  0.4610961377620697
train gradient:  0.4435952114076052
iteration : 1174
train acc:  0.75
train loss:  0.5381253957748413
train gradient:  0.7045639616683333
iteration : 1175
train acc:  0.796875
train loss:  0.47138094902038574
train gradient:  0.5231807833002784
iteration : 1176
train acc:  0.7734375
train loss:  0.488594651222229
train gradient:  0.47496264986512243
iteration : 1177
train acc:  0.84375
train loss:  0.34284818172454834
train gradient:  0.3296541068742181
iteration : 1178
train acc:  0.828125
train loss:  0.3707203269004822
train gradient:  0.2842642782825357
iteration : 1179
train acc:  0.859375
train loss:  0.37854790687561035
train gradient:  0.3986093337312064
iteration : 1180
train acc:  0.7734375
train loss:  0.4835570752620697
train gradient:  0.5200620754867695
iteration : 1181
train acc:  0.828125
train loss:  0.3948635160923004
train gradient:  0.3946091846754565
iteration : 1182
train acc:  0.8203125
train loss:  0.3848118484020233
train gradient:  0.46999517739311214
iteration : 1183
train acc:  0.828125
train loss:  0.4669026732444763
train gradient:  0.4309128058840068
iteration : 1184
train acc:  0.765625
train loss:  0.48299074172973633
train gradient:  0.6347759742829528
iteration : 1185
train acc:  0.71875
train loss:  0.46819525957107544
train gradient:  0.5640541243497201
iteration : 1186
train acc:  0.828125
train loss:  0.42118850350379944
train gradient:  0.39673893450738745
iteration : 1187
train acc:  0.8515625
train loss:  0.393954873085022
train gradient:  0.4473282723601468
iteration : 1188
train acc:  0.71875
train loss:  0.5488892793655396
train gradient:  0.6944408969906356
iteration : 1189
train acc:  0.7734375
train loss:  0.4386039972305298
train gradient:  0.42751072738941115
iteration : 1190
train acc:  0.78125
train loss:  0.4447411596775055
train gradient:  0.5203058691427683
iteration : 1191
train acc:  0.8046875
train loss:  0.4488254189491272
train gradient:  0.42715646890699727
iteration : 1192
train acc:  0.7578125
train loss:  0.4340810775756836
train gradient:  0.4497494296149529
iteration : 1193
train acc:  0.796875
train loss:  0.3746797442436218
train gradient:  0.3348282617989037
iteration : 1194
train acc:  0.75
train loss:  0.4900505840778351
train gradient:  0.5644334666231268
iteration : 1195
train acc:  0.8203125
train loss:  0.42258980870246887
train gradient:  0.5078389181535176
iteration : 1196
train acc:  0.7734375
train loss:  0.44604870676994324
train gradient:  0.41906719838488143
iteration : 1197
train acc:  0.828125
train loss:  0.4211973249912262
train gradient:  0.4617023287445049
iteration : 1198
train acc:  0.8125
train loss:  0.4656783640384674
train gradient:  0.6386267485375086
iteration : 1199
train acc:  0.765625
train loss:  0.42791157960891724
train gradient:  0.5107659693175317
iteration : 1200
train acc:  0.7890625
train loss:  0.44796305894851685
train gradient:  0.5776258849901406
iteration : 1201
train acc:  0.7890625
train loss:  0.5023830533027649
train gradient:  0.7125875607002309
iteration : 1202
train acc:  0.8125
train loss:  0.40317273139953613
train gradient:  0.4182518229534546
iteration : 1203
train acc:  0.7890625
train loss:  0.4438130259513855
train gradient:  0.5523762229294628
iteration : 1204
train acc:  0.78125
train loss:  0.4618391990661621
train gradient:  0.5701334114116348
iteration : 1205
train acc:  0.7734375
train loss:  0.46552592515945435
train gradient:  0.7843608208078323
iteration : 1206
train acc:  0.7734375
train loss:  0.45011210441589355
train gradient:  0.35427971858061785
iteration : 1207
train acc:  0.8046875
train loss:  0.4466071128845215
train gradient:  0.39166752607995114
iteration : 1208
train acc:  0.734375
train loss:  0.5281235575675964
train gradient:  0.6172715278663099
iteration : 1209
train acc:  0.8125
train loss:  0.43555691838264465
train gradient:  0.676440081721364
iteration : 1210
train acc:  0.78125
train loss:  0.44482797384262085
train gradient:  0.5237317613183291
iteration : 1211
train acc:  0.7578125
train loss:  0.4673462212085724
train gradient:  0.5576016944064626
iteration : 1212
train acc:  0.796875
train loss:  0.4645090699195862
train gradient:  0.4599637473466116
iteration : 1213
train acc:  0.8203125
train loss:  0.4431401789188385
train gradient:  0.6286291432475114
iteration : 1214
train acc:  0.71875
train loss:  0.4686911106109619
train gradient:  0.5434816208755288
iteration : 1215
train acc:  0.796875
train loss:  0.38373658061027527
train gradient:  0.5091057579981462
iteration : 1216
train acc:  0.8359375
train loss:  0.39184027910232544
train gradient:  0.42328084784544795
iteration : 1217
train acc:  0.8125
train loss:  0.42027464509010315
train gradient:  0.48871315705189783
iteration : 1218
train acc:  0.7890625
train loss:  0.42645084857940674
train gradient:  0.6769864802075054
iteration : 1219
train acc:  0.7734375
train loss:  0.46227914094924927
train gradient:  0.6827081498790917
iteration : 1220
train acc:  0.71875
train loss:  0.5612903833389282
train gradient:  0.9847973872156658
iteration : 1221
train acc:  0.7578125
train loss:  0.4845063090324402
train gradient:  0.7019722959279383
iteration : 1222
train acc:  0.84375
train loss:  0.4111648499965668
train gradient:  0.5274945780453855
iteration : 1223
train acc:  0.8125
train loss:  0.4399229884147644
train gradient:  0.46736605183136765
iteration : 1224
train acc:  0.7734375
train loss:  0.47280025482177734
train gradient:  0.6457851845135407
iteration : 1225
train acc:  0.703125
train loss:  0.554944634437561
train gradient:  0.6192135006412393
iteration : 1226
train acc:  0.7578125
train loss:  0.5123816132545471
train gradient:  0.6957279947870431
iteration : 1227
train acc:  0.828125
train loss:  0.3837635815143585
train gradient:  0.49172584517868756
iteration : 1228
train acc:  0.8203125
train loss:  0.3824657201766968
train gradient:  0.44072222768313496
iteration : 1229
train acc:  0.7890625
train loss:  0.4715227782726288
train gradient:  0.6242550730672589
iteration : 1230
train acc:  0.7265625
train loss:  0.5473155975341797
train gradient:  0.8420714965561513
iteration : 1231
train acc:  0.7890625
train loss:  0.401424765586853
train gradient:  0.4511266608369601
iteration : 1232
train acc:  0.765625
train loss:  0.4572533667087555
train gradient:  0.4309702828924137
iteration : 1233
train acc:  0.75
train loss:  0.49943381547927856
train gradient:  0.5120671762124624
iteration : 1234
train acc:  0.8046875
train loss:  0.3900079131126404
train gradient:  0.33393092071054614
iteration : 1235
train acc:  0.75
train loss:  0.5204347372055054
train gradient:  0.6072974339864488
iteration : 1236
train acc:  0.8515625
train loss:  0.3793483376502991
train gradient:  0.4541152911467367
iteration : 1237
train acc:  0.734375
train loss:  0.5109776258468628
train gradient:  0.6985858667840776
iteration : 1238
train acc:  0.7890625
train loss:  0.44041526317596436
train gradient:  0.44346474339238984
iteration : 1239
train acc:  0.828125
train loss:  0.44909465312957764
train gradient:  0.40857655585784675
iteration : 1240
train acc:  0.78125
train loss:  0.5105858445167542
train gradient:  0.6944939526312943
iteration : 1241
train acc:  0.828125
train loss:  0.4109460711479187
train gradient:  0.5436891980026752
iteration : 1242
train acc:  0.765625
train loss:  0.4501795172691345
train gradient:  0.3866547005098286
iteration : 1243
train acc:  0.75
train loss:  0.540054202079773
train gradient:  0.6384673055011334
iteration : 1244
train acc:  0.75
train loss:  0.4777742624282837
train gradient:  0.5701307723990705
iteration : 1245
train acc:  0.7890625
train loss:  0.4369747042655945
train gradient:  0.4273028222934477
iteration : 1246
train acc:  0.7734375
train loss:  0.49652889370918274
train gradient:  0.5038137533715559
iteration : 1247
train acc:  0.78125
train loss:  0.4186888337135315
train gradient:  0.36722374203632113
iteration : 1248
train acc:  0.796875
train loss:  0.42928245663642883
train gradient:  0.38097241017895467
iteration : 1249
train acc:  0.7890625
train loss:  0.4409726560115814
train gradient:  0.4770348458328566
iteration : 1250
train acc:  0.7734375
train loss:  0.45906829833984375
train gradient:  0.45769376957780056
iteration : 1251
train acc:  0.796875
train loss:  0.420615553855896
train gradient:  0.3994450664063881
iteration : 1252
train acc:  0.7890625
train loss:  0.49052324891090393
train gradient:  0.5632873519019785
iteration : 1253
train acc:  0.828125
train loss:  0.44805118441581726
train gradient:  0.45430277816100684
iteration : 1254
train acc:  0.8046875
train loss:  0.44861483573913574
train gradient:  0.4448011434710183
iteration : 1255
train acc:  0.7734375
train loss:  0.46392568945884705
train gradient:  0.5377827878501531
iteration : 1256
train acc:  0.8046875
train loss:  0.40877479314804077
train gradient:  0.40704965507670193
iteration : 1257
train acc:  0.765625
train loss:  0.4359246492385864
train gradient:  0.46892065114538845
iteration : 1258
train acc:  0.78125
train loss:  0.4349828362464905
train gradient:  0.3712586963864639
iteration : 1259
train acc:  0.828125
train loss:  0.4148550033569336
train gradient:  0.4144717395708018
iteration : 1260
train acc:  0.7890625
train loss:  0.43668562173843384
train gradient:  0.4943792323003741
iteration : 1261
train acc:  0.828125
train loss:  0.437929630279541
train gradient:  0.4509642532870875
iteration : 1262
train acc:  0.765625
train loss:  0.46265357732772827
train gradient:  0.49831637970022497
iteration : 1263
train acc:  0.8125
train loss:  0.46619653701782227
train gradient:  0.5390234092279468
iteration : 1264
train acc:  0.8203125
train loss:  0.3898783326148987
train gradient:  0.4641198566339619
iteration : 1265
train acc:  0.78125
train loss:  0.4754534363746643
train gradient:  0.611509854457244
iteration : 1266
train acc:  0.7578125
train loss:  0.5021570920944214
train gradient:  0.5136419652031623
iteration : 1267
train acc:  0.828125
train loss:  0.4165453016757965
train gradient:  0.36635762173206843
iteration : 1268
train acc:  0.7421875
train loss:  0.5080435276031494
train gradient:  0.6140674963650865
iteration : 1269
train acc:  0.8203125
train loss:  0.4106205105781555
train gradient:  0.3759908530072525
iteration : 1270
train acc:  0.7578125
train loss:  0.4739896357059479
train gradient:  0.4733339397136305
iteration : 1271
train acc:  0.8125
train loss:  0.3963484764099121
train gradient:  0.49065795472603924
iteration : 1272
train acc:  0.8046875
train loss:  0.43295592069625854
train gradient:  0.5841752673207646
iteration : 1273
train acc:  0.765625
train loss:  0.42760610580444336
train gradient:  0.5561582293376937
iteration : 1274
train acc:  0.8125
train loss:  0.44523119926452637
train gradient:  0.42915535577936426
iteration : 1275
train acc:  0.8046875
train loss:  0.49960651993751526
train gradient:  0.43732855016794375
iteration : 1276
train acc:  0.796875
train loss:  0.42442989349365234
train gradient:  0.48018011590704684
iteration : 1277
train acc:  0.765625
train loss:  0.4386172890663147
train gradient:  0.466639597544582
iteration : 1278
train acc:  0.8046875
train loss:  0.4314030110836029
train gradient:  0.49295922519066665
iteration : 1279
train acc:  0.8359375
train loss:  0.37336885929107666
train gradient:  0.36848102482680595
iteration : 1280
train acc:  0.765625
train loss:  0.44603538513183594
train gradient:  0.46601245448618167
iteration : 1281
train acc:  0.8046875
train loss:  0.3752003312110901
train gradient:  0.3412606308485743
iteration : 1282
train acc:  0.8671875
train loss:  0.3504972457885742
train gradient:  0.359364312770222
iteration : 1283
train acc:  0.8359375
train loss:  0.40716004371643066
train gradient:  0.32111033716145365
iteration : 1284
train acc:  0.875
train loss:  0.3350297808647156
train gradient:  0.27950831806433457
iteration : 1285
train acc:  0.8203125
train loss:  0.4468640089035034
train gradient:  1.4112244342274631
iteration : 1286
train acc:  0.765625
train loss:  0.45780178904533386
train gradient:  0.4859496518723408
iteration : 1287
train acc:  0.765625
train loss:  0.4708859622478485
train gradient:  0.5090211460426955
iteration : 1288
train acc:  0.8515625
train loss:  0.3837515115737915
train gradient:  0.33703225934376735
iteration : 1289
train acc:  0.734375
train loss:  0.5387659072875977
train gradient:  0.6708971787312703
iteration : 1290
train acc:  0.8203125
train loss:  0.45646432042121887
train gradient:  0.49730896407236436
iteration : 1291
train acc:  0.796875
train loss:  0.3444004952907562
train gradient:  0.408414564938014
iteration : 1292
train acc:  0.8125
train loss:  0.4469994306564331
train gradient:  0.4442150250364098
iteration : 1293
train acc:  0.78125
train loss:  0.48448312282562256
train gradient:  0.6480345415856181
iteration : 1294
train acc:  0.796875
train loss:  0.4296034872531891
train gradient:  0.47758487504523484
iteration : 1295
train acc:  0.8125
train loss:  0.368940144777298
train gradient:  0.4823346725751823
iteration : 1296
train acc:  0.828125
train loss:  0.4066327214241028
train gradient:  0.41535550479842603
iteration : 1297
train acc:  0.78125
train loss:  0.44226014614105225
train gradient:  0.3944614120268519
iteration : 1298
train acc:  0.8515625
train loss:  0.4022664427757263
train gradient:  0.5099909211187632
iteration : 1299
train acc:  0.765625
train loss:  0.4868556559085846
train gradient:  0.5366214622489851
iteration : 1300
train acc:  0.84375
train loss:  0.3890187740325928
train gradient:  0.4921264825961131
iteration : 1301
train acc:  0.828125
train loss:  0.48996224999427795
train gradient:  0.6718474207828256
iteration : 1302
train acc:  0.78125
train loss:  0.46255236864089966
train gradient:  0.47011230582163216
iteration : 1303
train acc:  0.8203125
train loss:  0.39920657873153687
train gradient:  0.46259205548420906
iteration : 1304
train acc:  0.796875
train loss:  0.39027583599090576
train gradient:  0.30815567072982003
iteration : 1305
train acc:  0.7421875
train loss:  0.4925228953361511
train gradient:  0.49673497948324147
iteration : 1306
train acc:  0.796875
train loss:  0.4395946264266968
train gradient:  0.4409320392763324
iteration : 1307
train acc:  0.8203125
train loss:  0.44850337505340576
train gradient:  0.4581577017787972
iteration : 1308
train acc:  0.796875
train loss:  0.43439605832099915
train gradient:  0.579905062594749
iteration : 1309
train acc:  0.7578125
train loss:  0.5329756736755371
train gradient:  0.5948816984764892
iteration : 1310
train acc:  0.7890625
train loss:  0.4509585499763489
train gradient:  0.537433989500709
iteration : 1311
train acc:  0.796875
train loss:  0.4673921465873718
train gradient:  0.508409021157846
iteration : 1312
train acc:  0.8671875
train loss:  0.361307829618454
train gradient:  0.356980220531897
iteration : 1313
train acc:  0.8203125
train loss:  0.44494014978408813
train gradient:  0.5377198212955416
iteration : 1314
train acc:  0.7890625
train loss:  0.4911031723022461
train gradient:  0.8099312496298887
iteration : 1315
train acc:  0.75
train loss:  0.4764799475669861
train gradient:  0.5185749736481904
iteration : 1316
train acc:  0.84375
train loss:  0.4153553247451782
train gradient:  0.35851627010144227
iteration : 1317
train acc:  0.8515625
train loss:  0.40649333596229553
train gradient:  0.4377651001887586
iteration : 1318
train acc:  0.75
train loss:  0.46060097217559814
train gradient:  0.6056848427370927
iteration : 1319
train acc:  0.828125
train loss:  0.4047565460205078
train gradient:  0.42783865821644457
iteration : 1320
train acc:  0.8671875
train loss:  0.3444695472717285
train gradient:  0.34397038524045775
iteration : 1321
train acc:  0.8203125
train loss:  0.446388304233551
train gradient:  0.84503386928744
iteration : 1322
train acc:  0.8359375
train loss:  0.37764665484428406
train gradient:  0.3215841016628945
iteration : 1323
train acc:  0.8203125
train loss:  0.38909995555877686
train gradient:  0.4410837451605155
iteration : 1324
train acc:  0.78125
train loss:  0.46546584367752075
train gradient:  0.6083173203605119
iteration : 1325
train acc:  0.7734375
train loss:  0.4371883273124695
train gradient:  0.42092563339290895
iteration : 1326
train acc:  0.78125
train loss:  0.46527254581451416
train gradient:  0.6707956203182591
iteration : 1327
train acc:  0.7890625
train loss:  0.4296864867210388
train gradient:  0.5676909983109178
iteration : 1328
train acc:  0.859375
train loss:  0.3689560890197754
train gradient:  0.4778647302607497
iteration : 1329
train acc:  0.7421875
train loss:  0.468249648809433
train gradient:  0.530692459274025
iteration : 1330
train acc:  0.78125
train loss:  0.4559268057346344
train gradient:  0.4377609591343648
iteration : 1331
train acc:  0.8671875
train loss:  0.3719395697116852
train gradient:  0.3868208918894616
iteration : 1332
train acc:  0.75
train loss:  0.475175678730011
train gradient:  0.6534598566631623
iteration : 1333
train acc:  0.828125
train loss:  0.4234459400177002
train gradient:  0.475462802418613
iteration : 1334
train acc:  0.7890625
train loss:  0.4173184037208557
train gradient:  0.5992715912942579
iteration : 1335
train acc:  0.8046875
train loss:  0.44233304262161255
train gradient:  0.5833109816340537
iteration : 1336
train acc:  0.8515625
train loss:  0.3891241252422333
train gradient:  0.4581866639808886
iteration : 1337
train acc:  0.7890625
train loss:  0.4530811309814453
train gradient:  0.6098295730637584
iteration : 1338
train acc:  0.796875
train loss:  0.42808833718299866
train gradient:  0.5211812301210771
iteration : 1339
train acc:  0.75
train loss:  0.4401625394821167
train gradient:  0.6951041475357729
iteration : 1340
train acc:  0.8125
train loss:  0.433230459690094
train gradient:  0.449608031091072
iteration : 1341
train acc:  0.734375
train loss:  0.494418203830719
train gradient:  0.7164423687683135
iteration : 1342
train acc:  0.7890625
train loss:  0.43782317638397217
train gradient:  0.5414656706397575
iteration : 1343
train acc:  0.7578125
train loss:  0.47940659523010254
train gradient:  0.538508861070466
iteration : 1344
train acc:  0.7578125
train loss:  0.48327934741973877
train gradient:  0.5889027407833467
iteration : 1345
train acc:  0.8125
train loss:  0.45219433307647705
train gradient:  0.5106733087760406
iteration : 1346
train acc:  0.84375
train loss:  0.4279935657978058
train gradient:  0.5332523126979116
iteration : 1347
train acc:  0.8203125
train loss:  0.35962235927581787
train gradient:  0.3076406995395929
iteration : 1348
train acc:  0.875
train loss:  0.38135814666748047
train gradient:  0.3471480479305838
iteration : 1349
train acc:  0.84375
train loss:  0.40910202264785767
train gradient:  0.49661776064015506
iteration : 1350
train acc:  0.7890625
train loss:  0.4361003339290619
train gradient:  0.5089460548476723
iteration : 1351
train acc:  0.8359375
train loss:  0.38526690006256104
train gradient:  0.40833959622490124
iteration : 1352
train acc:  0.734375
train loss:  0.46908700466156006
train gradient:  0.5471250468076256
iteration : 1353
train acc:  0.7421875
train loss:  0.5466964840888977
train gradient:  0.6799895823276203
iteration : 1354
train acc:  0.859375
train loss:  0.36918509006500244
train gradient:  0.431261587239421
iteration : 1355
train acc:  0.8046875
train loss:  0.41947755217552185
train gradient:  0.39780017532476436
iteration : 1356
train acc:  0.7890625
train loss:  0.44519031047821045
train gradient:  0.49822860101110533
iteration : 1357
train acc:  0.8359375
train loss:  0.3962866961956024
train gradient:  0.4020810146699877
iteration : 1358
train acc:  0.78125
train loss:  0.4232250452041626
train gradient:  0.5586277143531513
iteration : 1359
train acc:  0.75
train loss:  0.483524352312088
train gradient:  0.4413486102190137
iteration : 1360
train acc:  0.7109375
train loss:  0.4937893748283386
train gradient:  0.6073787668056813
iteration : 1361
train acc:  0.8203125
train loss:  0.4121202230453491
train gradient:  0.47603666952524504
iteration : 1362
train acc:  0.7734375
train loss:  0.4285602271556854
train gradient:  0.3600620655797025
iteration : 1363
train acc:  0.8046875
train loss:  0.4152382016181946
train gradient:  0.4861418148104884
iteration : 1364
train acc:  0.7890625
train loss:  0.44192761182785034
train gradient:  0.5542819432542971
iteration : 1365
train acc:  0.8125
train loss:  0.4347362220287323
train gradient:  0.5592490502992465
iteration : 1366
train acc:  0.8046875
train loss:  0.40505850315093994
train gradient:  0.554845807289974
iteration : 1367
train acc:  0.7734375
train loss:  0.43785741925239563
train gradient:  0.5528906569430383
iteration : 1368
train acc:  0.796875
train loss:  0.43073803186416626
train gradient:  0.42739606798019997
iteration : 1369
train acc:  0.78125
train loss:  0.5246472954750061
train gradient:  0.7216985374114537
iteration : 1370
train acc:  0.8515625
train loss:  0.3500378131866455
train gradient:  0.3301077130990754
iteration : 1371
train acc:  0.8203125
train loss:  0.3750489354133606
train gradient:  0.3389612934885775
iteration : 1372
train acc:  0.7578125
train loss:  0.4865970313549042
train gradient:  0.6378239166136566
iteration : 1373
train acc:  0.7890625
train loss:  0.38465869426727295
train gradient:  0.4190164672019754
iteration : 1374
train acc:  0.8046875
train loss:  0.3941129744052887
train gradient:  0.4245939386910065
iteration : 1375
train acc:  0.765625
train loss:  0.43845996260643005
train gradient:  0.47314096946220263
iteration : 1376
train acc:  0.875
train loss:  0.3489152491092682
train gradient:  0.427176197197672
iteration : 1377
train acc:  0.7421875
train loss:  0.519607663154602
train gradient:  0.665468645636891
iteration : 1378
train acc:  0.75
train loss:  0.46550849080085754
train gradient:  0.6110484537092823
iteration : 1379
train acc:  0.828125
train loss:  0.37867850065231323
train gradient:  0.29765391185300333
iteration : 1380
train acc:  0.75
train loss:  0.5173713564872742
train gradient:  0.5845225899568669
iteration : 1381
train acc:  0.734375
train loss:  0.5026253461837769
train gradient:  0.5619372410889496
iteration : 1382
train acc:  0.796875
train loss:  0.4395656883716583
train gradient:  0.39963486028300704
iteration : 1383
train acc:  0.8203125
train loss:  0.4080900251865387
train gradient:  0.42621127666190717
iteration : 1384
train acc:  0.78125
train loss:  0.505174994468689
train gradient:  0.6542203563557638
iteration : 1385
train acc:  0.796875
train loss:  0.48676684498786926
train gradient:  0.5203918517533135
iteration : 1386
train acc:  0.7890625
train loss:  0.4786771535873413
train gradient:  0.687256942226789
iteration : 1387
train acc:  0.796875
train loss:  0.45138436555862427
train gradient:  0.5243164834901157
iteration : 1388
train acc:  0.7734375
train loss:  0.52742600440979
train gradient:  1.0674489462355727
iteration : 1389
train acc:  0.765625
train loss:  0.4762868285179138
train gradient:  0.5521143706999885
iteration : 1390
train acc:  0.7109375
train loss:  0.5125243067741394
train gradient:  0.7175890252637482
iteration : 1391
train acc:  0.8671875
train loss:  0.3501814603805542
train gradient:  0.4048512353227139
iteration : 1392
train acc:  0.8203125
train loss:  0.4066045880317688
train gradient:  0.44287624951718135
iteration : 1393
train acc:  0.75
train loss:  0.4642084836959839
train gradient:  0.4573388952211368
iteration : 1394
train acc:  0.8203125
train loss:  0.3829733729362488
train gradient:  0.26730166435296276
iteration : 1395
train acc:  0.7890625
train loss:  0.465288907289505
train gradient:  0.5355624197789118
iteration : 1396
train acc:  0.75
train loss:  0.4610680043697357
train gradient:  0.4905414390883628
iteration : 1397
train acc:  0.78125
train loss:  0.5066720247268677
train gradient:  0.5391642948809078
iteration : 1398
train acc:  0.8046875
train loss:  0.4412199854850769
train gradient:  0.3875749337625676
iteration : 1399
train acc:  0.90625
train loss:  0.33100026845932007
train gradient:  0.24537124742089117
iteration : 1400
train acc:  0.8203125
train loss:  0.4236529469490051
train gradient:  0.3980447011337894
iteration : 1401
train acc:  0.765625
train loss:  0.47337740659713745
train gradient:  0.5278930626550791
iteration : 1402
train acc:  0.796875
train loss:  0.4856800436973572
train gradient:  0.6359019950759115
iteration : 1403
train acc:  0.765625
train loss:  0.4278043806552887
train gradient:  0.4005273807978912
iteration : 1404
train acc:  0.7734375
train loss:  0.44133925437927246
train gradient:  0.3295949222623888
iteration : 1405
train acc:  0.796875
train loss:  0.48403364419937134
train gradient:  0.40989816795056805
iteration : 1406
train acc:  0.7734375
train loss:  0.4834848642349243
train gradient:  0.5827274563685447
iteration : 1407
train acc:  0.8359375
train loss:  0.42675530910491943
train gradient:  0.4266998029384314
iteration : 1408
train acc:  0.78125
train loss:  0.47035467624664307
train gradient:  0.43088249230071074
iteration : 1409
train acc:  0.78125
train loss:  0.43657663464546204
train gradient:  0.4657603601278894
iteration : 1410
train acc:  0.8203125
train loss:  0.3889533281326294
train gradient:  0.42229811742137136
iteration : 1411
train acc:  0.7734375
train loss:  0.4710133671760559
train gradient:  0.5246588216004455
iteration : 1412
train acc:  0.796875
train loss:  0.4801338016986847
train gradient:  0.5691777637229907
iteration : 1413
train acc:  0.78125
train loss:  0.4535650610923767
train gradient:  0.3471976872336534
iteration : 1414
train acc:  0.8359375
train loss:  0.4129484295845032
train gradient:  0.4257558023257743
iteration : 1415
train acc:  0.7578125
train loss:  0.4755181074142456
train gradient:  0.4068818467751202
iteration : 1416
train acc:  0.84375
train loss:  0.4137720763683319
train gradient:  0.5118377488050414
iteration : 1417
train acc:  0.7890625
train loss:  0.4245994985103607
train gradient:  0.4985557452590964
iteration : 1418
train acc:  0.796875
train loss:  0.4541170001029968
train gradient:  0.48988893745957146
iteration : 1419
train acc:  0.828125
train loss:  0.45394736528396606
train gradient:  0.3700215916920038
iteration : 1420
train acc:  0.78125
train loss:  0.43034905195236206
train gradient:  0.39751972681315106
iteration : 1421
train acc:  0.8203125
train loss:  0.3839293122291565
train gradient:  0.33817977296004986
iteration : 1422
train acc:  0.796875
train loss:  0.42894992232322693
train gradient:  0.679364708324312
iteration : 1423
train acc:  0.8046875
train loss:  0.40039658546447754
train gradient:  0.3493248570158685
iteration : 1424
train acc:  0.765625
train loss:  0.5272907018661499
train gradient:  0.42997871953959826
iteration : 1425
train acc:  0.8046875
train loss:  0.4476735591888428
train gradient:  0.412070162153493
iteration : 1426
train acc:  0.78125
train loss:  0.43115317821502686
train gradient:  0.3543269359050046
iteration : 1427
train acc:  0.84375
train loss:  0.404852956533432
train gradient:  0.34543404533457533
iteration : 1428
train acc:  0.828125
train loss:  0.40161001682281494
train gradient:  0.5288485534715883
iteration : 1429
train acc:  0.7265625
train loss:  0.5000936985015869
train gradient:  0.4971515849281409
iteration : 1430
train acc:  0.8359375
train loss:  0.42993617057800293
train gradient:  0.3223021875251262
iteration : 1431
train acc:  0.859375
train loss:  0.3874609172344208
train gradient:  0.3782403960733159
iteration : 1432
train acc:  0.8125
train loss:  0.41021236777305603
train gradient:  0.5227928083783071
iteration : 1433
train acc:  0.765625
train loss:  0.4427551031112671
train gradient:  0.5818542829239944
iteration : 1434
train acc:  0.8125
train loss:  0.36254239082336426
train gradient:  0.3517025710970929
iteration : 1435
train acc:  0.90625
train loss:  0.3020537495613098
train gradient:  0.24820435149651754
iteration : 1436
train acc:  0.8828125
train loss:  0.3554578721523285
train gradient:  0.3333640234064328
iteration : 1437
train acc:  0.8515625
train loss:  0.388020783662796
train gradient:  0.4087553212748141
iteration : 1438
train acc:  0.8515625
train loss:  0.3468872606754303
train gradient:  0.30699689019338583
iteration : 1439
train acc:  0.8203125
train loss:  0.42547714710235596
train gradient:  0.3393830375340194
iteration : 1440
train acc:  0.7890625
train loss:  0.41900643706321716
train gradient:  0.33359252177455984
iteration : 1441
train acc:  0.703125
train loss:  0.5033003091812134
train gradient:  0.6184909494682191
iteration : 1442
train acc:  0.7890625
train loss:  0.4773503541946411
train gradient:  0.8611412224362214
iteration : 1443
train acc:  0.8359375
train loss:  0.39785701036453247
train gradient:  0.34155069205477684
iteration : 1444
train acc:  0.765625
train loss:  0.4750736951828003
train gradient:  0.5057572377649314
iteration : 1445
train acc:  0.796875
train loss:  0.4175226092338562
train gradient:  0.3635492440113756
iteration : 1446
train acc:  0.828125
train loss:  0.38476669788360596
train gradient:  0.383234018686721
iteration : 1447
train acc:  0.78125
train loss:  0.4961854815483093
train gradient:  0.6365843896423489
iteration : 1448
train acc:  0.8125
train loss:  0.4875800609588623
train gradient:  0.5233742137686331
iteration : 1449
train acc:  0.828125
train loss:  0.4310327172279358
train gradient:  0.4176785601848492
iteration : 1450
train acc:  0.7890625
train loss:  0.4663538336753845
train gradient:  0.4559101524240258
iteration : 1451
train acc:  0.8515625
train loss:  0.37021249532699585
train gradient:  0.31732256119098284
iteration : 1452
train acc:  0.8046875
train loss:  0.3885633945465088
train gradient:  0.3681999496895929
iteration : 1453
train acc:  0.8203125
train loss:  0.40938863158226013
train gradient:  0.4391770507792546
iteration : 1454
train acc:  0.8515625
train loss:  0.3607160449028015
train gradient:  0.40055637331131894
iteration : 1455
train acc:  0.8046875
train loss:  0.4094846248626709
train gradient:  0.4444924713848215
iteration : 1456
train acc:  0.8125
train loss:  0.38546839356422424
train gradient:  0.3049656617029442
iteration : 1457
train acc:  0.796875
train loss:  0.5196319818496704
train gradient:  0.3772536292656007
iteration : 1458
train acc:  0.8203125
train loss:  0.3566089868545532
train gradient:  0.38803866279640353
iteration : 1459
train acc:  0.8359375
train loss:  0.36944299936294556
train gradient:  0.39757937359953405
iteration : 1460
train acc:  0.859375
train loss:  0.32485198974609375
train gradient:  0.33145701521810256
iteration : 1461
train acc:  0.8125
train loss:  0.3967626988887787
train gradient:  0.50858371217792
iteration : 1462
train acc:  0.7734375
train loss:  0.4816421866416931
train gradient:  0.5673537015800278
iteration : 1463
train acc:  0.8203125
train loss:  0.4194740056991577
train gradient:  0.46240997319989724
iteration : 1464
train acc:  0.84375
train loss:  0.3847644627094269
train gradient:  0.35438964090306035
iteration : 1465
train acc:  0.78125
train loss:  0.4829823076725006
train gradient:  0.6325353012935042
iteration : 1466
train acc:  0.7890625
train loss:  0.4898720383644104
train gradient:  0.6057575075907525
iteration : 1467
train acc:  0.8203125
train loss:  0.372165322303772
train gradient:  0.34170184335854276
iteration : 1468
train acc:  0.84375
train loss:  0.3946141004562378
train gradient:  0.3900269406704637
iteration : 1469
train acc:  0.75
train loss:  0.44039806723594666
train gradient:  0.5162388723137913
iteration : 1470
train acc:  0.875
train loss:  0.32285821437835693
train gradient:  0.44358559066645276
iteration : 1471
train acc:  0.8828125
train loss:  0.29495593905448914
train gradient:  0.2797750448370637
iteration : 1472
train acc:  0.875
train loss:  0.36762455105781555
train gradient:  0.3569807748545856
iteration : 1473
train acc:  0.8125
train loss:  0.4160558581352234
train gradient:  0.4235642902016392
iteration : 1474
train acc:  0.8359375
train loss:  0.35663485527038574
train gradient:  0.4150939541843579
iteration : 1475
train acc:  0.828125
train loss:  0.453286349773407
train gradient:  0.692693331884389
iteration : 1476
train acc:  0.796875
train loss:  0.48177236318588257
train gradient:  0.6140829067089015
iteration : 1477
train acc:  0.796875
train loss:  0.41340288519859314
train gradient:  0.4599818918809295
iteration : 1478
train acc:  0.8046875
train loss:  0.40501564741134644
train gradient:  0.45532438079310195
iteration : 1479
train acc:  0.8359375
train loss:  0.3983215093612671
train gradient:  0.5951392482244359
iteration : 1480
train acc:  0.84375
train loss:  0.37832051515579224
train gradient:  0.37565080255703487
iteration : 1481
train acc:  0.8359375
train loss:  0.37734168767929077
train gradient:  0.43754955970471665
iteration : 1482
train acc:  0.8125
train loss:  0.3478219509124756
train gradient:  0.4164433074568491
iteration : 1483
train acc:  0.8046875
train loss:  0.4767420291900635
train gradient:  0.6127307892268906
iteration : 1484
train acc:  0.8125
train loss:  0.3771078586578369
train gradient:  0.47464983009249107
iteration : 1485
train acc:  0.8125
train loss:  0.3650324046611786
train gradient:  0.386240292831401
iteration : 1486
train acc:  0.765625
train loss:  0.45504921674728394
train gradient:  0.538446042323706
iteration : 1487
train acc:  0.8828125
train loss:  0.3423781096935272
train gradient:  0.3814878908647445
iteration : 1488
train acc:  0.7734375
train loss:  0.4047439694404602
train gradient:  0.6098004615027839
iteration : 1489
train acc:  0.8125
train loss:  0.46460890769958496
train gradient:  0.5663369017951125
iteration : 1490
train acc:  0.828125
train loss:  0.3842369019985199
train gradient:  0.41673936281579294
iteration : 1491
train acc:  0.7890625
train loss:  0.4295254945755005
train gradient:  0.5197647629745649
iteration : 1492
train acc:  0.765625
train loss:  0.4053749442100525
train gradient:  0.48068825566778545
iteration : 1493
train acc:  0.7734375
train loss:  0.47385740280151367
train gradient:  0.4461796054047637
iteration : 1494
train acc:  0.8359375
train loss:  0.3575265407562256
train gradient:  0.330622067698753
iteration : 1495
train acc:  0.796875
train loss:  0.41962379217147827
train gradient:  0.39177135941742625
iteration : 1496
train acc:  0.8046875
train loss:  0.38318270444869995
train gradient:  0.4798189075664722
iteration : 1497
train acc:  0.796875
train loss:  0.47480303049087524
train gradient:  0.6777570943538009
iteration : 1498
train acc:  0.8203125
train loss:  0.4501996636390686
train gradient:  0.45612311495527
iteration : 1499
train acc:  0.828125
train loss:  0.42744678258895874
train gradient:  0.6387250837999272
iteration : 1500
train acc:  0.84375
train loss:  0.4002354145050049
train gradient:  0.38228345850797246
iteration : 1501
train acc:  0.8203125
train loss:  0.4176029562950134
train gradient:  0.44143809647709153
iteration : 1502
train acc:  0.8046875
train loss:  0.4308566749095917
train gradient:  0.6099891807660729
iteration : 1503
train acc:  0.765625
train loss:  0.4734352231025696
train gradient:  0.6069867922005442
iteration : 1504
train acc:  0.8203125
train loss:  0.3953564167022705
train gradient:  0.5215028997827457
iteration : 1505
train acc:  0.78125
train loss:  0.44509226083755493
train gradient:  0.3762907872875217
iteration : 1506
train acc:  0.8515625
train loss:  0.37939125299453735
train gradient:  0.4443313442342734
iteration : 1507
train acc:  0.7578125
train loss:  0.49285411834716797
train gradient:  0.7209239671511761
iteration : 1508
train acc:  0.78125
train loss:  0.456045925617218
train gradient:  0.6502225375650967
iteration : 1509
train acc:  0.828125
train loss:  0.37817567586898804
train gradient:  0.38960739796962396
iteration : 1510
train acc:  0.75
train loss:  0.5559466481208801
train gradient:  0.763555815822174
iteration : 1511
train acc:  0.796875
train loss:  0.44823092222213745
train gradient:  0.5464911906826765
iteration : 1512
train acc:  0.8671875
train loss:  0.37707018852233887
train gradient:  0.31234318069524386
iteration : 1513
train acc:  0.8671875
train loss:  0.36961624026298523
train gradient:  0.41011138800685587
iteration : 1514
train acc:  0.84375
train loss:  0.39831405878067017
train gradient:  0.5474480535139611
iteration : 1515
train acc:  0.8125
train loss:  0.4139975309371948
train gradient:  0.45851446058051937
iteration : 1516
train acc:  0.7421875
train loss:  0.548879861831665
train gradient:  0.8382920129587694
iteration : 1517
train acc:  0.796875
train loss:  0.41428041458129883
train gradient:  0.49172256145343907
iteration : 1518
train acc:  0.8203125
train loss:  0.40506136417388916
train gradient:  0.4109303239447525
iteration : 1519
train acc:  0.7578125
train loss:  0.4851323664188385
train gradient:  0.5898991985653746
iteration : 1520
train acc:  0.75
train loss:  0.5011224150657654
train gradient:  0.6944982569566129
iteration : 1521
train acc:  0.7578125
train loss:  0.4506111145019531
train gradient:  0.6282946549617222
iteration : 1522
train acc:  0.8515625
train loss:  0.39195287227630615
train gradient:  0.5607940249825348
iteration : 1523
train acc:  0.7890625
train loss:  0.4404904246330261
train gradient:  0.49039989529951405
iteration : 1524
train acc:  0.8125
train loss:  0.45987462997436523
train gradient:  0.5669849391274803
iteration : 1525
train acc:  0.7734375
train loss:  0.4430775046348572
train gradient:  0.6604971361635573
iteration : 1526
train acc:  0.7578125
train loss:  0.4420948624610901
train gradient:  0.5083974692371398
iteration : 1527
train acc:  0.765625
train loss:  0.4634847044944763
train gradient:  0.4964162012652718
iteration : 1528
train acc:  0.8359375
train loss:  0.3393973112106323
train gradient:  0.3349113843838121
iteration : 1529
train acc:  0.8359375
train loss:  0.3922596573829651
train gradient:  0.4981158201338171
iteration : 1530
train acc:  0.796875
train loss:  0.4797150492668152
train gradient:  0.5655404618990982
iteration : 1531
train acc:  0.78125
train loss:  0.459370881319046
train gradient:  0.4692038324494247
iteration : 1532
train acc:  0.796875
train loss:  0.47249287366867065
train gradient:  0.5524558232746467
iteration : 1533
train acc:  0.84375
train loss:  0.36789947748184204
train gradient:  0.37785172229731817
iteration : 1534
train acc:  0.8203125
train loss:  0.3864840865135193
train gradient:  0.3068116014590757
iteration : 1535
train acc:  0.8125
train loss:  0.3657001256942749
train gradient:  0.40796032780828195
iteration : 1536
train acc:  0.75
train loss:  0.4738393723964691
train gradient:  0.4899540062654176
iteration : 1537
train acc:  0.765625
train loss:  0.42226529121398926
train gradient:  0.4461980864144637
iteration : 1538
train acc:  0.859375
train loss:  0.33621999621391296
train gradient:  0.36272437256428725
iteration : 1539
train acc:  0.8203125
train loss:  0.4455782473087311
train gradient:  0.512470289005197
iteration : 1540
train acc:  0.8203125
train loss:  0.41971611976623535
train gradient:  0.6945835654051521
iteration : 1541
train acc:  0.765625
train loss:  0.4994773864746094
train gradient:  0.593208860739203
iteration : 1542
train acc:  0.7421875
train loss:  0.5228142738342285
train gradient:  0.7438251371284188
iteration : 1543
train acc:  0.8046875
train loss:  0.4415549039840698
train gradient:  0.5363333482963292
iteration : 1544
train acc:  0.84375
train loss:  0.43646442890167236
train gradient:  0.402949398923652
iteration : 1545
train acc:  0.7890625
train loss:  0.4149025082588196
train gradient:  0.3880117109872394
iteration : 1546
train acc:  0.765625
train loss:  0.4737258553504944
train gradient:  0.44495047558378464
iteration : 1547
train acc:  0.8515625
train loss:  0.38035327196121216
train gradient:  0.30198763574656096
iteration : 1548
train acc:  0.7265625
train loss:  0.526453971862793
train gradient:  0.7985687647645592
iteration : 1549
train acc:  0.828125
train loss:  0.4032493233680725
train gradient:  0.39840271284736317
iteration : 1550
train acc:  0.828125
train loss:  0.466938853263855
train gradient:  0.4468072619098329
iteration : 1551
train acc:  0.890625
train loss:  0.31023266911506653
train gradient:  0.27061737359743615
iteration : 1552
train acc:  0.828125
train loss:  0.3476492762565613
train gradient:  0.28395434368196554
iteration : 1553
train acc:  0.8046875
train loss:  0.40254032611846924
train gradient:  0.42499815878858843
iteration : 1554
train acc:  0.8515625
train loss:  0.3570736348628998
train gradient:  0.3244717207080198
iteration : 1555
train acc:  0.8203125
train loss:  0.47040966153144836
train gradient:  0.6038229469918605
iteration : 1556
train acc:  0.8671875
train loss:  0.39070701599121094
train gradient:  0.39182971437985137
iteration : 1557
train acc:  0.7890625
train loss:  0.4148601293563843
train gradient:  0.5087302069480043
iteration : 1558
train acc:  0.8125
train loss:  0.40988287329673767
train gradient:  0.4182095817632827
iteration : 1559
train acc:  0.765625
train loss:  0.5283859968185425
train gradient:  0.6495142405449537
iteration : 1560
train acc:  0.75
train loss:  0.5009190440177917
train gradient:  0.4462148017159416
iteration : 1561
train acc:  0.8125
train loss:  0.4939701557159424
train gradient:  0.45787655355154855
iteration : 1562
train acc:  0.75
train loss:  0.5515327453613281
train gradient:  0.7813470548076475
iteration : 1563
train acc:  0.8046875
train loss:  0.4125449061393738
train gradient:  0.38832717378887527
iteration : 1564
train acc:  0.8046875
train loss:  0.43840914964675903
train gradient:  0.570879636096281
iteration : 1565
train acc:  0.8046875
train loss:  0.41933390498161316
train gradient:  0.3825838037254399
iteration : 1566
train acc:  0.8125
train loss:  0.38180071115493774
train gradient:  0.3480580821287566
iteration : 1567
train acc:  0.8359375
train loss:  0.40448683500289917
train gradient:  0.47346912514310274
iteration : 1568
train acc:  0.7578125
train loss:  0.4774055778980255
train gradient:  0.5639212160081035
iteration : 1569
train acc:  0.8515625
train loss:  0.37454307079315186
train gradient:  0.2916828656010213
iteration : 1570
train acc:  0.7578125
train loss:  0.5016855597496033
train gradient:  0.5012118056359459
iteration : 1571
train acc:  0.78125
train loss:  0.46190398931503296
train gradient:  0.4355495147090062
iteration : 1572
train acc:  0.75
train loss:  0.4999557137489319
train gradient:  0.5998647253018523
iteration : 1573
train acc:  0.7890625
train loss:  0.4612354040145874
train gradient:  0.4735388089669846
iteration : 1574
train acc:  0.765625
train loss:  0.45936769247055054
train gradient:  0.47322770690167415
iteration : 1575
train acc:  0.78125
train loss:  0.4549785256385803
train gradient:  0.3994269661789717
iteration : 1576
train acc:  0.71875
train loss:  0.4708057641983032
train gradient:  0.4447682860725465
iteration : 1577
train acc:  0.8203125
train loss:  0.40207570791244507
train gradient:  0.3147399982852984
iteration : 1578
train acc:  0.8359375
train loss:  0.4032225012779236
train gradient:  0.40685486789722897
iteration : 1579
train acc:  0.8359375
train loss:  0.38943666219711304
train gradient:  0.33382179089933905
iteration : 1580
train acc:  0.796875
train loss:  0.4321402907371521
train gradient:  0.29905364553277725
iteration : 1581
train acc:  0.8203125
train loss:  0.4147161841392517
train gradient:  0.3976887633674752
iteration : 1582
train acc:  0.7890625
train loss:  0.4336059093475342
train gradient:  0.5655971693229881
iteration : 1583
train acc:  0.78125
train loss:  0.43688416481018066
train gradient:  0.40354872018125426
iteration : 1584
train acc:  0.8046875
train loss:  0.4390571713447571
train gradient:  0.41385491589140466
iteration : 1585
train acc:  0.7890625
train loss:  0.47926339507102966
train gradient:  0.46407775425654507
iteration : 1586
train acc:  0.7578125
train loss:  0.49607813358306885
train gradient:  0.5023684216928621
iteration : 1587
train acc:  0.8359375
train loss:  0.4267892837524414
train gradient:  0.30429451590611534
iteration : 1588
train acc:  0.734375
train loss:  0.49558618664741516
train gradient:  0.5805655989296338
iteration : 1589
train acc:  0.78125
train loss:  0.45147159695625305
train gradient:  0.5437536288595051
iteration : 1590
train acc:  0.875
train loss:  0.37011170387268066
train gradient:  0.26751866574298755
iteration : 1591
train acc:  0.8671875
train loss:  0.37537646293640137
train gradient:  0.2845035199586764
iteration : 1592
train acc:  0.828125
train loss:  0.41898757219314575
train gradient:  0.4641347879665387
iteration : 1593
train acc:  0.8046875
train loss:  0.43819180130958557
train gradient:  0.5667694285447784
iteration : 1594
train acc:  0.796875
train loss:  0.4740551710128784
train gradient:  0.5436414156903098
iteration : 1595
train acc:  0.8046875
train loss:  0.44660520553588867
train gradient:  0.35753143210502875
iteration : 1596
train acc:  0.8828125
train loss:  0.32741251587867737
train gradient:  0.2642363846391683
iteration : 1597
train acc:  0.7578125
train loss:  0.43907660245895386
train gradient:  0.4789449689653897
iteration : 1598
train acc:  0.7734375
train loss:  0.5044066309928894
train gradient:  0.4698703136425663
iteration : 1599
train acc:  0.828125
train loss:  0.45685404539108276
train gradient:  0.4017867300017074
iteration : 1600
train acc:  0.828125
train loss:  0.4196895360946655
train gradient:  0.3122592113249868
iteration : 1601
train acc:  0.8125
train loss:  0.39214614033699036
train gradient:  0.2695576708207303
iteration : 1602
train acc:  0.765625
train loss:  0.5217149257659912
train gradient:  0.5472428852671818
iteration : 1603
train acc:  0.75
train loss:  0.5177345275878906
train gradient:  0.47744374005380913
iteration : 1604
train acc:  0.8359375
train loss:  0.41973787546157837
train gradient:  0.3932648839587178
iteration : 1605
train acc:  0.8203125
train loss:  0.38508883118629456
train gradient:  0.34761239651161674
iteration : 1606
train acc:  0.796875
train loss:  0.4263354539871216
train gradient:  0.4806841004913285
iteration : 1607
train acc:  0.78125
train loss:  0.4153537154197693
train gradient:  0.36937903961750884
iteration : 1608
train acc:  0.75
train loss:  0.48966842889785767
train gradient:  0.48992132106472286
iteration : 1609
train acc:  0.7578125
train loss:  0.5048694610595703
train gradient:  0.5443717649011097
iteration : 1610
train acc:  0.8203125
train loss:  0.4624664783477783
train gradient:  0.4940378747718113
iteration : 1611
train acc:  0.828125
train loss:  0.3612891435623169
train gradient:  0.43319715719111296
iteration : 1612
train acc:  0.7734375
train loss:  0.44057604670524597
train gradient:  0.3727280973812428
iteration : 1613
train acc:  0.8125
train loss:  0.41323918104171753
train gradient:  0.31235715888072585
iteration : 1614
train acc:  0.7578125
train loss:  0.5019342303276062
train gradient:  0.5452607659582052
iteration : 1615
train acc:  0.7734375
train loss:  0.5089027881622314
train gradient:  0.3866375796012066
iteration : 1616
train acc:  0.796875
train loss:  0.4525250196456909
train gradient:  0.5774917281438381
iteration : 1617
train acc:  0.8515625
train loss:  0.36081844568252563
train gradient:  0.3394704776303465
iteration : 1618
train acc:  0.8125
train loss:  0.4243583679199219
train gradient:  0.4209439433635675
iteration : 1619
train acc:  0.8046875
train loss:  0.4097203016281128
train gradient:  0.4880544001976392
iteration : 1620
train acc:  0.765625
train loss:  0.48528996109962463
train gradient:  0.5235306719720244
iteration : 1621
train acc:  0.8125
train loss:  0.4371018409729004
train gradient:  0.45161290980766283
iteration : 1622
train acc:  0.828125
train loss:  0.3910481035709381
train gradient:  0.30789451635197285
iteration : 1623
train acc:  0.7890625
train loss:  0.4430316090583801
train gradient:  0.34430372999157427
iteration : 1624
train acc:  0.7734375
train loss:  0.43563294410705566
train gradient:  0.5460775347432378
iteration : 1625
train acc:  0.8671875
train loss:  0.32927191257476807
train gradient:  0.31654680507152433
iteration : 1626
train acc:  0.828125
train loss:  0.36169111728668213
train gradient:  0.371475166308819
iteration : 1627
train acc:  0.828125
train loss:  0.46062690019607544
train gradient:  0.4201497865571558
iteration : 1628
train acc:  0.8046875
train loss:  0.43087106943130493
train gradient:  0.4240712899990205
iteration : 1629
train acc:  0.7890625
train loss:  0.4789910912513733
train gradient:  0.4634942281427655
iteration : 1630
train acc:  0.765625
train loss:  0.4697134792804718
train gradient:  0.31179206639858664
iteration : 1631
train acc:  0.8125
train loss:  0.41791754961013794
train gradient:  0.6303926153887545
iteration : 1632
train acc:  0.7890625
train loss:  0.4901142716407776
train gradient:  0.6334085091583113
iteration : 1633
train acc:  0.828125
train loss:  0.40247684717178345
train gradient:  0.41668161800396364
iteration : 1634
train acc:  0.828125
train loss:  0.34827131032943726
train gradient:  0.29471764832254893
iteration : 1635
train acc:  0.8203125
train loss:  0.3805953562259674
train gradient:  0.4518613226188746
iteration : 1636
train acc:  0.765625
train loss:  0.4991176128387451
train gradient:  0.5608488780755181
iteration : 1637
train acc:  0.7578125
train loss:  0.44273415207862854
train gradient:  0.4184304921532861
iteration : 1638
train acc:  0.71875
train loss:  0.4800529479980469
train gradient:  0.7121386416503763
iteration : 1639
train acc:  0.796875
train loss:  0.4461181163787842
train gradient:  0.46573047881709884
iteration : 1640
train acc:  0.8671875
train loss:  0.3542000949382782
train gradient:  0.38243458853538487
iteration : 1641
train acc:  0.8203125
train loss:  0.4486512541770935
train gradient:  0.3821030247480968
iteration : 1642
train acc:  0.8359375
train loss:  0.3867873549461365
train gradient:  0.5463042998850635
iteration : 1643
train acc:  0.84375
train loss:  0.37679558992385864
train gradient:  0.3377802556500704
iteration : 1644
train acc:  0.828125
train loss:  0.37043869495391846
train gradient:  0.3122500101255171
iteration : 1645
train acc:  0.8046875
train loss:  0.48332977294921875
train gradient:  0.4226270887793523
iteration : 1646
train acc:  0.84375
train loss:  0.36578625440597534
train gradient:  0.4405473118448472
iteration : 1647
train acc:  0.7734375
train loss:  0.4333932399749756
train gradient:  0.4038455784156349
iteration : 1648
train acc:  0.8046875
train loss:  0.4139690101146698
train gradient:  0.2988112260974296
iteration : 1649
train acc:  0.8125
train loss:  0.43679147958755493
train gradient:  0.6508529453183045
iteration : 1650
train acc:  0.765625
train loss:  0.4978017807006836
train gradient:  0.5029082988736411
iteration : 1651
train acc:  0.7578125
train loss:  0.43827319145202637
train gradient:  0.40167245789802636
iteration : 1652
train acc:  0.7890625
train loss:  0.43891167640686035
train gradient:  0.3309320700145714
iteration : 1653
train acc:  0.7734375
train loss:  0.46592336893081665
train gradient:  0.3666738319284853
iteration : 1654
train acc:  0.8125
train loss:  0.41936954855918884
train gradient:  0.32499137712986287
iteration : 1655
train acc:  0.796875
train loss:  0.5056623220443726
train gradient:  0.48236134377177997
iteration : 1656
train acc:  0.8125
train loss:  0.4041364789009094
train gradient:  0.3830222356526038
iteration : 1657
train acc:  0.78125
train loss:  0.474653422832489
train gradient:  0.4862807601158979
iteration : 1658
train acc:  0.8359375
train loss:  0.37636664509773254
train gradient:  0.3394004555140446
iteration : 1659
train acc:  0.8125
train loss:  0.4649600088596344
train gradient:  0.4690189655389588
iteration : 1660
train acc:  0.8203125
train loss:  0.3650614619255066
train gradient:  0.4195922965363541
iteration : 1661
train acc:  0.8203125
train loss:  0.43958839774131775
train gradient:  0.4790787596593857
iteration : 1662
train acc:  0.8046875
train loss:  0.39677074551582336
train gradient:  0.3802614046155404
iteration : 1663
train acc:  0.828125
train loss:  0.39031848311424255
train gradient:  0.4580863578706342
iteration : 1664
train acc:  0.828125
train loss:  0.3909335434436798
train gradient:  0.37426523895499053
iteration : 1665
train acc:  0.8203125
train loss:  0.41079944372177124
train gradient:  0.4250179641834774
iteration : 1666
train acc:  0.828125
train loss:  0.40857458114624023
train gradient:  0.4211200981771537
iteration : 1667
train acc:  0.71875
train loss:  0.5467197895050049
train gradient:  0.6759232041956008
iteration : 1668
train acc:  0.78125
train loss:  0.42196714878082275
train gradient:  0.410130237916258
iteration : 1669
train acc:  0.828125
train loss:  0.3678628206253052
train gradient:  0.2538135150829165
iteration : 1670
train acc:  0.78125
train loss:  0.4771665334701538
train gradient:  0.47020785841856505
iteration : 1671
train acc:  0.84375
train loss:  0.33125048875808716
train gradient:  0.3058644675076894
iteration : 1672
train acc:  0.8125
train loss:  0.46593981981277466
train gradient:  0.4365889564755753
iteration : 1673
train acc:  0.8203125
train loss:  0.40265876054763794
train gradient:  0.3409803015453056
iteration : 1674
train acc:  0.7734375
train loss:  0.5115154385566711
train gradient:  0.4409422732433579
iteration : 1675
train acc:  0.7578125
train loss:  0.43725699186325073
train gradient:  0.3753721070991246
iteration : 1676
train acc:  0.8046875
train loss:  0.4163348376750946
train gradient:  0.4512614821993354
iteration : 1677
train acc:  0.8203125
train loss:  0.4191977381706238
train gradient:  0.39394097265957373
iteration : 1678
train acc:  0.8671875
train loss:  0.38030073046684265
train gradient:  0.33330736570958946
iteration : 1679
train acc:  0.8203125
train loss:  0.4108901023864746
train gradient:  0.35016681160468727
iteration : 1680
train acc:  0.8046875
train loss:  0.4414268732070923
train gradient:  0.5378315044573714
iteration : 1681
train acc:  0.78125
train loss:  0.44050848484039307
train gradient:  0.3930167264712321
iteration : 1682
train acc:  0.8046875
train loss:  0.42762693762779236
train gradient:  0.3262301894589279
iteration : 1683
train acc:  0.734375
train loss:  0.512648344039917
train gradient:  0.48999793932888897
iteration : 1684
train acc:  0.875
train loss:  0.3337803781032562
train gradient:  0.35448713052458897
iteration : 1685
train acc:  0.796875
train loss:  0.47418251633644104
train gradient:  0.5441385549470615
iteration : 1686
train acc:  0.8359375
train loss:  0.35796231031417847
train gradient:  0.33765504137079966
iteration : 1687
train acc:  0.8125
train loss:  0.3939603269100189
train gradient:  0.36702604590832916
iteration : 1688
train acc:  0.8203125
train loss:  0.39263492822647095
train gradient:  0.6567194061808175
iteration : 1689
train acc:  0.8515625
train loss:  0.35379132628440857
train gradient:  0.44804157257088656
iteration : 1690
train acc:  0.765625
train loss:  0.445170521736145
train gradient:  0.43106406025158356
iteration : 1691
train acc:  0.8515625
train loss:  0.3987463712692261
train gradient:  0.36139170217883354
iteration : 1692
train acc:  0.84375
train loss:  0.3534642159938812
train gradient:  0.2998251461559049
iteration : 1693
train acc:  0.8515625
train loss:  0.3712643086910248
train gradient:  0.3842849679683635
iteration : 1694
train acc:  0.8359375
train loss:  0.4060117304325104
train gradient:  0.49894758438700554
iteration : 1695
train acc:  0.7890625
train loss:  0.5036314725875854
train gradient:  0.6032221399987054
iteration : 1696
train acc:  0.796875
train loss:  0.4293921887874603
train gradient:  0.4050125927161788
iteration : 1697
train acc:  0.7734375
train loss:  0.46792852878570557
train gradient:  0.45705501551315253
iteration : 1698
train acc:  0.7890625
train loss:  0.4079625606536865
train gradient:  0.3351931561631567
iteration : 1699
train acc:  0.8203125
train loss:  0.413404643535614
train gradient:  0.44797116028132583
iteration : 1700
train acc:  0.8046875
train loss:  0.4102584421634674
train gradient:  0.3839638718118012
iteration : 1701
train acc:  0.7890625
train loss:  0.43983137607574463
train gradient:  0.5224625302036772
iteration : 1702
train acc:  0.828125
train loss:  0.41291046142578125
train gradient:  0.47827143315469867
iteration : 1703
train acc:  0.8046875
train loss:  0.4333101511001587
train gradient:  0.407459609065897
iteration : 1704
train acc:  0.8203125
train loss:  0.38794028759002686
train gradient:  0.3888494375563436
iteration : 1705
train acc:  0.890625
train loss:  0.30950629711151123
train gradient:  0.23987525743449142
iteration : 1706
train acc:  0.84375
train loss:  0.3804340958595276
train gradient:  0.3586046218415153
iteration : 1707
train acc:  0.734375
train loss:  0.4954230487346649
train gradient:  0.5770572902523737
iteration : 1708
train acc:  0.890625
train loss:  0.29748767614364624
train gradient:  0.3473342806559102
iteration : 1709
train acc:  0.765625
train loss:  0.49677377939224243
train gradient:  0.5873799662618964
iteration : 1710
train acc:  0.828125
train loss:  0.44544923305511475
train gradient:  0.5062996880420046
iteration : 1711
train acc:  0.8046875
train loss:  0.41559410095214844
train gradient:  0.3680992541910569
iteration : 1712
train acc:  0.8203125
train loss:  0.4297458231449127
train gradient:  0.3153328488831478
iteration : 1713
train acc:  0.8125
train loss:  0.468696653842926
train gradient:  0.3591095737963333
iteration : 1714
train acc:  0.7109375
train loss:  0.518546462059021
train gradient:  0.5481558550040695
iteration : 1715
train acc:  0.8125
train loss:  0.4518098831176758
train gradient:  0.46427317320948797
iteration : 1716
train acc:  0.796875
train loss:  0.4163280725479126
train gradient:  0.45979733705872433
iteration : 1717
train acc:  0.7578125
train loss:  0.4263850450515747
train gradient:  0.502915920369047
iteration : 1718
train acc:  0.796875
train loss:  0.4108316898345947
train gradient:  0.41503946884814374
iteration : 1719
train acc:  0.7890625
train loss:  0.379405677318573
train gradient:  0.2987689319028307
iteration : 1720
train acc:  0.8125
train loss:  0.4135957360267639
train gradient:  0.49643477842446315
iteration : 1721
train acc:  0.8046875
train loss:  0.4476984739303589
train gradient:  0.40358168168847414
iteration : 1722
train acc:  0.796875
train loss:  0.4545295834541321
train gradient:  0.4228357763424556
iteration : 1723
train acc:  0.8046875
train loss:  0.45448631048202515
train gradient:  0.31104085159722594
iteration : 1724
train acc:  0.8125
train loss:  0.41140758991241455
train gradient:  0.40410152255997595
iteration : 1725
train acc:  0.7890625
train loss:  0.3943261206150055
train gradient:  0.3301374856179246
iteration : 1726
train acc:  0.8125
train loss:  0.3842979371547699
train gradient:  0.30961607411436765
iteration : 1727
train acc:  0.7734375
train loss:  0.47461366653442383
train gradient:  0.5164507532268516
iteration : 1728
train acc:  0.875
train loss:  0.3085271716117859
train gradient:  0.22057855522516107
iteration : 1729
train acc:  0.8046875
train loss:  0.3696975111961365
train gradient:  0.3623667176500475
iteration : 1730
train acc:  0.84375
train loss:  0.3646133542060852
train gradient:  0.3039871378051052
iteration : 1731
train acc:  0.7265625
train loss:  0.46431347727775574
train gradient:  0.4499291375260273
iteration : 1732
train acc:  0.8359375
train loss:  0.3619346618652344
train gradient:  0.35258475939647443
iteration : 1733
train acc:  0.8203125
train loss:  0.3778083920478821
train gradient:  0.348603442377431
iteration : 1734
train acc:  0.8203125
train loss:  0.3889935612678528
train gradient:  0.3547793154234007
iteration : 1735
train acc:  0.765625
train loss:  0.4564434885978699
train gradient:  0.46597784984032914
iteration : 1736
train acc:  0.8203125
train loss:  0.44585543870925903
train gradient:  0.3831762299361732
iteration : 1737
train acc:  0.875
train loss:  0.33654847741127014
train gradient:  0.3466053875629698
iteration : 1738
train acc:  0.8515625
train loss:  0.36112314462661743
train gradient:  0.33422355246010066
iteration : 1739
train acc:  0.8046875
train loss:  0.40396684408187866
train gradient:  0.40390265267299524
iteration : 1740
train acc:  0.828125
train loss:  0.39659589529037476
train gradient:  0.3852529924127999
iteration : 1741
train acc:  0.796875
train loss:  0.40993621945381165
train gradient:  0.35711390180449426
iteration : 1742
train acc:  0.828125
train loss:  0.37863287329673767
train gradient:  0.4247886480035081
iteration : 1743
train acc:  0.8203125
train loss:  0.40975624322891235
train gradient:  0.4789968893392503
iteration : 1744
train acc:  0.8046875
train loss:  0.41781213879585266
train gradient:  0.409255376247599
iteration : 1745
train acc:  0.8359375
train loss:  0.4519857168197632
train gradient:  0.5278698216131277
iteration : 1746
train acc:  0.8046875
train loss:  0.41983139514923096
train gradient:  0.3644899964535856
iteration : 1747
train acc:  0.8828125
train loss:  0.3755873441696167
train gradient:  0.3933422779264049
iteration : 1748
train acc:  0.8125
train loss:  0.42396217584609985
train gradient:  0.47633710873349355
iteration : 1749
train acc:  0.859375
train loss:  0.3364112675189972
train gradient:  0.33902494238006187
iteration : 1750
train acc:  0.8359375
train loss:  0.3975469470024109
train gradient:  0.6264085168096819
iteration : 1751
train acc:  0.796875
train loss:  0.40358424186706543
train gradient:  0.329648472088889
iteration : 1752
train acc:  0.7734375
train loss:  0.47026655077934265
train gradient:  0.5257458987185512
iteration : 1753
train acc:  0.859375
train loss:  0.3724062442779541
train gradient:  0.2888075626382002
iteration : 1754
train acc:  0.828125
train loss:  0.3975934386253357
train gradient:  0.4756580127221243
iteration : 1755
train acc:  0.8359375
train loss:  0.37922582030296326
train gradient:  0.4547526787507202
iteration : 1756
train acc:  0.796875
train loss:  0.4548228979110718
train gradient:  0.5116365220856973
iteration : 1757
train acc:  0.84375
train loss:  0.34418636560440063
train gradient:  0.3980951868113885
iteration : 1758
train acc:  0.796875
train loss:  0.4365437626838684
train gradient:  0.6174269566823924
iteration : 1759
train acc:  0.796875
train loss:  0.4181840419769287
train gradient:  0.43443637337938523
iteration : 1760
train acc:  0.7578125
train loss:  0.4527258276939392
train gradient:  0.6357328744958376
iteration : 1761
train acc:  0.8125
train loss:  0.3811744749546051
train gradient:  0.5963344051238206
iteration : 1762
train acc:  0.765625
train loss:  0.45723581314086914
train gradient:  0.5618705070529073
iteration : 1763
train acc:  0.84375
train loss:  0.3722107708454132
train gradient:  0.30102755292942457
iteration : 1764
train acc:  0.796875
train loss:  0.4343467056751251
train gradient:  0.5803967857712495
iteration : 1765
train acc:  0.78125
train loss:  0.39287269115448
train gradient:  0.3611740523178892
iteration : 1766
train acc:  0.796875
train loss:  0.4295034408569336
train gradient:  0.5827765758206938
iteration : 1767
train acc:  0.796875
train loss:  0.41825881600379944
train gradient:  0.4400299734237414
iteration : 1768
train acc:  0.78125
train loss:  0.4235731363296509
train gradient:  0.5341111681319198
iteration : 1769
train acc:  0.796875
train loss:  0.4729762077331543
train gradient:  0.4816724104662739
iteration : 1770
train acc:  0.828125
train loss:  0.36712223291397095
train gradient:  0.3239006690803449
iteration : 1771
train acc:  0.8125
train loss:  0.45200851559638977
train gradient:  0.44141640689799405
iteration : 1772
train acc:  0.8359375
train loss:  0.35323771834373474
train gradient:  0.2789513911626032
iteration : 1773
train acc:  0.796875
train loss:  0.42541995644569397
train gradient:  0.39076272763774245
iteration : 1774
train acc:  0.7890625
train loss:  0.4447709918022156
train gradient:  0.4541860703406369
iteration : 1775
train acc:  0.8046875
train loss:  0.48370206356048584
train gradient:  0.4355362223629936
iteration : 1776
train acc:  0.859375
train loss:  0.36365216970443726
train gradient:  0.30551497307643244
iteration : 1777
train acc:  0.859375
train loss:  0.3645980656147003
train gradient:  0.3661159377886351
iteration : 1778
train acc:  0.8125
train loss:  0.3957411050796509
train gradient:  0.37202312171445545
iteration : 1779
train acc:  0.8046875
train loss:  0.38583052158355713
train gradient:  0.3778697500159041
iteration : 1780
train acc:  0.78125
train loss:  0.44854336977005005
train gradient:  0.611783639837188
iteration : 1781
train acc:  0.8203125
train loss:  0.42052730917930603
train gradient:  0.6202656876815652
iteration : 1782
train acc:  0.828125
train loss:  0.539705753326416
train gradient:  0.6215469244848457
iteration : 1783
train acc:  0.734375
train loss:  0.5024983882904053
train gradient:  0.5801278485896718
iteration : 1784
train acc:  0.7578125
train loss:  0.49572116136550903
train gradient:  0.440145595839932
iteration : 1785
train acc:  0.8125
train loss:  0.35895952582359314
train gradient:  0.3240597488699755
iteration : 1786
train acc:  0.796875
train loss:  0.4068693220615387
train gradient:  0.3738471468379788
iteration : 1787
train acc:  0.8359375
train loss:  0.4649289548397064
train gradient:  0.46846422029804713
iteration : 1788
train acc:  0.75
train loss:  0.48971086740493774
train gradient:  0.6217987981197357
iteration : 1789
train acc:  0.6796875
train loss:  0.5311914682388306
train gradient:  0.5745573428578781
iteration : 1790
train acc:  0.75
train loss:  0.4958909749984741
train gradient:  0.8220518849102822
iteration : 1791
train acc:  0.78125
train loss:  0.44701799750328064
train gradient:  0.5102360121511864
iteration : 1792
train acc:  0.8359375
train loss:  0.3915671110153198
train gradient:  0.33113037665636463
iteration : 1793
train acc:  0.78125
train loss:  0.4227648377418518
train gradient:  0.36783309747481635
iteration : 1794
train acc:  0.796875
train loss:  0.4689185321331024
train gradient:  0.4653817731971777
iteration : 1795
train acc:  0.84375
train loss:  0.3624040186405182
train gradient:  0.3324992569178923
iteration : 1796
train acc:  0.765625
train loss:  0.416321724653244
train gradient:  0.41024495823215545
iteration : 1797
train acc:  0.765625
train loss:  0.46450501680374146
train gradient:  0.36115873791795267
iteration : 1798
train acc:  0.8515625
train loss:  0.4677552878856659
train gradient:  0.5430504178769738
iteration : 1799
train acc:  0.859375
train loss:  0.360343873500824
train gradient:  0.6897931788617111
iteration : 1800
train acc:  0.75
train loss:  0.4604933261871338
train gradient:  0.5096721493657801
iteration : 1801
train acc:  0.8359375
train loss:  0.38786059617996216
train gradient:  0.3997017744291894
iteration : 1802
train acc:  0.7734375
train loss:  0.44184455275535583
train gradient:  0.43698107092361554
iteration : 1803
train acc:  0.8046875
train loss:  0.43272149562835693
train gradient:  0.46580018980607024
iteration : 1804
train acc:  0.765625
train loss:  0.45080119371414185
train gradient:  0.5102363555043445
iteration : 1805
train acc:  0.8203125
train loss:  0.4082403779029846
train gradient:  0.4534476893619057
iteration : 1806
train acc:  0.8125
train loss:  0.37806618213653564
train gradient:  0.3490718595646136
iteration : 1807
train acc:  0.7890625
train loss:  0.39173680543899536
train gradient:  0.3821105926192917
iteration : 1808
train acc:  0.84375
train loss:  0.35981035232543945
train gradient:  0.34049398397629904
iteration : 1809
train acc:  0.7734375
train loss:  0.43109363317489624
train gradient:  0.3772985393126071
iteration : 1810
train acc:  0.7734375
train loss:  0.4752870202064514
train gradient:  0.5148185771403841
iteration : 1811
train acc:  0.7421875
train loss:  0.5056965351104736
train gradient:  0.6259413525476782
iteration : 1812
train acc:  0.8046875
train loss:  0.40272057056427
train gradient:  0.505444226856795
iteration : 1813
train acc:  0.8359375
train loss:  0.38134121894836426
train gradient:  0.38474359950764
iteration : 1814
train acc:  0.9140625
train loss:  0.3024158477783203
train gradient:  0.20097979544204345
iteration : 1815
train acc:  0.7890625
train loss:  0.43686842918395996
train gradient:  0.40927746798533526
iteration : 1816
train acc:  0.828125
train loss:  0.36492154002189636
train gradient:  0.2835565910182876
iteration : 1817
train acc:  0.7890625
train loss:  0.3567849397659302
train gradient:  0.2903490815714384
iteration : 1818
train acc:  0.8203125
train loss:  0.39926502108573914
train gradient:  0.39055639540174886
iteration : 1819
train acc:  0.8359375
train loss:  0.3901638090610504
train gradient:  0.29087673255351887
iteration : 1820
train acc:  0.8046875
train loss:  0.38291722536087036
train gradient:  0.4414845096866663
iteration : 1821
train acc:  0.7890625
train loss:  0.39997535943984985
train gradient:  0.3978874363896533
iteration : 1822
train acc:  0.859375
train loss:  0.37329360842704773
train gradient:  0.3053855476235404
iteration : 1823
train acc:  0.7421875
train loss:  0.49577319622039795
train gradient:  0.598631192219931
iteration : 1824
train acc:  0.8359375
train loss:  0.401588499546051
train gradient:  0.5285292479274258
iteration : 1825
train acc:  0.8125
train loss:  0.3575993478298187
train gradient:  0.3550278409403098
iteration : 1826
train acc:  0.828125
train loss:  0.3934738039970398
train gradient:  0.5130580435919856
iteration : 1827
train acc:  0.8515625
train loss:  0.36165618896484375
train gradient:  0.30847582664984524
iteration : 1828
train acc:  0.8203125
train loss:  0.4062250256538391
train gradient:  0.4422522806755137
iteration : 1829
train acc:  0.84375
train loss:  0.32762861251831055
train gradient:  0.3611181453699596
iteration : 1830
train acc:  0.78125
train loss:  0.41327548027038574
train gradient:  0.5055499864269584
iteration : 1831
train acc:  0.8515625
train loss:  0.4324115812778473
train gradient:  0.326539419407885
iteration : 1832
train acc:  0.796875
train loss:  0.4360924959182739
train gradient:  0.34812428777335275
iteration : 1833
train acc:  0.84375
train loss:  0.3871488571166992
train gradient:  0.35383211418205296
iteration : 1834
train acc:  0.8359375
train loss:  0.34512361884117126
train gradient:  0.2915445009700471
iteration : 1835
train acc:  0.78125
train loss:  0.44614899158477783
train gradient:  0.40368137973753704
iteration : 1836
train acc:  0.7890625
train loss:  0.4157649874687195
train gradient:  0.5111993973980635
iteration : 1837
train acc:  0.859375
train loss:  0.31575918197631836
train gradient:  0.260824596692022
iteration : 1838
train acc:  0.8125
train loss:  0.3994333744049072
train gradient:  0.3898697481723712
iteration : 1839
train acc:  0.828125
train loss:  0.41215258836746216
train gradient:  0.3979823891024141
iteration : 1840
train acc:  0.859375
train loss:  0.4262631833553314
train gradient:  0.4726559394816957
iteration : 1841
train acc:  0.8125
train loss:  0.43473726511001587
train gradient:  0.41990505145357565
iteration : 1842
train acc:  0.84375
train loss:  0.3363749384880066
train gradient:  0.2929855786163698
iteration : 1843
train acc:  0.859375
train loss:  0.3655069172382355
train gradient:  0.36694461185226157
iteration : 1844
train acc:  0.796875
train loss:  0.41410893201828003
train gradient:  0.49479209278924846
iteration : 1845
train acc:  0.8671875
train loss:  0.3999757766723633
train gradient:  0.49494979836687913
iteration : 1846
train acc:  0.875
train loss:  0.3410138487815857
train gradient:  0.4211918684955363
iteration : 1847
train acc:  0.8046875
train loss:  0.42794761061668396
train gradient:  0.6721576176475577
iteration : 1848
train acc:  0.9453125
train loss:  0.275071918964386
train gradient:  0.24975630418975647
iteration : 1849
train acc:  0.7109375
train loss:  0.5232253074645996
train gradient:  0.5891613170527441
iteration : 1850
train acc:  0.84375
train loss:  0.3719954788684845
train gradient:  0.3494491658371681
iteration : 1851
train acc:  0.796875
train loss:  0.41260895133018494
train gradient:  0.5648861239937699
iteration : 1852
train acc:  0.765625
train loss:  0.48061996698379517
train gradient:  0.5455987066997116
iteration : 1853
train acc:  0.7734375
train loss:  0.44424861669540405
train gradient:  0.5078900866300737
iteration : 1854
train acc:  0.8125
train loss:  0.45001232624053955
train gradient:  0.43641064480055974
iteration : 1855
train acc:  0.828125
train loss:  0.3901534676551819
train gradient:  0.3609483930528557
iteration : 1856
train acc:  0.8203125
train loss:  0.3885306119918823
train gradient:  0.38792002490418326
iteration : 1857
train acc:  0.7890625
train loss:  0.4115362763404846
train gradient:  0.41858258910732027
iteration : 1858
train acc:  0.78125
train loss:  0.4617495834827423
train gradient:  0.6354126788465086
iteration : 1859
train acc:  0.8203125
train loss:  0.4027861952781677
train gradient:  0.4330100111630424
iteration : 1860
train acc:  0.78125
train loss:  0.38075289130210876
train gradient:  0.49629800692259135
iteration : 1861
train acc:  0.8515625
train loss:  0.3940269947052002
train gradient:  0.3681550867974077
iteration : 1862
train acc:  0.828125
train loss:  0.38324183225631714
train gradient:  0.44161501466937214
iteration : 1863
train acc:  0.875
train loss:  0.36828696727752686
train gradient:  0.3785187955197703
iteration : 1864
train acc:  0.8125
train loss:  0.3542664051055908
train gradient:  0.35714097649168824
iteration : 1865
train acc:  0.75
train loss:  0.5280419588088989
train gradient:  0.6795915310686166
iteration : 1866
train acc:  0.796875
train loss:  0.4567996859550476
train gradient:  0.5318359415881697
iteration : 1867
train acc:  0.859375
train loss:  0.3834196627140045
train gradient:  0.3956607535559027
iteration : 1868
train acc:  0.8125
train loss:  0.41092202067375183
train gradient:  0.3686635965407643
iteration : 1869
train acc:  0.828125
train loss:  0.38386911153793335
train gradient:  0.3088328993782562
iteration : 1870
train acc:  0.8203125
train loss:  0.3916175663471222
train gradient:  0.38273286859677247
iteration : 1871
train acc:  0.8046875
train loss:  0.3987266719341278
train gradient:  0.5587558907359591
iteration : 1872
train acc:  0.8046875
train loss:  0.43108275532722473
train gradient:  0.528777102028184
iteration : 1873
train acc:  0.828125
train loss:  0.35098886489868164
train gradient:  0.3255685286433649
iteration : 1874
train acc:  0.84375
train loss:  0.34760111570358276
train gradient:  0.3566367468766266
iteration : 1875
train acc:  0.8359375
train loss:  0.3777512013912201
train gradient:  0.39761075948643154
iteration : 1876
train acc:  0.8203125
train loss:  0.39115971326828003
train gradient:  0.3635567656100214
iteration : 1877
train acc:  0.78125
train loss:  0.492198646068573
train gradient:  0.5973158819758222
iteration : 1878
train acc:  0.8671875
train loss:  0.3267917335033417
train gradient:  1.2253852415789073
iteration : 1879
train acc:  0.8203125
train loss:  0.37465810775756836
train gradient:  0.31274282329466896
iteration : 1880
train acc:  0.78125
train loss:  0.43583396077156067
train gradient:  0.48336762899822416
iteration : 1881
train acc:  0.7890625
train loss:  0.4763849675655365
train gradient:  0.5815970662104408
iteration : 1882
train acc:  0.75
train loss:  0.4603814482688904
train gradient:  0.5314404194107986
iteration : 1883
train acc:  0.78125
train loss:  0.4303717613220215
train gradient:  0.4795456567739827
iteration : 1884
train acc:  0.7734375
train loss:  0.3797866106033325
train gradient:  0.4361494207355063
iteration : 1885
train acc:  0.796875
train loss:  0.4757784307003021
train gradient:  0.6301145271588795
iteration : 1886
train acc:  0.7890625
train loss:  0.40399229526519775
train gradient:  0.3987705357194542
iteration : 1887
train acc:  0.828125
train loss:  0.3616272807121277
train gradient:  0.2936787182106272
iteration : 1888
train acc:  0.7734375
train loss:  0.40226608514785767
train gradient:  0.4255333333388967
iteration : 1889
train acc:  0.7578125
train loss:  0.4078572392463684
train gradient:  0.41332631091963645
iteration : 1890
train acc:  0.8515625
train loss:  0.377360999584198
train gradient:  0.3339730420928006
iteration : 1891
train acc:  0.859375
train loss:  0.38807350397109985
train gradient:  0.41202381377399744
iteration : 1892
train acc:  0.7890625
train loss:  0.4341650903224945
train gradient:  0.4553722866232696
iteration : 1893
train acc:  0.8046875
train loss:  0.4458100199699402
train gradient:  0.5543503064352394
iteration : 1894
train acc:  0.8046875
train loss:  0.4854589104652405
train gradient:  0.5711145177959717
iteration : 1895
train acc:  0.828125
train loss:  0.4484086036682129
train gradient:  0.5109777362219983
iteration : 1896
train acc:  0.8125
train loss:  0.4298965334892273
train gradient:  0.390691608552227
iteration : 1897
train acc:  0.796875
train loss:  0.4548938274383545
train gradient:  0.35669287700727126
iteration : 1898
train acc:  0.8203125
train loss:  0.48974087834358215
train gradient:  0.4403574820677268
iteration : 1899
train acc:  0.7890625
train loss:  0.38987845182418823
train gradient:  0.4732985974737249
iteration : 1900
train acc:  0.8125
train loss:  0.4168124496936798
train gradient:  0.42757561920027004
iteration : 1901
train acc:  0.7890625
train loss:  0.4252209961414337
train gradient:  0.6506160643364727
iteration : 1902
train acc:  0.796875
train loss:  0.4165644347667694
train gradient:  0.4037940276352896
iteration : 1903
train acc:  0.8359375
train loss:  0.42628389596939087
train gradient:  0.42317924670744633
iteration : 1904
train acc:  0.8515625
train loss:  0.35320818424224854
train gradient:  0.31879652132674563
iteration : 1905
train acc:  0.8359375
train loss:  0.37613868713378906
train gradient:  0.3889365460976113
iteration : 1906
train acc:  0.859375
train loss:  0.34527960419654846
train gradient:  0.2953690985511289
iteration : 1907
train acc:  0.78125
train loss:  0.40893805027008057
train gradient:  0.5211476516012273
iteration : 1908
train acc:  0.796875
train loss:  0.46591097116470337
train gradient:  0.5864343585030469
iteration : 1909
train acc:  0.8359375
train loss:  0.3996150493621826
train gradient:  0.3987972518640861
iteration : 1910
train acc:  0.796875
train loss:  0.44139397144317627
train gradient:  0.40314481402552993
iteration : 1911
train acc:  0.8359375
train loss:  0.41858747601509094
train gradient:  0.4942003986186267
iteration : 1912
train acc:  0.8203125
train loss:  0.4033616781234741
train gradient:  0.33251536430621886
iteration : 1913
train acc:  0.765625
train loss:  0.42585843801498413
train gradient:  0.30716572568951417
iteration : 1914
train acc:  0.8359375
train loss:  0.39241480827331543
train gradient:  0.40804969480457765
iteration : 1915
train acc:  0.8203125
train loss:  0.3845728039741516
train gradient:  0.2865758773134081
iteration : 1916
train acc:  0.8359375
train loss:  0.3477376103401184
train gradient:  0.33518828419297464
iteration : 1917
train acc:  0.7890625
train loss:  0.4519375264644623
train gradient:  0.30831029664302323
iteration : 1918
train acc:  0.8125
train loss:  0.4177315831184387
train gradient:  0.34282961507756443
iteration : 1919
train acc:  0.78125
train loss:  0.474166601896286
train gradient:  0.4387413612613028
iteration : 1920
train acc:  0.796875
train loss:  0.47997915744781494
train gradient:  0.47931570899616877
iteration : 1921
train acc:  0.796875
train loss:  0.41320231556892395
train gradient:  0.4420754751437763
iteration : 1922
train acc:  0.8125
train loss:  0.3861140310764313
train gradient:  0.30825745663021376
iteration : 1923
train acc:  0.828125
train loss:  0.3289058804512024
train gradient:  0.3085123951711232
iteration : 1924
train acc:  0.796875
train loss:  0.4888656437397003
train gradient:  0.397155553983374
iteration : 1925
train acc:  0.7890625
train loss:  0.4293941557407379
train gradient:  0.34688540987430566
iteration : 1926
train acc:  0.7421875
train loss:  0.48245370388031006
train gradient:  0.45355443379585425
iteration : 1927
train acc:  0.828125
train loss:  0.4023555815219879
train gradient:  0.29344778276596123
iteration : 1928
train acc:  0.8125
train loss:  0.39920634031295776
train gradient:  0.3439127642478355
iteration : 1929
train acc:  0.8671875
train loss:  0.33498069643974304
train gradient:  0.22893439947648628
iteration : 1930
train acc:  0.796875
train loss:  0.40952789783477783
train gradient:  0.41649345611473954
iteration : 1931
train acc:  0.84375
train loss:  0.3502357602119446
train gradient:  0.2973126769398815
iteration : 1932
train acc:  0.828125
train loss:  0.3746219873428345
train gradient:  0.34615331930453835
iteration : 1933
train acc:  0.90625
train loss:  0.29884254932403564
train gradient:  0.30191744197071657
iteration : 1934
train acc:  0.8359375
train loss:  0.3967519998550415
train gradient:  0.38022322388172997
iteration : 1935
train acc:  0.8046875
train loss:  0.3983645439147949
train gradient:  0.36120771432502785
iteration : 1936
train acc:  0.765625
train loss:  0.4708501696586609
train gradient:  0.5448248999624836
iteration : 1937
train acc:  0.796875
train loss:  0.3914062976837158
train gradient:  0.3696328949725384
iteration : 1938
train acc:  0.8125
train loss:  0.41422075033187866
train gradient:  0.4786194710225099
iteration : 1939
train acc:  0.828125
train loss:  0.4336327910423279
train gradient:  0.44606241167864563
iteration : 1940
train acc:  0.8203125
train loss:  0.3838025629520416
train gradient:  0.45159382291534184
iteration : 1941
train acc:  0.8203125
train loss:  0.4299379587173462
train gradient:  0.44283252371542153
iteration : 1942
train acc:  0.796875
train loss:  0.3834301233291626
train gradient:  0.34012929447071466
iteration : 1943
train acc:  0.796875
train loss:  0.4229623079299927
train gradient:  0.3883911830441939
iteration : 1944
train acc:  0.8359375
train loss:  0.36462706327438354
train gradient:  0.2897484463218222
iteration : 1945
train acc:  0.828125
train loss:  0.37618333101272583
train gradient:  0.2608220755978034
iteration : 1946
train acc:  0.859375
train loss:  0.32524359226226807
train gradient:  0.21515257945706706
iteration : 1947
train acc:  0.8203125
train loss:  0.40048813819885254
train gradient:  0.3034535092923702
iteration : 1948
train acc:  0.8125
train loss:  0.4520610272884369
train gradient:  0.4278705759222179
iteration : 1949
train acc:  0.7578125
train loss:  0.4800495207309723
train gradient:  0.5627856260967452
iteration : 1950
train acc:  0.796875
train loss:  0.42451536655426025
train gradient:  0.35349862508877605
iteration : 1951
train acc:  0.8125
train loss:  0.4475400149822235
train gradient:  0.46255054295465825
iteration : 1952
train acc:  0.8125
train loss:  0.37528741359710693
train gradient:  0.3113761244395192
iteration : 1953
train acc:  0.8203125
train loss:  0.39031243324279785
train gradient:  0.3823860685471108
iteration : 1954
train acc:  0.8203125
train loss:  0.36769652366638184
train gradient:  0.4494756656328459
iteration : 1955
train acc:  0.8359375
train loss:  0.40764498710632324
train gradient:  0.43576738697703954
iteration : 1956
train acc:  0.8046875
train loss:  0.3654136061668396
train gradient:  0.4015805083566266
iteration : 1957
train acc:  0.8125
train loss:  0.3855563700199127
train gradient:  0.30005749570806256
iteration : 1958
train acc:  0.8359375
train loss:  0.37935885787010193
train gradient:  0.4145173738861748
iteration : 1959
train acc:  0.8515625
train loss:  0.4097972810268402
train gradient:  0.3280123077500126
iteration : 1960
train acc:  0.8515625
train loss:  0.40177154541015625
train gradient:  0.28681088649979736
iteration : 1961
train acc:  0.84375
train loss:  0.38181060552597046
train gradient:  0.3754551872922934
iteration : 1962
train acc:  0.8125
train loss:  0.43163859844207764
train gradient:  0.4775200575261644
iteration : 1963
train acc:  0.8515625
train loss:  0.3558821380138397
train gradient:  0.2733932791866776
iteration : 1964
train acc:  0.8203125
train loss:  0.3816326856613159
train gradient:  0.4071095720272188
iteration : 1965
train acc:  0.8359375
train loss:  0.3617958426475525
train gradient:  0.35452745931897833
iteration : 1966
train acc:  0.828125
train loss:  0.38526850938796997
train gradient:  0.3255823481622892
iteration : 1967
train acc:  0.8359375
train loss:  0.354041188955307
train gradient:  0.21190468201623203
iteration : 1968
train acc:  0.828125
train loss:  0.4672088027000427
train gradient:  0.45082739618250817
iteration : 1969
train acc:  0.8046875
train loss:  0.4991317689418793
train gradient:  0.6422588042876682
iteration : 1970
train acc:  0.7578125
train loss:  0.46616870164871216
train gradient:  0.455736762136953
iteration : 1971
train acc:  0.796875
train loss:  0.435349702835083
train gradient:  0.42716170777057616
iteration : 1972
train acc:  0.78125
train loss:  0.41222327947616577
train gradient:  0.46224696844735524
iteration : 1973
train acc:  0.8203125
train loss:  0.3970097601413727
train gradient:  0.3740883718776049
iteration : 1974
train acc:  0.7890625
train loss:  0.41565361618995667
train gradient:  0.35170561335472605
iteration : 1975
train acc:  0.875
train loss:  0.3696877360343933
train gradient:  0.3165190817382237
iteration : 1976
train acc:  0.8203125
train loss:  0.4030904173851013
train gradient:  0.3522818430425459
iteration : 1977
train acc:  0.828125
train loss:  0.38546082377433777
train gradient:  0.2809861937561971
iteration : 1978
train acc:  0.8359375
train loss:  0.3488083481788635
train gradient:  0.25991620347010935
iteration : 1979
train acc:  0.7890625
train loss:  0.48532068729400635
train gradient:  0.4122510144086764
iteration : 1980
train acc:  0.828125
train loss:  0.4208952784538269
train gradient:  0.36714833102173
iteration : 1981
train acc:  0.8046875
train loss:  0.40891069173812866
train gradient:  0.3982615342052001
iteration : 1982
train acc:  0.78125
train loss:  0.44836288690567017
train gradient:  0.39418412422299387
iteration : 1983
train acc:  0.8671875
train loss:  0.34448736906051636
train gradient:  0.37221321945207736
iteration : 1984
train acc:  0.84375
train loss:  0.34337177872657776
train gradient:  0.25878816106920993
iteration : 1985
train acc:  0.8515625
train loss:  0.43901413679122925
train gradient:  0.5646518303747
iteration : 1986
train acc:  0.8203125
train loss:  0.4478478729724884
train gradient:  0.37066619918805815
iteration : 1987
train acc:  0.78125
train loss:  0.502705454826355
train gradient:  0.6858095484838512
iteration : 1988
train acc:  0.8515625
train loss:  0.3725239336490631
train gradient:  0.49988103545421975
iteration : 1989
train acc:  0.8046875
train loss:  0.4015815258026123
train gradient:  0.4898009692376536
iteration : 1990
train acc:  0.8359375
train loss:  0.3542487323284149
train gradient:  0.29093157442374445
iteration : 1991
train acc:  0.8359375
train loss:  0.4175247550010681
train gradient:  0.5023093222811337
iteration : 1992
train acc:  0.8046875
train loss:  0.4241197109222412
train gradient:  0.6204678895383193
iteration : 1993
train acc:  0.765625
train loss:  0.4329664707183838
train gradient:  0.47910999222339434
iteration : 1994
train acc:  0.8125
train loss:  0.3917374312877655
train gradient:  0.40356914269745753
iteration : 1995
train acc:  0.8828125
train loss:  0.3190762400627136
train gradient:  0.2816412797041374
iteration : 1996
train acc:  0.796875
train loss:  0.4221407473087311
train gradient:  0.471389857387719
iteration : 1997
train acc:  0.7890625
train loss:  0.4400672912597656
train gradient:  0.6182513297005487
iteration : 1998
train acc:  0.78125
train loss:  0.46538156270980835
train gradient:  0.47741133551872106
iteration : 1999
train acc:  0.8515625
train loss:  0.3569290041923523
train gradient:  0.4032204881708147
iteration : 2000
train acc:  0.7734375
train loss:  0.4765881896018982
train gradient:  0.46735533205094604
iteration : 2001
train acc:  0.7890625
train loss:  0.3803524076938629
train gradient:  0.4009740978945635
iteration : 2002
train acc:  0.8671875
train loss:  0.3574700355529785
train gradient:  0.37650742121340386
iteration : 2003
train acc:  0.859375
train loss:  0.36011701822280884
train gradient:  0.27608320881079224
iteration : 2004
train acc:  0.828125
train loss:  0.3972504138946533
train gradient:  0.3718312957423146
iteration : 2005
train acc:  0.8203125
train loss:  0.35338377952575684
train gradient:  0.30725736446409607
iteration : 2006
train acc:  0.796875
train loss:  0.5325683355331421
train gradient:  0.5860427019555703
iteration : 2007
train acc:  0.796875
train loss:  0.4071402847766876
train gradient:  0.47626852749382065
iteration : 2008
train acc:  0.8046875
train loss:  0.39258626103401184
train gradient:  0.44714252241475433
iteration : 2009
train acc:  0.828125
train loss:  0.3635002672672272
train gradient:  0.4162434846172846
iteration : 2010
train acc:  0.8125
train loss:  0.3674520254135132
train gradient:  0.27005407990624136
iteration : 2011
train acc:  0.8046875
train loss:  0.3974916338920593
train gradient:  0.26732417762085325
iteration : 2012
train acc:  0.828125
train loss:  0.42096641659736633
train gradient:  0.30430259426362866
iteration : 2013
train acc:  0.8359375
train loss:  0.42221787571907043
train gradient:  0.3369106122058632
iteration : 2014
train acc:  0.8359375
train loss:  0.39572224020957947
train gradient:  0.3761154374652015
iteration : 2015
train acc:  0.828125
train loss:  0.4163854420185089
train gradient:  0.38148151075086645
iteration : 2016
train acc:  0.8125
train loss:  0.4205681383609772
train gradient:  0.36034924753411796
iteration : 2017
train acc:  0.8515625
train loss:  0.42281973361968994
train gradient:  0.5077981125355744
iteration : 2018
train acc:  0.8359375
train loss:  0.3759123682975769
train gradient:  0.2944505022759411
iteration : 2019
train acc:  0.7734375
train loss:  0.46482691168785095
train gradient:  0.44622138754581275
iteration : 2020
train acc:  0.8203125
train loss:  0.38472896814346313
train gradient:  0.3540092095512682
iteration : 2021
train acc:  0.8515625
train loss:  0.3458069860935211
train gradient:  0.3178559088431164
iteration : 2022
train acc:  0.78125
train loss:  0.3892195224761963
train gradient:  0.36242336720835455
iteration : 2023
train acc:  0.7578125
train loss:  0.4582233130931854
train gradient:  0.5080105088566036
iteration : 2024
train acc:  0.8671875
train loss:  0.3231223225593567
train gradient:  0.2510587489357515
iteration : 2025
train acc:  0.828125
train loss:  0.4308677911758423
train gradient:  0.3781144033690808
iteration : 2026
train acc:  0.8515625
train loss:  0.34559065103530884
train gradient:  0.3427408615464311
iteration : 2027
train acc:  0.8359375
train loss:  0.35648366808891296
train gradient:  0.39619003705126693
iteration : 2028
train acc:  0.875
train loss:  0.31339094042778015
train gradient:  0.26981799864328593
iteration : 2029
train acc:  0.84375
train loss:  0.37934863567352295
train gradient:  0.38060106462768595
iteration : 2030
train acc:  0.7578125
train loss:  0.5371596217155457
train gradient:  0.6897077534333894
iteration : 2031
train acc:  0.78125
train loss:  0.374237596988678
train gradient:  0.3574171050503959
iteration : 2032
train acc:  0.8671875
train loss:  0.33187127113342285
train gradient:  0.335142978453317
iteration : 2033
train acc:  0.8046875
train loss:  0.4272274374961853
train gradient:  0.3119629478037202
iteration : 2034
train acc:  0.8046875
train loss:  0.4191349148750305
train gradient:  0.40057554883300245
iteration : 2035
train acc:  0.8125
train loss:  0.3899587392807007
train gradient:  0.37815644032960977
iteration : 2036
train acc:  0.8125
train loss:  0.4315906763076782
train gradient:  0.31721037986271733
iteration : 2037
train acc:  0.8515625
train loss:  0.3452192544937134
train gradient:  0.29574451771462607
iteration : 2038
train acc:  0.796875
train loss:  0.4021427631378174
train gradient:  0.3656346169768267
iteration : 2039
train acc:  0.84375
train loss:  0.4676918089389801
train gradient:  0.585231636914034
iteration : 2040
train acc:  0.84375
train loss:  0.33908963203430176
train gradient:  0.2689681116703354
iteration : 2041
train acc:  0.828125
train loss:  0.38976341485977173
train gradient:  0.47279078453491186
iteration : 2042
train acc:  0.78125
train loss:  0.39736491441726685
train gradient:  0.4327084686153285
iteration : 2043
train acc:  0.859375
train loss:  0.378301739692688
train gradient:  0.34737609203404013
iteration : 2044
train acc:  0.7890625
train loss:  0.49851319193840027
train gradient:  0.6535888314711444
iteration : 2045
train acc:  0.8046875
train loss:  0.4068986773490906
train gradient:  0.4246770658706375
iteration : 2046
train acc:  0.8828125
train loss:  0.34037089347839355
train gradient:  0.37592463137094334
iteration : 2047
train acc:  0.8203125
train loss:  0.40781015157699585
train gradient:  0.4528157050832906
iteration : 2048
train acc:  0.796875
train loss:  0.4299430847167969
train gradient:  0.5028974967816073
iteration : 2049
train acc:  0.78125
train loss:  0.5092223286628723
train gradient:  0.5693953155011433
iteration : 2050
train acc:  0.8203125
train loss:  0.4146445393562317
train gradient:  0.4588739245302787
iteration : 2051
train acc:  0.8125
train loss:  0.410799503326416
train gradient:  0.3399523985379915
iteration : 2052
train acc:  0.765625
train loss:  0.4888136684894562
train gradient:  0.5110189338544195
iteration : 2053
train acc:  0.8046875
train loss:  0.41656142473220825
train gradient:  0.35724317777540626
iteration : 2054
train acc:  0.8515625
train loss:  0.35965797305107117
train gradient:  0.2666210224194377
iteration : 2055
train acc:  0.8125
train loss:  0.3917457163333893
train gradient:  0.347558317244512
iteration : 2056
train acc:  0.796875
train loss:  0.37272611260414124
train gradient:  0.3850409037115885
iteration : 2057
train acc:  0.8125
train loss:  0.41406238079071045
train gradient:  0.36027066952953174
iteration : 2058
train acc:  0.890625
train loss:  0.33308786153793335
train gradient:  0.2864710216912933
iteration : 2059
train acc:  0.828125
train loss:  0.40746915340423584
train gradient:  0.310140568796516
iteration : 2060
train acc:  0.828125
train loss:  0.36403030157089233
train gradient:  0.31594093898733183
iteration : 2061
train acc:  0.765625
train loss:  0.4933902621269226
train gradient:  0.5091863426792498
iteration : 2062
train acc:  0.84375
train loss:  0.40345820784568787
train gradient:  0.45468739506424094
iteration : 2063
train acc:  0.78125
train loss:  0.4917317032814026
train gradient:  0.6576892975777326
iteration : 2064
train acc:  0.8515625
train loss:  0.348979651927948
train gradient:  0.3432274128128834
iteration : 2065
train acc:  0.7421875
train loss:  0.5021167993545532
train gradient:  0.6702733209241326
iteration : 2066
train acc:  0.7578125
train loss:  0.5603815317153931
train gradient:  0.5990041948606778
iteration : 2067
train acc:  0.84375
train loss:  0.3789569139480591
train gradient:  0.28047319949910166
iteration : 2068
train acc:  0.84375
train loss:  0.41072311997413635
train gradient:  0.34667985936163204
iteration : 2069
train acc:  0.84375
train loss:  0.4175085723400116
train gradient:  0.37166714775192194
iteration : 2070
train acc:  0.8046875
train loss:  0.43032845854759216
train gradient:  0.5027927297293848
iteration : 2071
train acc:  0.78125
train loss:  0.4593323767185211
train gradient:  0.41122602567505423
iteration : 2072
train acc:  0.78125
train loss:  0.42928749322891235
train gradient:  0.3635836672157355
iteration : 2073
train acc:  0.8046875
train loss:  0.4122345447540283
train gradient:  0.4694402981069628
iteration : 2074
train acc:  0.84375
train loss:  0.4136489927768707
train gradient:  0.4693875115487031
iteration : 2075
train acc:  0.8125
train loss:  0.3793635070323944
train gradient:  0.47056840443049913
iteration : 2076
train acc:  0.859375
train loss:  0.37082308530807495
train gradient:  0.2688019809790091
iteration : 2077
train acc:  0.796875
train loss:  0.46910494565963745
train gradient:  0.3985074989095412
iteration : 2078
train acc:  0.8203125
train loss:  0.3204387426376343
train gradient:  0.24411699295754127
iteration : 2079
train acc:  0.8359375
train loss:  0.4101298451423645
train gradient:  0.33183477086676405
iteration : 2080
train acc:  0.8046875
train loss:  0.39328908920288086
train gradient:  0.46758290329487684
iteration : 2081
train acc:  0.7734375
train loss:  0.38310471177101135
train gradient:  0.419909055900121
iteration : 2082
train acc:  0.84375
train loss:  0.3227270841598511
train gradient:  0.2269122779024508
iteration : 2083
train acc:  0.8125
train loss:  0.42616036534309387
train gradient:  0.3368164405083177
iteration : 2084
train acc:  0.859375
train loss:  0.37430092692375183
train gradient:  0.24572159769812885
iteration : 2085
train acc:  0.7890625
train loss:  0.4391392469406128
train gradient:  0.49508079631523405
iteration : 2086
train acc:  0.7421875
train loss:  0.5004376173019409
train gradient:  0.59473340281499
iteration : 2087
train acc:  0.859375
train loss:  0.36416304111480713
train gradient:  0.3233635264472801
iteration : 2088
train acc:  0.8046875
train loss:  0.4719140827655792
train gradient:  0.6255276931630673
iteration : 2089
train acc:  0.84375
train loss:  0.391114741563797
train gradient:  0.3517394979542099
iteration : 2090
train acc:  0.7578125
train loss:  0.41759419441223145
train gradient:  0.47689415692627524
iteration : 2091
train acc:  0.8125
train loss:  0.4012405276298523
train gradient:  0.2858036508769761
iteration : 2092
train acc:  0.7578125
train loss:  0.5106044411659241
train gradient:  0.4946153033660887
iteration : 2093
train acc:  0.84375
train loss:  0.3967877924442291
train gradient:  0.3034636406516659
iteration : 2094
train acc:  0.7265625
train loss:  0.49783262610435486
train gradient:  0.6500568694434972
iteration : 2095
train acc:  0.796875
train loss:  0.4279593229293823
train gradient:  0.4840977429356117
iteration : 2096
train acc:  0.796875
train loss:  0.37008023262023926
train gradient:  0.4081528887520122
iteration : 2097
train acc:  0.859375
train loss:  0.3787146806716919
train gradient:  0.2513358952038626
iteration : 2098
train acc:  0.8359375
train loss:  0.35897672176361084
train gradient:  0.33172246680874845
iteration : 2099
train acc:  0.8671875
train loss:  0.367167592048645
train gradient:  0.27966892815078337
iteration : 2100
train acc:  0.78125
train loss:  0.4367596507072449
train gradient:  0.45157427428731545
iteration : 2101
train acc:  0.765625
train loss:  0.4237781763076782
train gradient:  0.4637102508437022
iteration : 2102
train acc:  0.8671875
train loss:  0.3665509819984436
train gradient:  0.3596742765603046
iteration : 2103
train acc:  0.765625
train loss:  0.48016661405563354
train gradient:  0.4664959435768196
iteration : 2104
train acc:  0.84375
train loss:  0.30517855286598206
train gradient:  0.22183248251128945
iteration : 2105
train acc:  0.765625
train loss:  0.4880118668079376
train gradient:  0.5270351867272494
iteration : 2106
train acc:  0.8984375
train loss:  0.3062976896762848
train gradient:  0.24169136996413798
iteration : 2107
train acc:  0.8203125
train loss:  0.4281638264656067
train gradient:  0.5360643866510959
iteration : 2108
train acc:  0.8359375
train loss:  0.37164950370788574
train gradient:  0.34656738795185016
iteration : 2109
train acc:  0.7890625
train loss:  0.4747436046600342
train gradient:  0.6348448514267171
iteration : 2110
train acc:  0.8515625
train loss:  0.3616393506526947
train gradient:  0.3320171476418626
iteration : 2111
train acc:  0.8515625
train loss:  0.4001084566116333
train gradient:  0.3513932508151315
iteration : 2112
train acc:  0.890625
train loss:  0.3410525321960449
train gradient:  0.3358997800261668
iteration : 2113
train acc:  0.8671875
train loss:  0.3397761583328247
train gradient:  0.402454310215494
iteration : 2114
train acc:  0.828125
train loss:  0.39454957842826843
train gradient:  0.3215978657275441
iteration : 2115
train acc:  0.7734375
train loss:  0.43544843792915344
train gradient:  0.4603186629112565
iteration : 2116
train acc:  0.8046875
train loss:  0.4649357199668884
train gradient:  0.3517045718868627
iteration : 2117
train acc:  0.8359375
train loss:  0.36430567502975464
train gradient:  0.31012839735356007
iteration : 2118
train acc:  0.8046875
train loss:  0.38766127824783325
train gradient:  0.26439695535128993
iteration : 2119
train acc:  0.8671875
train loss:  0.3411012291908264
train gradient:  0.31672213705067526
iteration : 2120
train acc:  0.828125
train loss:  0.3676520884037018
train gradient:  0.30495129401388943
iteration : 2121
train acc:  0.875
train loss:  0.32401758432388306
train gradient:  0.18642456353929748
iteration : 2122
train acc:  0.7890625
train loss:  0.3953777551651001
train gradient:  0.4654414026604782
iteration : 2123
train acc:  0.8203125
train loss:  0.3441951870918274
train gradient:  0.27036086535270776
iteration : 2124
train acc:  0.78125
train loss:  0.4117421507835388
train gradient:  0.43941320916592064
iteration : 2125
train acc:  0.8671875
train loss:  0.27604544162750244
train gradient:  0.19534984042333256
iteration : 2126
train acc:  0.875
train loss:  0.35625261068344116
train gradient:  0.28818236337697056
iteration : 2127
train acc:  0.8125
train loss:  0.4343206286430359
train gradient:  0.35939547893860807
iteration : 2128
train acc:  0.8046875
train loss:  0.4343302845954895
train gradient:  0.39675205620277604
iteration : 2129
train acc:  0.8203125
train loss:  0.3813750147819519
train gradient:  0.3456698990601418
iteration : 2130
train acc:  0.7890625
train loss:  0.4546852707862854
train gradient:  0.40753391510122033
iteration : 2131
train acc:  0.859375
train loss:  0.32896625995635986
train gradient:  0.28625469602437253
iteration : 2132
train acc:  0.8203125
train loss:  0.4303291440010071
train gradient:  0.47132999520567503
iteration : 2133
train acc:  0.875
train loss:  0.31804358959198
train gradient:  0.20219503261330773
iteration : 2134
train acc:  0.8359375
train loss:  0.4036421477794647
train gradient:  0.3691330103761812
iteration : 2135
train acc:  0.765625
train loss:  0.49974608421325684
train gradient:  0.5684409574372038
iteration : 2136
train acc:  0.8125
train loss:  0.4350825548171997
train gradient:  0.4000429883087654
iteration : 2137
train acc:  0.7578125
train loss:  0.4601532220840454
train gradient:  0.39214091211809227
iteration : 2138
train acc:  0.765625
train loss:  0.4601539373397827
train gradient:  0.39063856617056675
iteration : 2139
train acc:  0.828125
train loss:  0.3798035681247711
train gradient:  0.2875371031076427
iteration : 2140
train acc:  0.8359375
train loss:  0.3653585910797119
train gradient:  0.35326259688006906
iteration : 2141
train acc:  0.828125
train loss:  0.44878634810447693
train gradient:  0.4317060892578078
iteration : 2142
train acc:  0.84375
train loss:  0.3913872539997101
train gradient:  0.33692525820900854
iteration : 2143
train acc:  0.8515625
train loss:  0.3223535418510437
train gradient:  0.36590426129255404
iteration : 2144
train acc:  0.8203125
train loss:  0.3525894284248352
train gradient:  0.3632043347670631
iteration : 2145
train acc:  0.84375
train loss:  0.40229034423828125
train gradient:  0.5430794328785812
iteration : 2146
train acc:  0.828125
train loss:  0.49008750915527344
train gradient:  0.5375955698771819
iteration : 2147
train acc:  0.8046875
train loss:  0.4004706144332886
train gradient:  0.3473205609482094
iteration : 2148
train acc:  0.8203125
train loss:  0.37052327394485474
train gradient:  0.35853394988112547
iteration : 2149
train acc:  0.7734375
train loss:  0.38219213485717773
train gradient:  0.3061607821892423
iteration : 2150
train acc:  0.78125
train loss:  0.47315776348114014
train gradient:  0.5030380099629685
iteration : 2151
train acc:  0.8203125
train loss:  0.4240695536136627
train gradient:  0.36599862736043376
iteration : 2152
train acc:  0.8125
train loss:  0.4220980405807495
train gradient:  0.41728060545658524
iteration : 2153
train acc:  0.8125
train loss:  0.4190870523452759
train gradient:  0.4881446060164827
iteration : 2154
train acc:  0.8515625
train loss:  0.38780897855758667
train gradient:  0.34361099260720346
iteration : 2155
train acc:  0.8046875
train loss:  0.4128752648830414
train gradient:  0.3948612081436433
iteration : 2156
train acc:  0.8125
train loss:  0.42427217960357666
train gradient:  0.4556713749224913
iteration : 2157
train acc:  0.828125
train loss:  0.41340431571006775
train gradient:  0.49069371456637123
iteration : 2158
train acc:  0.8125
train loss:  0.38802164793014526
train gradient:  0.3645912393722962
iteration : 2159
train acc:  0.8046875
train loss:  0.46150076389312744
train gradient:  0.5625998724578118
iteration : 2160
train acc:  0.8125
train loss:  0.41032111644744873
train gradient:  0.4295996532655936
iteration : 2161
train acc:  0.8359375
train loss:  0.36434251070022583
train gradient:  0.27222741270706663
iteration : 2162
train acc:  0.8046875
train loss:  0.41654032468795776
train gradient:  0.33132216087441313
iteration : 2163
train acc:  0.9140625
train loss:  0.27270883321762085
train gradient:  0.30631983112494443
iteration : 2164
train acc:  0.78125
train loss:  0.482372522354126
train gradient:  0.38989212170726695
iteration : 2165
train acc:  0.890625
train loss:  0.32598787546157837
train gradient:  0.336569673689567
iteration : 2166
train acc:  0.8046875
train loss:  0.36841484904289246
train gradient:  0.3360061278229879
iteration : 2167
train acc:  0.828125
train loss:  0.37002965807914734
train gradient:  0.2869764089889897
iteration : 2168
train acc:  0.796875
train loss:  0.4963512420654297
train gradient:  0.6298450868435524
iteration : 2169
train acc:  0.7578125
train loss:  0.497200608253479
train gradient:  0.5798943590062271
iteration : 2170
train acc:  0.78125
train loss:  0.4293515384197235
train gradient:  0.43118667075299466
iteration : 2171
train acc:  0.8046875
train loss:  0.4013514220714569
train gradient:  0.45002182015558156
iteration : 2172
train acc:  0.859375
train loss:  0.3309074640274048
train gradient:  0.23872764081444214
iteration : 2173
train acc:  0.84375
train loss:  0.3692356050014496
train gradient:  0.3567637399657434
iteration : 2174
train acc:  0.78125
train loss:  0.4368191063404083
train gradient:  0.5138607594970166
iteration : 2175
train acc:  0.7421875
train loss:  0.4653260111808777
train gradient:  0.5373254271354164
iteration : 2176
train acc:  0.796875
train loss:  0.39657941460609436
train gradient:  0.4205517011938056
iteration : 2177
train acc:  0.8125
train loss:  0.3935418128967285
train gradient:  0.3377532112522505
iteration : 2178
train acc:  0.859375
train loss:  0.36182358860969543
train gradient:  0.27724776237207277
iteration : 2179
train acc:  0.78125
train loss:  0.41331353783607483
train gradient:  0.384034868810262
iteration : 2180
train acc:  0.84375
train loss:  0.3421964943408966
train gradient:  0.33416333459807346
iteration : 2181
train acc:  0.78125
train loss:  0.4513234496116638
train gradient:  0.4370917441132466
iteration : 2182
train acc:  0.8203125
train loss:  0.3914068341255188
train gradient:  0.3858826795221121
iteration : 2183
train acc:  0.78125
train loss:  0.42872190475463867
train gradient:  0.6665645413239057
iteration : 2184
train acc:  0.8203125
train loss:  0.3892228603363037
train gradient:  0.4256444656251991
iteration : 2185
train acc:  0.9296875
train loss:  0.26848194003105164
train gradient:  0.3209314337489225
iteration : 2186
train acc:  0.8203125
train loss:  0.40937596559524536
train gradient:  0.39342472670377193
iteration : 2187
train acc:  0.8515625
train loss:  0.36148208379745483
train gradient:  0.31555362766381284
iteration : 2188
train acc:  0.796875
train loss:  0.44389504194259644
train gradient:  0.48155376432278263
iteration : 2189
train acc:  0.8671875
train loss:  0.35138288140296936
train gradient:  0.35241044691572476
iteration : 2190
train acc:  0.828125
train loss:  0.3621980547904968
train gradient:  0.3205755707573796
iteration : 2191
train acc:  0.90625
train loss:  0.27816593647003174
train gradient:  0.19140587637101825
iteration : 2192
train acc:  0.7890625
train loss:  0.40608304738998413
train gradient:  0.2994893176853073
iteration : 2193
train acc:  0.84375
train loss:  0.3659318685531616
train gradient:  0.34863177514895843
iteration : 2194
train acc:  0.8671875
train loss:  0.3410920798778534
train gradient:  0.38944678240001535
iteration : 2195
train acc:  0.8203125
train loss:  0.39667561650276184
train gradient:  0.3846078139367057
iteration : 2196
train acc:  0.84375
train loss:  0.3400466740131378
train gradient:  0.28006401503067246
iteration : 2197
train acc:  0.8359375
train loss:  0.374432772397995
train gradient:  0.4223699405381265
iteration : 2198
train acc:  0.7734375
train loss:  0.5537283420562744
train gradient:  0.7163712914144615
iteration : 2199
train acc:  0.8359375
train loss:  0.4072091579437256
train gradient:  0.463701665924531
iteration : 2200
train acc:  0.796875
train loss:  0.43068593740463257
train gradient:  0.3586875017790824
iteration : 2201
train acc:  0.75
train loss:  0.5227582454681396
train gradient:  0.6051544846992278
iteration : 2202
train acc:  0.875
train loss:  0.33609798550605774
train gradient:  0.3071874819482591
iteration : 2203
train acc:  0.8515625
train loss:  0.3746066391468048
train gradient:  0.39539998540536486
iteration : 2204
train acc:  0.8046875
train loss:  0.41906338930130005
train gradient:  0.38669332854745264
iteration : 2205
train acc:  0.828125
train loss:  0.41088664531707764
train gradient:  0.3889131557126429
iteration : 2206
train acc:  0.8671875
train loss:  0.34339970350265503
train gradient:  0.27600776536365307
iteration : 2207
train acc:  0.8125
train loss:  0.4358355700969696
train gradient:  0.39124698350261977
iteration : 2208
train acc:  0.78125
train loss:  0.4411002993583679
train gradient:  0.3243878965535811
iteration : 2209
train acc:  0.8203125
train loss:  0.383261501789093
train gradient:  0.5297604929954149
iteration : 2210
train acc:  0.7734375
train loss:  0.46015751361846924
train gradient:  0.39895434401897706
iteration : 2211
train acc:  0.8359375
train loss:  0.4315645098686218
train gradient:  0.27730986333757346
iteration : 2212
train acc:  0.828125
train loss:  0.37172842025756836
train gradient:  0.47201605356735327
iteration : 2213
train acc:  0.8359375
train loss:  0.3968709111213684
train gradient:  0.2542180767211375
iteration : 2214
train acc:  0.828125
train loss:  0.3890625238418579
train gradient:  0.40083524872920623
iteration : 2215
train acc:  0.8203125
train loss:  0.41373395919799805
train gradient:  0.3788977732694614
iteration : 2216
train acc:  0.84375
train loss:  0.36039042472839355
train gradient:  0.2686960455317067
iteration : 2217
train acc:  0.828125
train loss:  0.3692668378353119
train gradient:  0.26993588010504627
iteration : 2218
train acc:  0.8828125
train loss:  0.3715019226074219
train gradient:  0.23602823031370865
iteration : 2219
train acc:  0.7734375
train loss:  0.42733457684516907
train gradient:  0.3538895965218682
iteration : 2220
train acc:  0.84375
train loss:  0.3859582543373108
train gradient:  0.38661723587006713
iteration : 2221
train acc:  0.8671875
train loss:  0.3258712887763977
train gradient:  0.2878859138257191
iteration : 2222
train acc:  0.8125
train loss:  0.40157008171081543
train gradient:  0.3711901332042897
iteration : 2223
train acc:  0.84375
train loss:  0.3552413582801819
train gradient:  0.2995675253757149
iteration : 2224
train acc:  0.8046875
train loss:  0.40571630001068115
train gradient:  0.32827228992285384
iteration : 2225
train acc:  0.8359375
train loss:  0.3670848608016968
train gradient:  0.29736339027851055
iteration : 2226
train acc:  0.7734375
train loss:  0.41003358364105225
train gradient:  0.7345086376143994
iteration : 2227
train acc:  0.8515625
train loss:  0.303495317697525
train gradient:  0.19905690190458283
iteration : 2228
train acc:  0.8046875
train loss:  0.42017969489097595
train gradient:  0.4094510372992483
iteration : 2229
train acc:  0.8046875
train loss:  0.49315494298934937
train gradient:  0.44700639382426266
iteration : 2230
train acc:  0.8203125
train loss:  0.4475107789039612
train gradient:  0.47103729789314924
iteration : 2231
train acc:  0.859375
train loss:  0.39478737115859985
train gradient:  0.3851260418218637
iteration : 2232
train acc:  0.8203125
train loss:  0.34915080666542053
train gradient:  0.2366643623840887
iteration : 2233
train acc:  0.796875
train loss:  0.4568779766559601
train gradient:  0.47816742841530385
iteration : 2234
train acc:  0.828125
train loss:  0.43362563848495483
train gradient:  0.3704275960693045
iteration : 2235
train acc:  0.78125
train loss:  0.41755178570747375
train gradient:  0.3765147907124956
iteration : 2236
train acc:  0.8203125
train loss:  0.3932320475578308
train gradient:  0.3846807732609972
iteration : 2237
train acc:  0.828125
train loss:  0.43066710233688354
train gradient:  0.390851339593274
iteration : 2238
train acc:  0.8515625
train loss:  0.3581966161727905
train gradient:  0.23165196139109
iteration : 2239
train acc:  0.84375
train loss:  0.38077372312545776
train gradient:  0.2697895540796896
iteration : 2240
train acc:  0.765625
train loss:  0.46597525477409363
train gradient:  0.6390908931901691
iteration : 2241
train acc:  0.8203125
train loss:  0.4028739333152771
train gradient:  0.2638518095530235
iteration : 2242
train acc:  0.8203125
train loss:  0.40187665820121765
train gradient:  0.45820938368442404
iteration : 2243
train acc:  0.8359375
train loss:  0.3366643786430359
train gradient:  0.2179285083934627
iteration : 2244
train acc:  0.8359375
train loss:  0.37096208333969116
train gradient:  0.2950401831248214
iteration : 2245
train acc:  0.8515625
train loss:  0.33428955078125
train gradient:  0.3479250155636016
iteration : 2246
train acc:  0.796875
train loss:  0.37615108489990234
train gradient:  0.360575924908811
iteration : 2247
train acc:  0.8125
train loss:  0.39274194836616516
train gradient:  0.2852366403493828
iteration : 2248
train acc:  0.859375
train loss:  0.3828790485858917
train gradient:  0.3413085959020314
iteration : 2249
train acc:  0.8125
train loss:  0.40806782245635986
train gradient:  0.32671199485966546
iteration : 2250
train acc:  0.796875
train loss:  0.40878838300704956
train gradient:  0.36992529354100684
iteration : 2251
train acc:  0.828125
train loss:  0.4176967740058899
train gradient:  0.3541164619485522
iteration : 2252
train acc:  0.8671875
train loss:  0.32098376750946045
train gradient:  0.3221886225221439
iteration : 2253
train acc:  0.7578125
train loss:  0.42083436250686646
train gradient:  0.43580679122118693
iteration : 2254
train acc:  0.796875
train loss:  0.4093282222747803
train gradient:  0.5436126425023009
iteration : 2255
train acc:  0.78125
train loss:  0.4432188868522644
train gradient:  0.31431101806131423
iteration : 2256
train acc:  0.796875
train loss:  0.42932239174842834
train gradient:  0.40992214891574086
iteration : 2257
train acc:  0.796875
train loss:  0.3912423551082611
train gradient:  0.2473396431542905
iteration : 2258
train acc:  0.8515625
train loss:  0.3267948031425476
train gradient:  0.3226984328545029
iteration : 2259
train acc:  0.8515625
train loss:  0.3466018736362457
train gradient:  0.3002709935407986
iteration : 2260
train acc:  0.8359375
train loss:  0.39401087164878845
train gradient:  0.3474421686017115
iteration : 2261
train acc:  0.8046875
train loss:  0.40987923741340637
train gradient:  0.4815613429840709
iteration : 2262
train acc:  0.8515625
train loss:  0.4207002818584442
train gradient:  0.38499293150574243
iteration : 2263
train acc:  0.84375
train loss:  0.34938889741897583
train gradient:  0.3311261534785934
iteration : 2264
train acc:  0.828125
train loss:  0.3832281231880188
train gradient:  0.5846111915684788
iteration : 2265
train acc:  0.8359375
train loss:  0.37171027064323425
train gradient:  0.43299723369970344
iteration : 2266
train acc:  0.9140625
train loss:  0.298335999250412
train gradient:  0.269601772564078
iteration : 2267
train acc:  0.7890625
train loss:  0.4203367531299591
train gradient:  0.4269507486953235
iteration : 2268
train acc:  0.796875
train loss:  0.4401387572288513
train gradient:  0.42211734424492964
iteration : 2269
train acc:  0.8359375
train loss:  0.40008851885795593
train gradient:  0.28007304779984293
iteration : 2270
train acc:  0.8203125
train loss:  0.3486693799495697
train gradient:  0.3024645230049828
iteration : 2271
train acc:  0.7578125
train loss:  0.5275526642799377
train gradient:  0.5151320338188975
iteration : 2272
train acc:  0.8515625
train loss:  0.3407505750656128
train gradient:  0.25764870606541523
iteration : 2273
train acc:  0.8359375
train loss:  0.38713914155960083
train gradient:  0.5358172664093803
iteration : 2274
train acc:  0.828125
train loss:  0.42578744888305664
train gradient:  0.5595916327281596
iteration : 2275
train acc:  0.796875
train loss:  0.3906531035900116
train gradient:  0.3842852777095581
iteration : 2276
train acc:  0.84375
train loss:  0.36931121349334717
train gradient:  0.3652103477418106
iteration : 2277
train acc:  0.8125
train loss:  0.4272518754005432
train gradient:  0.30570324614921945
iteration : 2278
train acc:  0.7578125
train loss:  0.47635287046432495
train gradient:  0.470951510311294
iteration : 2279
train acc:  0.828125
train loss:  0.37068337202072144
train gradient:  0.2871630735810203
iteration : 2280
train acc:  0.84375
train loss:  0.3424452543258667
train gradient:  0.32921092589175194
iteration : 2281
train acc:  0.8203125
train loss:  0.4737486243247986
train gradient:  0.5181127987162307
iteration : 2282
train acc:  0.8203125
train loss:  0.4058780074119568
train gradient:  0.35105735667004645
iteration : 2283
train acc:  0.84375
train loss:  0.4099429249763489
train gradient:  0.29890445372004254
iteration : 2284
train acc:  0.8359375
train loss:  0.3536112308502197
train gradient:  0.3907899184849288
iteration : 2285
train acc:  0.796875
train loss:  0.4490339159965515
train gradient:  0.4930708788948757
iteration : 2286
train acc:  0.828125
train loss:  0.3829883933067322
train gradient:  0.26392924730600054
iteration : 2287
train acc:  0.8515625
train loss:  0.3143276572227478
train gradient:  0.3137370497166207
iteration : 2288
train acc:  0.7578125
train loss:  0.49335843324661255
train gradient:  0.5871935568464018
iteration : 2289
train acc:  0.8125
train loss:  0.4146393835544586
train gradient:  0.3547148977982586
iteration : 2290
train acc:  0.796875
train loss:  0.4211544990539551
train gradient:  0.4112441804747195
iteration : 2291
train acc:  0.8359375
train loss:  0.3665453791618347
train gradient:  0.29382526250450286
iteration : 2292
train acc:  0.8046875
train loss:  0.4219183623790741
train gradient:  0.36758344116026775
iteration : 2293
train acc:  0.8203125
train loss:  0.4309845566749573
train gradient:  0.42781349328410895
iteration : 2294
train acc:  0.8515625
train loss:  0.32307320833206177
train gradient:  0.25531696327684783
iteration : 2295
train acc:  0.8828125
train loss:  0.3531219959259033
train gradient:  0.3573090789664776
iteration : 2296
train acc:  0.8515625
train loss:  0.3501795530319214
train gradient:  0.3137020688695455
iteration : 2297
train acc:  0.8671875
train loss:  0.33608075976371765
train gradient:  0.23551359783286646
iteration : 2298
train acc:  0.75
train loss:  0.4347464442253113
train gradient:  0.4695637698569942
iteration : 2299
train acc:  0.8203125
train loss:  0.42693644762039185
train gradient:  0.3420926392103926
iteration : 2300
train acc:  0.859375
train loss:  0.3321443796157837
train gradient:  0.335887482071477
iteration : 2301
train acc:  0.828125
train loss:  0.3730011582374573
train gradient:  0.24113814318765742
iteration : 2302
train acc:  0.8359375
train loss:  0.34658288955688477
train gradient:  0.27793887548669477
iteration : 2303
train acc:  0.8671875
train loss:  0.30087828636169434
train gradient:  0.31222163145624954
iteration : 2304
train acc:  0.7734375
train loss:  0.45333483815193176
train gradient:  0.36045790792410876
iteration : 2305
train acc:  0.7734375
train loss:  0.4597492218017578
train gradient:  0.8171170459494157
iteration : 2306
train acc:  0.8203125
train loss:  0.3674619793891907
train gradient:  0.28655291250542964
iteration : 2307
train acc:  0.8125
train loss:  0.40354418754577637
train gradient:  0.3706539733354647
iteration : 2308
train acc:  0.8203125
train loss:  0.39817050099372864
train gradient:  0.4751961001246459
iteration : 2309
train acc:  0.8671875
train loss:  0.3176211416721344
train gradient:  0.2911228429094809
iteration : 2310
train acc:  0.859375
train loss:  0.3939148783683777
train gradient:  0.29527148463722175
iteration : 2311
train acc:  0.84375
train loss:  0.3561587333679199
train gradient:  0.2989770527899058
iteration : 2312
train acc:  0.8203125
train loss:  0.35318803787231445
train gradient:  0.4707068006722818
iteration : 2313
train acc:  0.875
train loss:  0.35577914118766785
train gradient:  0.3202670921760118
iteration : 2314
train acc:  0.90625
train loss:  0.3118785321712494
train gradient:  0.29545608281167945
iteration : 2315
train acc:  0.78125
train loss:  0.39384669065475464
train gradient:  0.35144895945226545
iteration : 2316
train acc:  0.8125
train loss:  0.3817056119441986
train gradient:  0.40346253808482785
iteration : 2317
train acc:  0.875
train loss:  0.3079991042613983
train gradient:  0.2343637185395852
iteration : 2318
train acc:  0.875
train loss:  0.34843242168426514
train gradient:  0.25234503709476847
iteration : 2319
train acc:  0.7734375
train loss:  0.44717779755592346
train gradient:  0.6287656744617298
iteration : 2320
train acc:  0.828125
train loss:  0.38198122382164
train gradient:  0.4722653582061484
iteration : 2321
train acc:  0.8046875
train loss:  0.37794065475463867
train gradient:  0.41558632838400866
iteration : 2322
train acc:  0.828125
train loss:  0.41653603315353394
train gradient:  0.40700055560075876
iteration : 2323
train acc:  0.875
train loss:  0.33098679780960083
train gradient:  0.5305783713585128
iteration : 2324
train acc:  0.8046875
train loss:  0.35062915086746216
train gradient:  0.2855928755421163
iteration : 2325
train acc:  0.8203125
train loss:  0.3540264368057251
train gradient:  0.3362771202674969
iteration : 2326
train acc:  0.8125
train loss:  0.3980236053466797
train gradient:  0.5030590853934518
iteration : 2327
train acc:  0.7890625
train loss:  0.37995511293411255
train gradient:  0.43187802200166664
iteration : 2328
train acc:  0.796875
train loss:  0.38573116064071655
train gradient:  0.38222811866786194
iteration : 2329
train acc:  0.875
train loss:  0.2840335965156555
train gradient:  0.2466132048069291
iteration : 2330
train acc:  0.8203125
train loss:  0.3889199495315552
train gradient:  0.5587267504599059
iteration : 2331
train acc:  0.8359375
train loss:  0.33728936314582825
train gradient:  0.3438735981709919
iteration : 2332
train acc:  0.7734375
train loss:  0.431450217962265
train gradient:  0.4721713192063731
iteration : 2333
train acc:  0.8359375
train loss:  0.45294490456581116
train gradient:  0.4851495645173687
iteration : 2334
train acc:  0.8515625
train loss:  0.3623338043689728
train gradient:  0.3824382655919833
iteration : 2335
train acc:  0.8046875
train loss:  0.407792329788208
train gradient:  0.47273138954607474
iteration : 2336
train acc:  0.84375
train loss:  0.4162631332874298
train gradient:  0.37455201535302374
iteration : 2337
train acc:  0.8203125
train loss:  0.41824162006378174
train gradient:  0.38130557979478036
iteration : 2338
train acc:  0.8046875
train loss:  0.37166059017181396
train gradient:  0.4708382322468858
iteration : 2339
train acc:  0.90625
train loss:  0.2729155421257019
train gradient:  0.2838504795996266
iteration : 2340
train acc:  0.8203125
train loss:  0.3632892966270447
train gradient:  0.47721250269649024
iteration : 2341
train acc:  0.8515625
train loss:  0.39237603545188904
train gradient:  0.37112004422299427
iteration : 2342
train acc:  0.78125
train loss:  0.42761847376823425
train gradient:  0.49146140088474627
iteration : 2343
train acc:  0.8046875
train loss:  0.4011268615722656
train gradient:  0.42797532096936564
iteration : 2344
train acc:  0.8359375
train loss:  0.3824062943458557
train gradient:  0.6026878626812318
iteration : 2345
train acc:  0.7890625
train loss:  0.3836454153060913
train gradient:  0.363495711918039
iteration : 2346
train acc:  0.765625
train loss:  0.4865328073501587
train gradient:  0.6161498342703875
iteration : 2347
train acc:  0.8125
train loss:  0.4706193208694458
train gradient:  0.5058632524532896
iteration : 2348
train acc:  0.8515625
train loss:  0.36262446641921997
train gradient:  0.2778815831428214
iteration : 2349
train acc:  0.84375
train loss:  0.34308579564094543
train gradient:  0.2700125908236076
iteration : 2350
train acc:  0.84375
train loss:  0.3589928150177002
train gradient:  0.34350829011669265
iteration : 2351
train acc:  0.84375
train loss:  0.34273600578308105
train gradient:  0.44457260459244374
iteration : 2352
train acc:  0.859375
train loss:  0.3300783634185791
train gradient:  0.24351517717816223
iteration : 2353
train acc:  0.8515625
train loss:  0.3453824520111084
train gradient:  0.27777276738707857
iteration : 2354
train acc:  0.7890625
train loss:  0.48594170808792114
train gradient:  0.4499770751985281
iteration : 2355
train acc:  0.8828125
train loss:  0.3248818516731262
train gradient:  0.24171958122280937
iteration : 2356
train acc:  0.8203125
train loss:  0.4153754413127899
train gradient:  0.6208376278424705
iteration : 2357
train acc:  0.90625
train loss:  0.27054357528686523
train gradient:  0.25071019859965515
iteration : 2358
train acc:  0.8828125
train loss:  0.2964065670967102
train gradient:  0.28407603971195455
iteration : 2359
train acc:  0.7734375
train loss:  0.5370839238166809
train gradient:  0.6746049154023454
iteration : 2360
train acc:  0.8671875
train loss:  0.3393939137458801
train gradient:  0.2807024509509166
iteration : 2361
train acc:  0.7734375
train loss:  0.4852794408798218
train gradient:  0.5146227174421113
iteration : 2362
train acc:  0.8203125
train loss:  0.4348078966140747
train gradient:  0.720179138411019
iteration : 2363
train acc:  0.8203125
train loss:  0.3719480037689209
train gradient:  0.32727520007082045
iteration : 2364
train acc:  0.734375
train loss:  0.4786602556705475
train gradient:  0.4101765774969562
iteration : 2365
train acc:  0.8671875
train loss:  0.36991196870803833
train gradient:  0.40165191391845206
iteration : 2366
train acc:  0.78125
train loss:  0.4513603746891022
train gradient:  0.44864652569158664
iteration : 2367
train acc:  0.859375
train loss:  0.32595953345298767
train gradient:  0.33857393159973115
iteration : 2368
train acc:  0.8046875
train loss:  0.4172022342681885
train gradient:  0.5415180591853667
iteration : 2369
train acc:  0.796875
train loss:  0.45013347268104553
train gradient:  0.44922215957926426
iteration : 2370
train acc:  0.8046875
train loss:  0.4746074974536896
train gradient:  0.5940154233794728
iteration : 2371
train acc:  0.8515625
train loss:  0.3710198998451233
train gradient:  0.25461401318827237
iteration : 2372
train acc:  0.84375
train loss:  0.3563670814037323
train gradient:  0.2635721557763065
iteration : 2373
train acc:  0.8046875
train loss:  0.3617035448551178
train gradient:  0.4491191974751112
iteration : 2374
train acc:  0.8203125
train loss:  0.39127713441848755
train gradient:  0.4055400302498925
iteration : 2375
train acc:  0.8359375
train loss:  0.376293420791626
train gradient:  0.3263896853419097
iteration : 2376
train acc:  0.796875
train loss:  0.43948131799697876
train gradient:  0.420859873978736
iteration : 2377
train acc:  0.8125
train loss:  0.40068748593330383
train gradient:  0.34070702720277607
iteration : 2378
train acc:  0.8984375
train loss:  0.3203759491443634
train gradient:  0.2105548653977172
iteration : 2379
train acc:  0.765625
train loss:  0.47797852754592896
train gradient:  0.41543844312544376
iteration : 2380
train acc:  0.828125
train loss:  0.40431904792785645
train gradient:  0.342928330326823
iteration : 2381
train acc:  0.828125
train loss:  0.4147717356681824
train gradient:  0.4109559412112837
iteration : 2382
train acc:  0.765625
train loss:  0.4830624759197235
train gradient:  0.5129103631934514
iteration : 2383
train acc:  0.8359375
train loss:  0.35843658447265625
train gradient:  0.24803693737831808
iteration : 2384
train acc:  0.75
train loss:  0.44480860233306885
train gradient:  0.4036282127977056
iteration : 2385
train acc:  0.8515625
train loss:  0.32655757665634155
train gradient:  0.16228746356697937
iteration : 2386
train acc:  0.859375
train loss:  0.3160926401615143
train gradient:  0.3073157742371184
iteration : 2387
train acc:  0.8203125
train loss:  0.3625836968421936
train gradient:  0.3218767805418238
iteration : 2388
train acc:  0.8359375
train loss:  0.3854310214519501
train gradient:  0.3517940725946354
iteration : 2389
train acc:  0.8203125
train loss:  0.44456130266189575
train gradient:  0.46701880403907997
iteration : 2390
train acc:  0.90625
train loss:  0.31969448924064636
train gradient:  0.2705449442112696
iteration : 2391
train acc:  0.7734375
train loss:  0.4472321569919586
train gradient:  0.3750709373335563
iteration : 2392
train acc:  0.8203125
train loss:  0.37313133478164673
train gradient:  0.23631280698644208
iteration : 2393
train acc:  0.78125
train loss:  0.4197266697883606
train gradient:  0.3673826795482317
iteration : 2394
train acc:  0.7890625
train loss:  0.40107062458992004
train gradient:  0.43244155665995143
iteration : 2395
train acc:  0.84375
train loss:  0.3922863006591797
train gradient:  0.23812545797156393
iteration : 2396
train acc:  0.84375
train loss:  0.33958691358566284
train gradient:  0.29282462920407115
iteration : 2397
train acc:  0.796875
train loss:  0.4478219747543335
train gradient:  0.5156793570794722
iteration : 2398
train acc:  0.8203125
train loss:  0.38269567489624023
train gradient:  0.2661141107450393
iteration : 2399
train acc:  0.765625
train loss:  0.45090508460998535
train gradient:  0.3816867234200341
iteration : 2400
train acc:  0.8203125
train loss:  0.36038708686828613
train gradient:  0.31231213599029417
iteration : 2401
train acc:  0.8125
train loss:  0.40004289150238037
train gradient:  0.37735881954452716
iteration : 2402
train acc:  0.7890625
train loss:  0.403559148311615
train gradient:  0.4714210361750649
iteration : 2403
train acc:  0.8203125
train loss:  0.3876691460609436
train gradient:  0.3440467322137238
iteration : 2404
train acc:  0.7578125
train loss:  0.44719383120536804
train gradient:  0.4704235750850706
iteration : 2405
train acc:  0.796875
train loss:  0.4255561828613281
train gradient:  0.5244111236899587
iteration : 2406
train acc:  0.84375
train loss:  0.3440539240837097
train gradient:  0.2574792426872209
iteration : 2407
train acc:  0.8046875
train loss:  0.38892051577568054
train gradient:  0.32400992717973937
iteration : 2408
train acc:  0.8125
train loss:  0.3944290280342102
train gradient:  0.34251567724967574
iteration : 2409
train acc:  0.828125
train loss:  0.36271780729293823
train gradient:  0.3076229514489065
iteration : 2410
train acc:  0.796875
train loss:  0.4355628490447998
train gradient:  0.37404501142728125
iteration : 2411
train acc:  0.84375
train loss:  0.38637199997901917
train gradient:  0.35717840604377316
iteration : 2412
train acc:  0.8359375
train loss:  0.348738431930542
train gradient:  0.30808414476510254
iteration : 2413
train acc:  0.7890625
train loss:  0.4571933150291443
train gradient:  0.5495424733885067
iteration : 2414
train acc:  0.78125
train loss:  0.41194307804107666
train gradient:  0.53286723895526
iteration : 2415
train acc:  0.8515625
train loss:  0.3598240911960602
train gradient:  0.3082290818034518
iteration : 2416
train acc:  0.8046875
train loss:  0.3639547824859619
train gradient:  0.2970993732010435
iteration : 2417
train acc:  0.8515625
train loss:  0.37425661087036133
train gradient:  0.29317658704893973
iteration : 2418
train acc:  0.8359375
train loss:  0.364251971244812
train gradient:  0.32161275263509814
iteration : 2419
train acc:  0.8125
train loss:  0.4196392297744751
train gradient:  0.3929101560940566
iteration : 2420
train acc:  0.90625
train loss:  0.2900140583515167
train gradient:  0.1986900952952252
iteration : 2421
train acc:  0.8203125
train loss:  0.37241289019584656
train gradient:  0.3013433178735803
iteration : 2422
train acc:  0.8515625
train loss:  0.3394601047039032
train gradient:  0.2525433395300812
iteration : 2423
train acc:  0.828125
train loss:  0.41697049140930176
train gradient:  0.3202142710090593
iteration : 2424
train acc:  0.84375
train loss:  0.3444218337535858
train gradient:  0.22913531069069376
iteration : 2425
train acc:  0.7890625
train loss:  0.49149346351623535
train gradient:  0.4828740998371018
iteration : 2426
train acc:  0.8203125
train loss:  0.4757312834262848
train gradient:  0.4438170086796699
iteration : 2427
train acc:  0.796875
train loss:  0.4201759099960327
train gradient:  0.4425332480197658
iteration : 2428
train acc:  0.828125
train loss:  0.36890000104904175
train gradient:  0.36510287336841496
iteration : 2429
train acc:  0.8125
train loss:  0.37721970677375793
train gradient:  0.27582288217343803
iteration : 2430
train acc:  0.8671875
train loss:  0.2973578870296478
train gradient:  0.22724887001207278
iteration : 2431
train acc:  0.8359375
train loss:  0.3441373407840729
train gradient:  0.3443459986271774
iteration : 2432
train acc:  0.7734375
train loss:  0.44503775238990784
train gradient:  0.5129944624784013
iteration : 2433
train acc:  0.8046875
train loss:  0.41713619232177734
train gradient:  0.45377371986088066
iteration : 2434
train acc:  0.890625
train loss:  0.2988520860671997
train gradient:  0.2085354045325754
iteration : 2435
train acc:  0.8046875
train loss:  0.4423612654209137
train gradient:  0.3934188824709952
iteration : 2436
train acc:  0.765625
train loss:  0.44011205434799194
train gradient:  0.5280241254784197
iteration : 2437
train acc:  0.8125
train loss:  0.38865968585014343
train gradient:  0.3129530175478298
iteration : 2438
train acc:  0.8125
train loss:  0.3844289183616638
train gradient:  0.3671769296440839
iteration : 2439
train acc:  0.828125
train loss:  0.4031548500061035
train gradient:  0.3780019516423377
iteration : 2440
train acc:  0.8125
train loss:  0.36212724447250366
train gradient:  0.23968234825293147
iteration : 2441
train acc:  0.765625
train loss:  0.4269941449165344
train gradient:  0.3807581142349785
iteration : 2442
train acc:  0.8671875
train loss:  0.2823418378829956
train gradient:  0.1702436907185046
iteration : 2443
train acc:  0.78125
train loss:  0.4491382837295532
train gradient:  0.5304955547591852
iteration : 2444
train acc:  0.8046875
train loss:  0.3951164186000824
train gradient:  0.4257738820212875
iteration : 2445
train acc:  0.8125
train loss:  0.4163663983345032
train gradient:  0.3850396071154569
iteration : 2446
train acc:  0.828125
train loss:  0.34763631224632263
train gradient:  0.36269746245924467
iteration : 2447
train acc:  0.84375
train loss:  0.4476284980773926
train gradient:  0.48742131006588046
iteration : 2448
train acc:  0.7734375
train loss:  0.42881837487220764
train gradient:  0.49626434639269457
iteration : 2449
train acc:  0.7734375
train loss:  0.4793330132961273
train gradient:  0.4780939691183576
iteration : 2450
train acc:  0.8203125
train loss:  0.33540111780166626
train gradient:  0.2584167986878189
iteration : 2451
train acc:  0.875
train loss:  0.3039971888065338
train gradient:  0.23746465038062503
iteration : 2452
train acc:  0.8828125
train loss:  0.3413273096084595
train gradient:  0.29031815938688865
iteration : 2453
train acc:  0.828125
train loss:  0.4094027876853943
train gradient:  0.450309789825925
iteration : 2454
train acc:  0.7578125
train loss:  0.4802859425544739
train gradient:  0.42867450014723324
iteration : 2455
train acc:  0.8046875
train loss:  0.40698304772377014
train gradient:  0.3764152102932203
iteration : 2456
train acc:  0.8046875
train loss:  0.4561759829521179
train gradient:  0.4146783072582842
iteration : 2457
train acc:  0.859375
train loss:  0.3378918766975403
train gradient:  0.30189100118928447
iteration : 2458
train acc:  0.8046875
train loss:  0.37713056802749634
train gradient:  0.33805063399994306
iteration : 2459
train acc:  0.8046875
train loss:  0.4219866693019867
train gradient:  0.4060239215513843
iteration : 2460
train acc:  0.8046875
train loss:  0.41327086091041565
train gradient:  0.34305456100186954
iteration : 2461
train acc:  0.8125
train loss:  0.41467440128326416
train gradient:  0.6893497687993589
iteration : 2462
train acc:  0.8203125
train loss:  0.4084649384021759
train gradient:  0.30707464219305136
iteration : 2463
train acc:  0.8203125
train loss:  0.3978981375694275
train gradient:  0.3363334137026515
iteration : 2464
train acc:  0.8203125
train loss:  0.38820528984069824
train gradient:  0.4247218879858899
iteration : 2465
train acc:  0.8203125
train loss:  0.43410784006118774
train gradient:  0.5096365414431756
iteration : 2466
train acc:  0.875
train loss:  0.3229134678840637
train gradient:  0.3082876327194483
iteration : 2467
train acc:  0.796875
train loss:  0.39440053701400757
train gradient:  0.3588167681513113
iteration : 2468
train acc:  0.84375
train loss:  0.33702290058135986
train gradient:  0.29170222909604443
iteration : 2469
train acc:  0.8515625
train loss:  0.34982120990753174
train gradient:  0.3349471955593252
iteration : 2470
train acc:  0.90625
train loss:  0.2927764654159546
train gradient:  0.23363711241037038
iteration : 2471
train acc:  0.84375
train loss:  0.36340975761413574
train gradient:  0.40134762128309964
iteration : 2472
train acc:  0.8671875
train loss:  0.3649422526359558
train gradient:  0.31397104792485897
iteration : 2473
train acc:  0.8046875
train loss:  0.4147028923034668
train gradient:  0.41269581766011465
iteration : 2474
train acc:  0.8046875
train loss:  0.4482584595680237
train gradient:  0.4984448071889315
iteration : 2475
train acc:  0.8359375
train loss:  0.37118440866470337
train gradient:  0.2379527066489726
iteration : 2476
train acc:  0.7890625
train loss:  0.46517741680145264
train gradient:  0.5388187961415297
iteration : 2477
train acc:  0.8828125
train loss:  0.38718870282173157
train gradient:  0.5173776479464551
iteration : 2478
train acc:  0.8671875
train loss:  0.37264102697372437
train gradient:  0.30766145183611254
iteration : 2479
train acc:  0.7421875
train loss:  0.4758318066596985
train gradient:  0.5365117631949465
iteration : 2480
train acc:  0.8359375
train loss:  0.36968308687210083
train gradient:  0.4067540679751552
iteration : 2481
train acc:  0.796875
train loss:  0.4895992875099182
train gradient:  0.5954527534440106
iteration : 2482
train acc:  0.84375
train loss:  0.3529112935066223
train gradient:  0.4064711298357812
iteration : 2483
train acc:  0.7578125
train loss:  0.49152782559394836
train gradient:  0.592214558177225
iteration : 2484
train acc:  0.8671875
train loss:  0.282953679561615
train gradient:  0.2210963116605923
iteration : 2485
train acc:  0.8046875
train loss:  0.44479119777679443
train gradient:  0.43206240798529083
iteration : 2486
train acc:  0.8125
train loss:  0.42666441202163696
train gradient:  0.44103038353325325
iteration : 2487
train acc:  0.8203125
train loss:  0.3506543040275574
train gradient:  0.35520806637916585
iteration : 2488
train acc:  0.8203125
train loss:  0.4400691092014313
train gradient:  0.4217036691533586
iteration : 2489
train acc:  0.8515625
train loss:  0.40010708570480347
train gradient:  0.41783440102130415
iteration : 2490
train acc:  0.8203125
train loss:  0.41487377882003784
train gradient:  0.3244459437387253
iteration : 2491
train acc:  0.8515625
train loss:  0.3814607262611389
train gradient:  0.4608688448100356
iteration : 2492
train acc:  0.7265625
train loss:  0.4919080436229706
train gradient:  0.6065280206334649
iteration : 2493
train acc:  0.796875
train loss:  0.41778481006622314
train gradient:  0.3974327152110007
iteration : 2494
train acc:  0.7734375
train loss:  0.43104296922683716
train gradient:  0.49018630145709235
iteration : 2495
train acc:  0.7734375
train loss:  0.4832022786140442
train gradient:  0.4963280438824651
iteration : 2496
train acc:  0.84375
train loss:  0.38839107751846313
train gradient:  0.3742106109019907
iteration : 2497
train acc:  0.828125
train loss:  0.38878825306892395
train gradient:  0.2821619285918946
iteration : 2498
train acc:  0.84375
train loss:  0.3761442005634308
train gradient:  0.2656591705272307
iteration : 2499
train acc:  0.8359375
train loss:  0.3968680500984192
train gradient:  0.4051050732546033
iteration : 2500
train acc:  0.75
train loss:  0.4671723246574402
train gradient:  0.45095847626898317
iteration : 2501
train acc:  0.7421875
train loss:  0.4496614634990692
train gradient:  0.39595466694574905
iteration : 2502
train acc:  0.828125
train loss:  0.3736673593521118
train gradient:  0.24311741138092702
iteration : 2503
train acc:  0.8671875
train loss:  0.395687997341156
train gradient:  0.43921583865180885
iteration : 2504
train acc:  0.7421875
train loss:  0.5199184417724609
train gradient:  0.5093261326250995
iteration : 2505
train acc:  0.8125
train loss:  0.39147162437438965
train gradient:  0.374592018905795
iteration : 2506
train acc:  0.8203125
train loss:  0.3576046824455261
train gradient:  0.4369944368479025
iteration : 2507
train acc:  0.8359375
train loss:  0.3610858917236328
train gradient:  0.2689670275024473
iteration : 2508
train acc:  0.8046875
train loss:  0.3950713872909546
train gradient:  0.2956570444538142
iteration : 2509
train acc:  0.78125
train loss:  0.4672369062900543
train gradient:  0.3851633282127992
iteration : 2510
train acc:  0.8046875
train loss:  0.380623459815979
train gradient:  0.2930413945431331
iteration : 2511
train acc:  0.8046875
train loss:  0.39889422059059143
train gradient:  0.3589028293592824
iteration : 2512
train acc:  0.8125
train loss:  0.3733142614364624
train gradient:  0.2725488712607502
iteration : 2513
train acc:  0.828125
train loss:  0.4096907675266266
train gradient:  0.36855477931964614
iteration : 2514
train acc:  0.8046875
train loss:  0.42671531438827515
train gradient:  0.31938959794569555
iteration : 2515
train acc:  0.8203125
train loss:  0.4292764663696289
train gradient:  0.48625473267260233
iteration : 2516
train acc:  0.8515625
train loss:  0.35330840945243835
train gradient:  0.24028307498635906
iteration : 2517
train acc:  0.8515625
train loss:  0.3476767838001251
train gradient:  0.3393973870189906
iteration : 2518
train acc:  0.8046875
train loss:  0.4052255153656006
train gradient:  0.35365520847467563
iteration : 2519
train acc:  0.8828125
train loss:  0.3163846731185913
train gradient:  0.2217174862343516
iteration : 2520
train acc:  0.84375
train loss:  0.3676779568195343
train gradient:  0.30275907295907845
iteration : 2521
train acc:  0.7265625
train loss:  0.5910075902938843
train gradient:  0.838961545293708
iteration : 2522
train acc:  0.78125
train loss:  0.40252166986465454
train gradient:  0.34111830533645393
iteration : 2523
train acc:  0.7578125
train loss:  0.4307996928691864
train gradient:  0.3714361151427183
iteration : 2524
train acc:  0.7734375
train loss:  0.5581479072570801
train gradient:  0.6250398829682995
iteration : 2525
train acc:  0.8203125
train loss:  0.48062288761138916
train gradient:  0.42189969757492
iteration : 2526
train acc:  0.8671875
train loss:  0.30477064847946167
train gradient:  0.2636854144382767
iteration : 2527
train acc:  0.78125
train loss:  0.41349008679389954
train gradient:  0.3035174056096062
iteration : 2528
train acc:  0.875
train loss:  0.3278904855251312
train gradient:  0.23108074876044576
iteration : 2529
train acc:  0.8359375
train loss:  0.3362576365470886
train gradient:  0.29597982928953726
iteration : 2530
train acc:  0.84375
train loss:  0.33670181035995483
train gradient:  0.2445335727799825
iteration : 2531
train acc:  0.8125
train loss:  0.39940664172172546
train gradient:  0.34248970615026564
iteration : 2532
train acc:  0.796875
train loss:  0.4437468647956848
train gradient:  0.4721548626874681
iteration : 2533
train acc:  0.8359375
train loss:  0.424148827791214
train gradient:  0.4190668631017927
iteration : 2534
train acc:  0.84375
train loss:  0.3510943055152893
train gradient:  0.19583718830962057
iteration : 2535
train acc:  0.8984375
train loss:  0.29025959968566895
train gradient:  0.2690630253416668
iteration : 2536
train acc:  0.8515625
train loss:  0.3631465435028076
train gradient:  0.2679149898357162
iteration : 2537
train acc:  0.8046875
train loss:  0.4278688430786133
train gradient:  0.3682086169828798
iteration : 2538
train acc:  0.8203125
train loss:  0.42261749505996704
train gradient:  0.3882254056902664
iteration : 2539
train acc:  0.8046875
train loss:  0.3870866000652313
train gradient:  0.3737048452194706
iteration : 2540
train acc:  0.828125
train loss:  0.3324759006500244
train gradient:  0.20958633566473156
iteration : 2541
train acc:  0.875
train loss:  0.2968238592147827
train gradient:  0.21413413318285304
iteration : 2542
train acc:  0.8203125
train loss:  0.38783562183380127
train gradient:  0.6890082016221807
iteration : 2543
train acc:  0.8671875
train loss:  0.3076058328151703
train gradient:  0.23005352312490618
iteration : 2544
train acc:  0.8046875
train loss:  0.42172497510910034
train gradient:  0.6142313972918478
iteration : 2545
train acc:  0.84375
train loss:  0.35333067178726196
train gradient:  0.3685666978769366
iteration : 2546
train acc:  0.765625
train loss:  0.4680289030075073
train gradient:  0.5299533211593042
iteration : 2547
train acc:  0.8671875
train loss:  0.3789055347442627
train gradient:  0.31016815983909146
iteration : 2548
train acc:  0.8125
train loss:  0.4387810230255127
train gradient:  0.44332518404232396
iteration : 2549
train acc:  0.7890625
train loss:  0.5153969526290894
train gradient:  0.6142389278051417
iteration : 2550
train acc:  0.84375
train loss:  0.3719276487827301
train gradient:  0.299262978937579
iteration : 2551
train acc:  0.8203125
train loss:  0.3986726403236389
train gradient:  0.46353769952615753
iteration : 2552
train acc:  0.7734375
train loss:  0.415982186794281
train gradient:  0.4879556204624743
iteration : 2553
train acc:  0.859375
train loss:  0.34271663427352905
train gradient:  0.3820966116670391
iteration : 2554
train acc:  0.8046875
train loss:  0.4068666100502014
train gradient:  0.3553145053841963
iteration : 2555
train acc:  0.8125
train loss:  0.3642539381980896
train gradient:  0.29027915489787054
iteration : 2556
train acc:  0.8046875
train loss:  0.40497276186943054
train gradient:  0.466210938524206
iteration : 2557
train acc:  0.828125
train loss:  0.3694608211517334
train gradient:  0.31398302500848024
iteration : 2558
train acc:  0.765625
train loss:  0.46142280101776123
train gradient:  0.42702757941954395
iteration : 2559
train acc:  0.796875
train loss:  0.42317044734954834
train gradient:  0.4166702939034333
iteration : 2560
train acc:  0.8515625
train loss:  0.4118522107601166
train gradient:  0.33565150645830133
iteration : 2561
train acc:  0.890625
train loss:  0.2965323328971863
train gradient:  0.25656123989005886
iteration : 2562
train acc:  0.8515625
train loss:  0.3262326419353485
train gradient:  0.3379602722483793
iteration : 2563
train acc:  0.796875
train loss:  0.4143619239330292
train gradient:  0.31842543810001717
iteration : 2564
train acc:  0.8515625
train loss:  0.3346095085144043
train gradient:  0.24019948101882727
iteration : 2565
train acc:  0.828125
train loss:  0.4008176922798157
train gradient:  0.29847810576847783
iteration : 2566
train acc:  0.78125
train loss:  0.4500434994697571
train gradient:  0.3285892354814144
iteration : 2567
train acc:  0.7890625
train loss:  0.37539392709732056
train gradient:  0.2846599196822718
iteration : 2568
train acc:  0.8046875
train loss:  0.3781278133392334
train gradient:  0.27043384533816595
iteration : 2569
train acc:  0.7734375
train loss:  0.43159016966819763
train gradient:  0.4159789021349445
iteration : 2570
train acc:  0.8359375
train loss:  0.3572937846183777
train gradient:  0.4097902246909117
iteration : 2571
train acc:  0.8046875
train loss:  0.42193084955215454
train gradient:  0.3589355162950728
iteration : 2572
train acc:  0.8515625
train loss:  0.33484500646591187
train gradient:  0.3299290402465535
iteration : 2573
train acc:  0.8046875
train loss:  0.4046388864517212
train gradient:  0.3539773752078309
iteration : 2574
train acc:  0.8515625
train loss:  0.40446969866752625
train gradient:  0.32650710597226246
iteration : 2575
train acc:  0.8046875
train loss:  0.4092782735824585
train gradient:  0.47735148356728957
iteration : 2576
train acc:  0.8515625
train loss:  0.3947426974773407
train gradient:  0.28872856768756894
iteration : 2577
train acc:  0.875
train loss:  0.36510586738586426
train gradient:  0.24042740610758156
iteration : 2578
train acc:  0.8203125
train loss:  0.41137808561325073
train gradient:  0.3949007880161391
iteration : 2579
train acc:  0.8515625
train loss:  0.33863693475723267
train gradient:  0.2903818294346188
iteration : 2580
train acc:  0.8515625
train loss:  0.34922683238983154
train gradient:  0.3203505413817631
iteration : 2581
train acc:  0.8203125
train loss:  0.4013954699039459
train gradient:  0.3192812715742184
iteration : 2582
train acc:  0.8515625
train loss:  0.3741014003753662
train gradient:  0.35055606815433416
iteration : 2583
train acc:  0.7890625
train loss:  0.43080422282218933
train gradient:  0.4356275378318481
iteration : 2584
train acc:  0.796875
train loss:  0.4007421135902405
train gradient:  0.48627227829723113
iteration : 2585
train acc:  0.7734375
train loss:  0.44492894411087036
train gradient:  0.46554241204731267
iteration : 2586
train acc:  0.875
train loss:  0.3185068964958191
train gradient:  0.2847906786288269
iteration : 2587
train acc:  0.8203125
train loss:  0.42984384298324585
train gradient:  0.34050129677416513
iteration : 2588
train acc:  0.796875
train loss:  0.4129001796245575
train gradient:  0.3805355650277111
iteration : 2589
train acc:  0.8359375
train loss:  0.3646206855773926
train gradient:  0.31021500799854806
iteration : 2590
train acc:  0.8125
train loss:  0.4492056965827942
train gradient:  0.4608125027687032
iteration : 2591
train acc:  0.828125
train loss:  0.3833671510219574
train gradient:  0.3289809243754726
iteration : 2592
train acc:  0.890625
train loss:  0.26599058508872986
train gradient:  0.19438513936287105
iteration : 2593
train acc:  0.828125
train loss:  0.37203049659729004
train gradient:  0.4172947968129572
iteration : 2594
train acc:  0.7734375
train loss:  0.43670153617858887
train gradient:  0.30590335134613467
iteration : 2595
train acc:  0.78125
train loss:  0.435356080532074
train gradient:  0.51305577483793
iteration : 2596
train acc:  0.8515625
train loss:  0.38850998878479004
train gradient:  0.3738585624135438
iteration : 2597
train acc:  0.84375
train loss:  0.3770827054977417
train gradient:  0.31672927255957634
iteration : 2598
train acc:  0.8515625
train loss:  0.3146321177482605
train gradient:  0.2657697049311717
iteration : 2599
train acc:  0.8203125
train loss:  0.36362481117248535
train gradient:  0.3741195523436366
iteration : 2600
train acc:  0.828125
train loss:  0.3788319230079651
train gradient:  0.32570717826247036
iteration : 2601
train acc:  0.796875
train loss:  0.4039055109024048
train gradient:  0.388623416215223
iteration : 2602
train acc:  0.7890625
train loss:  0.4188969135284424
train gradient:  0.3186649562083975
iteration : 2603
train acc:  0.8515625
train loss:  0.38851580023765564
train gradient:  0.4030190344994579
iteration : 2604
train acc:  0.828125
train loss:  0.4207347333431244
train gradient:  0.340117705209607
iteration : 2605
train acc:  0.8046875
train loss:  0.4281584918498993
train gradient:  0.35975197752536536
iteration : 2606
train acc:  0.859375
train loss:  0.38598302006721497
train gradient:  0.35018345514754995
iteration : 2607
train acc:  0.8515625
train loss:  0.36797448992729187
train gradient:  0.34072413461883644
iteration : 2608
train acc:  0.7890625
train loss:  0.4415985345840454
train gradient:  0.6036883689722183
iteration : 2609
train acc:  0.7890625
train loss:  0.3557843267917633
train gradient:  0.2959036669210174
iteration : 2610
train acc:  0.859375
train loss:  0.3505564332008362
train gradient:  0.2199490936995002
iteration : 2611
train acc:  0.8203125
train loss:  0.44735655188560486
train gradient:  0.42176086924248274
iteration : 2612
train acc:  0.8046875
train loss:  0.40532857179641724
train gradient:  0.32086552246791883
iteration : 2613
train acc:  0.7890625
train loss:  0.41735079884529114
train gradient:  0.49351517095497655
iteration : 2614
train acc:  0.8359375
train loss:  0.35715052485466003
train gradient:  0.3184632285353745
iteration : 2615
train acc:  0.796875
train loss:  0.43169429898262024
train gradient:  0.5580231491391041
iteration : 2616
train acc:  0.8671875
train loss:  0.3433685898780823
train gradient:  0.4489523179097971
iteration : 2617
train acc:  0.828125
train loss:  0.36049896478652954
train gradient:  0.2965975616018864
iteration : 2618
train acc:  0.84375
train loss:  0.3950006067752838
train gradient:  0.37896367056630437
iteration : 2619
train acc:  0.8984375
train loss:  0.3310254216194153
train gradient:  0.23263255636576835
iteration : 2620
train acc:  0.8203125
train loss:  0.35373973846435547
train gradient:  0.24156126838994507
iteration : 2621
train acc:  0.8828125
train loss:  0.3488852381706238
train gradient:  0.29781689672499
iteration : 2622
train acc:  0.8046875
train loss:  0.4391353726387024
train gradient:  0.3395175820491235
iteration : 2623
train acc:  0.875
train loss:  0.3246493339538574
train gradient:  0.2518287171548804
iteration : 2624
train acc:  0.8671875
train loss:  0.32320207357406616
train gradient:  0.19894933716538263
iteration : 2625
train acc:  0.7734375
train loss:  0.4293336272239685
train gradient:  0.44233776037270456
iteration : 2626
train acc:  0.875
train loss:  0.34509575366973877
train gradient:  0.2782801228204118
iteration : 2627
train acc:  0.828125
train loss:  0.387593150138855
train gradient:  0.35293134152616185
iteration : 2628
train acc:  0.7734375
train loss:  0.4903510808944702
train gradient:  0.5078392019576938
iteration : 2629
train acc:  0.8203125
train loss:  0.3868379592895508
train gradient:  0.40109877717678344
iteration : 2630
train acc:  0.8125
train loss:  0.3726562261581421
train gradient:  0.34627111478834444
iteration : 2631
train acc:  0.7890625
train loss:  0.463914692401886
train gradient:  0.3925843789013321
iteration : 2632
train acc:  0.8515625
train loss:  0.33316007256507874
train gradient:  0.30206414153848693
iteration : 2633
train acc:  0.84375
train loss:  0.41864097118377686
train gradient:  0.4167252263967672
iteration : 2634
train acc:  0.875
train loss:  0.28447389602661133
train gradient:  0.2252542215922435
iteration : 2635
train acc:  0.78125
train loss:  0.45116186141967773
train gradient:  0.3885384576044279
iteration : 2636
train acc:  0.8125
train loss:  0.40242812037467957
train gradient:  0.3485098235085411
iteration : 2637
train acc:  0.84375
train loss:  0.380113810300827
train gradient:  0.30932937032896773
iteration : 2638
train acc:  0.859375
train loss:  0.34621870517730713
train gradient:  0.36759315707182805
iteration : 2639
train acc:  0.8203125
train loss:  0.36055028438568115
train gradient:  0.4101450221439076
iteration : 2640
train acc:  0.7734375
train loss:  0.39364558458328247
train gradient:  0.415934887725764
iteration : 2641
train acc:  0.8125
train loss:  0.41434985399246216
train gradient:  0.3559590106350585
iteration : 2642
train acc:  0.8046875
train loss:  0.3838484287261963
train gradient:  0.4862772819253861
iteration : 2643
train acc:  0.8359375
train loss:  0.3482436537742615
train gradient:  0.29015310590960347
iteration : 2644
train acc:  0.9453125
train loss:  0.23127008974552155
train gradient:  0.20762866883613923
iteration : 2645
train acc:  0.8984375
train loss:  0.2747728228569031
train gradient:  0.18344168087119422
iteration : 2646
train acc:  0.859375
train loss:  0.3634517192840576
train gradient:  0.2870029072797789
iteration : 2647
train acc:  0.8359375
train loss:  0.3587237596511841
train gradient:  0.27278132187357357
iteration : 2648
train acc:  0.796875
train loss:  0.4322156012058258
train gradient:  0.5406665864715934
iteration : 2649
train acc:  0.84375
train loss:  0.3421770930290222
train gradient:  0.2444218698953588
iteration : 2650
train acc:  0.8359375
train loss:  0.40122851729393005
train gradient:  0.3931143652003076
iteration : 2651
train acc:  0.875
train loss:  0.3451552391052246
train gradient:  0.2636243846484699
iteration : 2652
train acc:  0.8203125
train loss:  0.3655474781990051
train gradient:  0.29712647821879334
iteration : 2653
train acc:  0.7734375
train loss:  0.4172653555870056
train gradient:  0.3395659841154294
iteration : 2654
train acc:  0.890625
train loss:  0.3014969527721405
train gradient:  0.2739177730815915
iteration : 2655
train acc:  0.859375
train loss:  0.3108364939689636
train gradient:  0.2249877131516067
iteration : 2656
train acc:  0.84375
train loss:  0.34115809202194214
train gradient:  0.3951544760639589
iteration : 2657
train acc:  0.84375
train loss:  0.3754197955131531
train gradient:  0.2686616484525494
iteration : 2658
train acc:  0.875
train loss:  0.3495004177093506
train gradient:  0.24799894707744283
iteration : 2659
train acc:  0.8515625
train loss:  0.32055842876434326
train gradient:  0.31628206320614094
iteration : 2660
train acc:  0.8046875
train loss:  0.4077235758304596
train gradient:  0.5164280011835461
iteration : 2661
train acc:  0.84375
train loss:  0.3563667833805084
train gradient:  0.3419907113282769
iteration : 2662
train acc:  0.84375
train loss:  0.3654760718345642
train gradient:  0.307169389252937
iteration : 2663
train acc:  0.796875
train loss:  0.4490407407283783
train gradient:  0.5254272439934943
iteration : 2664
train acc:  0.828125
train loss:  0.38362354040145874
train gradient:  0.3666811126389477
iteration : 2665
train acc:  0.8203125
train loss:  0.3709346055984497
train gradient:  0.31432305397263144
iteration : 2666
train acc:  0.8046875
train loss:  0.4112412929534912
train gradient:  0.4508575471707939
iteration : 2667
train acc:  0.8828125
train loss:  0.3724711835384369
train gradient:  0.26282937378569743
iteration : 2668
train acc:  0.8046875
train loss:  0.4440087378025055
train gradient:  0.42731462267760445
iteration : 2669
train acc:  0.734375
train loss:  0.5601385831832886
train gradient:  0.607146653563962
iteration : 2670
train acc:  0.8671875
train loss:  0.3239765763282776
train gradient:  0.3118665852300133
iteration : 2671
train acc:  0.7890625
train loss:  0.4420013427734375
train gradient:  0.3150355038340441
iteration : 2672
train acc:  0.8125
train loss:  0.4590824246406555
train gradient:  0.48500228031159087
iteration : 2673
train acc:  0.796875
train loss:  0.5064178705215454
train gradient:  0.5122833277233844
iteration : 2674
train acc:  0.765625
train loss:  0.3849843144416809
train gradient:  0.317577929308868
iteration : 2675
train acc:  0.7890625
train loss:  0.4109761416912079
train gradient:  0.4350376196795218
iteration : 2676
train acc:  0.78125
train loss:  0.38566386699676514
train gradient:  0.3568434175511083
iteration : 2677
train acc:  0.8984375
train loss:  0.29496800899505615
train gradient:  0.3189549387670435
iteration : 2678
train acc:  0.8125
train loss:  0.41964688897132874
train gradient:  0.35502096055779614
iteration : 2679
train acc:  0.8359375
train loss:  0.36473405361175537
train gradient:  0.29234554694410664
iteration : 2680
train acc:  0.8359375
train loss:  0.35070478916168213
train gradient:  0.32933294816709763
iteration : 2681
train acc:  0.8359375
train loss:  0.3693700432777405
train gradient:  0.46042296797415
iteration : 2682
train acc:  0.796875
train loss:  0.4281706213951111
train gradient:  0.3501768694219418
iteration : 2683
train acc:  0.84375
train loss:  0.36604881286621094
train gradient:  0.3420826946553012
iteration : 2684
train acc:  0.8515625
train loss:  0.32263481616973877
train gradient:  0.25275283102617135
iteration : 2685
train acc:  0.8203125
train loss:  0.40830889344215393
train gradient:  0.34716001564178184
iteration : 2686
train acc:  0.7734375
train loss:  0.42003798484802246
train gradient:  0.7931451048355356
iteration : 2687
train acc:  0.796875
train loss:  0.3645622730255127
train gradient:  0.330737531695005
iteration : 2688
train acc:  0.8125
train loss:  0.432269811630249
train gradient:  0.4797557644489465
iteration : 2689
train acc:  0.84375
train loss:  0.34651243686676025
train gradient:  0.2916757314396354
iteration : 2690
train acc:  0.8515625
train loss:  0.3613628149032593
train gradient:  0.3530188704670068
iteration : 2691
train acc:  0.8671875
train loss:  0.32026970386505127
train gradient:  0.26284399656660323
iteration : 2692
train acc:  0.7734375
train loss:  0.4505244493484497
train gradient:  0.4007804952432026
iteration : 2693
train acc:  0.8046875
train loss:  0.4350733160972595
train gradient:  0.44877506509414133
iteration : 2694
train acc:  0.8515625
train loss:  0.3822290897369385
train gradient:  0.2849285996914874
iteration : 2695
train acc:  0.8359375
train loss:  0.3836459517478943
train gradient:  0.5009807602197447
iteration : 2696
train acc:  0.796875
train loss:  0.45118170976638794
train gradient:  0.3978321677141123
iteration : 2697
train acc:  0.8515625
train loss:  0.3115418553352356
train gradient:  0.22851412859347142
iteration : 2698
train acc:  0.8515625
train loss:  0.33536064624786377
train gradient:  0.5834578656328129
iteration : 2699
train acc:  0.8203125
train loss:  0.39522784948349
train gradient:  0.32727682471989816
iteration : 2700
train acc:  0.859375
train loss:  0.34086403250694275
train gradient:  0.34129828641605037
iteration : 2701
train acc:  0.75
train loss:  0.4972435534000397
train gradient:  0.49864692566937274
iteration : 2702
train acc:  0.8671875
train loss:  0.33556708693504333
train gradient:  0.3143089523810183
iteration : 2703
train acc:  0.828125
train loss:  0.38356253504753113
train gradient:  0.3275831416809421
iteration : 2704
train acc:  0.8203125
train loss:  0.35694563388824463
train gradient:  0.28660736859429026
iteration : 2705
train acc:  0.8515625
train loss:  0.3662155866622925
train gradient:  0.28438601265247393
iteration : 2706
train acc:  0.8515625
train loss:  0.3149683475494385
train gradient:  0.2401202354151165
iteration : 2707
train acc:  0.78125
train loss:  0.41852378845214844
train gradient:  0.3065161742480914
iteration : 2708
train acc:  0.8046875
train loss:  0.39470523595809937
train gradient:  0.31579269734539395
iteration : 2709
train acc:  0.765625
train loss:  0.5588535666465759
train gradient:  0.9039864043853133
iteration : 2710
train acc:  0.78125
train loss:  0.4089670181274414
train gradient:  0.4235161642243474
iteration : 2711
train acc:  0.8203125
train loss:  0.35615336894989014
train gradient:  0.2589640167530638
iteration : 2712
train acc:  0.7890625
train loss:  0.4770049750804901
train gradient:  0.4574948502455482
iteration : 2713
train acc:  0.84375
train loss:  0.3685494065284729
train gradient:  0.33066205857194
iteration : 2714
train acc:  0.8203125
train loss:  0.39670708775520325
train gradient:  0.3919720894946792
iteration : 2715
train acc:  0.78125
train loss:  0.40566110610961914
train gradient:  0.39320988915552185
iteration : 2716
train acc:  0.84375
train loss:  0.31954425573349
train gradient:  0.32373843863122215
iteration : 2717
train acc:  0.8828125
train loss:  0.3036752939224243
train gradient:  0.3010043804753515
iteration : 2718
train acc:  0.8125
train loss:  0.42474621534347534
train gradient:  0.45929901392970357
iteration : 2719
train acc:  0.828125
train loss:  0.36939480900764465
train gradient:  0.31504128297152667
iteration : 2720
train acc:  0.8046875
train loss:  0.4160946011543274
train gradient:  0.4114034901673119
iteration : 2721
train acc:  0.7578125
train loss:  0.5145469903945923
train gradient:  0.5272552568986931
iteration : 2722
train acc:  0.8203125
train loss:  0.3344614803791046
train gradient:  0.32305027298372896
iteration : 2723
train acc:  0.7890625
train loss:  0.4003901481628418
train gradient:  0.3108482395878271
iteration : 2724
train acc:  0.8046875
train loss:  0.3449184000492096
train gradient:  0.31759992188217817
iteration : 2725
train acc:  0.859375
train loss:  0.3840875029563904
train gradient:  0.35882960872026975
iteration : 2726
train acc:  0.8515625
train loss:  0.405473530292511
train gradient:  0.29490649036729705
iteration : 2727
train acc:  0.8515625
train loss:  0.3265685737133026
train gradient:  0.2610574809016463
iteration : 2728
train acc:  0.7890625
train loss:  0.3974112868309021
train gradient:  0.3767135413856054
iteration : 2729
train acc:  0.8203125
train loss:  0.36205220222473145
train gradient:  0.33099480016784144
iteration : 2730
train acc:  0.8203125
train loss:  0.3755192756652832
train gradient:  0.5566887625352503
iteration : 2731
train acc:  0.84375
train loss:  0.3883511424064636
train gradient:  0.2863503163443402
iteration : 2732
train acc:  0.8359375
train loss:  0.41774338483810425
train gradient:  0.42768907974006093
iteration : 2733
train acc:  0.8359375
train loss:  0.3685573935508728
train gradient:  0.316653934345245
iteration : 2734
train acc:  0.859375
train loss:  0.3418455719947815
train gradient:  0.4176968220201181
iteration : 2735
train acc:  0.796875
train loss:  0.3355216085910797
train gradient:  0.337276865323243
iteration : 2736
train acc:  0.8125
train loss:  0.4102209806442261
train gradient:  0.45009578272769385
iteration : 2737
train acc:  0.78125
train loss:  0.42576849460601807
train gradient:  0.3885465527428873
iteration : 2738
train acc:  0.8359375
train loss:  0.347072571516037
train gradient:  0.24221349377840795
iteration : 2739
train acc:  0.828125
train loss:  0.31232619285583496
train gradient:  0.26457283353349
iteration : 2740
train acc:  0.796875
train loss:  0.43548789620399475
train gradient:  0.36714362948108753
iteration : 2741
train acc:  0.796875
train loss:  0.40488576889038086
train gradient:  0.39818417182040117
iteration : 2742
train acc:  0.7890625
train loss:  0.42821717262268066
train gradient:  0.4677697346971776
iteration : 2743
train acc:  0.8046875
train loss:  0.4120289385318756
train gradient:  0.4188941108047865
iteration : 2744
train acc:  0.828125
train loss:  0.3952676057815552
train gradient:  0.28706918602735987
iteration : 2745
train acc:  0.765625
train loss:  0.5180867314338684
train gradient:  0.6218102264093724
iteration : 2746
train acc:  0.8828125
train loss:  0.3114587068557739
train gradient:  0.28187542468298593
iteration : 2747
train acc:  0.828125
train loss:  0.38463926315307617
train gradient:  0.3745778846745009
iteration : 2748
train acc:  0.8359375
train loss:  0.3792153000831604
train gradient:  0.3024015371986302
iteration : 2749
train acc:  0.828125
train loss:  0.3925919234752655
train gradient:  0.35313977108522726
iteration : 2750
train acc:  0.8046875
train loss:  0.4426133334636688
train gradient:  0.3836510524181849
iteration : 2751
train acc:  0.8046875
train loss:  0.42134571075439453
train gradient:  0.35932235765900883
iteration : 2752
train acc:  0.7890625
train loss:  0.440810889005661
train gradient:  0.3685669516906047
iteration : 2753
train acc:  0.8203125
train loss:  0.397477388381958
train gradient:  0.3572135846369911
iteration : 2754
train acc:  0.8515625
train loss:  0.3657580018043518
train gradient:  0.2789270828153647
iteration : 2755
train acc:  0.8125
train loss:  0.4226651191711426
train gradient:  0.49325639340921335
iteration : 2756
train acc:  0.875
train loss:  0.3241325616836548
train gradient:  0.32099474851890153
iteration : 2757
train acc:  0.859375
train loss:  0.36430788040161133
train gradient:  0.289697052099396
iteration : 2758
train acc:  0.8125
train loss:  0.39034509658813477
train gradient:  0.33782257718827036
iteration : 2759
train acc:  0.78125
train loss:  0.3863660991191864
train gradient:  0.34836854070177026
iteration : 2760
train acc:  0.8125
train loss:  0.3575317859649658
train gradient:  0.34516649779500536
iteration : 2761
train acc:  0.828125
train loss:  0.38992539048194885
train gradient:  0.30123996373609363
iteration : 2762
train acc:  0.84375
train loss:  0.34975773096084595
train gradient:  0.3374331227477819
iteration : 2763
train acc:  0.828125
train loss:  0.42397481203079224
train gradient:  0.32909075643222974
iteration : 2764
train acc:  0.875
train loss:  0.3155963718891144
train gradient:  0.2617356595640078
iteration : 2765
train acc:  0.75
train loss:  0.4304317533969879
train gradient:  0.4874929723532256
iteration : 2766
train acc:  0.8125
train loss:  0.36465296149253845
train gradient:  0.23261364422569225
iteration : 2767
train acc:  0.8359375
train loss:  0.31895071268081665
train gradient:  0.19308507909244915
iteration : 2768
train acc:  0.7890625
train loss:  0.4103386402130127
train gradient:  0.39621483613007014
iteration : 2769
train acc:  0.8671875
train loss:  0.3633703291416168
train gradient:  0.2761454905607831
iteration : 2770
train acc:  0.8671875
train loss:  0.2998780608177185
train gradient:  0.2037701379603614
iteration : 2771
train acc:  0.8671875
train loss:  0.3148146867752075
train gradient:  0.27064771345968397
iteration : 2772
train acc:  0.8046875
train loss:  0.39719846844673157
train gradient:  0.3287729516875174
iteration : 2773
train acc:  0.8515625
train loss:  0.32971903681755066
train gradient:  0.2229465407933405
iteration : 2774
train acc:  0.8125
train loss:  0.42223960161209106
train gradient:  0.4180523737205027
iteration : 2775
train acc:  0.828125
train loss:  0.39879029989242554
train gradient:  0.3852844939085047
iteration : 2776
train acc:  0.8359375
train loss:  0.37269484996795654
train gradient:  0.38951103421264954
iteration : 2777
train acc:  0.8359375
train loss:  0.4010850489139557
train gradient:  0.3853046862715725
iteration : 2778
train acc:  0.71875
train loss:  0.5872396230697632
train gradient:  0.580032223876083
iteration : 2779
train acc:  0.8359375
train loss:  0.39927858114242554
train gradient:  0.40018861186113425
iteration : 2780
train acc:  0.8125
train loss:  0.41622403264045715
train gradient:  0.3372738580702719
iteration : 2781
train acc:  0.7421875
train loss:  0.45123517513275146
train gradient:  0.5330853319948904
iteration : 2782
train acc:  0.875
train loss:  0.3472168445587158
train gradient:  0.1872473642334003
iteration : 2783
train acc:  0.84375
train loss:  0.3843560516834259
train gradient:  0.29456500991350654
iteration : 2784
train acc:  0.8515625
train loss:  0.3524795472621918
train gradient:  0.20907507318292745
iteration : 2785
train acc:  0.828125
train loss:  0.39890816807746887
train gradient:  0.3439422790254613
iteration : 2786
train acc:  0.84375
train loss:  0.35758131742477417
train gradient:  0.29623503068121715
iteration : 2787
train acc:  0.8359375
train loss:  0.4033803343772888
train gradient:  0.4073855705481872
iteration : 2788
train acc:  0.90625
train loss:  0.29234039783477783
train gradient:  0.17033854497874495
iteration : 2789
train acc:  0.9140625
train loss:  0.2552865147590637
train gradient:  0.2162548574625116
iteration : 2790
train acc:  0.8515625
train loss:  0.4074629545211792
train gradient:  0.2880907592835989
iteration : 2791
train acc:  0.8203125
train loss:  0.37590694427490234
train gradient:  0.34981758097103405
iteration : 2792
train acc:  0.8125
train loss:  0.37964582443237305
train gradient:  0.3138770620051266
iteration : 2793
train acc:  0.84375
train loss:  0.36255794763565063
train gradient:  0.35583424808947056
iteration : 2794
train acc:  0.8046875
train loss:  0.41069430112838745
train gradient:  0.35606507129846504
iteration : 2795
train acc:  0.8125
train loss:  0.34767282009124756
train gradient:  0.30406369027569585
iteration : 2796
train acc:  0.8203125
train loss:  0.44457221031188965
train gradient:  0.46999893401538373
iteration : 2797
train acc:  0.8515625
train loss:  0.3229626417160034
train gradient:  0.33401885699518596
iteration : 2798
train acc:  0.828125
train loss:  0.3126433789730072
train gradient:  0.3533178091584991
iteration : 2799
train acc:  0.8671875
train loss:  0.35105353593826294
train gradient:  0.3444475710735143
iteration : 2800
train acc:  0.796875
train loss:  0.39645424485206604
train gradient:  0.3110402669518066
iteration : 2801
train acc:  0.8671875
train loss:  0.32207298278808594
train gradient:  0.2718555633271176
iteration : 2802
train acc:  0.8203125
train loss:  0.44249051809310913
train gradient:  0.4145157145884533
iteration : 2803
train acc:  0.7890625
train loss:  0.3716374337673187
train gradient:  0.2941157504083644
iteration : 2804
train acc:  0.875
train loss:  0.3251223564147949
train gradient:  0.27451817752871455
iteration : 2805
train acc:  0.8515625
train loss:  0.3788575530052185
train gradient:  0.40655270054814535
iteration : 2806
train acc:  0.7890625
train loss:  0.4145689010620117
train gradient:  0.572702109777815
iteration : 2807
train acc:  0.8359375
train loss:  0.3646201193332672
train gradient:  0.2913161906596242
iteration : 2808
train acc:  0.8203125
train loss:  0.38909584283828735
train gradient:  0.40242993782248415
iteration : 2809
train acc:  0.8046875
train loss:  0.4611518085002899
train gradient:  0.5517131211276773
iteration : 2810
train acc:  0.8046875
train loss:  0.42782190442085266
train gradient:  0.5577060962464762
iteration : 2811
train acc:  0.7890625
train loss:  0.5108898878097534
train gradient:  0.5013864216894014
iteration : 2812
train acc:  0.8203125
train loss:  0.4021525979042053
train gradient:  0.39359543025569493
iteration : 2813
train acc:  0.84375
train loss:  0.4019041061401367
train gradient:  0.5154697330140644
iteration : 2814
train acc:  0.8828125
train loss:  0.32023805379867554
train gradient:  0.20485744487721985
iteration : 2815
train acc:  0.765625
train loss:  0.45296168327331543
train gradient:  0.4499422055103993
iteration : 2816
train acc:  0.8046875
train loss:  0.4079468250274658
train gradient:  0.3189571090002591
iteration : 2817
train acc:  0.8203125
train loss:  0.4652356803417206
train gradient:  0.4751006637706788
iteration : 2818
train acc:  0.8359375
train loss:  0.3703778386116028
train gradient:  0.2706615167053413
iteration : 2819
train acc:  0.7890625
train loss:  0.37913328409194946
train gradient:  0.3007030877010891
iteration : 2820
train acc:  0.828125
train loss:  0.4165623188018799
train gradient:  0.33579405432690584
iteration : 2821
train acc:  0.7578125
train loss:  0.44338634610176086
train gradient:  0.4026607586597759
iteration : 2822
train acc:  0.828125
train loss:  0.3377825617790222
train gradient:  0.3145008382924754
iteration : 2823
train acc:  0.8359375
train loss:  0.34467196464538574
train gradient:  0.2895914170213579
iteration : 2824
train acc:  0.8046875
train loss:  0.4207334518432617
train gradient:  0.3684650356927697
iteration : 2825
train acc:  0.8203125
train loss:  0.436188668012619
train gradient:  0.4493258470660301
iteration : 2826
train acc:  0.8125
train loss:  0.4340395927429199
train gradient:  0.5531225598443053
iteration : 2827
train acc:  0.8125
train loss:  0.3819767236709595
train gradient:  0.5168240611694019
iteration : 2828
train acc:  0.890625
train loss:  0.36554789543151855
train gradient:  0.3678441885008433
iteration : 2829
train acc:  0.8046875
train loss:  0.47053369879722595
train gradient:  0.3846514572001425
iteration : 2830
train acc:  0.8046875
train loss:  0.3875228762626648
train gradient:  0.41820441399082714
iteration : 2831
train acc:  0.8515625
train loss:  0.3567821681499481
train gradient:  0.2499541245283195
iteration : 2832
train acc:  0.8125
train loss:  0.44100067019462585
train gradient:  0.4094678174378187
iteration : 2833
train acc:  0.8359375
train loss:  0.34757110476493835
train gradient:  0.488371282927141
iteration : 2834
train acc:  0.84375
train loss:  0.34774160385131836
train gradient:  0.26093858872479436
iteration : 2835
train acc:  0.84375
train loss:  0.407964825630188
train gradient:  0.34770780515234023
iteration : 2836
train acc:  0.8203125
train loss:  0.34783700108528137
train gradient:  0.25182283269177563
iteration : 2837
train acc:  0.8515625
train loss:  0.3441343307495117
train gradient:  0.26113268656591476
iteration : 2838
train acc:  0.8125
train loss:  0.3589318096637726
train gradient:  0.27894544719308517
iteration : 2839
train acc:  0.8359375
train loss:  0.37700632214546204
train gradient:  0.25008045426218706
iteration : 2840
train acc:  0.84375
train loss:  0.35616666078567505
train gradient:  0.20852480301482135
iteration : 2841
train acc:  0.828125
train loss:  0.4364454746246338
train gradient:  0.3842771067193964
iteration : 2842
train acc:  0.8359375
train loss:  0.38080528378486633
train gradient:  0.40255766075920457
iteration : 2843
train acc:  0.7890625
train loss:  0.43861377239227295
train gradient:  0.27331180448603853
iteration : 2844
train acc:  0.84375
train loss:  0.35642093420028687
train gradient:  0.297108690352951
iteration : 2845
train acc:  0.8359375
train loss:  0.3736427128314972
train gradient:  0.31576657985931356
iteration : 2846
train acc:  0.765625
train loss:  0.42259448766708374
train gradient:  0.4106826617130317
iteration : 2847
train acc:  0.8359375
train loss:  0.37468066811561584
train gradient:  0.35258455528657073
iteration : 2848
train acc:  0.796875
train loss:  0.3863510489463806
train gradient:  0.29405979618109224
iteration : 2849
train acc:  0.7734375
train loss:  0.40955185890197754
train gradient:  0.39448620422686054
iteration : 2850
train acc:  0.859375
train loss:  0.32969948649406433
train gradient:  0.24168736840619232
iteration : 2851
train acc:  0.84375
train loss:  0.29903972148895264
train gradient:  0.24824080348139094
iteration : 2852
train acc:  0.828125
train loss:  0.355537474155426
train gradient:  0.29653930103823367
iteration : 2853
train acc:  0.796875
train loss:  0.4384784698486328
train gradient:  0.39670231564223307
iteration : 2854
train acc:  0.8046875
train loss:  0.4335659146308899
train gradient:  0.4356204570434563
iteration : 2855
train acc:  0.8125
train loss:  0.38125017285346985
train gradient:  0.34584441397029203
iteration : 2856
train acc:  0.828125
train loss:  0.4144914746284485
train gradient:  2.742727019989159
iteration : 2857
train acc:  0.84375
train loss:  0.3510168790817261
train gradient:  0.25546860965541873
iteration : 2858
train acc:  0.7578125
train loss:  0.4897983968257904
train gradient:  0.447391701954956
iteration : 2859
train acc:  0.84375
train loss:  0.3709915280342102
train gradient:  0.3126724170375632
iteration : 2860
train acc:  0.8359375
train loss:  0.44669094681739807
train gradient:  0.5265589032996197
iteration : 2861
train acc:  0.8203125
train loss:  0.35816919803619385
train gradient:  0.33535309259427337
iteration : 2862
train acc:  0.8203125
train loss:  0.36960098147392273
train gradient:  0.27921815997529875
iteration : 2863
train acc:  0.8359375
train loss:  0.3696538805961609
train gradient:  0.3471326653606189
iteration : 2864
train acc:  0.7734375
train loss:  0.4020804762840271
train gradient:  0.3755436047602682
iteration : 2865
train acc:  0.8515625
train loss:  0.4012303650379181
train gradient:  0.329909558480108
iteration : 2866
train acc:  0.859375
train loss:  0.34623968601226807
train gradient:  0.3151224112800319
iteration : 2867
train acc:  0.796875
train loss:  0.4464569091796875
train gradient:  0.4753173684317421
iteration : 2868
train acc:  0.8515625
train loss:  0.36479681730270386
train gradient:  0.31288029954789603
iteration : 2869
train acc:  0.8046875
train loss:  0.40712738037109375
train gradient:  0.3918471512144245
iteration : 2870
train acc:  0.8203125
train loss:  0.4080773591995239
train gradient:  0.34290957941015576
iteration : 2871
train acc:  0.8046875
train loss:  0.4200819432735443
train gradient:  0.3882466361578514
iteration : 2872
train acc:  0.765625
train loss:  0.5173447132110596
train gradient:  0.74552741948731
iteration : 2873
train acc:  0.8671875
train loss:  0.3386147618293762
train gradient:  0.31212660615459104
iteration : 2874
train acc:  0.8125
train loss:  0.43736904859542847
train gradient:  0.4725824803938562
iteration : 2875
train acc:  0.8515625
train loss:  0.3798883259296417
train gradient:  0.43954283073825634
iteration : 2876
train acc:  0.8515625
train loss:  0.3807290196418762
train gradient:  0.3095088884187672
iteration : 2877
train acc:  0.8046875
train loss:  0.42266106605529785
train gradient:  0.5217326317048503
iteration : 2878
train acc:  0.875
train loss:  0.3663750886917114
train gradient:  0.3056524899907202
iteration : 2879
train acc:  0.8515625
train loss:  0.36691057682037354
train gradient:  0.2787287963866734
iteration : 2880
train acc:  0.875
train loss:  0.32812759280204773
train gradient:  0.3199310052909287
iteration : 2881
train acc:  0.828125
train loss:  0.3923649787902832
train gradient:  0.2892740197459247
iteration : 2882
train acc:  0.84375
train loss:  0.40312591195106506
train gradient:  0.27435764472016505
iteration : 2883
train acc:  0.8671875
train loss:  0.318803608417511
train gradient:  0.26573065566107434
iteration : 2884
train acc:  0.8125
train loss:  0.37079936265945435
train gradient:  0.23295588591829527
iteration : 2885
train acc:  0.8359375
train loss:  0.36679017543792725
train gradient:  0.2567327648612162
iteration : 2886
train acc:  0.8046875
train loss:  0.4171673655509949
train gradient:  0.28591408661747375
iteration : 2887
train acc:  0.859375
train loss:  0.2938995957374573
train gradient:  0.18547646416123725
iteration : 2888
train acc:  0.8359375
train loss:  0.3838494122028351
train gradient:  0.3457904194671696
iteration : 2889
train acc:  0.828125
train loss:  0.4281962215900421
train gradient:  0.30783888190429193
iteration : 2890
train acc:  0.828125
train loss:  0.3480866551399231
train gradient:  0.28331708271562767
iteration : 2891
train acc:  0.8359375
train loss:  0.39134883880615234
train gradient:  0.34904678311713266
iteration : 2892
train acc:  0.8828125
train loss:  0.2923625707626343
train gradient:  0.20622361619889173
iteration : 2893
train acc:  0.8203125
train loss:  0.3614739179611206
train gradient:  0.3307907718662717
iteration : 2894
train acc:  0.8828125
train loss:  0.3120846748352051
train gradient:  0.24756646547887717
iteration : 2895
train acc:  0.8359375
train loss:  0.3734532594680786
train gradient:  0.35984564684285575
iteration : 2896
train acc:  0.84375
train loss:  0.3655313551425934
train gradient:  0.2600377484540796
iteration : 2897
train acc:  0.859375
train loss:  0.33765068650245667
train gradient:  0.3548451788904579
iteration : 2898
train acc:  0.8671875
train loss:  0.3220430016517639
train gradient:  0.3007209076133773
iteration : 2899
train acc:  0.765625
train loss:  0.39288413524627686
train gradient:  0.30235023543982664
iteration : 2900
train acc:  0.765625
train loss:  0.4967574179172516
train gradient:  0.45739479402790106
iteration : 2901
train acc:  0.8046875
train loss:  0.3966060280799866
train gradient:  0.32380143352687474
iteration : 2902
train acc:  0.875
train loss:  0.3211999535560608
train gradient:  0.26167214520342036
iteration : 2903
train acc:  0.796875
train loss:  0.42150020599365234
train gradient:  0.47657763576117984
iteration : 2904
train acc:  0.8125
train loss:  0.35439056158065796
train gradient:  0.24800973511942176
iteration : 2905
train acc:  0.8203125
train loss:  0.3561161160469055
train gradient:  0.23004257557066254
iteration : 2906
train acc:  0.765625
train loss:  0.40231871604919434
train gradient:  0.30339154490917336
iteration : 2907
train acc:  0.8671875
train loss:  0.32283249497413635
train gradient:  0.22362170738325626
iteration : 2908
train acc:  0.84375
train loss:  0.3969630002975464
train gradient:  0.35093875878212943
iteration : 2909
train acc:  0.8671875
train loss:  0.3017391860485077
train gradient:  0.2769832954609338
iteration : 2910
train acc:  0.8671875
train loss:  0.33336538076400757
train gradient:  0.3409951955904679
iteration : 2911
train acc:  0.7890625
train loss:  0.38664448261260986
train gradient:  0.36847127818656294
iteration : 2912
train acc:  0.7890625
train loss:  0.41435641050338745
train gradient:  0.4076006897770892
iteration : 2913
train acc:  0.84375
train loss:  0.3200731873512268
train gradient:  0.33651368094552897
iteration : 2914
train acc:  0.828125
train loss:  0.32463258504867554
train gradient:  0.2663406666621053
iteration : 2915
train acc:  0.84375
train loss:  0.388597309589386
train gradient:  0.26275734277215734
iteration : 2916
train acc:  0.859375
train loss:  0.3898499608039856
train gradient:  0.36058257497059687
iteration : 2917
train acc:  0.8046875
train loss:  0.4252612590789795
train gradient:  0.34746280786009914
iteration : 2918
train acc:  0.8515625
train loss:  0.37148240208625793
train gradient:  0.29720740214798713
iteration : 2919
train acc:  0.8046875
train loss:  0.47831791639328003
train gradient:  0.39314516175472464
iteration : 2920
train acc:  0.8671875
train loss:  0.3370041847229004
train gradient:  0.2241002749142955
iteration : 2921
train acc:  0.828125
train loss:  0.4067017436027527
train gradient:  0.28193952001997435
iteration : 2922
train acc:  0.8515625
train loss:  0.3012639880180359
train gradient:  0.21790701263337353
iteration : 2923
train acc:  0.7890625
train loss:  0.44719117879867554
train gradient:  0.3595052373435857
iteration : 2924
train acc:  0.8359375
train loss:  0.37164974212646484
train gradient:  0.4472561041198142
iteration : 2925
train acc:  0.828125
train loss:  0.3635457158088684
train gradient:  0.3605430624118649
iteration : 2926
train acc:  0.859375
train loss:  0.34082335233688354
train gradient:  0.29289828602735213
iteration : 2927
train acc:  0.8515625
train loss:  0.34461578726768494
train gradient:  0.3219269417960666
iteration : 2928
train acc:  0.78125
train loss:  0.4625100791454315
train gradient:  0.4104821486601735
iteration : 2929
train acc:  0.8984375
train loss:  0.2791597247123718
train gradient:  0.19619986460459063
iteration : 2930
train acc:  0.828125
train loss:  0.3511006832122803
train gradient:  0.2863673799491978
iteration : 2931
train acc:  0.8125
train loss:  0.3715537190437317
train gradient:  0.2984728438545498
iteration : 2932
train acc:  0.7578125
train loss:  0.4709373116493225
train gradient:  0.5569183974626486
iteration : 2933
train acc:  0.8203125
train loss:  0.36169904470443726
train gradient:  0.31936548160638784
iteration : 2934
train acc:  0.8046875
train loss:  0.37062737345695496
train gradient:  0.3460922640274396
iteration : 2935
train acc:  0.8125
train loss:  0.40141481161117554
train gradient:  0.3779635388681329
iteration : 2936
train acc:  0.78125
train loss:  0.3969613015651703
train gradient:  0.34220184557467465
iteration : 2937
train acc:  0.8203125
train loss:  0.4323415458202362
train gradient:  0.3440532414237219
iteration : 2938
train acc:  0.8046875
train loss:  0.4014248847961426
train gradient:  0.39478029156489414
iteration : 2939
train acc:  0.8203125
train loss:  0.3393051028251648
train gradient:  0.32370443446830405
iteration : 2940
train acc:  0.8359375
train loss:  0.36851322650909424
train gradient:  0.2934810866080364
iteration : 2941
train acc:  0.7734375
train loss:  0.48092013597488403
train gradient:  0.4538255900776208
iteration : 2942
train acc:  0.8046875
train loss:  0.4098905324935913
train gradient:  0.42287412845374867
iteration : 2943
train acc:  0.8515625
train loss:  0.3601697087287903
train gradient:  0.37227449649362215
iteration : 2944
train acc:  0.8515625
train loss:  0.3614356517791748
train gradient:  0.3143822543955793
iteration : 2945
train acc:  0.7890625
train loss:  0.4324137568473816
train gradient:  0.5391716991502864
iteration : 2946
train acc:  0.8984375
train loss:  0.26841527223587036
train gradient:  0.18539490050459728
iteration : 2947
train acc:  0.8828125
train loss:  0.33065909147262573
train gradient:  0.38929250934105875
iteration : 2948
train acc:  0.8046875
train loss:  0.41743922233581543
train gradient:  0.34458936465169904
iteration : 2949
train acc:  0.8515625
train loss:  0.3483361005783081
train gradient:  0.33902747632249564
iteration : 2950
train acc:  0.828125
train loss:  0.4123595356941223
train gradient:  0.3720428063983324
iteration : 2951
train acc:  0.7890625
train loss:  0.42605355381965637
train gradient:  0.44798935026771863
iteration : 2952
train acc:  0.859375
train loss:  0.3234705924987793
train gradient:  0.2565440064148667
iteration : 2953
train acc:  0.8515625
train loss:  0.40117529034614563
train gradient:  0.4590997210357834
iteration : 2954
train acc:  0.859375
train loss:  0.33492279052734375
train gradient:  0.27781521636036033
iteration : 2955
train acc:  0.8671875
train loss:  0.32537466287612915
train gradient:  0.2729313616289278
iteration : 2956
train acc:  0.875
train loss:  0.3466917872428894
train gradient:  0.26681397545071833
iteration : 2957
train acc:  0.859375
train loss:  0.35000818967819214
train gradient:  0.2771986700471782
iteration : 2958
train acc:  0.78125
train loss:  0.5180193185806274
train gradient:  0.5494629254979044
iteration : 2959
train acc:  0.8359375
train loss:  0.39430251717567444
train gradient:  0.44651559940888597
iteration : 2960
train acc:  0.7890625
train loss:  0.42850613594055176
train gradient:  0.4190070969482327
iteration : 2961
train acc:  0.8359375
train loss:  0.420503169298172
train gradient:  0.372792319243955
iteration : 2962
train acc:  0.859375
train loss:  0.3398749828338623
train gradient:  0.25126559406216215
iteration : 2963
train acc:  0.859375
train loss:  0.3520769476890564
train gradient:  0.24357992903517908
iteration : 2964
train acc:  0.859375
train loss:  0.31581228971481323
train gradient:  0.19149120857370341
iteration : 2965
train acc:  0.828125
train loss:  0.3233219385147095
train gradient:  0.24498687137011987
iteration : 2966
train acc:  0.8203125
train loss:  0.3649013638496399
train gradient:  0.36218981651878435
iteration : 2967
train acc:  0.78125
train loss:  0.43891656398773193
train gradient:  0.452331447713473
iteration : 2968
train acc:  0.859375
train loss:  0.4246259927749634
train gradient:  0.36737223476642267
iteration : 2969
train acc:  0.890625
train loss:  0.35670143365859985
train gradient:  0.2890583585991725
iteration : 2970
train acc:  0.8203125
train loss:  0.3689701557159424
train gradient:  0.3009695035159004
iteration : 2971
train acc:  0.859375
train loss:  0.34321415424346924
train gradient:  0.351667265155065
iteration : 2972
train acc:  0.796875
train loss:  0.42428505420684814
train gradient:  0.49329875190352973
iteration : 2973
train acc:  0.8671875
train loss:  0.2958294451236725
train gradient:  0.26612858285178415
iteration : 2974
train acc:  0.8203125
train loss:  0.4201670289039612
train gradient:  0.3691789552390033
iteration : 2975
train acc:  0.8515625
train loss:  0.34216034412384033
train gradient:  0.3413697344126312
iteration : 2976
train acc:  0.828125
train loss:  0.3865482807159424
train gradient:  0.3210338580883228
iteration : 2977
train acc:  0.8515625
train loss:  0.336895614862442
train gradient:  0.32141312667572564
iteration : 2978
train acc:  0.8671875
train loss:  0.3628387153148651
train gradient:  0.26379078080017787
iteration : 2979
train acc:  0.7890625
train loss:  0.417410284280777
train gradient:  0.36036219529578345
iteration : 2980
train acc:  0.828125
train loss:  0.3679239749908447
train gradient:  0.3096706713907686
iteration : 2981
train acc:  0.84375
train loss:  0.3811447024345398
train gradient:  0.4587403408874714
iteration : 2982
train acc:  0.8671875
train loss:  0.3476722836494446
train gradient:  0.2779011661912058
iteration : 2983
train acc:  0.859375
train loss:  0.36159461736679077
train gradient:  0.24659279214045296
iteration : 2984
train acc:  0.8203125
train loss:  0.3942054510116577
train gradient:  0.438490891453038
iteration : 2985
train acc:  0.8515625
train loss:  0.3911997377872467
train gradient:  0.3025090299640125
iteration : 2986
train acc:  0.8828125
train loss:  0.3390745520591736
train gradient:  0.26024305632169287
iteration : 2987
train acc:  0.890625
train loss:  0.3065413236618042
train gradient:  0.21772955451838136
iteration : 2988
train acc:  0.8359375
train loss:  0.3809943199157715
train gradient:  0.3136122141007751
iteration : 2989
train acc:  0.75
train loss:  0.4876159429550171
train gradient:  0.5288812076719522
iteration : 2990
train acc:  0.8125
train loss:  0.4031140208244324
train gradient:  0.40805174020151336
iteration : 2991
train acc:  0.8203125
train loss:  0.3683941066265106
train gradient:  0.393018968097166
iteration : 2992
train acc:  0.796875
train loss:  0.37985414266586304
train gradient:  0.4210537622996285
iteration : 2993
train acc:  0.828125
train loss:  0.37730371952056885
train gradient:  0.22095841521101822
iteration : 2994
train acc:  0.8984375
train loss:  0.26057231426239014
train gradient:  0.19427446207535587
iteration : 2995
train acc:  0.7890625
train loss:  0.4286317527294159
train gradient:  0.45216806222134176
iteration : 2996
train acc:  0.828125
train loss:  0.3931523561477661
train gradient:  0.5260705383684441
iteration : 2997
train acc:  0.8515625
train loss:  0.3472326397895813
train gradient:  0.2790216780422879
iteration : 2998
train acc:  0.828125
train loss:  0.4155229926109314
train gradient:  0.40728714114590414
iteration : 2999
train acc:  0.8359375
train loss:  0.3937098979949951
train gradient:  0.33566076394695743
iteration : 3000
train acc:  0.8203125
train loss:  0.35343432426452637
train gradient:  0.3789553798381756
iteration : 3001
train acc:  0.8203125
train loss:  0.39510393142700195
train gradient:  0.40132680825247785
iteration : 3002
train acc:  0.8046875
train loss:  0.41617563366889954
train gradient:  0.34651278064521046
iteration : 3003
train acc:  0.8125
train loss:  0.3883492648601532
train gradient:  0.43317336539099505
iteration : 3004
train acc:  0.8515625
train loss:  0.3496578335762024
train gradient:  0.37187729945337994
iteration : 3005
train acc:  0.78125
train loss:  0.5057693719863892
train gradient:  0.5364433922020146
iteration : 3006
train acc:  0.875
train loss:  0.31026962399482727
train gradient:  0.39167688587896204
iteration : 3007
train acc:  0.828125
train loss:  0.3785467743873596
train gradient:  0.35671787728434984
iteration : 3008
train acc:  0.8203125
train loss:  0.4148726761341095
train gradient:  0.3863251686790546
iteration : 3009
train acc:  0.8046875
train loss:  0.4671725630760193
train gradient:  0.3836179352203361
iteration : 3010
train acc:  0.7890625
train loss:  0.4519958198070526
train gradient:  0.3965444902877739
iteration : 3011
train acc:  0.8203125
train loss:  0.3920668959617615
train gradient:  0.37630308497867565
iteration : 3012
train acc:  0.84375
train loss:  0.3336467146873474
train gradient:  0.23192372667787373
iteration : 3013
train acc:  0.8671875
train loss:  0.34597185254096985
train gradient:  0.4000816631919123
iteration : 3014
train acc:  0.84375
train loss:  0.3763415217399597
train gradient:  0.7173906557593355
iteration : 3015
train acc:  0.8203125
train loss:  0.4466556906700134
train gradient:  0.3516880762917604
iteration : 3016
train acc:  0.84375
train loss:  0.34563952684402466
train gradient:  0.3931339492804303
iteration : 3017
train acc:  0.8203125
train loss:  0.38983821868896484
train gradient:  0.2812508190766135
iteration : 3018
train acc:  0.8125
train loss:  0.3312406539916992
train gradient:  0.5558590719234611
iteration : 3019
train acc:  0.859375
train loss:  0.31405404210090637
train gradient:  0.19378779218649156
iteration : 3020
train acc:  0.8046875
train loss:  0.40318623185157776
train gradient:  0.39818221179201935
iteration : 3021
train acc:  0.7734375
train loss:  0.4563477635383606
train gradient:  0.5758586640813752
iteration : 3022
train acc:  0.84375
train loss:  0.31831586360931396
train gradient:  0.2242301259935258
iteration : 3023
train acc:  0.859375
train loss:  0.36904993653297424
train gradient:  0.3191994869282278
iteration : 3024
train acc:  0.84375
train loss:  0.3674393892288208
train gradient:  0.40651903026269315
iteration : 3025
train acc:  0.7890625
train loss:  0.45518094301223755
train gradient:  0.6598260895878338
iteration : 3026
train acc:  0.84375
train loss:  0.33188751339912415
train gradient:  0.293970559946693
iteration : 3027
train acc:  0.796875
train loss:  0.4228876829147339
train gradient:  0.36354440994489995
iteration : 3028
train acc:  0.7890625
train loss:  0.42147666215896606
train gradient:  0.4348426193459508
iteration : 3029
train acc:  0.8515625
train loss:  0.3054356575012207
train gradient:  0.1862260577906067
iteration : 3030
train acc:  0.859375
train loss:  0.3622020483016968
train gradient:  0.2894494817841635
iteration : 3031
train acc:  0.859375
train loss:  0.3370997905731201
train gradient:  0.33171806548858235
iteration : 3032
train acc:  0.8125
train loss:  0.44496285915374756
train gradient:  0.4996619381059235
iteration : 3033
train acc:  0.84375
train loss:  0.4158446192741394
train gradient:  0.4198813916327591
iteration : 3034
train acc:  0.828125
train loss:  0.3682413697242737
train gradient:  0.2965290203714113
iteration : 3035
train acc:  0.75
train loss:  0.5183881521224976
train gradient:  0.5341492600280815
iteration : 3036
train acc:  0.8515625
train loss:  0.4207835793495178
train gradient:  0.3324588747104606
iteration : 3037
train acc:  0.84375
train loss:  0.3616045415401459
train gradient:  0.19949322347053233
iteration : 3038
train acc:  0.8671875
train loss:  0.33944544196128845
train gradient:  0.2630744855599162
iteration : 3039
train acc:  0.875
train loss:  0.29516899585723877
train gradient:  0.2251989533722837
iteration : 3040
train acc:  0.8828125
train loss:  0.3194195032119751
train gradient:  0.3631779543804207
iteration : 3041
train acc:  0.828125
train loss:  0.3609685003757477
train gradient:  0.47215996109672254
iteration : 3042
train acc:  0.8046875
train loss:  0.48788541555404663
train gradient:  0.35715745159091716
iteration : 3043
train acc:  0.8359375
train loss:  0.40664607286453247
train gradient:  0.3262471302052003
iteration : 3044
train acc:  0.8359375
train loss:  0.36311501264572144
train gradient:  0.24211633776819402
iteration : 3045
train acc:  0.84375
train loss:  0.3804418444633484
train gradient:  0.353441459173675
iteration : 3046
train acc:  0.8359375
train loss:  0.3604998290538788
train gradient:  0.19863999587124662
iteration : 3047
train acc:  0.8203125
train loss:  0.35158535838127136
train gradient:  0.33186710154331267
iteration : 3048
train acc:  0.8828125
train loss:  0.29795408248901367
train gradient:  0.2617895953174386
iteration : 3049
train acc:  0.8515625
train loss:  0.3225458264350891
train gradient:  0.24017701818022
iteration : 3050
train acc:  0.8515625
train loss:  0.3210473358631134
train gradient:  0.24323782928884397
iteration : 3051
train acc:  0.8515625
train loss:  0.3277534544467926
train gradient:  0.2514614967429807
iteration : 3052
train acc:  0.7890625
train loss:  0.5177223086357117
train gradient:  0.6901178397086842
iteration : 3053
train acc:  0.78125
train loss:  0.4372674822807312
train gradient:  0.5995924021005844
iteration : 3054
train acc:  0.84375
train loss:  0.3729572296142578
train gradient:  0.28584017670473855
iteration : 3055
train acc:  0.8828125
train loss:  0.28250083327293396
train gradient:  0.23437118722225161
iteration : 3056
train acc:  0.828125
train loss:  0.3805621266365051
train gradient:  0.28355695577515655
iteration : 3057
train acc:  0.8984375
train loss:  0.3667714595794678
train gradient:  0.3635293861964161
iteration : 3058
train acc:  0.8515625
train loss:  0.37489068508148193
train gradient:  0.3187912956109951
iteration : 3059
train acc:  0.8828125
train loss:  0.3398665189743042
train gradient:  0.30038514942965294
iteration : 3060
train acc:  0.8203125
train loss:  0.39832761883735657
train gradient:  0.35931985486576684
iteration : 3061
train acc:  0.8046875
train loss:  0.4128614664077759
train gradient:  0.368544906531824
iteration : 3062
train acc:  0.84375
train loss:  0.3987218737602234
train gradient:  0.30470150150391145
iteration : 3063
train acc:  0.8828125
train loss:  0.28054821491241455
train gradient:  0.2582522725036372
iteration : 3064
train acc:  0.8125
train loss:  0.38237589597702026
train gradient:  0.3183454693265624
iteration : 3065
train acc:  0.8203125
train loss:  0.34384214878082275
train gradient:  0.311520736911164
iteration : 3066
train acc:  0.8515625
train loss:  0.38952216506004333
train gradient:  0.26658619294639824
iteration : 3067
train acc:  0.859375
train loss:  0.3251897990703583
train gradient:  0.23374589514134292
iteration : 3068
train acc:  0.828125
train loss:  0.39289775490760803
train gradient:  0.4536856596867606
iteration : 3069
train acc:  0.8515625
train loss:  0.330565869808197
train gradient:  0.2687829474608016
iteration : 3070
train acc:  0.8515625
train loss:  0.34651458263397217
train gradient:  0.3241298770749504
iteration : 3071
train acc:  0.8359375
train loss:  0.35519224405288696
train gradient:  0.21913858593488916
iteration : 3072
train acc:  0.8515625
train loss:  0.37211382389068604
train gradient:  0.30053717742037134
iteration : 3073
train acc:  0.8671875
train loss:  0.32068318128585815
train gradient:  0.2694972742163389
iteration : 3074
train acc:  0.7578125
train loss:  0.48407599329948425
train gradient:  0.5682744409561409
iteration : 3075
train acc:  0.84375
train loss:  0.3559291362762451
train gradient:  0.28560990945682174
iteration : 3076
train acc:  0.8671875
train loss:  0.3607942461967468
train gradient:  0.3783388726484266
iteration : 3077
train acc:  0.8359375
train loss:  0.37673741579055786
train gradient:  0.2974909321090318
iteration : 3078
train acc:  0.7890625
train loss:  0.40880435705184937
train gradient:  0.4247025526753566
iteration : 3079
train acc:  0.8515625
train loss:  0.384056031703949
train gradient:  0.23582554622493368
iteration : 3080
train acc:  0.859375
train loss:  0.3566873073577881
train gradient:  0.32709888737861653
iteration : 3081
train acc:  0.8828125
train loss:  0.29782089591026306
train gradient:  0.16749294466930625
iteration : 3082
train acc:  0.8125
train loss:  0.38592079281806946
train gradient:  0.4023472840518863
iteration : 3083
train acc:  0.8046875
train loss:  0.38909775018692017
train gradient:  0.4176702750268682
iteration : 3084
train acc:  0.8359375
train loss:  0.32231438159942627
train gradient:  0.31711228725448753
iteration : 3085
train acc:  0.8203125
train loss:  0.3972984552383423
train gradient:  0.38110760321653087
iteration : 3086
train acc:  0.84375
train loss:  0.3720548450946808
train gradient:  0.32595434876839857
iteration : 3087
train acc:  0.8359375
train loss:  0.3733382225036621
train gradient:  0.4244607887562783
iteration : 3088
train acc:  0.8359375
train loss:  0.3150649070739746
train gradient:  0.26077638784715684
iteration : 3089
train acc:  0.859375
train loss:  0.32115358114242554
train gradient:  0.3185084700932649
iteration : 3090
train acc:  0.8359375
train loss:  0.3910371661186218
train gradient:  0.3186285018378935
iteration : 3091
train acc:  0.8671875
train loss:  0.34911811351776123
train gradient:  0.2737145377562587
iteration : 3092
train acc:  0.8359375
train loss:  0.35853785276412964
train gradient:  0.33532383318183967
iteration : 3093
train acc:  0.7734375
train loss:  0.4612404406070709
train gradient:  0.6376494251930669
iteration : 3094
train acc:  0.8359375
train loss:  0.4145902395248413
train gradient:  0.30762267569085006
iteration : 3095
train acc:  0.8515625
train loss:  0.3549696207046509
train gradient:  0.2839076911258278
iteration : 3096
train acc:  0.7734375
train loss:  0.4187436103820801
train gradient:  0.5187598564293388
iteration : 3097
train acc:  0.796875
train loss:  0.4106234312057495
train gradient:  0.3083647364208473
iteration : 3098
train acc:  0.8203125
train loss:  0.3870648443698883
train gradient:  0.27823146426088574
iteration : 3099
train acc:  0.8125
train loss:  0.41928181052207947
train gradient:  0.35099055754672304
iteration : 3100
train acc:  0.796875
train loss:  0.44029828906059265
train gradient:  0.47472604275349367
iteration : 3101
train acc:  0.796875
train loss:  0.38365256786346436
train gradient:  0.34142633946781553
iteration : 3102
train acc:  0.8515625
train loss:  0.34610438346862793
train gradient:  0.32930533950435814
iteration : 3103
train acc:  0.8203125
train loss:  0.42345893383026123
train gradient:  0.34267401954282023
iteration : 3104
train acc:  0.796875
train loss:  0.4449344277381897
train gradient:  0.3102939640764562
iteration : 3105
train acc:  0.8046875
train loss:  0.4058241844177246
train gradient:  0.3881597084577851
iteration : 3106
train acc:  0.828125
train loss:  0.3498767614364624
train gradient:  0.4424107014424306
iteration : 3107
train acc:  0.7890625
train loss:  0.3701454997062683
train gradient:  0.38241124110550734
iteration : 3108
train acc:  0.859375
train loss:  0.3274921178817749
train gradient:  0.38502704615581385
iteration : 3109
train acc:  0.8828125
train loss:  0.309050977230072
train gradient:  0.1855661656890695
iteration : 3110
train acc:  0.796875
train loss:  0.42477935552597046
train gradient:  0.413149349761209
iteration : 3111
train acc:  0.8828125
train loss:  0.3443338871002197
train gradient:  0.3093694204304178
iteration : 3112
train acc:  0.859375
train loss:  0.333607017993927
train gradient:  0.26928454991438405
iteration : 3113
train acc:  0.8359375
train loss:  0.36190927028656006
train gradient:  0.2955268024258216
iteration : 3114
train acc:  0.8515625
train loss:  0.3496423065662384
train gradient:  0.2653744808828003
iteration : 3115
train acc:  0.9140625
train loss:  0.30595868825912476
train gradient:  0.24444998133099033
iteration : 3116
train acc:  0.8203125
train loss:  0.40233123302459717
train gradient:  0.27656742581464755
iteration : 3117
train acc:  0.7890625
train loss:  0.4060787558555603
train gradient:  0.28031454782368853
iteration : 3118
train acc:  0.8046875
train loss:  0.4205359220504761
train gradient:  0.3815645570714235
iteration : 3119
train acc:  0.8359375
train loss:  0.3355327248573303
train gradient:  0.24172870982904296
iteration : 3120
train acc:  0.796875
train loss:  0.44941580295562744
train gradient:  0.5257827246519735
iteration : 3121
train acc:  0.7890625
train loss:  0.43252575397491455
train gradient:  0.35391032126038857
iteration : 3122
train acc:  0.796875
train loss:  0.4281207323074341
train gradient:  0.2863151632145528
iteration : 3123
train acc:  0.859375
train loss:  0.3151453137397766
train gradient:  0.254556125373993
iteration : 3124
train acc:  0.859375
train loss:  0.30483290553092957
train gradient:  0.22886302412155154
iteration : 3125
train acc:  0.7890625
train loss:  0.409697026014328
train gradient:  0.279616344365986
iteration : 3126
train acc:  0.7578125
train loss:  0.45924288034439087
train gradient:  0.3804521279846764
iteration : 3127
train acc:  0.8046875
train loss:  0.44507455825805664
train gradient:  0.36578519218662003
iteration : 3128
train acc:  0.8515625
train loss:  0.35063838958740234
train gradient:  0.426026020229971
iteration : 3129
train acc:  0.875
train loss:  0.2861561179161072
train gradient:  0.22272543766581207
iteration : 3130
train acc:  0.890625
train loss:  0.26087477803230286
train gradient:  0.19549788371103147
iteration : 3131
train acc:  0.84375
train loss:  0.3364085555076599
train gradient:  0.2705437815622856
iteration : 3132
train acc:  0.796875
train loss:  0.4250822067260742
train gradient:  0.2817795326811455
iteration : 3133
train acc:  0.8359375
train loss:  0.36729180812835693
train gradient:  0.3220414823659419
iteration : 3134
train acc:  0.7890625
train loss:  0.42244118452072144
train gradient:  0.38606531938436467
iteration : 3135
train acc:  0.796875
train loss:  0.4127197861671448
train gradient:  0.2996418826581866
iteration : 3136
train acc:  0.8984375
train loss:  0.32868248224258423
train gradient:  0.1973574165129512
iteration : 3137
train acc:  0.8125
train loss:  0.3754517436027527
train gradient:  0.35115578376649637
iteration : 3138
train acc:  0.859375
train loss:  0.3411703109741211
train gradient:  0.23540936534295467
iteration : 3139
train acc:  0.8515625
train loss:  0.37998971343040466
train gradient:  0.2964986188058903
iteration : 3140
train acc:  0.8125
train loss:  0.4273681640625
train gradient:  0.4503044503168444
iteration : 3141
train acc:  0.828125
train loss:  0.37068691849708557
train gradient:  0.3421454233078429
iteration : 3142
train acc:  0.8515625
train loss:  0.2978816628456116
train gradient:  0.31096214928785276
iteration : 3143
train acc:  0.828125
train loss:  0.41529279947280884
train gradient:  0.39901670867865735
iteration : 3144
train acc:  0.8671875
train loss:  0.32541030645370483
train gradient:  0.2617116903454826
iteration : 3145
train acc:  0.84375
train loss:  0.37412118911743164
train gradient:  0.3007083038354984
iteration : 3146
train acc:  0.8515625
train loss:  0.36004871129989624
train gradient:  0.26689410971730615
iteration : 3147
train acc:  0.8046875
train loss:  0.38558438420295715
train gradient:  0.31148470641219134
iteration : 3148
train acc:  0.828125
train loss:  0.33790743350982666
train gradient:  0.27241542408702657
iteration : 3149
train acc:  0.8828125
train loss:  0.30231061577796936
train gradient:  0.2300637819663937
iteration : 3150
train acc:  0.75
train loss:  0.6072983145713806
train gradient:  0.790768145461128
iteration : 3151
train acc:  0.8359375
train loss:  0.3950786292552948
train gradient:  0.2885675880773705
iteration : 3152
train acc:  0.7890625
train loss:  0.39541947841644287
train gradient:  0.6014511562067277
iteration : 3153
train acc:  0.8671875
train loss:  0.35601475834846497
train gradient:  0.2666097769106714
iteration : 3154
train acc:  0.765625
train loss:  0.4839403033256531
train gradient:  0.5560071651764695
iteration : 3155
train acc:  0.8671875
train loss:  0.3308621048927307
train gradient:  0.2595969621914331
iteration : 3156
train acc:  0.75
train loss:  0.4593765139579773
train gradient:  0.49086731679024764
iteration : 3157
train acc:  0.859375
train loss:  0.3154956102371216
train gradient:  0.21661938229577415
iteration : 3158
train acc:  0.8515625
train loss:  0.34683847427368164
train gradient:  0.34161750339602165
iteration : 3159
train acc:  0.8359375
train loss:  0.360849529504776
train gradient:  0.31974675015446374
iteration : 3160
train acc:  0.7734375
train loss:  0.37786200642585754
train gradient:  0.33906206350327334
iteration : 3161
train acc:  0.8671875
train loss:  0.30559754371643066
train gradient:  0.16653571444857673
iteration : 3162
train acc:  0.828125
train loss:  0.38453397154808044
train gradient:  0.46375771344276284
iteration : 3163
train acc:  0.8515625
train loss:  0.3034624755382538
train gradient:  0.21693369389577846
iteration : 3164
train acc:  0.8203125
train loss:  0.3821573257446289
train gradient:  0.28294523518777004
iteration : 3165
train acc:  0.8359375
train loss:  0.3775233030319214
train gradient:  0.3858624501322109
iteration : 3166
train acc:  0.8515625
train loss:  0.31171631813049316
train gradient:  0.2012434784837288
iteration : 3167
train acc:  0.7890625
train loss:  0.5279150009155273
train gradient:  0.502263881019121
iteration : 3168
train acc:  0.8203125
train loss:  0.3565525412559509
train gradient:  0.3307610106483788
iteration : 3169
train acc:  0.84375
train loss:  0.3639345169067383
train gradient:  0.25098795983759403
iteration : 3170
train acc:  0.828125
train loss:  0.3822477161884308
train gradient:  0.25992525660052407
iteration : 3171
train acc:  0.8125
train loss:  0.4422871470451355
train gradient:  0.5432309616865798
iteration : 3172
train acc:  0.890625
train loss:  0.298275887966156
train gradient:  0.2664731226821594
iteration : 3173
train acc:  0.890625
train loss:  0.32664138078689575
train gradient:  0.3597658535821653
iteration : 3174
train acc:  0.828125
train loss:  0.4147545099258423
train gradient:  0.32352033803126
iteration : 3175
train acc:  0.8125
train loss:  0.3658527731895447
train gradient:  0.2701942262489495
iteration : 3176
train acc:  0.84375
train loss:  0.36750417947769165
train gradient:  0.35074547360423614
iteration : 3177
train acc:  0.84375
train loss:  0.33393827080726624
train gradient:  0.24223580998609612
iteration : 3178
train acc:  0.84375
train loss:  0.350820928812027
train gradient:  0.27503562605188225
iteration : 3179
train acc:  0.8125
train loss:  0.4328952729701996
train gradient:  0.3832802960108343
iteration : 3180
train acc:  0.796875
train loss:  0.40241605043411255
train gradient:  0.3150831226033421
iteration : 3181
train acc:  0.84375
train loss:  0.354566752910614
train gradient:  0.5253394622096386
iteration : 3182
train acc:  0.8515625
train loss:  0.3618146777153015
train gradient:  0.28441662686700675
iteration : 3183
train acc:  0.8203125
train loss:  0.4377566874027252
train gradient:  0.5428969373576872
iteration : 3184
train acc:  0.84375
train loss:  0.3804094195365906
train gradient:  0.2964542867441524
iteration : 3185
train acc:  0.828125
train loss:  0.402119517326355
train gradient:  0.559223418763525
iteration : 3186
train acc:  0.8203125
train loss:  0.363878458738327
train gradient:  0.278378955256622
iteration : 3187
train acc:  0.8515625
train loss:  0.34000736474990845
train gradient:  0.266245100275333
iteration : 3188
train acc:  0.84375
train loss:  0.3424830436706543
train gradient:  0.2958263413907772
iteration : 3189
train acc:  0.8046875
train loss:  0.4324052333831787
train gradient:  0.3703451784507049
iteration : 3190
train acc:  0.78125
train loss:  0.4671175181865692
train gradient:  0.4587907690147809
iteration : 3191
train acc:  0.9140625
train loss:  0.2647501826286316
train gradient:  0.2053509786146846
iteration : 3192
train acc:  0.84375
train loss:  0.37348634004592896
train gradient:  0.3718290266037174
iteration : 3193
train acc:  0.8359375
train loss:  0.37692123651504517
train gradient:  0.3495129273951966
iteration : 3194
train acc:  0.796875
train loss:  0.4094886779785156
train gradient:  0.4312176856833915
iteration : 3195
train acc:  0.8359375
train loss:  0.345337837934494
train gradient:  0.21463759625863985
iteration : 3196
train acc:  0.828125
train loss:  0.39007431268692017
train gradient:  0.3674946418025871
iteration : 3197
train acc:  0.90625
train loss:  0.30392593145370483
train gradient:  0.21379854014888733
iteration : 3198
train acc:  0.78125
train loss:  0.4923161268234253
train gradient:  0.49564379673927667
iteration : 3199
train acc:  0.78125
train loss:  0.46329063177108765
train gradient:  0.5838173597124261
iteration : 3200
train acc:  0.8359375
train loss:  0.3658350110054016
train gradient:  0.31363231311770073
iteration : 3201
train acc:  0.8046875
train loss:  0.4276587665081024
train gradient:  0.2877197441781377
iteration : 3202
train acc:  0.7578125
train loss:  0.44462743401527405
train gradient:  0.42128108061438363
iteration : 3203
train acc:  0.8515625
train loss:  0.3572830259799957
train gradient:  0.3165467228292304
iteration : 3204
train acc:  0.8359375
train loss:  0.3667735457420349
train gradient:  0.28650374401652334
iteration : 3205
train acc:  0.8125
train loss:  0.3432363271713257
train gradient:  0.3319299691045529
iteration : 3206
train acc:  0.828125
train loss:  0.38118696212768555
train gradient:  0.27808472178403587
iteration : 3207
train acc:  0.8125
train loss:  0.3517954349517822
train gradient:  0.3524725359416009
iteration : 3208
train acc:  0.8125
train loss:  0.3925623297691345
train gradient:  0.3557717584927521
iteration : 3209
train acc:  0.8515625
train loss:  0.3358980417251587
train gradient:  0.27236213629916534
iteration : 3210
train acc:  0.8359375
train loss:  0.3573119342327118
train gradient:  0.23322341585096196
iteration : 3211
train acc:  0.8359375
train loss:  0.4297310709953308
train gradient:  0.3098228844886212
iteration : 3212
train acc:  0.7890625
train loss:  0.43607664108276367
train gradient:  0.3915721101623638
iteration : 3213
train acc:  0.78125
train loss:  0.43768176436424255
train gradient:  0.332303419196496
iteration : 3214
train acc:  0.7890625
train loss:  0.43475639820098877
train gradient:  0.3932481229078489
iteration : 3215
train acc:  0.8515625
train loss:  0.31714725494384766
train gradient:  0.2276335823170616
iteration : 3216
train acc:  0.84375
train loss:  0.35156679153442383
train gradient:  0.26403166277044965
iteration : 3217
train acc:  0.8515625
train loss:  0.34689709544181824
train gradient:  0.2119149282801876
iteration : 3218
train acc:  0.8828125
train loss:  0.37483465671539307
train gradient:  0.45597450963083996
iteration : 3219
train acc:  0.8203125
train loss:  0.3409510552883148
train gradient:  0.3900252389800749
iteration : 3220
train acc:  0.875
train loss:  0.3347334861755371
train gradient:  0.2957913379448263
iteration : 3221
train acc:  0.84375
train loss:  0.3594246506690979
train gradient:  0.23371395889393576
iteration : 3222
train acc:  0.84375
train loss:  0.34608641266822815
train gradient:  0.1473849069111131
iteration : 3223
train acc:  0.8359375
train loss:  0.34210264682769775
train gradient:  0.2239699733037469
iteration : 3224
train acc:  0.7890625
train loss:  0.3819410502910614
train gradient:  0.3018533752155396
iteration : 3225
train acc:  0.890625
train loss:  0.2967662811279297
train gradient:  0.18871267590987928
iteration : 3226
train acc:  0.8515625
train loss:  0.3649969696998596
train gradient:  0.2990699157076277
iteration : 3227
train acc:  0.859375
train loss:  0.37015312910079956
train gradient:  0.2451365509034948
iteration : 3228
train acc:  0.9140625
train loss:  0.3104550540447235
train gradient:  0.18570487368873267
iteration : 3229
train acc:  0.890625
train loss:  0.29061222076416016
train gradient:  0.19270987868684697
iteration : 3230
train acc:  0.8359375
train loss:  0.3685913383960724
train gradient:  0.36944309354543103
iteration : 3231
train acc:  0.8125
train loss:  0.41050985455513
train gradient:  0.42474346809372177
iteration : 3232
train acc:  0.8046875
train loss:  0.48442763090133667
train gradient:  0.6698524429207524
iteration : 3233
train acc:  0.7890625
train loss:  0.5147707462310791
train gradient:  0.552024914340207
iteration : 3234
train acc:  0.8359375
train loss:  0.3679672181606293
train gradient:  0.27022592536123097
iteration : 3235
train acc:  0.8203125
train loss:  0.38336145877838135
train gradient:  0.4088111071314527
iteration : 3236
train acc:  0.7890625
train loss:  0.42398327589035034
train gradient:  0.2890486910281962
iteration : 3237
train acc:  0.765625
train loss:  0.42704468965530396
train gradient:  0.4360174606016804
iteration : 3238
train acc:  0.7421875
train loss:  0.4297912120819092
train gradient:  0.3526912857512983
iteration : 3239
train acc:  0.84375
train loss:  0.3852853775024414
train gradient:  0.26591070016246093
iteration : 3240
train acc:  0.7734375
train loss:  0.40829765796661377
train gradient:  0.2851698412712504
iteration : 3241
train acc:  0.828125
train loss:  0.38944458961486816
train gradient:  0.3461658503288842
iteration : 3242
train acc:  0.859375
train loss:  0.3739094138145447
train gradient:  0.33761682186872133
iteration : 3243
train acc:  0.8671875
train loss:  0.3150772452354431
train gradient:  0.26085503571848656
iteration : 3244
train acc:  0.8203125
train loss:  0.40675169229507446
train gradient:  0.43263585401418914
iteration : 3245
train acc:  0.828125
train loss:  0.3519909977912903
train gradient:  0.28422037437439424
iteration : 3246
train acc:  0.8828125
train loss:  0.32638150453567505
train gradient:  0.37882809772801274
iteration : 3247
train acc:  0.8671875
train loss:  0.34602174162864685
train gradient:  0.3776443389250472
iteration : 3248
train acc:  0.859375
train loss:  0.32781633734703064
train gradient:  0.297218356304842
iteration : 3249
train acc:  0.8203125
train loss:  0.3938254714012146
train gradient:  0.2743588077974371
iteration : 3250
train acc:  0.875
train loss:  0.3069012463092804
train gradient:  0.29101041630112234
iteration : 3251
train acc:  0.8515625
train loss:  0.35697251558303833
train gradient:  0.27110054308883386
iteration : 3252
train acc:  0.8125
train loss:  0.3729342222213745
train gradient:  0.2818699771260084
iteration : 3253
train acc:  0.7734375
train loss:  0.4414106607437134
train gradient:  0.37347250734148946
iteration : 3254
train acc:  0.8046875
train loss:  0.4324890375137329
train gradient:  0.3605878506007496
iteration : 3255
train acc:  0.8671875
train loss:  0.3050512671470642
train gradient:  0.2507029561712654
iteration : 3256
train acc:  0.7890625
train loss:  0.4040050506591797
train gradient:  0.2609090996104618
iteration : 3257
train acc:  0.8671875
train loss:  0.31840935349464417
train gradient:  0.2777900125488427
iteration : 3258
train acc:  0.8671875
train loss:  0.3089953362941742
train gradient:  0.1929943197406963
iteration : 3259
train acc:  0.875
train loss:  0.3177233636379242
train gradient:  0.23367373972084532
iteration : 3260
train acc:  0.84375
train loss:  0.4005020260810852
train gradient:  0.4467381253977304
iteration : 3261
train acc:  0.8125
train loss:  0.4238540530204773
train gradient:  0.2843281694879043
iteration : 3262
train acc:  0.796875
train loss:  0.4369674324989319
train gradient:  0.3257549164819824
iteration : 3263
train acc:  0.84375
train loss:  0.3847750425338745
train gradient:  0.31292802864450464
iteration : 3264
train acc:  0.84375
train loss:  0.3318687677383423
train gradient:  0.24541451192835725
iteration : 3265
train acc:  0.7734375
train loss:  0.5052564144134521
train gradient:  0.46128731731599243
iteration : 3266
train acc:  0.8359375
train loss:  0.38518133759498596
train gradient:  0.2581898889267741
iteration : 3267
train acc:  0.765625
train loss:  0.4584479033946991
train gradient:  0.36484449995544566
iteration : 3268
train acc:  0.8125
train loss:  0.37117186188697815
train gradient:  0.34702363430500816
iteration : 3269
train acc:  0.8125
train loss:  0.4168168008327484
train gradient:  0.2663898378868345
iteration : 3270
train acc:  0.8671875
train loss:  0.3185681104660034
train gradient:  0.22901812252016462
iteration : 3271
train acc:  0.8515625
train loss:  0.34418734908103943
train gradient:  0.2790922890259384
iteration : 3272
train acc:  0.828125
train loss:  0.3547469675540924
train gradient:  0.2905922260057618
iteration : 3273
train acc:  0.8203125
train loss:  0.43490540981292725
train gradient:  0.49540278787395553
iteration : 3274
train acc:  0.84375
train loss:  0.35549068450927734
train gradient:  0.3112770058296384
iteration : 3275
train acc:  0.8046875
train loss:  0.4537162482738495
train gradient:  0.43046523158947997
iteration : 3276
train acc:  0.8984375
train loss:  0.2733118236064911
train gradient:  0.202021838940005
iteration : 3277
train acc:  0.875
train loss:  0.33173099160194397
train gradient:  0.23357562630389817
iteration : 3278
train acc:  0.859375
train loss:  0.32612261176109314
train gradient:  0.2796801708360513
iteration : 3279
train acc:  0.8203125
train loss:  0.37161970138549805
train gradient:  0.25864387188374816
iteration : 3280
train acc:  0.7578125
train loss:  0.416646271944046
train gradient:  0.41109014993496124
iteration : 3281
train acc:  0.8203125
train loss:  0.37589383125305176
train gradient:  0.23436652214349799
iteration : 3282
train acc:  0.8515625
train loss:  0.35556188225746155
train gradient:  0.22261573075625843
iteration : 3283
train acc:  0.8203125
train loss:  0.3837445080280304
train gradient:  0.2705826937591678
iteration : 3284
train acc:  0.84375
train loss:  0.4146187901496887
train gradient:  0.275305485230254
iteration : 3285
train acc:  0.8359375
train loss:  0.378354012966156
train gradient:  0.3721166424977516
iteration : 3286
train acc:  0.796875
train loss:  0.5140676498413086
train gradient:  0.37640914190430935
iteration : 3287
train acc:  0.8125
train loss:  0.3722543716430664
train gradient:  0.31786413682058673
iteration : 3288
train acc:  0.859375
train loss:  0.317527174949646
train gradient:  0.24322909998309844
iteration : 3289
train acc:  0.8359375
train loss:  0.3960973918437958
train gradient:  0.30181057403754297
iteration : 3290
train acc:  0.8359375
train loss:  0.3460194170475006
train gradient:  0.38078549945024576
iteration : 3291
train acc:  0.875
train loss:  0.2907365560531616
train gradient:  0.23702269112640453
iteration : 3292
train acc:  0.8125
train loss:  0.44155824184417725
train gradient:  0.406680516196878
iteration : 3293
train acc:  0.84375
train loss:  0.4055090844631195
train gradient:  0.365847121120217
iteration : 3294
train acc:  0.8359375
train loss:  0.35722658038139343
train gradient:  0.24447377231937925
iteration : 3295
train acc:  0.75
train loss:  0.5079114437103271
train gradient:  0.5128132890732533
iteration : 3296
train acc:  0.78125
train loss:  0.4589928984642029
train gradient:  0.333163924472635
iteration : 3297
train acc:  0.859375
train loss:  0.36447834968566895
train gradient:  0.22906473303584557
iteration : 3298
train acc:  0.8125
train loss:  0.36329758167266846
train gradient:  0.23032413840302607
iteration : 3299
train acc:  0.828125
train loss:  0.3812462091445923
train gradient:  0.3273043609738661
iteration : 3300
train acc:  0.8203125
train loss:  0.40543124079704285
train gradient:  0.3779610301871677
iteration : 3301
train acc:  0.8203125
train loss:  0.43508923053741455
train gradient:  0.45456716798778074
iteration : 3302
train acc:  0.8125
train loss:  0.40677374601364136
train gradient:  0.3239909328151573
iteration : 3303
train acc:  0.828125
train loss:  0.3769686222076416
train gradient:  0.2450720276050381
iteration : 3304
train acc:  0.8515625
train loss:  0.3501061797142029
train gradient:  0.35016610567208417
iteration : 3305
train acc:  0.8046875
train loss:  0.3680804967880249
train gradient:  0.29050990957196043
iteration : 3306
train acc:  0.8359375
train loss:  0.3445037007331848
train gradient:  0.26725639492398284
iteration : 3307
train acc:  0.859375
train loss:  0.35161399841308594
train gradient:  0.3038764237684711
iteration : 3308
train acc:  0.8515625
train loss:  0.33503034710884094
train gradient:  0.2863021514269438
iteration : 3309
train acc:  0.875
train loss:  0.3243480920791626
train gradient:  0.28894812793730107
iteration : 3310
train acc:  0.8203125
train loss:  0.36539456248283386
train gradient:  0.21379948608377047
iteration : 3311
train acc:  0.7421875
train loss:  0.5280362367630005
train gradient:  0.501530092446393
iteration : 3312
train acc:  0.8828125
train loss:  0.3257114291191101
train gradient:  0.2829208715440517
iteration : 3313
train acc:  0.8515625
train loss:  0.3717551827430725
train gradient:  0.2782556361413055
iteration : 3314
train acc:  0.8046875
train loss:  0.4408552646636963
train gradient:  0.3218835933405949
iteration : 3315
train acc:  0.8828125
train loss:  0.2901012897491455
train gradient:  0.24611315574842346
iteration : 3316
train acc:  0.8671875
train loss:  0.3492165207862854
train gradient:  0.26822032539565205
iteration : 3317
train acc:  0.8203125
train loss:  0.36221104860305786
train gradient:  0.31902147825564137
iteration : 3318
train acc:  0.84375
train loss:  0.4362477660179138
train gradient:  0.36009950824322184
iteration : 3319
train acc:  0.8359375
train loss:  0.36730924248695374
train gradient:  0.2631063126697195
iteration : 3320
train acc:  0.84375
train loss:  0.38519006967544556
train gradient:  0.4166332926030898
iteration : 3321
train acc:  0.8515625
train loss:  0.3841816484928131
train gradient:  0.3065617665769564
iteration : 3322
train acc:  0.828125
train loss:  0.43483227491378784
train gradient:  0.3674364187002981
iteration : 3323
train acc:  0.8359375
train loss:  0.3252759575843811
train gradient:  0.23689121985085398
iteration : 3324
train acc:  0.8203125
train loss:  0.41855865716934204
train gradient:  0.25253402206214753
iteration : 3325
train acc:  0.765625
train loss:  0.39190950989723206
train gradient:  0.3786072070124742
iteration : 3326
train acc:  0.84375
train loss:  0.32231202721595764
train gradient:  0.35727914910454744
iteration : 3327
train acc:  0.84375
train loss:  0.3656170666217804
train gradient:  0.35552247699477824
iteration : 3328
train acc:  0.828125
train loss:  0.38002660870552063
train gradient:  0.3149139529472759
iteration : 3329
train acc:  0.8984375
train loss:  0.301680326461792
train gradient:  0.19318612383606706
iteration : 3330
train acc:  0.8515625
train loss:  0.3302403390407562
train gradient:  0.3498211637419919
iteration : 3331
train acc:  0.875
train loss:  0.31628260016441345
train gradient:  0.273038527817686
iteration : 3332
train acc:  0.8125
train loss:  0.34511056542396545
train gradient:  0.33783096797866696
iteration : 3333
train acc:  0.8125
train loss:  0.38001447916030884
train gradient:  0.5530634845817374
iteration : 3334
train acc:  0.8359375
train loss:  0.3981265723705292
train gradient:  0.3085534837630632
iteration : 3335
train acc:  0.828125
train loss:  0.3900116980075836
train gradient:  0.3262010399315448
iteration : 3336
train acc:  0.84375
train loss:  0.3867029547691345
train gradient:  0.3450468023216176
iteration : 3337
train acc:  0.8046875
train loss:  0.37696343660354614
train gradient:  0.28541260430123555
iteration : 3338
train acc:  0.8828125
train loss:  0.27475497126579285
train gradient:  0.20172098123718266
iteration : 3339
train acc:  0.8515625
train loss:  0.3918112516403198
train gradient:  0.5861211845504385
iteration : 3340
train acc:  0.8515625
train loss:  0.3473038673400879
train gradient:  0.30523540106114666
iteration : 3341
train acc:  0.8203125
train loss:  0.36369213461875916
train gradient:  0.28452720191898045
iteration : 3342
train acc:  0.796875
train loss:  0.39617136120796204
train gradient:  0.3101050514480706
iteration : 3343
train acc:  0.84375
train loss:  0.31348997354507446
train gradient:  0.34447597162676424
iteration : 3344
train acc:  0.8359375
train loss:  0.39220690727233887
train gradient:  0.40094689307174247
iteration : 3345
train acc:  0.8203125
train loss:  0.3579128384590149
train gradient:  0.3656403128669348
iteration : 3346
train acc:  0.8671875
train loss:  0.3082658350467682
train gradient:  0.3012611677695534
iteration : 3347
train acc:  0.84375
train loss:  0.3281896114349365
train gradient:  0.3597428948733651
iteration : 3348
train acc:  0.90625
train loss:  0.29439494013786316
train gradient:  0.2565645085368405
iteration : 3349
train acc:  0.8125
train loss:  0.40741172432899475
train gradient:  0.34897122480391496
iteration : 3350
train acc:  0.828125
train loss:  0.48388198018074036
train gradient:  0.5785420418700753
iteration : 3351
train acc:  0.84375
train loss:  0.35360002517700195
train gradient:  0.3253152731607439
iteration : 3352
train acc:  0.8203125
train loss:  0.38186389207839966
train gradient:  0.30545589502834697
iteration : 3353
train acc:  0.8359375
train loss:  0.3571162223815918
train gradient:  0.2644473184676368
iteration : 3354
train acc:  0.8125
train loss:  0.3993418216705322
train gradient:  0.36027777944270356
iteration : 3355
train acc:  0.8671875
train loss:  0.34977036714553833
train gradient:  0.3331616841452635
iteration : 3356
train acc:  0.859375
train loss:  0.3936617970466614
train gradient:  0.41580886782293164
iteration : 3357
train acc:  0.796875
train loss:  0.4005575180053711
train gradient:  0.37153528045793954
iteration : 3358
train acc:  0.875
train loss:  0.2999383807182312
train gradient:  0.25456427136687554
iteration : 3359
train acc:  0.8125
train loss:  0.40003830194473267
train gradient:  0.47894341721571504
iteration : 3360
train acc:  0.8671875
train loss:  0.3226352334022522
train gradient:  0.28957817566303445
iteration : 3361
train acc:  0.8125
train loss:  0.3731011748313904
train gradient:  0.3937294351496037
iteration : 3362
train acc:  0.8046875
train loss:  0.4300785958766937
train gradient:  0.394938830683236
iteration : 3363
train acc:  0.84375
train loss:  0.37718525528907776
train gradient:  0.3866225652677615
iteration : 3364
train acc:  0.796875
train loss:  0.41336458921432495
train gradient:  0.3367324969613745
iteration : 3365
train acc:  0.84375
train loss:  0.404352605342865
train gradient:  0.3618272832403976
iteration : 3366
train acc:  0.828125
train loss:  0.4280005097389221
train gradient:  0.3444671774248486
iteration : 3367
train acc:  0.7734375
train loss:  0.4418278634548187
train gradient:  0.5199649236351842
iteration : 3368
train acc:  0.8203125
train loss:  0.39191290736198425
train gradient:  0.572450998633289
iteration : 3369
train acc:  0.8515625
train loss:  0.3395543694496155
train gradient:  0.34705044339223856
iteration : 3370
train acc:  0.8359375
train loss:  0.3948651850223541
train gradient:  0.27523552223897624
iteration : 3371
train acc:  0.78125
train loss:  0.5040481090545654
train gradient:  0.5302695487771437
iteration : 3372
train acc:  0.8125
train loss:  0.3832560181617737
train gradient:  0.41526294161336874
iteration : 3373
train acc:  0.828125
train loss:  0.3609457015991211
train gradient:  0.3276934680033832
iteration : 3374
train acc:  0.8515625
train loss:  0.3297703266143799
train gradient:  0.23162865351439751
iteration : 3375
train acc:  0.8359375
train loss:  0.36809372901916504
train gradient:  0.34348363912883595
iteration : 3376
train acc:  0.8515625
train loss:  0.36447465419769287
train gradient:  0.3267573625068171
iteration : 3377
train acc:  0.796875
train loss:  0.42379626631736755
train gradient:  0.3820548021109511
iteration : 3378
train acc:  0.890625
train loss:  0.34249168634414673
train gradient:  0.24632317821218674
iteration : 3379
train acc:  0.890625
train loss:  0.3193584382534027
train gradient:  0.22214020960354575
iteration : 3380
train acc:  0.8671875
train loss:  0.35657787322998047
train gradient:  0.30683393000790765
iteration : 3381
train acc:  0.875
train loss:  0.3047023415565491
train gradient:  0.22540020797326898
iteration : 3382
train acc:  0.890625
train loss:  0.3040039539337158
train gradient:  0.22778047711939953
iteration : 3383
train acc:  0.7890625
train loss:  0.4296257495880127
train gradient:  0.3333724138273965
iteration : 3384
train acc:  0.8359375
train loss:  0.3350909352302551
train gradient:  0.40734737807540033
iteration : 3385
train acc:  0.84375
train loss:  0.3410918712615967
train gradient:  0.2720301172441075
iteration : 3386
train acc:  0.859375
train loss:  0.3480450510978699
train gradient:  0.21686956585995187
iteration : 3387
train acc:  0.828125
train loss:  0.3292173743247986
train gradient:  0.29348261590384916
iteration : 3388
train acc:  0.84375
train loss:  0.3655775487422943
train gradient:  0.36867723716248063
iteration : 3389
train acc:  0.7734375
train loss:  0.5058407187461853
train gradient:  0.44362835781590165
iteration : 3390
train acc:  0.8203125
train loss:  0.4342063069343567
train gradient:  0.42419679753857237
iteration : 3391
train acc:  0.8828125
train loss:  0.329103022813797
train gradient:  0.33606909957407555
iteration : 3392
train acc:  0.8046875
train loss:  0.42812657356262207
train gradient:  0.4793077510821621
iteration : 3393
train acc:  0.8359375
train loss:  0.38907021284103394
train gradient:  0.29361283991947085
iteration : 3394
train acc:  0.875
train loss:  0.3087248206138611
train gradient:  0.2353741980263448
iteration : 3395
train acc:  0.7890625
train loss:  0.4588216245174408
train gradient:  0.49901944936697434
iteration : 3396
train acc:  0.796875
train loss:  0.3813607692718506
train gradient:  0.4121759341138742
iteration : 3397
train acc:  0.859375
train loss:  0.3120754659175873
train gradient:  0.27812083961722833
iteration : 3398
train acc:  0.8515625
train loss:  0.3294757604598999
train gradient:  0.20658670124406334
iteration : 3399
train acc:  0.890625
train loss:  0.2971150875091553
train gradient:  0.38847302014444074
iteration : 3400
train acc:  0.828125
train loss:  0.39252969622612
train gradient:  0.2637581176621725
iteration : 3401
train acc:  0.765625
train loss:  0.4028235673904419
train gradient:  0.5049754821634718
iteration : 3402
train acc:  0.8046875
train loss:  0.4281984269618988
train gradient:  0.46875217300605126
iteration : 3403
train acc:  0.8046875
train loss:  0.3869926631450653
train gradient:  0.39216102390805635
iteration : 3404
train acc:  0.8984375
train loss:  0.27906668186187744
train gradient:  0.1793921251168339
iteration : 3405
train acc:  0.828125
train loss:  0.4673992097377777
train gradient:  0.5389928841696023
iteration : 3406
train acc:  0.828125
train loss:  0.3410024046897888
train gradient:  0.44322028298502153
iteration : 3407
train acc:  0.8203125
train loss:  0.3521019220352173
train gradient:  0.3843402739402333
iteration : 3408
train acc:  0.8515625
train loss:  0.3782876133918762
train gradient:  0.2767972780477823
iteration : 3409
train acc:  0.8671875
train loss:  0.33961692452430725
train gradient:  0.3349150528795954
iteration : 3410
train acc:  0.875
train loss:  0.3564549386501312
train gradient:  0.4225352234211061
iteration : 3411
train acc:  0.8125
train loss:  0.4017294943332672
train gradient:  0.38498929718635183
iteration : 3412
train acc:  0.859375
train loss:  0.3253452777862549
train gradient:  0.3345645260669839
iteration : 3413
train acc:  0.8203125
train loss:  0.4189133942127228
train gradient:  0.5440994981207512
iteration : 3414
train acc:  0.828125
train loss:  0.34427472949028015
train gradient:  0.1932883007693884
iteration : 3415
train acc:  0.796875
train loss:  0.4416847229003906
train gradient:  0.5267541186134012
iteration : 3416
train acc:  0.828125
train loss:  0.4348614513874054
train gradient:  0.45474595638819626
iteration : 3417
train acc:  0.828125
train loss:  0.39726561307907104
train gradient:  0.4919924592046195
iteration : 3418
train acc:  0.84375
train loss:  0.4084779620170593
train gradient:  0.4173081561365185
iteration : 3419
train acc:  0.8203125
train loss:  0.3805950880050659
train gradient:  0.2995015207565707
iteration : 3420
train acc:  0.8046875
train loss:  0.4273058772087097
train gradient:  0.6246384215188956
iteration : 3421
train acc:  0.8203125
train loss:  0.37319833040237427
train gradient:  0.3138005175948806
iteration : 3422
train acc:  0.8203125
train loss:  0.4001413881778717
train gradient:  0.343017795457907
iteration : 3423
train acc:  0.78125
train loss:  0.5119448900222778
train gradient:  0.6372259471153583
iteration : 3424
train acc:  0.8359375
train loss:  0.404848575592041
train gradient:  0.39444224103804004
iteration : 3425
train acc:  0.8359375
train loss:  0.4811609983444214
train gradient:  0.420503145202908
iteration : 3426
train acc:  0.796875
train loss:  0.40462592244148254
train gradient:  0.3710242936452777
iteration : 3427
train acc:  0.8125
train loss:  0.45345503091812134
train gradient:  0.4131110279865977
iteration : 3428
train acc:  0.796875
train loss:  0.4877576529979706
train gradient:  0.6001976943079556
iteration : 3429
train acc:  0.8671875
train loss:  0.36226046085357666
train gradient:  0.3334229736722143
iteration : 3430
train acc:  0.8203125
train loss:  0.3754933476448059
train gradient:  0.3135788988740011
iteration : 3431
train acc:  0.8515625
train loss:  0.3481917381286621
train gradient:  0.357722222872204
iteration : 3432
train acc:  0.8046875
train loss:  0.40301400423049927
train gradient:  0.2894899257066589
iteration : 3433
train acc:  0.8203125
train loss:  0.3701239824295044
train gradient:  0.2969628104787092
iteration : 3434
train acc:  0.7578125
train loss:  0.4781292676925659
train gradient:  0.449332526371012
iteration : 3435
train acc:  0.8828125
train loss:  0.36818790435791016
train gradient:  0.27090671405145345
iteration : 3436
train acc:  0.8828125
train loss:  0.3368087410926819
train gradient:  0.36716458385013495
iteration : 3437
train acc:  0.859375
train loss:  0.3645220994949341
train gradient:  0.3405169311874038
iteration : 3438
train acc:  0.8828125
train loss:  0.2835768461227417
train gradient:  0.18766042906258956
iteration : 3439
train acc:  0.8359375
train loss:  0.3668900728225708
train gradient:  0.2863378948380945
iteration : 3440
train acc:  0.8125
train loss:  0.43866726756095886
train gradient:  0.4322893184583932
iteration : 3441
train acc:  0.828125
train loss:  0.36947715282440186
train gradient:  0.20217539514330468
iteration : 3442
train acc:  0.8359375
train loss:  0.35061168670654297
train gradient:  0.32096129051812905
iteration : 3443
train acc:  0.84375
train loss:  0.34400856494903564
train gradient:  0.38417344989406754
iteration : 3444
train acc:  0.8515625
train loss:  0.3528395891189575
train gradient:  0.39586789609728357
iteration : 3445
train acc:  0.8125
train loss:  0.3640964925289154
train gradient:  0.4219313503580556
iteration : 3446
train acc:  0.9140625
train loss:  0.30495044589042664
train gradient:  0.21085249401982453
iteration : 3447
train acc:  0.8359375
train loss:  0.3588526248931885
train gradient:  0.3516217258085183
iteration : 3448
train acc:  0.8515625
train loss:  0.38852912187576294
train gradient:  0.27786686353701656
iteration : 3449
train acc:  0.7890625
train loss:  0.42856258153915405
train gradient:  0.37987099430237825
iteration : 3450
train acc:  0.8515625
train loss:  0.322632372379303
train gradient:  0.23242672931640823
iteration : 3451
train acc:  0.8359375
train loss:  0.3588724732398987
train gradient:  0.22675355602399966
iteration : 3452
train acc:  0.8671875
train loss:  0.36974772810935974
train gradient:  0.25128210537810114
iteration : 3453
train acc:  0.84375
train loss:  0.38584598898887634
train gradient:  0.3657430061539682
iteration : 3454
train acc:  0.8046875
train loss:  0.3971651792526245
train gradient:  0.3312086418526479
iteration : 3455
train acc:  0.8203125
train loss:  0.40797102451324463
train gradient:  0.4309715584421923
iteration : 3456
train acc:  0.8125
train loss:  0.38371995091438293
train gradient:  0.31081537209889315
iteration : 3457
train acc:  0.7734375
train loss:  0.4755941927433014
train gradient:  0.5115307277964185
iteration : 3458
train acc:  0.859375
train loss:  0.35306304693222046
train gradient:  0.22233113994926268
iteration : 3459
train acc:  0.8046875
train loss:  0.4820597767829895
train gradient:  0.521090641479443
iteration : 3460
train acc:  0.7734375
train loss:  0.39776310324668884
train gradient:  0.3482894003775694
iteration : 3461
train acc:  0.8125
train loss:  0.4824959337711334
train gradient:  0.45810287857420584
iteration : 3462
train acc:  0.84375
train loss:  0.3421326279640198
train gradient:  0.21598622374609644
iteration : 3463
train acc:  0.828125
train loss:  0.37257814407348633
train gradient:  0.2755178124908125
iteration : 3464
train acc:  0.7890625
train loss:  0.4181445240974426
train gradient:  0.4031785969795259
iteration : 3465
train acc:  0.765625
train loss:  0.44695472717285156
train gradient:  0.40689018442605474
iteration : 3466
train acc:  0.8671875
train loss:  0.3476739823818207
train gradient:  0.2293954382397088
iteration : 3467
train acc:  0.890625
train loss:  0.2859877049922943
train gradient:  0.21672056116026744
iteration : 3468
train acc:  0.8359375
train loss:  0.35530638694763184
train gradient:  0.31676701675069396
iteration : 3469
train acc:  0.90625
train loss:  0.2671195864677429
train gradient:  0.2004358102327697
iteration : 3470
train acc:  0.84375
train loss:  0.35878679156303406
train gradient:  0.24209850165649438
iteration : 3471
train acc:  0.828125
train loss:  0.3275899887084961
train gradient:  0.24248501401755604
iteration : 3472
train acc:  0.890625
train loss:  0.2857980728149414
train gradient:  0.26285266030186893
iteration : 3473
train acc:  0.8671875
train loss:  0.3116266131401062
train gradient:  0.1821169955507148
iteration : 3474
train acc:  0.8515625
train loss:  0.3353827893733978
train gradient:  0.25200441710790594
iteration : 3475
train acc:  0.8828125
train loss:  0.36437350511550903
train gradient:  0.23320746788835206
iteration : 3476
train acc:  0.796875
train loss:  0.4191563129425049
train gradient:  0.3052785779614664
iteration : 3477
train acc:  0.8203125
train loss:  0.38952362537384033
train gradient:  0.3099549271048174
iteration : 3478
train acc:  0.84375
train loss:  0.3352651000022888
train gradient:  0.25479652801384794
iteration : 3479
train acc:  0.859375
train loss:  0.3082805275917053
train gradient:  0.2034441791854371
iteration : 3480
train acc:  0.875
train loss:  0.3044338822364807
train gradient:  0.15642341893562456
iteration : 3481
train acc:  0.84375
train loss:  0.3606184124946594
train gradient:  0.2867308970833179
iteration : 3482
train acc:  0.796875
train loss:  0.35479506850242615
train gradient:  0.4588473516590527
iteration : 3483
train acc:  0.8359375
train loss:  0.35938364267349243
train gradient:  0.29038314166599877
iteration : 3484
train acc:  0.8828125
train loss:  0.3081258535385132
train gradient:  0.2507176064487115
iteration : 3485
train acc:  0.828125
train loss:  0.3755096197128296
train gradient:  0.29137233846272687
iteration : 3486
train acc:  0.7890625
train loss:  0.39823246002197266
train gradient:  0.3240121305712203
iteration : 3487
train acc:  0.859375
train loss:  0.386701762676239
train gradient:  0.33100287047067395
iteration : 3488
train acc:  0.8046875
train loss:  0.44394242763519287
train gradient:  0.41050216159828323
iteration : 3489
train acc:  0.828125
train loss:  0.3430509865283966
train gradient:  0.28560020887329546
iteration : 3490
train acc:  0.8125
train loss:  0.3686653971672058
train gradient:  0.3364183121649657
iteration : 3491
train acc:  0.859375
train loss:  0.3815465271472931
train gradient:  0.30315267474873925
iteration : 3492
train acc:  0.78125
train loss:  0.37142300605773926
train gradient:  0.29530514637404787
iteration : 3493
train acc:  0.84375
train loss:  0.313856303691864
train gradient:  0.2434630104391951
iteration : 3494
train acc:  0.8515625
train loss:  0.3474586009979248
train gradient:  0.2789979047980118
iteration : 3495
train acc:  0.8203125
train loss:  0.3456524610519409
train gradient:  0.32019805608115587
iteration : 3496
train acc:  0.859375
train loss:  0.3511900305747986
train gradient:  0.3151716859516949
iteration : 3497
train acc:  0.8359375
train loss:  0.33608555793762207
train gradient:  0.2136039693231963
iteration : 3498
train acc:  0.8203125
train loss:  0.410905659198761
train gradient:  0.3782567354414047
iteration : 3499
train acc:  0.8125
train loss:  0.39060091972351074
train gradient:  0.42614979862415076
iteration : 3500
train acc:  0.7578125
train loss:  0.46859800815582275
train gradient:  0.62201669868776
iteration : 3501
train acc:  0.796875
train loss:  0.4034464955329895
train gradient:  0.5128004337724054
iteration : 3502
train acc:  0.859375
train loss:  0.3652098476886749
train gradient:  0.2657659256784459
iteration : 3503
train acc:  0.8203125
train loss:  0.4311116337776184
train gradient:  0.35502860556634863
iteration : 3504
train acc:  0.875
train loss:  0.30503615736961365
train gradient:  0.33916608611104837
iteration : 3505
train acc:  0.8046875
train loss:  0.3903849422931671
train gradient:  0.35828002447185453
iteration : 3506
train acc:  0.8671875
train loss:  0.3468281924724579
train gradient:  0.30222344087459585
iteration : 3507
train acc:  0.8671875
train loss:  0.319296658039093
train gradient:  0.21358405503226102
iteration : 3508
train acc:  0.8515625
train loss:  0.31869956851005554
train gradient:  0.24087926321404524
iteration : 3509
train acc:  0.8203125
train loss:  0.362617164850235
train gradient:  0.3738094107899705
iteration : 3510
train acc:  0.8203125
train loss:  0.3860431909561157
train gradient:  0.3281461659055751
iteration : 3511
train acc:  0.8359375
train loss:  0.3656434714794159
train gradient:  0.3016280209808755
iteration : 3512
train acc:  0.859375
train loss:  0.37416642904281616
train gradient:  0.2710329089388248
iteration : 3513
train acc:  0.8359375
train loss:  0.3462586998939514
train gradient:  0.3291149754479456
iteration : 3514
train acc:  0.84375
train loss:  0.3502062261104584
train gradient:  0.24499295127219192
iteration : 3515
train acc:  0.8359375
train loss:  0.3780108690261841
train gradient:  0.31772085120634497
iteration : 3516
train acc:  0.8125
train loss:  0.3621351420879364
train gradient:  0.2541015196995183
iteration : 3517
train acc:  0.8671875
train loss:  0.304993212223053
train gradient:  0.23248073647844564
iteration : 3518
train acc:  0.8359375
train loss:  0.35732215642929077
train gradient:  0.38403437281915126
iteration : 3519
train acc:  0.796875
train loss:  0.4577378034591675
train gradient:  0.5767760636901526
iteration : 3520
train acc:  0.84375
train loss:  0.37848562002182007
train gradient:  0.31379079998412407
iteration : 3521
train acc:  0.8203125
train loss:  0.43135449290275574
train gradient:  0.37908888571096694
iteration : 3522
train acc:  0.828125
train loss:  0.34155112504959106
train gradient:  0.3283467708810678
iteration : 3523
train acc:  0.8359375
train loss:  0.4049902558326721
train gradient:  0.35249669411842877
iteration : 3524
train acc:  0.8203125
train loss:  0.35082846879959106
train gradient:  0.22739577799209626
iteration : 3525
train acc:  0.8046875
train loss:  0.3829237222671509
train gradient:  0.31069307022915793
iteration : 3526
train acc:  0.78125
train loss:  0.45572328567504883
train gradient:  0.5965375988967998
iteration : 3527
train acc:  0.7734375
train loss:  0.4024031162261963
train gradient:  0.41894183381577244
iteration : 3528
train acc:  0.84375
train loss:  0.3619252145290375
train gradient:  0.3695854521652062
iteration : 3529
train acc:  0.7734375
train loss:  0.44699275493621826
train gradient:  0.3398743972874483
iteration : 3530
train acc:  0.828125
train loss:  0.42474374175071716
train gradient:  0.3968293984262691
iteration : 3531
train acc:  0.8203125
train loss:  0.3797987103462219
train gradient:  0.34675938621402413
iteration : 3532
train acc:  0.8515625
train loss:  0.3208266496658325
train gradient:  0.29002515622323094
iteration : 3533
train acc:  0.8828125
train loss:  0.26790398359298706
train gradient:  0.16602115832528988
iteration : 3534
train acc:  0.8671875
train loss:  0.30670708417892456
train gradient:  0.22301098723757168
iteration : 3535
train acc:  0.8984375
train loss:  0.25859615206718445
train gradient:  0.18091248634631338
iteration : 3536
train acc:  0.8515625
train loss:  0.37347692251205444
train gradient:  0.2738029503074925
iteration : 3537
train acc:  0.859375
train loss:  0.3137819766998291
train gradient:  0.20155083106926835
iteration : 3538
train acc:  0.8671875
train loss:  0.26038509607315063
train gradient:  0.20452820572739436
iteration : 3539
train acc:  0.8359375
train loss:  0.36848533153533936
train gradient:  0.24704562432578558
iteration : 3540
train acc:  0.875
train loss:  0.3281022310256958
train gradient:  0.2340581300854958
iteration : 3541
train acc:  0.828125
train loss:  0.41801226139068604
train gradient:  0.33572017522239345
iteration : 3542
train acc:  0.7890625
train loss:  0.41537460684776306
train gradient:  0.35314084779868826
iteration : 3543
train acc:  0.875
train loss:  0.3312072157859802
train gradient:  0.2210547824827674
iteration : 3544
train acc:  0.84375
train loss:  0.3348509669303894
train gradient:  0.24576441029949064
iteration : 3545
train acc:  0.84375
train loss:  0.36271506547927856
train gradient:  0.21371022169359383
iteration : 3546
train acc:  0.8515625
train loss:  0.3581063449382782
train gradient:  0.24110683476882544
iteration : 3547
train acc:  0.8671875
train loss:  0.30727618932724
train gradient:  0.18828013653390402
iteration : 3548
train acc:  0.8828125
train loss:  0.293687105178833
train gradient:  0.2351148872750946
iteration : 3549
train acc:  0.8671875
train loss:  0.35460662841796875
train gradient:  0.26405238423484984
iteration : 3550
train acc:  0.8359375
train loss:  0.3279516100883484
train gradient:  0.32760513107639444
iteration : 3551
train acc:  0.828125
train loss:  0.4091818928718567
train gradient:  0.33305099366967045
iteration : 3552
train acc:  0.8203125
train loss:  0.35872822999954224
train gradient:  0.34712664152874545
iteration : 3553
train acc:  0.796875
train loss:  0.3626255691051483
train gradient:  0.2845969341820671
iteration : 3554
train acc:  0.828125
train loss:  0.3670329749584198
train gradient:  0.29146711393996816
iteration : 3555
train acc:  0.796875
train loss:  0.40836817026138306
train gradient:  0.3425009688923032
iteration : 3556
train acc:  0.859375
train loss:  0.36388999223709106
train gradient:  0.40367761087316784
iteration : 3557
train acc:  0.8515625
train loss:  0.34017518162727356
train gradient:  0.29433379634513546
iteration : 3558
train acc:  0.78125
train loss:  0.49582159519195557
train gradient:  0.4474809707219354
iteration : 3559
train acc:  0.8359375
train loss:  0.3865453600883484
train gradient:  0.24276734882306572
iteration : 3560
train acc:  0.8359375
train loss:  0.4058474898338318
train gradient:  0.42357998794821766
iteration : 3561
train acc:  0.8125
train loss:  0.4565829634666443
train gradient:  0.45781585142266634
iteration : 3562
train acc:  0.8125
train loss:  0.40850120782852173
train gradient:  0.32464853625564005
iteration : 3563
train acc:  0.8203125
train loss:  0.3932168483734131
train gradient:  0.29841821624988
iteration : 3564
train acc:  0.8671875
train loss:  0.29828280210494995
train gradient:  0.22692906566981028
iteration : 3565
train acc:  0.8359375
train loss:  0.3761787712574005
train gradient:  0.4739776106999272
iteration : 3566
train acc:  0.8359375
train loss:  0.31408435106277466
train gradient:  0.29272242752642885
iteration : 3567
train acc:  0.84375
train loss:  0.36541563272476196
train gradient:  0.29017846344453124
iteration : 3568
train acc:  0.84375
train loss:  0.37940335273742676
train gradient:  0.3482357343780249
iteration : 3569
train acc:  0.8359375
train loss:  0.42233747243881226
train gradient:  0.37044252264673005
iteration : 3570
train acc:  0.8671875
train loss:  0.32771456241607666
train gradient:  0.21722910932149533
iteration : 3571
train acc:  0.8203125
train loss:  0.45359939336776733
train gradient:  0.3148683765365056
iteration : 3572
train acc:  0.828125
train loss:  0.41109228134155273
train gradient:  0.2858294265977556
iteration : 3573
train acc:  0.8359375
train loss:  0.3514854609966278
train gradient:  0.3336128450310288
iteration : 3574
train acc:  0.8671875
train loss:  0.38285911083221436
train gradient:  0.34775507632868347
iteration : 3575
train acc:  0.8515625
train loss:  0.35446375608444214
train gradient:  0.2787023589576674
iteration : 3576
train acc:  0.8515625
train loss:  0.34269675612449646
train gradient:  0.34208784052423935
iteration : 3577
train acc:  0.84375
train loss:  0.3610861003398895
train gradient:  0.32277562147595545
iteration : 3578
train acc:  0.859375
train loss:  0.3527120351791382
train gradient:  0.22536415554148967
iteration : 3579
train acc:  0.8046875
train loss:  0.4026676416397095
train gradient:  0.3802906999815678
iteration : 3580
train acc:  0.8671875
train loss:  0.35450446605682373
train gradient:  0.2276973297881308
iteration : 3581
train acc:  0.875
train loss:  0.2929954528808594
train gradient:  0.17884178367736342
iteration : 3582
train acc:  0.796875
train loss:  0.3446583151817322
train gradient:  0.27674877908486095
iteration : 3583
train acc:  0.8671875
train loss:  0.3234350085258484
train gradient:  0.23068440421723027
iteration : 3584
train acc:  0.8203125
train loss:  0.37863728404045105
train gradient:  0.29595504356357727
iteration : 3585
train acc:  0.8359375
train loss:  0.3961886167526245
train gradient:  0.36276727256771596
iteration : 3586
train acc:  0.8359375
train loss:  0.41035574674606323
train gradient:  0.33529785336742
iteration : 3587
train acc:  0.828125
train loss:  0.36868101358413696
train gradient:  0.3370836491204691
iteration : 3588
train acc:  0.84375
train loss:  0.3429722487926483
train gradient:  0.41136956957652704
iteration : 3589
train acc:  0.84375
train loss:  0.3722456693649292
train gradient:  0.25171189012469997
iteration : 3590
train acc:  0.8203125
train loss:  0.32236653566360474
train gradient:  0.25140562389562093
iteration : 3591
train acc:  0.859375
train loss:  0.3389129638671875
train gradient:  0.197598495202711
iteration : 3592
train acc:  0.796875
train loss:  0.40185046195983887
train gradient:  0.6012346361614322
iteration : 3593
train acc:  0.828125
train loss:  0.39246824383735657
train gradient:  0.38036862231724616
iteration : 3594
train acc:  0.8671875
train loss:  0.3373682498931885
train gradient:  0.29580182272537736
iteration : 3595
train acc:  0.9140625
train loss:  0.25588738918304443
train gradient:  0.1927360579878049
iteration : 3596
train acc:  0.875
train loss:  0.34060990810394287
train gradient:  0.2500892279917134
iteration : 3597
train acc:  0.8671875
train loss:  0.34414124488830566
train gradient:  0.32040047260184107
iteration : 3598
train acc:  0.8125
train loss:  0.41857418417930603
train gradient:  0.33321713347626647
iteration : 3599
train acc:  0.859375
train loss:  0.3610842227935791
train gradient:  0.4349501413800644
iteration : 3600
train acc:  0.84375
train loss:  0.3891453146934509
train gradient:  0.412387688377524
iteration : 3601
train acc:  0.8125
train loss:  0.41063404083251953
train gradient:  0.34093439005587567
iteration : 3602
train acc:  0.8359375
train loss:  0.34845733642578125
train gradient:  0.31232269853074923
iteration : 3603
train acc:  0.8828125
train loss:  0.31916898488998413
train gradient:  0.2900494112219419
iteration : 3604
train acc:  0.8203125
train loss:  0.39581984281539917
train gradient:  0.3073813294406321
iteration : 3605
train acc:  0.8515625
train loss:  0.3820182681083679
train gradient:  0.4025179492786455
iteration : 3606
train acc:  0.875
train loss:  0.33221763372421265
train gradient:  0.3162109662631278
iteration : 3607
train acc:  0.84375
train loss:  0.362813800573349
train gradient:  0.3104243377003148
iteration : 3608
train acc:  0.8203125
train loss:  0.35901540517807007
train gradient:  0.2119826547554466
iteration : 3609
train acc:  0.8125
train loss:  0.4550821781158447
train gradient:  0.47554253500613414
iteration : 3610
train acc:  0.8203125
train loss:  0.3678740859031677
train gradient:  0.32406992387965905
iteration : 3611
train acc:  0.796875
train loss:  0.40853893756866455
train gradient:  0.44471044251069364
iteration : 3612
train acc:  0.8125
train loss:  0.34797030687332153
train gradient:  0.3085889810051349
iteration : 3613
train acc:  0.8203125
train loss:  0.3426312804222107
train gradient:  0.41020303115519974
iteration : 3614
train acc:  0.828125
train loss:  0.36673885583877563
train gradient:  0.39699208550067805
iteration : 3615
train acc:  0.8046875
train loss:  0.39959442615509033
train gradient:  0.300261979713983
iteration : 3616
train acc:  0.8046875
train loss:  0.3670690655708313
train gradient:  0.3246411228094165
iteration : 3617
train acc:  0.875
train loss:  0.31364020705223083
train gradient:  0.25992346981636316
iteration : 3618
train acc:  0.859375
train loss:  0.3253595530986786
train gradient:  0.27930871262962875
iteration : 3619
train acc:  0.828125
train loss:  0.40327155590057373
train gradient:  0.405105375383316
iteration : 3620
train acc:  0.7890625
train loss:  0.43585771322250366
train gradient:  0.6265007643416144
iteration : 3621
train acc:  0.84375
train loss:  0.38269180059432983
train gradient:  0.34837742331355837
iteration : 3622
train acc:  0.84375
train loss:  0.31711623072624207
train gradient:  0.16872041177470798
iteration : 3623
train acc:  0.8671875
train loss:  0.3302367329597473
train gradient:  0.3208360610947788
iteration : 3624
train acc:  0.8828125
train loss:  0.28461676836013794
train gradient:  0.20353742058578292
iteration : 3625
train acc:  0.8984375
train loss:  0.2641516923904419
train gradient:  0.18397526438274342
iteration : 3626
train acc:  0.859375
train loss:  0.33860549330711365
train gradient:  0.32036830382869413
iteration : 3627
train acc:  0.7890625
train loss:  0.3936540484428406
train gradient:  0.48408148385763544
iteration : 3628
train acc:  0.796875
train loss:  0.4059733748435974
train gradient:  0.3579348450337218
iteration : 3629
train acc:  0.84375
train loss:  0.39906734228134155
train gradient:  0.3606905785750065
iteration : 3630
train acc:  0.8671875
train loss:  0.3141845762729645
train gradient:  0.24576010765711342
iteration : 3631
train acc:  0.859375
train loss:  0.3390759825706482
train gradient:  0.2553974870480154
iteration : 3632
train acc:  0.796875
train loss:  0.398729145526886
train gradient:  0.35398112122786085
iteration : 3633
train acc:  0.859375
train loss:  0.32648682594299316
train gradient:  0.2590145364007989
iteration : 3634
train acc:  0.765625
train loss:  0.47990575432777405
train gradient:  0.5791746812366774
iteration : 3635
train acc:  0.8671875
train loss:  0.3482368290424347
train gradient:  0.4604399097007959
iteration : 3636
train acc:  0.8359375
train loss:  0.36119359731674194
train gradient:  0.34953546593806833
iteration : 3637
train acc:  0.8359375
train loss:  0.33775219321250916
train gradient:  0.2520998372894248
iteration : 3638
train acc:  0.8984375
train loss:  0.31230831146240234
train gradient:  0.21175267078940455
iteration : 3639
train acc:  0.8203125
train loss:  0.36148178577423096
train gradient:  0.3268916322641745
iteration : 3640
train acc:  0.90625
train loss:  0.2938525080680847
train gradient:  0.18583357552099256
iteration : 3641
train acc:  0.859375
train loss:  0.2721866965293884
train gradient:  0.2480598958396264
iteration : 3642
train acc:  0.7890625
train loss:  0.39358586072921753
train gradient:  0.5123035186335091
iteration : 3643
train acc:  0.84375
train loss:  0.3356063961982727
train gradient:  0.3405298741959923
iteration : 3644
train acc:  0.859375
train loss:  0.3030359148979187
train gradient:  0.2501442030764315
iteration : 3645
train acc:  0.84375
train loss:  0.38852018117904663
train gradient:  0.31683593744166494
iteration : 3646
train acc:  0.84375
train loss:  0.35434067249298096
train gradient:  0.21804097113305432
iteration : 3647
train acc:  0.828125
train loss:  0.36101216077804565
train gradient:  0.40198193262435516
iteration : 3648
train acc:  0.8359375
train loss:  0.361735999584198
train gradient:  0.22980265598942928
iteration : 3649
train acc:  0.859375
train loss:  0.3641119599342346
train gradient:  0.3558719201546568
iteration : 3650
train acc:  0.8046875
train loss:  0.33848339319229126
train gradient:  0.3410543595277117
iteration : 3651
train acc:  0.828125
train loss:  0.41313326358795166
train gradient:  0.36905263397849036
iteration : 3652
train acc:  0.8125
train loss:  0.39874130487442017
train gradient:  0.531232156760904
iteration : 3653
train acc:  0.8125
train loss:  0.43981602787971497
train gradient:  0.34846858200070974
iteration : 3654
train acc:  0.828125
train loss:  0.38200223445892334
train gradient:  0.3363278045899732
iteration : 3655
train acc:  0.7890625
train loss:  0.4013175964355469
train gradient:  0.3313683376307933
iteration : 3656
train acc:  0.765625
train loss:  0.43498313426971436
train gradient:  0.31239082097892956
iteration : 3657
train acc:  0.796875
train loss:  0.4473668336868286
train gradient:  0.3977180610034314
iteration : 3658
train acc:  0.84375
train loss:  0.3673560321331024
train gradient:  0.5224251970454494
iteration : 3659
train acc:  0.84375
train loss:  0.3191268742084503
train gradient:  0.22997968519519357
iteration : 3660
train acc:  0.7890625
train loss:  0.4311230182647705
train gradient:  0.39116903329227015
iteration : 3661
train acc:  0.8828125
train loss:  0.3640803098678589
train gradient:  0.3083773254140409
iteration : 3662
train acc:  0.84375
train loss:  0.357629656791687
train gradient:  0.31705884500877696
iteration : 3663
train acc:  0.8359375
train loss:  0.33884769678115845
train gradient:  0.2809875334637
iteration : 3664
train acc:  0.796875
train loss:  0.3595185875892639
train gradient:  0.2575119770460911
iteration : 3665
train acc:  0.8125
train loss:  0.37920576333999634
train gradient:  0.41976410866809777
iteration : 3666
train acc:  0.8203125
train loss:  0.3949694037437439
train gradient:  0.4003381758411892
iteration : 3667
train acc:  0.8046875
train loss:  0.45413973927497864
train gradient:  0.4339098864426992
iteration : 3668
train acc:  0.7890625
train loss:  0.4089176654815674
train gradient:  0.31537003866472674
iteration : 3669
train acc:  0.875
train loss:  0.35044848918914795
train gradient:  0.458976259867086
iteration : 3670
train acc:  0.7890625
train loss:  0.45242956280708313
train gradient:  0.3367050011583872
iteration : 3671
train acc:  0.84375
train loss:  0.3189257085323334
train gradient:  0.24478287373337054
iteration : 3672
train acc:  0.8203125
train loss:  0.3371797204017639
train gradient:  0.2824342679476197
iteration : 3673
train acc:  0.8125
train loss:  0.4266785979270935
train gradient:  0.292651711586758
iteration : 3674
train acc:  0.796875
train loss:  0.35888391733169556
train gradient:  0.3540104349015222
iteration : 3675
train acc:  0.8359375
train loss:  0.37504589557647705
train gradient:  0.27394237322031223
iteration : 3676
train acc:  0.8359375
train loss:  0.4122074842453003
train gradient:  0.40261223366434284
iteration : 3677
train acc:  0.765625
train loss:  0.49962204694747925
train gradient:  0.5244976945972311
iteration : 3678
train acc:  0.8515625
train loss:  0.35608139634132385
train gradient:  0.3119391559351726
iteration : 3679
train acc:  0.8515625
train loss:  0.3138463497161865
train gradient:  0.19987071417938537
iteration : 3680
train acc:  0.8515625
train loss:  0.3671332001686096
train gradient:  0.24435675286330788
iteration : 3681
train acc:  0.7890625
train loss:  0.4464769959449768
train gradient:  0.4175818598189424
iteration : 3682
train acc:  0.796875
train loss:  0.40197813510894775
train gradient:  0.3869730940136418
iteration : 3683
train acc:  0.890625
train loss:  0.30901503562927246
train gradient:  0.22311430979259433
iteration : 3684
train acc:  0.8515625
train loss:  0.3304979205131531
train gradient:  0.19117388053263043
iteration : 3685
train acc:  0.859375
train loss:  0.33038318157196045
train gradient:  0.25026754102027243
iteration : 3686
train acc:  0.890625
train loss:  0.260572612285614
train gradient:  0.1869855788640754
iteration : 3687
train acc:  0.859375
train loss:  0.3321709930896759
train gradient:  0.353460951080396
iteration : 3688
train acc:  0.8515625
train loss:  0.37273144721984863
train gradient:  0.3580975553201073
iteration : 3689
train acc:  0.84375
train loss:  0.32918405532836914
train gradient:  0.2735746924249397
iteration : 3690
train acc:  0.828125
train loss:  0.36662739515304565
train gradient:  0.282183375780614
iteration : 3691
train acc:  0.84375
train loss:  0.31697243452072144
train gradient:  0.22574877090218243
iteration : 3692
train acc:  0.84375
train loss:  0.35123786330223083
train gradient:  0.32407415811410867
iteration : 3693
train acc:  0.84375
train loss:  0.3513253927230835
train gradient:  0.19671621617015886
iteration : 3694
train acc:  0.8359375
train loss:  0.3720504939556122
train gradient:  0.3145133099808937
iteration : 3695
train acc:  0.8203125
train loss:  0.37629425525665283
train gradient:  0.26909170503007185
iteration : 3696
train acc:  0.8828125
train loss:  0.3191032111644745
train gradient:  0.2162906636461339
iteration : 3697
train acc:  0.890625
train loss:  0.2935938835144043
train gradient:  0.21331144584101355
iteration : 3698
train acc:  0.828125
train loss:  0.357096791267395
train gradient:  0.35960906857824765
iteration : 3699
train acc:  0.796875
train loss:  0.4325723350048065
train gradient:  0.35947211573906335
iteration : 3700
train acc:  0.8125
train loss:  0.41184109449386597
train gradient:  0.3248546820345559
iteration : 3701
train acc:  0.921875
train loss:  0.266766756772995
train gradient:  0.19047107733023297
iteration : 3702
train acc:  0.859375
train loss:  0.3306208550930023
train gradient:  0.23122589655724418
iteration : 3703
train acc:  0.8359375
train loss:  0.3074800372123718
train gradient:  0.20404775786759474
iteration : 3704
train acc:  0.8125
train loss:  0.4166910946369171
train gradient:  0.3664301413273018
iteration : 3705
train acc:  0.8515625
train loss:  0.3840465247631073
train gradient:  0.31111782685565476
iteration : 3706
train acc:  0.8515625
train loss:  0.3388479948043823
train gradient:  0.3687078938070678
iteration : 3707
train acc:  0.8046875
train loss:  0.40633243322372437
train gradient:  0.4189077529952811
iteration : 3708
train acc:  0.890625
train loss:  0.3487626910209656
train gradient:  0.19762706906879846
iteration : 3709
train acc:  0.8359375
train loss:  0.35015326738357544
train gradient:  0.2934868041318342
iteration : 3710
train acc:  0.8125
train loss:  0.4233652949333191
train gradient:  0.3571642966713038
iteration : 3711
train acc:  0.875
train loss:  0.31967422366142273
train gradient:  0.19494901214422858
iteration : 3712
train acc:  0.765625
train loss:  0.42327064275741577
train gradient:  0.3428598910628458
iteration : 3713
train acc:  0.8984375
train loss:  0.28181207180023193
train gradient:  0.23300650776838966
iteration : 3714
train acc:  0.796875
train loss:  0.4038862884044647
train gradient:  0.27758962834268136
iteration : 3715
train acc:  0.8515625
train loss:  0.3058829605579376
train gradient:  0.16525437201843274
iteration : 3716
train acc:  0.78125
train loss:  0.42489194869995117
train gradient:  0.38577490292639766
iteration : 3717
train acc:  0.828125
train loss:  0.36515361070632935
train gradient:  0.3389886333692818
iteration : 3718
train acc:  0.8828125
train loss:  0.27632105350494385
train gradient:  0.20244057391873832
iteration : 3719
train acc:  0.796875
train loss:  0.3804685175418854
train gradient:  0.4603638264475426
iteration : 3720
train acc:  0.84375
train loss:  0.38510823249816895
train gradient:  0.30856158052557503
iteration : 3721
train acc:  0.796875
train loss:  0.36601927876472473
train gradient:  0.3468633406858585
iteration : 3722
train acc:  0.828125
train loss:  0.39677590131759644
train gradient:  0.3463424444767182
iteration : 3723
train acc:  0.859375
train loss:  0.3326256275177002
train gradient:  0.3264955685213665
iteration : 3724
train acc:  0.84375
train loss:  0.33548861742019653
train gradient:  0.27937081010788967
iteration : 3725
train acc:  0.859375
train loss:  0.3371294438838959
train gradient:  0.2612369934233278
iteration : 3726
train acc:  0.8828125
train loss:  0.25260254740715027
train gradient:  0.2297213685680456
iteration : 3727
train acc:  0.8359375
train loss:  0.37403562664985657
train gradient:  0.32524745292263824
iteration : 3728
train acc:  0.8203125
train loss:  0.3674197196960449
train gradient:  0.2675662808770853
iteration : 3729
train acc:  0.8359375
train loss:  0.327306866645813
train gradient:  0.25507275931450135
iteration : 3730
train acc:  0.8515625
train loss:  0.3000417947769165
train gradient:  0.22381996716080432
iteration : 3731
train acc:  0.8671875
train loss:  0.32481101155281067
train gradient:  0.7217158080864454
iteration : 3732
train acc:  0.90625
train loss:  0.26130563020706177
train gradient:  0.17221768459252762
iteration : 3733
train acc:  0.875
train loss:  0.31617408990859985
train gradient:  0.4025489778429144
iteration : 3734
train acc:  0.8515625
train loss:  0.3518102169036865
train gradient:  0.33901161548937603
iteration : 3735
train acc:  0.7890625
train loss:  0.41166961193084717
train gradient:  0.3938129859547487
iteration : 3736
train acc:  0.8046875
train loss:  0.44817379117012024
train gradient:  0.3989448577067954
iteration : 3737
train acc:  0.859375
train loss:  0.3400517404079437
train gradient:  0.2991852211565208
iteration : 3738
train acc:  0.8515625
train loss:  0.32811373472213745
train gradient:  0.26107191881710906
iteration : 3739
train acc:  0.84375
train loss:  0.37324875593185425
train gradient:  0.24280294875495512
iteration : 3740
train acc:  0.796875
train loss:  0.42296692728996277
train gradient:  0.35521990397156616
iteration : 3741
train acc:  0.8046875
train loss:  0.4127238988876343
train gradient:  0.4470193374213212
iteration : 3742
train acc:  0.8515625
train loss:  0.3177129626274109
train gradient:  0.2609574365570982
iteration : 3743
train acc:  0.84375
train loss:  0.37897470593452454
train gradient:  0.4678123382521022
iteration : 3744
train acc:  0.8984375
train loss:  0.3056259751319885
train gradient:  0.15284178710725324
iteration : 3745
train acc:  0.8125
train loss:  0.41677388548851013
train gradient:  0.3659856871868497
iteration : 3746
train acc:  0.7421875
train loss:  0.47455543279647827
train gradient:  0.4864818832353823
iteration : 3747
train acc:  0.8046875
train loss:  0.3647041916847229
train gradient:  0.31333779396808475
iteration : 3748
train acc:  0.78125
train loss:  0.43142274022102356
train gradient:  0.423572771225414
iteration : 3749
train acc:  0.859375
train loss:  0.3421436548233032
train gradient:  0.497639621198469
iteration : 3750
train acc:  0.7890625
train loss:  0.3898433446884155
train gradient:  0.2887303796575886
iteration : 3751
train acc:  0.828125
train loss:  0.39268648624420166
train gradient:  0.39524205499456444
iteration : 3752
train acc:  0.84375
train loss:  0.3798080086708069
train gradient:  0.324044017623181
iteration : 3753
train acc:  0.859375
train loss:  0.350142240524292
train gradient:  0.2537092605475842
iteration : 3754
train acc:  0.8671875
train loss:  0.39032670855522156
train gradient:  0.5230279473064077
iteration : 3755
train acc:  0.8046875
train loss:  0.35446274280548096
train gradient:  0.2959467256600009
iteration : 3756
train acc:  0.8671875
train loss:  0.34616193175315857
train gradient:  0.255111060616249
iteration : 3757
train acc:  0.8515625
train loss:  0.3507593274116516
train gradient:  0.3429809025286013
iteration : 3758
train acc:  0.84375
train loss:  0.3512529134750366
train gradient:  0.30630785303479163
iteration : 3759
train acc:  0.8203125
train loss:  0.40493786334991455
train gradient:  0.2978311612723956
iteration : 3760
train acc:  0.8984375
train loss:  0.28302979469299316
train gradient:  0.24783391781738653
iteration : 3761
train acc:  0.8359375
train loss:  0.43829280138015747
train gradient:  0.5528980108465513
iteration : 3762
train acc:  0.84375
train loss:  0.3658815026283264
train gradient:  0.38521746064846984
iteration : 3763
train acc:  0.84375
train loss:  0.31756436824798584
train gradient:  0.2721662633646481
iteration : 3764
train acc:  0.8203125
train loss:  0.39859163761138916
train gradient:  0.261708920989272
iteration : 3765
train acc:  0.84375
train loss:  0.40138310194015503
train gradient:  0.3596873063598622
iteration : 3766
train acc:  0.8046875
train loss:  0.45338770747184753
train gradient:  0.45297326804068205
iteration : 3767
train acc:  0.8046875
train loss:  0.3691713213920593
train gradient:  0.20339164364928022
iteration : 3768
train acc:  0.78125
train loss:  0.4639164209365845
train gradient:  0.27592714647439986
iteration : 3769
train acc:  0.8359375
train loss:  0.363067090511322
train gradient:  0.34794528959687293
iteration : 3770
train acc:  0.8046875
train loss:  0.38044965267181396
train gradient:  0.29624809308814715
iteration : 3771
train acc:  0.84375
train loss:  0.32781365513801575
train gradient:  0.20800481977330132
iteration : 3772
train acc:  0.84375
train loss:  0.33078479766845703
train gradient:  0.2837160950560809
iteration : 3773
train acc:  0.9140625
train loss:  0.27274349331855774
train gradient:  0.2919660008931282
iteration : 3774
train acc:  0.8984375
train loss:  0.26950186491012573
train gradient:  0.21140503465630647
iteration : 3775
train acc:  0.8125
train loss:  0.3940715789794922
train gradient:  0.2705318581178118
iteration : 3776
train acc:  0.765625
train loss:  0.44423437118530273
train gradient:  0.5470814509629452
iteration : 3777
train acc:  0.828125
train loss:  0.40183746814727783
train gradient:  0.29906221113332165
iteration : 3778
train acc:  0.8984375
train loss:  0.2759130597114563
train gradient:  0.1690841687936701
iteration : 3779
train acc:  0.8203125
train loss:  0.3654371500015259
train gradient:  0.24471012586964924
iteration : 3780
train acc:  0.8359375
train loss:  0.32717233896255493
train gradient:  0.2565112560736647
iteration : 3781
train acc:  0.8203125
train loss:  0.3877796232700348
train gradient:  0.267806190654012
iteration : 3782
train acc:  0.859375
train loss:  0.3149939775466919
train gradient:  0.23012236082637177
iteration : 3783
train acc:  0.8671875
train loss:  0.316450297832489
train gradient:  0.19626452172582023
iteration : 3784
train acc:  0.78125
train loss:  0.4090428650379181
train gradient:  0.34168870620649233
iteration : 3785
train acc:  0.8515625
train loss:  0.36329612135887146
train gradient:  0.26885085823972404
iteration : 3786
train acc:  0.8515625
train loss:  0.3612592816352844
train gradient:  0.3032187288354981
iteration : 3787
train acc:  0.8203125
train loss:  0.3098047971725464
train gradient:  0.22819888099577093
iteration : 3788
train acc:  0.8984375
train loss:  0.2661990523338318
train gradient:  0.16778402172886986
iteration : 3789
train acc:  0.8046875
train loss:  0.39637112617492676
train gradient:  0.3263877155325719
iteration : 3790
train acc:  0.84375
train loss:  0.3149106800556183
train gradient:  0.21692486546879722
iteration : 3791
train acc:  0.8515625
train loss:  0.34974217414855957
train gradient:  0.23748988781440328
iteration : 3792
train acc:  0.8125
train loss:  0.3698691725730896
train gradient:  0.2519692060006193
iteration : 3793
train acc:  0.8828125
train loss:  0.3404892086982727
train gradient:  0.3048909263416112
iteration : 3794
train acc:  0.8515625
train loss:  0.3469671607017517
train gradient:  0.1528389560676247
iteration : 3795
train acc:  0.828125
train loss:  0.36491915583610535
train gradient:  0.2598822907224936
iteration : 3796
train acc:  0.8203125
train loss:  0.38195061683654785
train gradient:  0.2524116255575752
iteration : 3797
train acc:  0.8125
train loss:  0.4072035551071167
train gradient:  0.3197241162165013
iteration : 3798
train acc:  0.8203125
train loss:  0.3341432511806488
train gradient:  0.34025397949542635
iteration : 3799
train acc:  0.8671875
train loss:  0.37941765785217285
train gradient:  0.3655168079441436
iteration : 3800
train acc:  0.84375
train loss:  0.36272352933883667
train gradient:  0.2616431525676963
iteration : 3801
train acc:  0.859375
train loss:  0.34797292947769165
train gradient:  0.2683038251461691
iteration : 3802
train acc:  0.7890625
train loss:  0.39898306131362915
train gradient:  0.33357798897780133
iteration : 3803
train acc:  0.8671875
train loss:  0.29997894167900085
train gradient:  0.2086671017536817
iteration : 3804
train acc:  0.8828125
train loss:  0.3148127496242523
train gradient:  0.20846521609037322
iteration : 3805
train acc:  0.8125
train loss:  0.4044173061847687
train gradient:  0.26798673015442565
iteration : 3806
train acc:  0.859375
train loss:  0.3410826325416565
train gradient:  0.21751929168600626
iteration : 3807
train acc:  0.8125
train loss:  0.42232441902160645
train gradient:  0.3880245551476005
iteration : 3808
train acc:  0.921875
train loss:  0.26589375734329224
train gradient:  0.16097157092613798
iteration : 3809
train acc:  0.8125
train loss:  0.4008696377277374
train gradient:  0.295236079914803
iteration : 3810
train acc:  0.8359375
train loss:  0.35880887508392334
train gradient:  0.2703594854037164
iteration : 3811
train acc:  0.8359375
train loss:  0.3425191044807434
train gradient:  0.2859562906927332
iteration : 3812
train acc:  0.84375
train loss:  0.35380399227142334
train gradient:  0.2557276558579152
iteration : 3813
train acc:  0.859375
train loss:  0.33345264196395874
train gradient:  0.32590701498446173
iteration : 3814
train acc:  0.859375
train loss:  0.35876667499542236
train gradient:  0.24192179154970564
iteration : 3815
train acc:  0.859375
train loss:  0.3141997456550598
train gradient:  0.2990156090229119
iteration : 3816
train acc:  0.8359375
train loss:  0.4647647440433502
train gradient:  0.4739052920010891
iteration : 3817
train acc:  0.8125
train loss:  0.40692999958992004
train gradient:  0.32888219709680305
iteration : 3818
train acc:  0.796875
train loss:  0.4156709909439087
train gradient:  0.26121289087460864
iteration : 3819
train acc:  0.84375
train loss:  0.31797707080841064
train gradient:  0.30306042499617913
iteration : 3820
train acc:  0.8359375
train loss:  0.38594233989715576
train gradient:  0.3053762821325824
iteration : 3821
train acc:  0.8203125
train loss:  0.3637726902961731
train gradient:  0.31930664241463136
iteration : 3822
train acc:  0.8359375
train loss:  0.3736763596534729
train gradient:  0.2461346570486764
iteration : 3823
train acc:  0.8359375
train loss:  0.36063891649246216
train gradient:  0.31687881337090035
iteration : 3824
train acc:  0.8203125
train loss:  0.3448314666748047
train gradient:  0.2620616365523872
iteration : 3825
train acc:  0.84375
train loss:  0.3318479657173157
train gradient:  0.2725647099871227
iteration : 3826
train acc:  0.796875
train loss:  0.4268564283847809
train gradient:  0.2996713725805965
iteration : 3827
train acc:  0.8515625
train loss:  0.33598852157592773
train gradient:  0.29040826252645763
iteration : 3828
train acc:  0.875
train loss:  0.3182481527328491
train gradient:  0.22611990833297382
iteration : 3829
train acc:  0.7890625
train loss:  0.3728390634059906
train gradient:  0.28778521361050396
iteration : 3830
train acc:  0.8671875
train loss:  0.32929959893226624
train gradient:  0.3213598222703129
iteration : 3831
train acc:  0.8515625
train loss:  0.2910589575767517
train gradient:  0.24555897014035394
iteration : 3832
train acc:  0.7890625
train loss:  0.4137130081653595
train gradient:  0.31041429490925365
iteration : 3833
train acc:  0.84375
train loss:  0.37402573227882385
train gradient:  0.26322733992677605
iteration : 3834
train acc:  0.8671875
train loss:  0.3398543894290924
train gradient:  0.20873655228233617
iteration : 3835
train acc:  0.8671875
train loss:  0.3593774437904358
train gradient:  0.32404165890204173
iteration : 3836
train acc:  0.828125
train loss:  0.4392056465148926
train gradient:  0.3222412466602587
iteration : 3837
train acc:  0.8359375
train loss:  0.37559425830841064
train gradient:  0.3614743252364378
iteration : 3838
train acc:  0.8203125
train loss:  0.3619285225868225
train gradient:  0.26291777272465056
iteration : 3839
train acc:  0.8359375
train loss:  0.34448808431625366
train gradient:  0.23396211192312305
iteration : 3840
train acc:  0.8359375
train loss:  0.38408055901527405
train gradient:  0.34162608940775474
iteration : 3841
train acc:  0.8515625
train loss:  0.35403576493263245
train gradient:  0.24714985241149837
iteration : 3842
train acc:  0.8984375
train loss:  0.252079039812088
train gradient:  0.15924062789730875
iteration : 3843
train acc:  0.8046875
train loss:  0.4104444980621338
train gradient:  0.3300758507339473
iteration : 3844
train acc:  0.859375
train loss:  0.3565075397491455
train gradient:  0.1857689673787754
iteration : 3845
train acc:  0.8046875
train loss:  0.4542364180088043
train gradient:  0.39932619387269913
iteration : 3846
train acc:  0.8515625
train loss:  0.31607145071029663
train gradient:  0.23550768524195873
iteration : 3847
train acc:  0.8359375
train loss:  0.4138168692588806
train gradient:  0.3399248849731579
iteration : 3848
train acc:  0.796875
train loss:  0.45482003688812256
train gradient:  0.33681343543774883
iteration : 3849
train acc:  0.8359375
train loss:  0.34766656160354614
train gradient:  0.2623959413762488
iteration : 3850
train acc:  0.90625
train loss:  0.26332271099090576
train gradient:  0.28514715979941435
iteration : 3851
train acc:  0.859375
train loss:  0.3069205582141876
train gradient:  0.187923740021315
iteration : 3852
train acc:  0.8125
train loss:  0.3864760994911194
train gradient:  0.24614235439384646
iteration : 3853
train acc:  0.8125
train loss:  0.3978024125099182
train gradient:  0.2870479555628217
iteration : 3854
train acc:  0.8515625
train loss:  0.3386876583099365
train gradient:  0.22911916278285885
iteration : 3855
train acc:  0.8359375
train loss:  0.36224961280822754
train gradient:  0.30192478567473
iteration : 3856
train acc:  0.8203125
train loss:  0.38400667905807495
train gradient:  0.3405136434185433
iteration : 3857
train acc:  0.8125
train loss:  0.4357818365097046
train gradient:  0.3067785364276475
iteration : 3858
train acc:  0.8359375
train loss:  0.3899056315422058
train gradient:  0.31650493478167685
iteration : 3859
train acc:  0.8125
train loss:  0.39395931363105774
train gradient:  0.1985142207312987
iteration : 3860
train acc:  0.8828125
train loss:  0.3145427703857422
train gradient:  0.26597366553048823
iteration : 3861
train acc:  0.8125
train loss:  0.3824526071548462
train gradient:  0.22764617702011314
iteration : 3862
train acc:  0.84375
train loss:  0.355502188205719
train gradient:  0.2351590564147471
iteration : 3863
train acc:  0.84375
train loss:  0.3266088366508484
train gradient:  0.22475564525795058
iteration : 3864
train acc:  0.8125
train loss:  0.41238200664520264
train gradient:  0.2940229911158237
iteration : 3865
train acc:  0.875
train loss:  0.30102282762527466
train gradient:  0.18709432687006936
iteration : 3866
train acc:  0.8203125
train loss:  0.3898487687110901
train gradient:  0.30865215828083964
iteration : 3867
train acc:  0.78125
train loss:  0.4548547863960266
train gradient:  0.3790573282295252
iteration : 3868
train acc:  0.828125
train loss:  0.40592101216316223
train gradient:  0.2289946296850299
iteration : 3869
train acc:  0.828125
train loss:  0.3945022523403168
train gradient:  0.2797906094838838
iteration : 3870
train acc:  0.8359375
train loss:  0.37953633069992065
train gradient:  0.2734606869395307
iteration : 3871
train acc:  0.8359375
train loss:  0.4313701093196869
train gradient:  0.43174695191831775
iteration : 3872
train acc:  0.8046875
train loss:  0.45021161437034607
train gradient:  0.27444442031460214
iteration : 3873
train acc:  0.8515625
train loss:  0.328092098236084
train gradient:  0.20095072335453817
iteration : 3874
train acc:  0.8203125
train loss:  0.3948679268360138
train gradient:  0.2637041647492324
iteration : 3875
train acc:  0.828125
train loss:  0.35103970766067505
train gradient:  0.2402280390902168
iteration : 3876
train acc:  0.8203125
train loss:  0.3400300145149231
train gradient:  0.38998031450436677
iteration : 3877
train acc:  0.796875
train loss:  0.37859779596328735
train gradient:  0.4203797899694413
iteration : 3878
train acc:  0.8828125
train loss:  0.30081647634506226
train gradient:  0.24523576433570468
iteration : 3879
train acc:  0.8515625
train loss:  0.31964850425720215
train gradient:  0.21087445535245208
iteration : 3880
train acc:  0.84375
train loss:  0.39801329374313354
train gradient:  0.30862149489493035
iteration : 3881
train acc:  0.8125
train loss:  0.49582648277282715
train gradient:  0.5150923595127233
iteration : 3882
train acc:  0.78125
train loss:  0.44448748230934143
train gradient:  0.28614841557091236
iteration : 3883
train acc:  0.8515625
train loss:  0.362018883228302
train gradient:  0.24609753599188267
iteration : 3884
train acc:  0.828125
train loss:  0.3954226076602936
train gradient:  0.4172722922233908
iteration : 3885
train acc:  0.8046875
train loss:  0.45206931233406067
train gradient:  0.2951451704738718
iteration : 3886
train acc:  0.75
train loss:  0.43706953525543213
train gradient:  0.38012524393741387
iteration : 3887
train acc:  0.8359375
train loss:  0.3686699867248535
train gradient:  0.2932706189211861
iteration : 3888
train acc:  0.875
train loss:  0.3348209261894226
train gradient:  0.26846275654383217
iteration : 3889
train acc:  0.875
train loss:  0.30637526512145996
train gradient:  0.2490835233466504
iteration : 3890
train acc:  0.84375
train loss:  0.3890540599822998
train gradient:  0.20740286080954
iteration : 3891
train acc:  0.875
train loss:  0.3473999798297882
train gradient:  0.22018225357251092
iteration : 3892
train acc:  0.8359375
train loss:  0.4248557686805725
train gradient:  0.2801937222417178
iteration : 3893
train acc:  0.8125
train loss:  0.37577319145202637
train gradient:  0.3057658329886128
iteration : 3894
train acc:  0.8515625
train loss:  0.2872423529624939
train gradient:  0.2478373660376962
iteration : 3895
train acc:  0.78125
train loss:  0.44208985567092896
train gradient:  0.30433178147974804
iteration : 3896
train acc:  0.828125
train loss:  0.367193341255188
train gradient:  0.2275207890704809
iteration : 3897
train acc:  0.8359375
train loss:  0.3524022102355957
train gradient:  0.23546842912572163
iteration : 3898
train acc:  0.8125
train loss:  0.362909197807312
train gradient:  0.32152922048712335
iteration : 3899
train acc:  0.8359375
train loss:  0.38367992639541626
train gradient:  0.31608544719835285
iteration : 3900
train acc:  0.859375
train loss:  0.3461534380912781
train gradient:  0.3236287106348503
iteration : 3901
train acc:  0.8671875
train loss:  0.2868293225765228
train gradient:  0.20109662363423386
iteration : 3902
train acc:  0.8046875
train loss:  0.381803959608078
train gradient:  0.3699248845625513
iteration : 3903
train acc:  0.84375
train loss:  0.3422527015209198
train gradient:  0.2903659291408214
iteration : 3904
train acc:  0.8515625
train loss:  0.4031273424625397
train gradient:  0.35460225294231434
iteration : 3905
train acc:  0.8828125
train loss:  0.282476007938385
train gradient:  0.1849400680405635
iteration : 3906
train acc:  0.8203125
train loss:  0.38246792554855347
train gradient:  0.33309833558309193
iteration : 3907
train acc:  0.8359375
train loss:  0.35277462005615234
train gradient:  0.18505955841813637
iteration : 3908
train acc:  0.8046875
train loss:  0.39557766914367676
train gradient:  0.22951890246449833
iteration : 3909
train acc:  0.8359375
train loss:  0.3383522033691406
train gradient:  0.17130919337635073
iteration : 3910
train acc:  0.84375
train loss:  0.33753177523612976
train gradient:  0.17763679900380186
iteration : 3911
train acc:  0.8984375
train loss:  0.272750586271286
train gradient:  0.15305711106976674
iteration : 3912
train acc:  0.84375
train loss:  0.3632606267929077
train gradient:  0.2526206183557782
iteration : 3913
train acc:  0.8828125
train loss:  0.4068315327167511
train gradient:  0.5892508653761648
iteration : 3914
train acc:  0.859375
train loss:  0.34086641669273376
train gradient:  0.19498794023827876
iteration : 3915
train acc:  0.8671875
train loss:  0.3289419412612915
train gradient:  0.22372634693490057
iteration : 3916
train acc:  0.8671875
train loss:  0.2914886474609375
train gradient:  0.25038814310610386
iteration : 3917
train acc:  0.875
train loss:  0.3346561789512634
train gradient:  0.29216350425441523
iteration : 3918
train acc:  0.8359375
train loss:  0.35365748405456543
train gradient:  0.24108496203697677
iteration : 3919
train acc:  0.796875
train loss:  0.38842737674713135
train gradient:  0.276765167817454
iteration : 3920
train acc:  0.8671875
train loss:  0.2894958257675171
train gradient:  0.14643869275895027
iteration : 3921
train acc:  0.828125
train loss:  0.38230010867118835
train gradient:  0.27061212617639313
iteration : 3922
train acc:  0.8359375
train loss:  0.38776588439941406
train gradient:  0.4098582246346506
iteration : 3923
train acc:  0.9140625
train loss:  0.273107647895813
train gradient:  0.27168232130476544
iteration : 3924
train acc:  0.8203125
train loss:  0.3099408745765686
train gradient:  0.2261298410294154
iteration : 3925
train acc:  0.7890625
train loss:  0.4260680675506592
train gradient:  0.3757865749924973
iteration : 3926
train acc:  0.8359375
train loss:  0.3549482226371765
train gradient:  0.3465021980197151
iteration : 3927
train acc:  0.859375
train loss:  0.363841712474823
train gradient:  0.3726120552657709
iteration : 3928
train acc:  0.859375
train loss:  0.3093276619911194
train gradient:  0.22051346790568727
iteration : 3929
train acc:  0.8203125
train loss:  0.42925989627838135
train gradient:  0.3005749504602123
iteration : 3930
train acc:  0.8671875
train loss:  0.3116599917411804
train gradient:  0.26154047894246846
iteration : 3931
train acc:  0.8828125
train loss:  0.3075098395347595
train gradient:  0.17829177265185386
iteration : 3932
train acc:  0.7890625
train loss:  0.4221147894859314
train gradient:  0.4157418401082534
iteration : 3933
train acc:  0.859375
train loss:  0.33782970905303955
train gradient:  0.3422708090989857
iteration : 3934
train acc:  0.84375
train loss:  0.3580799996852875
train gradient:  0.2489659228209867
iteration : 3935
train acc:  0.828125
train loss:  0.36655688285827637
train gradient:  0.23234556131193596
iteration : 3936
train acc:  0.8671875
train loss:  0.3428419530391693
train gradient:  0.25202918673120955
iteration : 3937
train acc:  0.8359375
train loss:  0.359988272190094
train gradient:  0.2656269442616897
iteration : 3938
train acc:  0.828125
train loss:  0.37074950337409973
train gradient:  0.29363384622942645
iteration : 3939
train acc:  0.8984375
train loss:  0.35019180178642273
train gradient:  0.25033430431331605
iteration : 3940
train acc:  0.8359375
train loss:  0.3443446159362793
train gradient:  0.2642620589343526
iteration : 3941
train acc:  0.875
train loss:  0.28435927629470825
train gradient:  0.18857376147533822
iteration : 3942
train acc:  0.8359375
train loss:  0.4033799469470978
train gradient:  0.29030893134235136
iteration : 3943
train acc:  0.8046875
train loss:  0.43471357226371765
train gradient:  0.3941904269206204
iteration : 3944
train acc:  0.828125
train loss:  0.3879773020744324
train gradient:  0.2754378306403155
iteration : 3945
train acc:  0.8203125
train loss:  0.47205299139022827
train gradient:  0.5318875475703395
iteration : 3946
train acc:  0.8125
train loss:  0.4034101963043213
train gradient:  0.43127917516211034
iteration : 3947
train acc:  0.8515625
train loss:  0.3690577745437622
train gradient:  0.27827009311652545
iteration : 3948
train acc:  0.8203125
train loss:  0.40108034014701843
train gradient:  0.3123337252648067
iteration : 3949
train acc:  0.8203125
train loss:  0.37203481793403625
train gradient:  0.19888788167293991
iteration : 3950
train acc:  0.90625
train loss:  0.2915894389152527
train gradient:  0.26908182479180676
iteration : 3951
train acc:  0.8671875
train loss:  0.35529446601867676
train gradient:  0.23105766861259605
iteration : 3952
train acc:  0.8515625
train loss:  0.3374181389808655
train gradient:  0.2162179095061517
iteration : 3953
train acc:  0.8671875
train loss:  0.37491440773010254
train gradient:  0.34014259764220023
iteration : 3954
train acc:  0.8671875
train loss:  0.341585248708725
train gradient:  0.26117516243913813
iteration : 3955
train acc:  0.828125
train loss:  0.4080011248588562
train gradient:  0.3510794020299727
iteration : 3956
train acc:  0.7890625
train loss:  0.5426926612854004
train gradient:  0.6463206842092084
iteration : 3957
train acc:  0.8359375
train loss:  0.3454185724258423
train gradient:  0.2875738626715234
iteration : 3958
train acc:  0.8046875
train loss:  0.4111068546772003
train gradient:  0.2397878626370215
iteration : 3959
train acc:  0.8671875
train loss:  0.3166753053665161
train gradient:  0.20122591412234642
iteration : 3960
train acc:  0.875
train loss:  0.3426935076713562
train gradient:  0.2891910085220048
iteration : 3961
train acc:  0.8046875
train loss:  0.35729336738586426
train gradient:  0.25240346413982784
iteration : 3962
train acc:  0.8359375
train loss:  0.3672332167625427
train gradient:  0.3017004879919469
iteration : 3963
train acc:  0.8359375
train loss:  0.3693598508834839
train gradient:  0.26151940608101254
iteration : 3964
train acc:  0.7734375
train loss:  0.4403217136859894
train gradient:  0.32792516580719394
iteration : 3965
train acc:  0.84375
train loss:  0.3656269907951355
train gradient:  0.2866099169880495
iteration : 3966
train acc:  0.8125
train loss:  0.4169645309448242
train gradient:  0.34564625183520625
iteration : 3967
train acc:  0.828125
train loss:  0.32943397760391235
train gradient:  0.2968027148683361
iteration : 3968
train acc:  0.890625
train loss:  0.2698418200016022
train gradient:  0.18799244544219734
iteration : 3969
train acc:  0.859375
train loss:  0.3799327611923218
train gradient:  0.3989446875921013
iteration : 3970
train acc:  0.8671875
train loss:  0.334339439868927
train gradient:  0.2292032932957097
iteration : 3971
train acc:  0.875
train loss:  0.2830037474632263
train gradient:  0.26094898435581393
iteration : 3972
train acc:  0.8515625
train loss:  0.3238471746444702
train gradient:  0.26423770268773905
iteration : 3973
train acc:  0.8671875
train loss:  0.3454713225364685
train gradient:  0.22064613373873954
iteration : 3974
train acc:  0.8125
train loss:  0.4832932949066162
train gradient:  0.3665115778814544
iteration : 3975
train acc:  0.859375
train loss:  0.2993180453777313
train gradient:  0.2323150364186874
iteration : 3976
train acc:  0.8515625
train loss:  0.37604624032974243
train gradient:  0.28900007037838465
iteration : 3977
train acc:  0.8203125
train loss:  0.38438063859939575
train gradient:  0.2982514636213756
iteration : 3978
train acc:  0.8359375
train loss:  0.3252275884151459
train gradient:  0.17931819961173714
iteration : 3979
train acc:  0.796875
train loss:  0.36179688572883606
train gradient:  0.2555551085044637
iteration : 3980
train acc:  0.8203125
train loss:  0.35786882042884827
train gradient:  0.29136882088380434
iteration : 3981
train acc:  0.84375
train loss:  0.37103110551834106
train gradient:  0.2890903043545393
iteration : 3982
train acc:  0.84375
train loss:  0.35181066393852234
train gradient:  0.3269570976368913
iteration : 3983
train acc:  0.9140625
train loss:  0.30480000376701355
train gradient:  0.18787822574754284
iteration : 3984
train acc:  0.859375
train loss:  0.3149909973144531
train gradient:  0.20078363103047575
iteration : 3985
train acc:  0.7890625
train loss:  0.40927594900131226
train gradient:  0.3347644150185708
iteration : 3986
train acc:  0.828125
train loss:  0.35675790905952454
train gradient:  0.23588383461313306
iteration : 3987
train acc:  0.78125
train loss:  0.4284170866012573
train gradient:  0.4734495911278059
iteration : 3988
train acc:  0.90625
train loss:  0.2860105633735657
train gradient:  0.27798977426044924
iteration : 3989
train acc:  0.8046875
train loss:  0.3500225245952606
train gradient:  0.29403159269437656
iteration : 3990
train acc:  0.796875
train loss:  0.3960748314857483
train gradient:  0.27687487845024183
iteration : 3991
train acc:  0.796875
train loss:  0.38443565368652344
train gradient:  0.3454814053668758
iteration : 3992
train acc:  0.8515625
train loss:  0.378465861082077
train gradient:  0.2727958376205669
iteration : 3993
train acc:  0.859375
train loss:  0.36322277784347534
train gradient:  0.28452913267446756
iteration : 3994
train acc:  0.8828125
train loss:  0.29744142293930054
train gradient:  0.188027828743712
iteration : 3995
train acc:  0.890625
train loss:  0.2859373688697815
train gradient:  0.23336534094878986
iteration : 3996
train acc:  0.8828125
train loss:  0.3093602657318115
train gradient:  0.18065148796412622
iteration : 3997
train acc:  0.84375
train loss:  0.3770145773887634
train gradient:  0.36726262477266014
iteration : 3998
train acc:  0.8671875
train loss:  0.31774774193763733
train gradient:  0.23789712155386838
iteration : 3999
train acc:  0.8359375
train loss:  0.365939199924469
train gradient:  0.3193351035677419
iteration : 4000
train acc:  0.8515625
train loss:  0.3645372986793518
train gradient:  0.29399021488710525
iteration : 4001
train acc:  0.8984375
train loss:  0.25879356265068054
train gradient:  0.1593038372147477
iteration : 4002
train acc:  0.875
train loss:  0.33429479598999023
train gradient:  0.25996958984030927
iteration : 4003
train acc:  0.8046875
train loss:  0.40494251251220703
train gradient:  0.33682984993521586
iteration : 4004
train acc:  0.796875
train loss:  0.4087214767932892
train gradient:  0.26456657489627505
iteration : 4005
train acc:  0.8515625
train loss:  0.3074289560317993
train gradient:  0.29369146389927214
iteration : 4006
train acc:  0.84375
train loss:  0.39457741379737854
train gradient:  0.3021044954894411
iteration : 4007
train acc:  0.8515625
train loss:  0.40753674507141113
train gradient:  0.37410907892467193
iteration : 4008
train acc:  0.8515625
train loss:  0.3541991412639618
train gradient:  0.3112618704076586
iteration : 4009
train acc:  0.84375
train loss:  0.32746899127960205
train gradient:  0.3114116830784593
iteration : 4010
train acc:  0.859375
train loss:  0.340947687625885
train gradient:  0.24552532127748866
iteration : 4011
train acc:  0.8359375
train loss:  0.33387327194213867
train gradient:  0.30703373457868116
iteration : 4012
train acc:  0.84375
train loss:  0.315129816532135
train gradient:  0.20500244078674612
iteration : 4013
train acc:  0.828125
train loss:  0.3942611813545227
train gradient:  0.288341062320634
iteration : 4014
train acc:  0.8984375
train loss:  0.2945675849914551
train gradient:  0.18397544017117132
iteration : 4015
train acc:  0.8671875
train loss:  0.3382764458656311
train gradient:  0.29489186326305655
iteration : 4016
train acc:  0.8359375
train loss:  0.3596579432487488
train gradient:  0.4142044029394552
iteration : 4017
train acc:  0.8359375
train loss:  0.39539018273353577
train gradient:  0.4512671369363101
iteration : 4018
train acc:  0.859375
train loss:  0.3480667173862457
train gradient:  0.30628613413188466
iteration : 4019
train acc:  0.8515625
train loss:  0.3249821066856384
train gradient:  0.2312181150455797
iteration : 4020
train acc:  0.8515625
train loss:  0.3243948817253113
train gradient:  0.2891200636890761
iteration : 4021
train acc:  0.8046875
train loss:  0.4029325842857361
train gradient:  0.3375532130581358
iteration : 4022
train acc:  0.84375
train loss:  0.3115077614784241
train gradient:  0.24181175506438168
iteration : 4023
train acc:  0.84375
train loss:  0.35798197984695435
train gradient:  0.3184452317849645
iteration : 4024
train acc:  0.859375
train loss:  0.3415929675102234
train gradient:  0.25914422311470886
iteration : 4025
train acc:  0.8828125
train loss:  0.29720935225486755
train gradient:  0.2990012263311535
iteration : 4026
train acc:  0.796875
train loss:  0.3727891147136688
train gradient:  0.37032975162304327
iteration : 4027
train acc:  0.8359375
train loss:  0.3964482247829437
train gradient:  0.3406403254277336
iteration : 4028
train acc:  0.796875
train loss:  0.5058042407035828
train gradient:  0.45886389365859304
iteration : 4029
train acc:  0.8671875
train loss:  0.37866824865341187
train gradient:  0.38802327904427014
iteration : 4030
train acc:  0.90625
train loss:  0.32704097032546997
train gradient:  0.27851379179644337
iteration : 4031
train acc:  0.828125
train loss:  0.3807404637336731
train gradient:  0.482942262468066
iteration : 4032
train acc:  0.8359375
train loss:  0.37014248967170715
train gradient:  0.23630099634546953
iteration : 4033
train acc:  0.8359375
train loss:  0.3765204846858978
train gradient:  0.24032960516947205
iteration : 4034
train acc:  0.828125
train loss:  0.40875911712646484
train gradient:  0.4044144031632771
iteration : 4035
train acc:  0.8828125
train loss:  0.27213793992996216
train gradient:  0.1712204444430429
iteration : 4036
train acc:  0.78125
train loss:  0.40212786197662354
train gradient:  0.38263524894307277
iteration : 4037
train acc:  0.875
train loss:  0.3139820098876953
train gradient:  0.27454687883523976
iteration : 4038
train acc:  0.8828125
train loss:  0.27751702070236206
train gradient:  0.17203927317231077
iteration : 4039
train acc:  0.8671875
train loss:  0.32803961634635925
train gradient:  0.2709768541381291
iteration : 4040
train acc:  0.765625
train loss:  0.4931965470314026
train gradient:  0.5458302177439991
iteration : 4041
train acc:  0.8359375
train loss:  0.3533065915107727
train gradient:  0.34282253995505624
iteration : 4042
train acc:  0.765625
train loss:  0.5053528547286987
train gradient:  0.6642774407441066
iteration : 4043
train acc:  0.84375
train loss:  0.37353259325027466
train gradient:  0.45395821809796144
iteration : 4044
train acc:  0.84375
train loss:  0.3822675943374634
train gradient:  0.2495560127636108
iteration : 4045
train acc:  0.8828125
train loss:  0.29222947359085083
train gradient:  0.20099537357983965
iteration : 4046
train acc:  0.84375
train loss:  0.35485243797302246
train gradient:  0.20295000029410354
iteration : 4047
train acc:  0.890625
train loss:  0.3449394106864929
train gradient:  0.24787302173710468
iteration : 4048
train acc:  0.828125
train loss:  0.3685912787914276
train gradient:  0.3393781975188924
iteration : 4049
train acc:  0.828125
train loss:  0.3758685290813446
train gradient:  0.20606694569965162
iteration : 4050
train acc:  0.7734375
train loss:  0.46164870262145996
train gradient:  0.5210433400883693
iteration : 4051
train acc:  0.8359375
train loss:  0.34480759501457214
train gradient:  0.31961800509662547
iteration : 4052
train acc:  0.875
train loss:  0.33446794748306274
train gradient:  0.2673242922511413
iteration : 4053
train acc:  0.8359375
train loss:  0.28103840351104736
train gradient:  0.20192819195486894
iteration : 4054
train acc:  0.8203125
train loss:  0.3834521770477295
train gradient:  0.2637712667921638
iteration : 4055
train acc:  0.7734375
train loss:  0.44659537076950073
train gradient:  0.4991134428075318
iteration : 4056
train acc:  0.859375
train loss:  0.329697847366333
train gradient:  0.19900108539044667
iteration : 4057
train acc:  0.7421875
train loss:  0.5089941024780273
train gradient:  0.4938226584639888
iteration : 4058
train acc:  0.8515625
train loss:  0.30837756395339966
train gradient:  0.3525273501638386
iteration : 4059
train acc:  0.8359375
train loss:  0.33219581842422485
train gradient:  0.21985902729406276
iteration : 4060
train acc:  0.8125
train loss:  0.38159996271133423
train gradient:  0.2607059087607757
iteration : 4061
train acc:  0.8203125
train loss:  0.4601516127586365
train gradient:  0.32568445626827036
iteration : 4062
train acc:  0.8203125
train loss:  0.3603024482727051
train gradient:  0.24100270174890295
iteration : 4063
train acc:  0.859375
train loss:  0.3390805721282959
train gradient:  0.26681072635016745
iteration : 4064
train acc:  0.828125
train loss:  0.397419810295105
train gradient:  0.4026153926208631
iteration : 4065
train acc:  0.8125
train loss:  0.46609023213386536
train gradient:  0.46543639345916066
iteration : 4066
train acc:  0.7890625
train loss:  0.4104342758655548
train gradient:  0.27074088046231404
iteration : 4067
train acc:  0.828125
train loss:  0.36506378650665283
train gradient:  0.24945118056187351
iteration : 4068
train acc:  0.8046875
train loss:  0.35819512605667114
train gradient:  0.3145864347832103
iteration : 4069
train acc:  0.859375
train loss:  0.3267301321029663
train gradient:  0.2201363605845127
iteration : 4070
train acc:  0.8359375
train loss:  0.37212878465652466
train gradient:  0.2456619211115832
iteration : 4071
train acc:  0.859375
train loss:  0.3789440393447876
train gradient:  0.29945673869370654
iteration : 4072
train acc:  0.859375
train loss:  0.3201945722103119
train gradient:  0.221538942098283
iteration : 4073
train acc:  0.8125
train loss:  0.37825512886047363
train gradient:  0.2580145087843831
iteration : 4074
train acc:  0.8203125
train loss:  0.3532644808292389
train gradient:  0.27395271756516243
iteration : 4075
train acc:  0.828125
train loss:  0.36152535676956177
train gradient:  0.278804697116011
iteration : 4076
train acc:  0.8515625
train loss:  0.3437032103538513
train gradient:  0.2194084146490623
iteration : 4077
train acc:  0.84375
train loss:  0.3766975998878479
train gradient:  0.2293505661339491
iteration : 4078
train acc:  0.8046875
train loss:  0.37942299246788025
train gradient:  0.6307949768011716
iteration : 4079
train acc:  0.8203125
train loss:  0.44731763005256653
train gradient:  0.4796099176050002
iteration : 4080
train acc:  0.8359375
train loss:  0.38147076964378357
train gradient:  0.25008426902923475
iteration : 4081
train acc:  0.8515625
train loss:  0.3397831618785858
train gradient:  0.3266556392707828
iteration : 4082
train acc:  0.8671875
train loss:  0.3260916471481323
train gradient:  0.23218088929324754
iteration : 4083
train acc:  0.8359375
train loss:  0.39736783504486084
train gradient:  0.2108264341708761
iteration : 4084
train acc:  0.90625
train loss:  0.25594431161880493
train gradient:  0.21405105766062418
iteration : 4085
train acc:  0.8515625
train loss:  0.3902741074562073
train gradient:  0.3316605557324316
iteration : 4086
train acc:  0.8671875
train loss:  0.36319035291671753
train gradient:  0.2682623550591951
iteration : 4087
train acc:  0.8515625
train loss:  0.33797842264175415
train gradient:  0.2065995024008297
iteration : 4088
train acc:  0.8515625
train loss:  0.3428802192211151
train gradient:  0.28822900185989264
iteration : 4089
train acc:  0.8515625
train loss:  0.3632480502128601
train gradient:  0.26137878412854443
iteration : 4090
train acc:  0.828125
train loss:  0.3338213562965393
train gradient:  0.15823689490163034
iteration : 4091
train acc:  0.8046875
train loss:  0.4191659688949585
train gradient:  0.4191920614811368
iteration : 4092
train acc:  0.875
train loss:  0.3284910321235657
train gradient:  0.20173022678957514
iteration : 4093
train acc:  0.890625
train loss:  0.32596540451049805
train gradient:  0.21648227300268114
iteration : 4094
train acc:  0.8984375
train loss:  0.30061110854148865
train gradient:  0.25771284020750707
iteration : 4095
train acc:  0.8359375
train loss:  0.38676780462265015
train gradient:  0.3586924741170584
iteration : 4096
train acc:  0.7890625
train loss:  0.4548604488372803
train gradient:  0.33935021920066516
iteration : 4097
train acc:  0.828125
train loss:  0.41504859924316406
train gradient:  0.3533926690760301
iteration : 4098
train acc:  0.8203125
train loss:  0.38454514741897583
train gradient:  0.2907160150463587
iteration : 4099
train acc:  0.8828125
train loss:  0.3063395619392395
train gradient:  0.24473506542258863
iteration : 4100
train acc:  0.90625
train loss:  0.32554328441619873
train gradient:  0.1276810860702819
iteration : 4101
train acc:  0.828125
train loss:  0.36560744047164917
train gradient:  0.21934272507712327
iteration : 4102
train acc:  0.8359375
train loss:  0.319690465927124
train gradient:  0.26329902474607747
iteration : 4103
train acc:  0.8046875
train loss:  0.3701445758342743
train gradient:  0.2510364717086067
iteration : 4104
train acc:  0.8359375
train loss:  0.37793827056884766
train gradient:  0.25231482500733726
iteration : 4105
train acc:  0.8515625
train loss:  0.3310631513595581
train gradient:  0.2240235860531063
iteration : 4106
train acc:  0.796875
train loss:  0.49348360300064087
train gradient:  0.4464820570731059
iteration : 4107
train acc:  0.828125
train loss:  0.4288184940814972
train gradient:  0.32468686141448666
iteration : 4108
train acc:  0.84375
train loss:  0.3107023239135742
train gradient:  0.16617289455908674
iteration : 4109
train acc:  0.875
train loss:  0.4098408818244934
train gradient:  0.26365208778893906
iteration : 4110
train acc:  0.7578125
train loss:  0.4532660245895386
train gradient:  0.3463861063258211
iteration : 4111
train acc:  0.8046875
train loss:  0.422856867313385
train gradient:  0.37418390354383124
iteration : 4112
train acc:  0.8203125
train loss:  0.40406739711761475
train gradient:  0.34585024566504574
iteration : 4113
train acc:  0.7734375
train loss:  0.4411151111125946
train gradient:  0.31682326396446187
iteration : 4114
train acc:  0.890625
train loss:  0.28652486205101013
train gradient:  0.14668194189229194
iteration : 4115
train acc:  0.8515625
train loss:  0.36163538694381714
train gradient:  0.30097668781058684
iteration : 4116
train acc:  0.875
train loss:  0.35148870944976807
train gradient:  0.2623816336221656
iteration : 4117
train acc:  0.8125
train loss:  0.3889196813106537
train gradient:  0.2343374505922181
iteration : 4118
train acc:  0.828125
train loss:  0.3893158435821533
train gradient:  0.3466876638211366
iteration : 4119
train acc:  0.84375
train loss:  0.349124550819397
train gradient:  0.3016082154595815
iteration : 4120
train acc:  0.8671875
train loss:  0.31585878133773804
train gradient:  0.2309357172176814
iteration : 4121
train acc:  0.875
train loss:  0.32824110984802246
train gradient:  0.28970031586084094
iteration : 4122
train acc:  0.8203125
train loss:  0.41379600763320923
train gradient:  0.2510182611109117
iteration : 4123
train acc:  0.8671875
train loss:  0.32406318187713623
train gradient:  0.16540517881272634
iteration : 4124
train acc:  0.8203125
train loss:  0.42389246821403503
train gradient:  0.3357427685137597
iteration : 4125
train acc:  0.8671875
train loss:  0.349711537361145
train gradient:  0.27429011135169185
iteration : 4126
train acc:  0.875
train loss:  0.2914127707481384
train gradient:  0.16782268511912135
iteration : 4127
train acc:  0.8046875
train loss:  0.4532414972782135
train gradient:  0.3554475695690342
iteration : 4128
train acc:  0.8125
train loss:  0.3527073264122009
train gradient:  0.2810204118255636
iteration : 4129
train acc:  0.84375
train loss:  0.35920199751853943
train gradient:  0.21600824784162165
iteration : 4130
train acc:  0.890625
train loss:  0.29764682054519653
train gradient:  0.18120225006432233
iteration : 4131
train acc:  0.8125
train loss:  0.4279117286205292
train gradient:  0.37915140255146446
iteration : 4132
train acc:  0.796875
train loss:  0.38922932744026184
train gradient:  0.28496291719290723
iteration : 4133
train acc:  0.7890625
train loss:  0.4557400345802307
train gradient:  0.43488138708011315
iteration : 4134
train acc:  0.7890625
train loss:  0.3546712100505829
train gradient:  0.21093940523632976
iteration : 4135
train acc:  0.8125
train loss:  0.409356951713562
train gradient:  0.2673101112774904
iteration : 4136
train acc:  0.8984375
train loss:  0.3133494555950165
train gradient:  0.18181493425740558
iteration : 4137
train acc:  0.7734375
train loss:  0.4481106996536255
train gradient:  0.37263721725353544
iteration : 4138
train acc:  0.7734375
train loss:  0.3937538266181946
train gradient:  0.3483248589790107
iteration : 4139
train acc:  0.8359375
train loss:  0.3446711599826813
train gradient:  0.2152222317227302
iteration : 4140
train acc:  0.8203125
train loss:  0.3832533359527588
train gradient:  0.26597907491898465
iteration : 4141
train acc:  0.8125
train loss:  0.38554131984710693
train gradient:  0.2824297836052007
iteration : 4142
train acc:  0.859375
train loss:  0.3353816866874695
train gradient:  0.21619674131163436
iteration : 4143
train acc:  0.8359375
train loss:  0.3537119925022125
train gradient:  0.2536957705707332
iteration : 4144
train acc:  0.8359375
train loss:  0.34007155895233154
train gradient:  0.3037028033801311
iteration : 4145
train acc:  0.8515625
train loss:  0.3315795660018921
train gradient:  0.19377566658651935
iteration : 4146
train acc:  0.8671875
train loss:  0.34697186946868896
train gradient:  0.4150839620359873
iteration : 4147
train acc:  0.8359375
train loss:  0.34793150424957275
train gradient:  0.2048960847825623
iteration : 4148
train acc:  0.859375
train loss:  0.37720048427581787
train gradient:  0.1916984043081585
iteration : 4149
train acc:  0.8359375
train loss:  0.3257980942726135
train gradient:  0.19979169663736296
iteration : 4150
train acc:  0.875
train loss:  0.3645496070384979
train gradient:  0.21897068768672945
iteration : 4151
train acc:  0.8671875
train loss:  0.32240790128707886
train gradient:  0.21376158037366502
iteration : 4152
train acc:  0.8203125
train loss:  0.39479872584342957
train gradient:  0.3133545773824911
iteration : 4153
train acc:  0.8984375
train loss:  0.26461684703826904
train gradient:  0.14395377087660344
iteration : 4154
train acc:  0.875
train loss:  0.32299530506134033
train gradient:  0.23089549428520945
iteration : 4155
train acc:  0.890625
train loss:  0.3178633451461792
train gradient:  0.17072194729646234
iteration : 4156
train acc:  0.8515625
train loss:  0.37561696767807007
train gradient:  0.19271965956382084
iteration : 4157
train acc:  0.90625
train loss:  0.31013399362564087
train gradient:  0.20149894633076973
iteration : 4158
train acc:  0.859375
train loss:  0.29636433720588684
train gradient:  0.17210801754706795
iteration : 4159
train acc:  0.890625
train loss:  0.25539422035217285
train gradient:  0.16417567423718554
iteration : 4160
train acc:  0.7578125
train loss:  0.4228712022304535
train gradient:  0.384129620864923
iteration : 4161
train acc:  0.859375
train loss:  0.3466074764728546
train gradient:  0.20805308556883634
iteration : 4162
train acc:  0.8671875
train loss:  0.28603509068489075
train gradient:  0.1859029533212092
iteration : 4163
train acc:  0.7890625
train loss:  0.4557654559612274
train gradient:  0.3533375341931151
iteration : 4164
train acc:  0.8046875
train loss:  0.35934561491012573
train gradient:  0.3000018730949629
iteration : 4165
train acc:  0.8359375
train loss:  0.331998348236084
train gradient:  0.2873776173016693
iteration : 4166
train acc:  0.890625
train loss:  0.27445459365844727
train gradient:  0.24795189755193006
iteration : 4167
train acc:  0.8671875
train loss:  0.2846294939517975
train gradient:  0.19872590424084033
iteration : 4168
train acc:  0.828125
train loss:  0.36931663751602173
train gradient:  0.25872498376552
iteration : 4169
train acc:  0.8203125
train loss:  0.38273242115974426
train gradient:  0.24976320365310584
iteration : 4170
train acc:  0.828125
train loss:  0.4156029224395752
train gradient:  0.2435936400337799
iteration : 4171
train acc:  0.8515625
train loss:  0.3396870791912079
train gradient:  0.23905595623510276
iteration : 4172
train acc:  0.8046875
train loss:  0.4017382264137268
train gradient:  0.39298338896075896
iteration : 4173
train acc:  0.828125
train loss:  0.4059918224811554
train gradient:  0.305424502822207
iteration : 4174
train acc:  0.875
train loss:  0.32149532437324524
train gradient:  0.23613818489863103
iteration : 4175
train acc:  0.90625
train loss:  0.3033464252948761
train gradient:  0.23574950458825006
iteration : 4176
train acc:  0.8125
train loss:  0.37544548511505127
train gradient:  0.33518956474882095
iteration : 4177
train acc:  0.8828125
train loss:  0.3236982822418213
train gradient:  0.20292229000660722
iteration : 4178
train acc:  0.84375
train loss:  0.3669030964374542
train gradient:  0.23604801939162623
iteration : 4179
train acc:  0.859375
train loss:  0.3758487403392792
train gradient:  0.3212062195477537
iteration : 4180
train acc:  0.8515625
train loss:  0.3457082509994507
train gradient:  0.2917667686461586
iteration : 4181
train acc:  0.8125
train loss:  0.4387727379798889
train gradient:  0.45121348604691486
iteration : 4182
train acc:  0.796875
train loss:  0.4856862425804138
train gradient:  0.582614923570931
iteration : 4183
train acc:  0.8984375
train loss:  0.3302808403968811
train gradient:  0.25745497079089474
iteration : 4184
train acc:  0.8359375
train loss:  0.45386913418769836
train gradient:  0.5055155278024797
iteration : 4185
train acc:  0.890625
train loss:  0.27902889251708984
train gradient:  0.22921072128686212
iteration : 4186
train acc:  0.8828125
train loss:  0.2767016887664795
train gradient:  0.18712580822205765
iteration : 4187
train acc:  0.875
train loss:  0.2981549799442291
train gradient:  0.30552527524082307
iteration : 4188
train acc:  0.8046875
train loss:  0.40114277601242065
train gradient:  0.37908273470854614
iteration : 4189
train acc:  0.8203125
train loss:  0.3929787278175354
train gradient:  0.2850048409629874
iteration : 4190
train acc:  0.828125
train loss:  0.3801814019680023
train gradient:  0.3828841056743543
iteration : 4191
train acc:  0.8671875
train loss:  0.33181893825531006
train gradient:  0.2096426542089015
iteration : 4192
train acc:  0.7734375
train loss:  0.4565107524394989
train gradient:  0.45181675643025293
iteration : 4193
train acc:  0.8203125
train loss:  0.3737437427043915
train gradient:  0.2713430716355201
iteration : 4194
train acc:  0.8359375
train loss:  0.37239253520965576
train gradient:  0.3530399511174276
iteration : 4195
train acc:  0.8671875
train loss:  0.36032748222351074
train gradient:  0.29145049699815007
iteration : 4196
train acc:  0.84375
train loss:  0.35677486658096313
train gradient:  0.32625685610223876
iteration : 4197
train acc:  0.8984375
train loss:  0.3258480429649353
train gradient:  0.3344023493791479
iteration : 4198
train acc:  0.8203125
train loss:  0.3743598759174347
train gradient:  0.35885446991094594
iteration : 4199
train acc:  0.8125
train loss:  0.39608922600746155
train gradient:  0.3599821459157744
iteration : 4200
train acc:  0.828125
train loss:  0.4250107407569885
train gradient:  0.49185564580998026
iteration : 4201
train acc:  0.8515625
train loss:  0.32055866718292236
train gradient:  0.20789901820294637
iteration : 4202
train acc:  0.84375
train loss:  0.377992182970047
train gradient:  0.3227566125630219
iteration : 4203
train acc:  0.921875
train loss:  0.25511324405670166
train gradient:  0.19321507635301544
iteration : 4204
train acc:  0.78125
train loss:  0.39677363634109497
train gradient:  0.39681615542211063
iteration : 4205
train acc:  0.7734375
train loss:  0.4762054979801178
train gradient:  0.4695151774496685
iteration : 4206
train acc:  0.8984375
train loss:  0.3434632122516632
train gradient:  0.3202435869173262
iteration : 4207
train acc:  0.859375
train loss:  0.37144601345062256
train gradient:  0.2564775475619948
iteration : 4208
train acc:  0.8515625
train loss:  0.3604893386363983
train gradient:  0.30690974560783674
iteration : 4209
train acc:  0.8203125
train loss:  0.39241814613342285
train gradient:  0.3037376821727387
iteration : 4210
train acc:  0.7421875
train loss:  0.4664285182952881
train gradient:  0.38865074231334573
iteration : 4211
train acc:  0.8125
train loss:  0.3521338105201721
train gradient:  0.30699061247787846
iteration : 4212
train acc:  0.796875
train loss:  0.43991899490356445
train gradient:  0.3354473491293396
iteration : 4213
train acc:  0.84375
train loss:  0.36088305711746216
train gradient:  0.2879413378361763
iteration : 4214
train acc:  0.859375
train loss:  0.3329392373561859
train gradient:  0.26224171700764193
iteration : 4215
train acc:  0.8515625
train loss:  0.3143014907836914
train gradient:  0.2615167944982751
iteration : 4216
train acc:  0.8515625
train loss:  0.3202691376209259
train gradient:  0.22550223139483655
iteration : 4217
train acc:  0.8515625
train loss:  0.2900102138519287
train gradient:  0.18729259136155685
iteration : 4218
train acc:  0.921875
train loss:  0.26215216517448425
train gradient:  0.15117564364473363
iteration : 4219
train acc:  0.8046875
train loss:  0.3911409378051758
train gradient:  0.29614549074768554
iteration : 4220
train acc:  0.8359375
train loss:  0.3404533267021179
train gradient:  0.19307761072809732
iteration : 4221
train acc:  0.8671875
train loss:  0.30637454986572266
train gradient:  0.1811978676662607
iteration : 4222
train acc:  0.84375
train loss:  0.30529528856277466
train gradient:  0.22322158812309112
iteration : 4223
train acc:  0.8671875
train loss:  0.33422768115997314
train gradient:  0.2246536367046228
iteration : 4224
train acc:  0.8359375
train loss:  0.34996068477630615
train gradient:  0.16163173844052348
iteration : 4225
train acc:  0.875
train loss:  0.3159427344799042
train gradient:  0.9869245207323888
iteration : 4226
train acc:  0.84375
train loss:  0.37250471115112305
train gradient:  0.2484651897312543
iteration : 4227
train acc:  0.8125
train loss:  0.43114912509918213
train gradient:  0.4250352301241789
iteration : 4228
train acc:  0.84375
train loss:  0.39230775833129883
train gradient:  0.284508402273281
iteration : 4229
train acc:  0.859375
train loss:  0.3775189518928528
train gradient:  0.40040648867119705
iteration : 4230
train acc:  0.796875
train loss:  0.43881839513778687
train gradient:  0.36489579193633975
iteration : 4231
train acc:  0.828125
train loss:  0.37877157330513
train gradient:  0.25742440610285716
iteration : 4232
train acc:  0.8359375
train loss:  0.41520214080810547
train gradient:  0.41699068520136257
iteration : 4233
train acc:  0.78125
train loss:  0.4641086459159851
train gradient:  0.35754597575648767
iteration : 4234
train acc:  0.8359375
train loss:  0.40015479922294617
train gradient:  0.44299214159699674
iteration : 4235
train acc:  0.875
train loss:  0.35500168800354004
train gradient:  0.23470037876969152
iteration : 4236
train acc:  0.84375
train loss:  0.32447487115859985
train gradient:  0.1563555054249997
iteration : 4237
train acc:  0.8203125
train loss:  0.3825474679470062
train gradient:  0.21632146424433313
iteration : 4238
train acc:  0.796875
train loss:  0.38214653730392456
train gradient:  0.2887067760999917
iteration : 4239
train acc:  0.8515625
train loss:  0.3390173017978668
train gradient:  0.24120715968113635
iteration : 4240
train acc:  0.875
train loss:  0.29208576679229736
train gradient:  0.22677271726227097
iteration : 4241
train acc:  0.828125
train loss:  0.39321720600128174
train gradient:  0.23001896103002184
iteration : 4242
train acc:  0.7734375
train loss:  0.4467337727546692
train gradient:  0.26754171373078933
iteration : 4243
train acc:  0.8515625
train loss:  0.29673004150390625
train gradient:  0.19651048198747623
iteration : 4244
train acc:  0.890625
train loss:  0.27637386322021484
train gradient:  0.13888914075254322
iteration : 4245
train acc:  0.8515625
train loss:  0.3649500608444214
train gradient:  0.19009971582591648
iteration : 4246
train acc:  0.890625
train loss:  0.3104296922683716
train gradient:  0.1530941840654118
iteration : 4247
train acc:  0.8984375
train loss:  0.33083194494247437
train gradient:  0.2520762513550826
iteration : 4248
train acc:  0.8984375
train loss:  0.286140501499176
train gradient:  0.21818880184287393
iteration : 4249
train acc:  0.8515625
train loss:  0.35191377997398376
train gradient:  0.28805448599571987
iteration : 4250
train acc:  0.875
train loss:  0.2895142734050751
train gradient:  0.18234306888907095
iteration : 4251
train acc:  0.859375
train loss:  0.31051892042160034
train gradient:  0.17732704155117762
iteration : 4252
train acc:  0.828125
train loss:  0.34999608993530273
train gradient:  1.1823932392496104
iteration : 4253
train acc:  0.890625
train loss:  0.28886866569519043
train gradient:  0.21682796759277218
iteration : 4254
train acc:  0.8125
train loss:  0.4185333549976349
train gradient:  0.4435362736150304
iteration : 4255
train acc:  0.8515625
train loss:  0.35555499792099
train gradient:  0.29893307723661616
iteration : 4256
train acc:  0.859375
train loss:  0.30391553044319153
train gradient:  0.20913256166083127
iteration : 4257
train acc:  0.875
train loss:  0.27496469020843506
train gradient:  0.17071124873192106
iteration : 4258
train acc:  0.8515625
train loss:  0.3918752670288086
train gradient:  0.3658067332896766
iteration : 4259
train acc:  0.875
train loss:  0.3384900391101837
train gradient:  0.228957768729438
iteration : 4260
train acc:  0.8359375
train loss:  0.34554553031921387
train gradient:  0.25061943776944606
iteration : 4261
train acc:  0.8125
train loss:  0.37837257981300354
train gradient:  0.3785119662057405
iteration : 4262
train acc:  0.8125
train loss:  0.43569692969322205
train gradient:  0.4026439224444572
iteration : 4263
train acc:  0.828125
train loss:  0.35110044479370117
train gradient:  0.25599182096792655
iteration : 4264
train acc:  0.8203125
train loss:  0.4207131862640381
train gradient:  0.29064121084308975
iteration : 4265
train acc:  0.875
train loss:  0.2894032597541809
train gradient:  0.19220659157469133
iteration : 4266
train acc:  0.90625
train loss:  0.2929074764251709
train gradient:  0.28929658875398884
iteration : 4267
train acc:  0.890625
train loss:  0.347583532333374
train gradient:  0.24434189943241755
iteration : 4268
train acc:  0.8515625
train loss:  0.38951462507247925
train gradient:  0.30849094350928397
iteration : 4269
train acc:  0.7890625
train loss:  0.46080589294433594
train gradient:  0.46677703870273307
iteration : 4270
train acc:  0.8125
train loss:  0.44612178206443787
train gradient:  0.3831654195716565
iteration : 4271
train acc:  0.8046875
train loss:  0.403192937374115
train gradient:  0.23950768018975915
iteration : 4272
train acc:  0.828125
train loss:  0.31811729073524475
train gradient:  0.2694805347013722
iteration : 4273
train acc:  0.828125
train loss:  0.3638646602630615
train gradient:  0.27345643645719053
iteration : 4274
train acc:  0.8203125
train loss:  0.44285640120506287
train gradient:  0.394911930045903
iteration : 4275
train acc:  0.8984375
train loss:  0.25349992513656616
train gradient:  0.14006865888555123
iteration : 4276
train acc:  0.8203125
train loss:  0.37970083951950073
train gradient:  0.24700743793135463
iteration : 4277
train acc:  0.8515625
train loss:  0.29022878408432007
train gradient:  0.23751405934449282
iteration : 4278
train acc:  0.8046875
train loss:  0.42699408531188965
train gradient:  0.4063523223301698
iteration : 4279
train acc:  0.890625
train loss:  0.3514346182346344
train gradient:  0.30169539540967255
iteration : 4280
train acc:  0.828125
train loss:  0.36972081661224365
train gradient:  0.3503471727806659
iteration : 4281
train acc:  0.890625
train loss:  0.2626405954360962
train gradient:  0.17292455071694757
iteration : 4282
train acc:  0.8515625
train loss:  0.3441848158836365
train gradient:  0.28795024584317075
iteration : 4283
train acc:  0.8125
train loss:  0.3883730173110962
train gradient:  0.3602223635781571
iteration : 4284
train acc:  0.84375
train loss:  0.39178887009620667
train gradient:  0.3298939616701807
iteration : 4285
train acc:  0.9296875
train loss:  0.22315125167369843
train gradient:  0.14965269239582574
iteration : 4286
train acc:  0.8671875
train loss:  0.3303242325782776
train gradient:  0.2282262608934793
iteration : 4287
train acc:  0.84375
train loss:  0.337240993976593
train gradient:  0.26012178928017415
iteration : 4288
train acc:  0.8515625
train loss:  0.33584216237068176
train gradient:  0.18348067583987812
iteration : 4289
train acc:  0.875
train loss:  0.3406703472137451
train gradient:  0.22245619193293123
iteration : 4290
train acc:  0.828125
train loss:  0.3948458731174469
train gradient:  0.33215822950952534
iteration : 4291
train acc:  0.828125
train loss:  0.34131452441215515
train gradient:  0.17591716581869116
iteration : 4292
train acc:  0.828125
train loss:  0.4298635721206665
train gradient:  0.3336220992562049
iteration : 4293
train acc:  0.828125
train loss:  0.3631078600883484
train gradient:  0.39688106523338695
iteration : 4294
train acc:  0.875
train loss:  0.3032366633415222
train gradient:  0.21648033090670812
iteration : 4295
train acc:  0.890625
train loss:  0.28050315380096436
train gradient:  0.158077364065541
iteration : 4296
train acc:  0.859375
train loss:  0.35266751050949097
train gradient:  0.27121057481073185
iteration : 4297
train acc:  0.890625
train loss:  0.28143733739852905
train gradient:  0.1549744571748581
iteration : 4298
train acc:  0.8125
train loss:  0.41635069251060486
train gradient:  0.27030815989240037
iteration : 4299
train acc:  0.796875
train loss:  0.45316457748413086
train gradient:  0.5158052250456746
iteration : 4300
train acc:  0.8359375
train loss:  0.37904998660087585
train gradient:  0.3109136700297851
iteration : 4301
train acc:  0.8515625
train loss:  0.33713752031326294
train gradient:  0.31260623834904927
iteration : 4302
train acc:  0.8984375
train loss:  0.30721515417099
train gradient:  0.18257132698542075
iteration : 4303
train acc:  0.8515625
train loss:  0.3114703297615051
train gradient:  0.22843656162269016
iteration : 4304
train acc:  0.890625
train loss:  0.27336156368255615
train gradient:  0.14575096788843828
iteration : 4305
train acc:  0.796875
train loss:  0.4381786286830902
train gradient:  0.39635504772906677
iteration : 4306
train acc:  0.8984375
train loss:  0.288999080657959
train gradient:  0.21216620884882928
iteration : 4307
train acc:  0.84375
train loss:  0.36023464798927307
train gradient:  0.2538964052799181
iteration : 4308
train acc:  0.8671875
train loss:  0.32908982038497925
train gradient:  0.2639628467266466
iteration : 4309
train acc:  0.796875
train loss:  0.41384512186050415
train gradient:  0.46537928073901563
iteration : 4310
train acc:  0.8359375
train loss:  0.32969412207603455
train gradient:  0.2643109414310111
iteration : 4311
train acc:  0.8515625
train loss:  0.32548630237579346
train gradient:  0.19242192389737792
iteration : 4312
train acc:  0.8515625
train loss:  0.35546770691871643
train gradient:  0.28429176472991685
iteration : 4313
train acc:  0.8515625
train loss:  0.38144543766975403
train gradient:  0.28845653199135357
iteration : 4314
train acc:  0.8359375
train loss:  0.3977794349193573
train gradient:  0.3901194803863109
iteration : 4315
train acc:  0.84375
train loss:  0.40282031893730164
train gradient:  0.38244635776453473
iteration : 4316
train acc:  0.859375
train loss:  0.3497893214225769
train gradient:  0.24996036972724806
iteration : 4317
train acc:  0.8359375
train loss:  0.40079769492149353
train gradient:  0.24861302916605643
iteration : 4318
train acc:  0.796875
train loss:  0.4503357410430908
train gradient:  0.29925098865896177
iteration : 4319
train acc:  0.8671875
train loss:  0.3060188889503479
train gradient:  0.2147905593444557
iteration : 4320
train acc:  0.8671875
train loss:  0.34661126136779785
train gradient:  0.27759217387626994
iteration : 4321
train acc:  0.890625
train loss:  0.2941562533378601
train gradient:  0.18958196718966475
iteration : 4322
train acc:  0.8671875
train loss:  0.3199348449707031
train gradient:  0.19660377078726116
iteration : 4323
train acc:  0.8828125
train loss:  0.3264295160770416
train gradient:  0.1766970656812777
iteration : 4324
train acc:  0.8359375
train loss:  0.36445727944374084
train gradient:  0.30456935524153195
iteration : 4325
train acc:  0.8125
train loss:  0.3717046082019806
train gradient:  0.21933509336529233
iteration : 4326
train acc:  0.8359375
train loss:  0.38261890411376953
train gradient:  0.37945504908343375
iteration : 4327
train acc:  0.8203125
train loss:  0.40152260661125183
train gradient:  0.2580399514295446
iteration : 4328
train acc:  0.890625
train loss:  0.27339327335357666
train gradient:  0.2584630583370936
iteration : 4329
train acc:  0.859375
train loss:  0.2895747721195221
train gradient:  0.2384286968165325
iteration : 4330
train acc:  0.8671875
train loss:  0.2845894694328308
train gradient:  0.23193152408460274
iteration : 4331
train acc:  0.890625
train loss:  0.2962656021118164
train gradient:  0.19386420829279483
iteration : 4332
train acc:  0.8359375
train loss:  0.39650586247444153
train gradient:  0.3594714323593876
iteration : 4333
train acc:  0.828125
train loss:  0.38608092069625854
train gradient:  0.48565052316626695
iteration : 4334
train acc:  0.8359375
train loss:  0.40418291091918945
train gradient:  0.3682166203361222
iteration : 4335
train acc:  0.828125
train loss:  0.35341644287109375
train gradient:  0.22954972385341937
iteration : 4336
train acc:  0.8046875
train loss:  0.40531742572784424
train gradient:  0.3400392048735836
iteration : 4337
train acc:  0.8203125
train loss:  0.40963879227638245
train gradient:  0.24685174762579576
iteration : 4338
train acc:  0.8359375
train loss:  0.3474307060241699
train gradient:  0.27115753837695117
iteration : 4339
train acc:  0.875
train loss:  0.3284544348716736
train gradient:  0.2016135240057636
iteration : 4340
train acc:  0.828125
train loss:  0.34070271253585815
train gradient:  0.30189313445309146
iteration : 4341
train acc:  0.8828125
train loss:  0.3068513870239258
train gradient:  0.15922502729307902
iteration : 4342
train acc:  0.8515625
train loss:  0.3260549306869507
train gradient:  0.21597269672053154
iteration : 4343
train acc:  0.8515625
train loss:  0.343994677066803
train gradient:  0.2629848026512859
iteration : 4344
train acc:  0.8671875
train loss:  0.31855863332748413
train gradient:  0.2677065515461059
iteration : 4345
train acc:  0.890625
train loss:  0.28180548548698425
train gradient:  0.21517041159267436
iteration : 4346
train acc:  0.7890625
train loss:  0.42018795013427734
train gradient:  0.3348253410748859
iteration : 4347
train acc:  0.78125
train loss:  0.3893395662307739
train gradient:  0.32265459205521463
iteration : 4348
train acc:  0.8671875
train loss:  0.3179018795490265
train gradient:  0.21665115455570144
iteration : 4349
train acc:  0.7421875
train loss:  0.4763057231903076
train gradient:  0.6833104797637468
iteration : 4350
train acc:  0.8828125
train loss:  0.3282983899116516
train gradient:  0.1740080766243373
iteration : 4351
train acc:  0.8671875
train loss:  0.32240724563598633
train gradient:  0.18376384409587826
iteration : 4352
train acc:  0.8125
train loss:  0.3755316138267517
train gradient:  0.32090856883811825
iteration : 4353
train acc:  0.8125
train loss:  0.4413434863090515
train gradient:  0.3800311624857219
iteration : 4354
train acc:  0.8046875
train loss:  0.36541152000427246
train gradient:  0.23733665512383773
iteration : 4355
train acc:  0.828125
train loss:  0.35533407330513
train gradient:  0.3518338717549748
iteration : 4356
train acc:  0.8984375
train loss:  0.26965025067329407
train gradient:  0.17948855959035376
iteration : 4357
train acc:  0.8671875
train loss:  0.33659952878952026
train gradient:  0.24613818348715077
iteration : 4358
train acc:  0.875
train loss:  0.2638583481311798
train gradient:  0.148097428340213
iteration : 4359
train acc:  0.8515625
train loss:  0.3265587389469147
train gradient:  0.2278747944799825
iteration : 4360
train acc:  0.8828125
train loss:  0.2815183699131012
train gradient:  0.18877210740220332
iteration : 4361
train acc:  0.8828125
train loss:  0.31270310282707214
train gradient:  0.16307439518657413
iteration : 4362
train acc:  0.875
train loss:  0.3591993451118469
train gradient:  0.22711682289362348
iteration : 4363
train acc:  0.796875
train loss:  0.39969688653945923
train gradient:  0.4381789325667154
iteration : 4364
train acc:  0.8515625
train loss:  0.33245956897735596
train gradient:  0.2618255665899344
iteration : 4365
train acc:  0.8828125
train loss:  0.2746585011482239
train gradient:  0.20781702988808531
iteration : 4366
train acc:  0.8359375
train loss:  0.35443127155303955
train gradient:  0.32456455798510875
iteration : 4367
train acc:  0.8671875
train loss:  0.39006951451301575
train gradient:  0.26822283594513496
iteration : 4368
train acc:  0.8984375
train loss:  0.2998056709766388
train gradient:  0.19622942535370871
iteration : 4369
train acc:  0.8671875
train loss:  0.35365375876426697
train gradient:  0.1986843066547228
iteration : 4370
train acc:  0.8671875
train loss:  0.34732291102409363
train gradient:  0.37048025894368414
iteration : 4371
train acc:  0.796875
train loss:  0.4626586437225342
train gradient:  0.3927555966028285
iteration : 4372
train acc:  0.828125
train loss:  0.3060181736946106
train gradient:  0.32704156876662926
iteration : 4373
train acc:  0.7890625
train loss:  0.4187142848968506
train gradient:  0.4294021484248954
iteration : 4374
train acc:  0.8671875
train loss:  0.32235246896743774
train gradient:  0.2855504174136211
iteration : 4375
train acc:  0.8671875
train loss:  0.305328369140625
train gradient:  0.21420819056268803
iteration : 4376
train acc:  0.8203125
train loss:  0.3847268223762512
train gradient:  0.32438757307065536
iteration : 4377
train acc:  0.8203125
train loss:  0.4125652015209198
train gradient:  0.36815483784020964
iteration : 4378
train acc:  0.90625
train loss:  0.2702212929725647
train gradient:  0.19802643944045517
iteration : 4379
train acc:  0.84375
train loss:  0.35191217064857483
train gradient:  0.3346099582533661
iteration : 4380
train acc:  0.890625
train loss:  0.3079250454902649
train gradient:  0.29825315958592535
iteration : 4381
train acc:  0.78125
train loss:  0.41675546765327454
train gradient:  0.33705063775293354
iteration : 4382
train acc:  0.8515625
train loss:  0.36005058884620667
train gradient:  0.2729372472694644
iteration : 4383
train acc:  0.8671875
train loss:  0.28978073596954346
train gradient:  0.25904923744221237
iteration : 4384
train acc:  0.8671875
train loss:  0.2960178256034851
train gradient:  0.22768007989069486
iteration : 4385
train acc:  0.8203125
train loss:  0.389933705329895
train gradient:  0.21229707525410194
iteration : 4386
train acc:  0.8515625
train loss:  0.34439846873283386
train gradient:  0.2671555834508703
iteration : 4387
train acc:  0.859375
train loss:  0.35642775893211365
train gradient:  0.39449946958549204
iteration : 4388
train acc:  0.8828125
train loss:  0.30231156945228577
train gradient:  0.24352009682873504
iteration : 4389
train acc:  0.890625
train loss:  0.2614847421646118
train gradient:  0.17359190761465712
iteration : 4390
train acc:  0.8125
train loss:  0.39870885014533997
train gradient:  0.3939903819830632
iteration : 4391
train acc:  0.8359375
train loss:  0.41579437255859375
train gradient:  0.36594942125742047
iteration : 4392
train acc:  0.8515625
train loss:  0.38255882263183594
train gradient:  0.23339891661468687
iteration : 4393
train acc:  0.84375
train loss:  0.4106684625148773
train gradient:  0.4371878006863197
iteration : 4394
train acc:  0.828125
train loss:  0.39094382524490356
train gradient:  0.35919792128735845
iteration : 4395
train acc:  0.828125
train loss:  0.3847578465938568
train gradient:  0.2789063115644803
iteration : 4396
train acc:  0.875
train loss:  0.3225962221622467
train gradient:  0.28495035855260636
iteration : 4397
train acc:  0.8359375
train loss:  0.3662174940109253
train gradient:  0.3002626776294686
iteration : 4398
train acc:  0.8828125
train loss:  0.2794201970100403
train gradient:  0.17110015028886477
iteration : 4399
train acc:  0.90625
train loss:  0.2770405411720276
train gradient:  0.251593239798985
iteration : 4400
train acc:  0.8359375
train loss:  0.39500904083251953
train gradient:  0.3282091860194351
iteration : 4401
train acc:  0.828125
train loss:  0.3467835485935211
train gradient:  0.2584526372453307
iteration : 4402
train acc:  0.8671875
train loss:  0.3333772122859955
train gradient:  0.30815685727191044
iteration : 4403
train acc:  0.8203125
train loss:  0.4384213984012604
train gradient:  0.3861001611906123
iteration : 4404
train acc:  0.8984375
train loss:  0.3088863492012024
train gradient:  0.2812786073287025
iteration : 4405
train acc:  0.8515625
train loss:  0.34158855676651
train gradient:  0.19155813735446242
iteration : 4406
train acc:  0.8046875
train loss:  0.36970043182373047
train gradient:  0.36608872078437843
iteration : 4407
train acc:  0.8125
train loss:  0.4077449440956116
train gradient:  0.4587368794923652
iteration : 4408
train acc:  0.8203125
train loss:  0.4009140729904175
train gradient:  0.35940361912538593
iteration : 4409
train acc:  0.8203125
train loss:  0.38884031772613525
train gradient:  0.434935602161006
iteration : 4410
train acc:  0.8125
train loss:  0.36331260204315186
train gradient:  0.30728476239730634
iteration : 4411
train acc:  0.8828125
train loss:  0.29125887155532837
train gradient:  0.2099436509110063
iteration : 4412
train acc:  0.8515625
train loss:  0.34812793135643005
train gradient:  0.32011637900792084
iteration : 4413
train acc:  0.8671875
train loss:  0.31731557846069336
train gradient:  0.20580286447664486
iteration : 4414
train acc:  0.8203125
train loss:  0.3516252636909485
train gradient:  0.243711278407289
iteration : 4415
train acc:  0.8046875
train loss:  0.3912825286388397
train gradient:  0.41960519012893543
iteration : 4416
train acc:  0.7734375
train loss:  0.4722232222557068
train gradient:  0.572016314626157
iteration : 4417
train acc:  0.8046875
train loss:  0.5358647704124451
train gradient:  0.5782552256703389
iteration : 4418
train acc:  0.8359375
train loss:  0.3909575343132019
train gradient:  0.25906436166765223
iteration : 4419
train acc:  0.8359375
train loss:  0.345919132232666
train gradient:  0.2601342876438572
iteration : 4420
train acc:  0.8671875
train loss:  0.3653910458087921
train gradient:  0.29026989493132677
iteration : 4421
train acc:  0.8515625
train loss:  0.3442343473434448
train gradient:  0.5001631113925058
iteration : 4422
train acc:  0.8125
train loss:  0.3811102509498596
train gradient:  0.24837819334269495
iteration : 4423
train acc:  0.84375
train loss:  0.3444862365722656
train gradient:  0.22780088440003918
iteration : 4424
train acc:  0.8515625
train loss:  0.40493083000183105
train gradient:  0.35349416857103194
iteration : 4425
train acc:  0.8984375
train loss:  0.2585430443286896
train gradient:  0.2235297755316124
iteration : 4426
train acc:  0.84375
train loss:  0.34559890627861023
train gradient:  0.19078908563302177
iteration : 4427
train acc:  0.8046875
train loss:  0.4235855042934418
train gradient:  0.4054704512862536
iteration : 4428
train acc:  0.8828125
train loss:  0.372203528881073
train gradient:  0.3017976375582968
iteration : 4429
train acc:  0.8671875
train loss:  0.34557563066482544
train gradient:  0.2875963293607231
iteration : 4430
train acc:  0.84375
train loss:  0.33575040102005005
train gradient:  0.2680761790540219
iteration : 4431
train acc:  0.8359375
train loss:  0.3976033627986908
train gradient:  0.3390564930650172
iteration : 4432
train acc:  0.8125
train loss:  0.3873558044433594
train gradient:  0.2566269231582507
iteration : 4433
train acc:  0.8359375
train loss:  0.3048552870750427
train gradient:  0.20875953109943035
iteration : 4434
train acc:  0.84375
train loss:  0.4435015320777893
train gradient:  0.29196531438455814
iteration : 4435
train acc:  0.8203125
train loss:  0.3962080776691437
train gradient:  0.31445180153498997
iteration : 4436
train acc:  0.8984375
train loss:  0.260972797870636
train gradient:  0.14866911303793234
iteration : 4437
train acc:  0.8984375
train loss:  0.2895317077636719
train gradient:  0.135752572087766
iteration : 4438
train acc:  0.8671875
train loss:  0.31794416904449463
train gradient:  0.25212037835239304
iteration : 4439
train acc:  0.8515625
train loss:  0.3151486814022064
train gradient:  0.23680266377249665
iteration : 4440
train acc:  0.875
train loss:  0.3081560730934143
train gradient:  0.2783789876647429
iteration : 4441
train acc:  0.8125
train loss:  0.3868604302406311
train gradient:  0.238917193340927
iteration : 4442
train acc:  0.8515625
train loss:  0.3348401188850403
train gradient:  0.3025944109928957
iteration : 4443
train acc:  0.8515625
train loss:  0.3249000608921051
train gradient:  0.23277182559483658
iteration : 4444
train acc:  0.8828125
train loss:  0.34042617678642273
train gradient:  0.1966913849133276
iteration : 4445
train acc:  0.8359375
train loss:  0.37619271874427795
train gradient:  0.2808473915820844
iteration : 4446
train acc:  0.84375
train loss:  0.376043438911438
train gradient:  0.22415243644791416
iteration : 4447
train acc:  0.890625
train loss:  0.2969299554824829
train gradient:  0.21201979381821634
iteration : 4448
train acc:  0.890625
train loss:  0.29815202951431274
train gradient:  0.21628158724717186
iteration : 4449
train acc:  0.796875
train loss:  0.42331838607788086
train gradient:  0.38733900394279197
iteration : 4450
train acc:  0.7890625
train loss:  0.4435240924358368
train gradient:  0.4974405291769233
iteration : 4451
train acc:  0.8046875
train loss:  0.38484734296798706
train gradient:  0.24045307763369878
iteration : 4452
train acc:  0.859375
train loss:  0.27507948875427246
train gradient:  0.18007057410346664
iteration : 4453
train acc:  0.8671875
train loss:  0.3080673813819885
train gradient:  0.2290683646879791
iteration : 4454
train acc:  0.7734375
train loss:  0.4128468930721283
train gradient:  0.4527511387735967
iteration : 4455
train acc:  0.84375
train loss:  0.36431828141212463
train gradient:  0.28440383711276856
iteration : 4456
train acc:  0.8203125
train loss:  0.3604108691215515
train gradient:  0.29249311926567345
iteration : 4457
train acc:  0.8671875
train loss:  0.3435506820678711
train gradient:  0.21084548703764516
iteration : 4458
train acc:  0.859375
train loss:  0.37642088532447815
train gradient:  0.2768639094009322
iteration : 4459
train acc:  0.8203125
train loss:  0.3331156075000763
train gradient:  0.2929598620120601
iteration : 4460
train acc:  0.8671875
train loss:  0.30823174118995667
train gradient:  0.22954546663902992
iteration : 4461
train acc:  0.8671875
train loss:  0.3146606981754303
train gradient:  0.19341618576036723
iteration : 4462
train acc:  0.8046875
train loss:  0.4203633666038513
train gradient:  0.3160167919026534
iteration : 4463
train acc:  0.828125
train loss:  0.37058165669441223
train gradient:  0.24955025623232815
iteration : 4464
train acc:  0.84375
train loss:  0.36147114634513855
train gradient:  0.35475261370117794
iteration : 4465
train acc:  0.828125
train loss:  0.4166199862957001
train gradient:  0.3636571050036279
iteration : 4466
train acc:  0.8125
train loss:  0.38819825649261475
train gradient:  0.313933370486201
iteration : 4467
train acc:  0.8046875
train loss:  0.44075730443000793
train gradient:  0.36846945170115064
iteration : 4468
train acc:  0.8359375
train loss:  0.3601914942264557
train gradient:  0.22185652698136665
iteration : 4469
train acc:  0.828125
train loss:  0.39960455894470215
train gradient:  0.26538152188903286
iteration : 4470
train acc:  0.828125
train loss:  0.3882594704627991
train gradient:  0.27077836746245615
iteration : 4471
train acc:  0.8359375
train loss:  0.3710823357105255
train gradient:  0.24913115936243987
iteration : 4472
train acc:  0.8046875
train loss:  0.4152216911315918
train gradient:  0.33161149163588116
iteration : 4473
train acc:  0.828125
train loss:  0.3413752317428589
train gradient:  0.2896413062304654
iteration : 4474
train acc:  0.84375
train loss:  0.3716772794723511
train gradient:  0.24181578968706696
iteration : 4475
train acc:  0.8359375
train loss:  0.336642861366272
train gradient:  0.3227887404702965
iteration : 4476
train acc:  0.75
train loss:  0.39518627524375916
train gradient:  0.2847339309343023
iteration : 4477
train acc:  0.8671875
train loss:  0.34177178144454956
train gradient:  0.24216853303582092
iteration : 4478
train acc:  0.78125
train loss:  0.44764938950538635
train gradient:  0.44217277339516214
iteration : 4479
train acc:  0.875
train loss:  0.3012286424636841
train gradient:  0.22867827263119647
iteration : 4480
train acc:  0.8359375
train loss:  0.3533371388912201
train gradient:  0.20963479945683963
iteration : 4481
train acc:  0.8515625
train loss:  0.308067262172699
train gradient:  0.26136843724572023
iteration : 4482
train acc:  0.796875
train loss:  0.3867337703704834
train gradient:  0.35027256001926227
iteration : 4483
train acc:  0.875
train loss:  0.315498411655426
train gradient:  0.20518163396559994
iteration : 4484
train acc:  0.84375
train loss:  0.36419862508773804
train gradient:  0.26909560524046944
iteration : 4485
train acc:  0.828125
train loss:  0.37476301193237305
train gradient:  0.29943603609900193
iteration : 4486
train acc:  0.8125
train loss:  0.39171215891838074
train gradient:  0.27564410352070534
iteration : 4487
train acc:  0.8125
train loss:  0.35028350353240967
train gradient:  0.28905455645466105
iteration : 4488
train acc:  0.859375
train loss:  0.33090662956237793
train gradient:  0.2651988290465621
iteration : 4489
train acc:  0.7890625
train loss:  0.491951048374176
train gradient:  0.7870971510269541
iteration : 4490
train acc:  0.8203125
train loss:  0.33315640687942505
train gradient:  0.3979079473070455
iteration : 4491
train acc:  0.8203125
train loss:  0.39994123578071594
train gradient:  0.27760279661978304
iteration : 4492
train acc:  0.8359375
train loss:  0.327766478061676
train gradient:  0.20050824452087707
iteration : 4493
train acc:  0.7890625
train loss:  0.4605329632759094
train gradient:  0.4790330047710499
iteration : 4494
train acc:  0.859375
train loss:  0.32847627997398376
train gradient:  0.17206240088560437
iteration : 4495
train acc:  0.7890625
train loss:  0.38285383582115173
train gradient:  0.31444328175225605
iteration : 4496
train acc:  0.8203125
train loss:  0.408605694770813
train gradient:  2.2132077133228503
iteration : 4497
train acc:  0.8515625
train loss:  0.39579206705093384
train gradient:  0.28233882170978963
iteration : 4498
train acc:  0.859375
train loss:  0.3259299695491791
train gradient:  0.2001230677779562
iteration : 4499
train acc:  0.8046875
train loss:  0.4314516484737396
train gradient:  0.3698270043960159
iteration : 4500
train acc:  0.8125
train loss:  0.3525012135505676
train gradient:  0.2593057780696889
iteration : 4501
train acc:  0.8359375
train loss:  0.35056212544441223
train gradient:  0.37418901446667197
iteration : 4502
train acc:  0.828125
train loss:  0.37525075674057007
train gradient:  0.291588777352224
iteration : 4503
train acc:  0.796875
train loss:  0.43852290511131287
train gradient:  0.2787981792958321
iteration : 4504
train acc:  0.8359375
train loss:  0.36747193336486816
train gradient:  0.2963431212723258
iteration : 4505
train acc:  0.84375
train loss:  0.3721252381801605
train gradient:  0.2771675872197704
iteration : 4506
train acc:  0.828125
train loss:  0.3520779609680176
train gradient:  0.3205743614222647
iteration : 4507
train acc:  0.875
train loss:  0.2967407703399658
train gradient:  0.20398808567152754
iteration : 4508
train acc:  0.875
train loss:  0.2966423034667969
train gradient:  0.26208480114973204
iteration : 4509
train acc:  0.8515625
train loss:  0.3145517110824585
train gradient:  0.2616788363014804
iteration : 4510
train acc:  0.84375
train loss:  0.3059137165546417
train gradient:  0.19904517967368365
iteration : 4511
train acc:  0.8359375
train loss:  0.41253146529197693
train gradient:  0.2947516119438577
iteration : 4512
train acc:  0.8828125
train loss:  0.3217676877975464
train gradient:  0.19184812332362805
iteration : 4513
train acc:  0.8203125
train loss:  0.4027474820613861
train gradient:  0.2528736352776508
iteration : 4514
train acc:  0.8671875
train loss:  0.3250736594200134
train gradient:  0.2832662242819974
iteration : 4515
train acc:  0.84375
train loss:  0.30635881423950195
train gradient:  0.17449698501079342
iteration : 4516
train acc:  0.828125
train loss:  0.36766213178634644
train gradient:  0.30222087548998866
iteration : 4517
train acc:  0.8984375
train loss:  0.26389259099960327
train gradient:  0.2698764461230022
iteration : 4518
train acc:  0.84375
train loss:  0.32122331857681274
train gradient:  0.23103870594580123
iteration : 4519
train acc:  0.7734375
train loss:  0.4005202651023865
train gradient:  0.520950275503948
iteration : 4520
train acc:  0.8515625
train loss:  0.3170378804206848
train gradient:  0.19461632154839698
iteration : 4521
train acc:  0.875
train loss:  0.340204656124115
train gradient:  0.29855209160659174
iteration : 4522
train acc:  0.7890625
train loss:  0.45673495531082153
train gradient:  0.5185201708037788
iteration : 4523
train acc:  0.78125
train loss:  0.41891199350357056
train gradient:  0.32243806047259277
iteration : 4524
train acc:  0.84375
train loss:  0.40430939197540283
train gradient:  0.3064903090803454
iteration : 4525
train acc:  0.8671875
train loss:  0.3188537359237671
train gradient:  0.20975626428178779
iteration : 4526
train acc:  0.890625
train loss:  0.30250826478004456
train gradient:  0.22373936136656286
iteration : 4527
train acc:  0.8359375
train loss:  0.3849366307258606
train gradient:  0.3486458677245173
iteration : 4528
train acc:  0.859375
train loss:  0.3188552260398865
train gradient:  0.20691514060690175
iteration : 4529
train acc:  0.8828125
train loss:  0.3040367364883423
train gradient:  0.1823576839150947
iteration : 4530
train acc:  0.8984375
train loss:  0.255953311920166
train gradient:  0.2010843234818075
iteration : 4531
train acc:  0.8125
train loss:  0.42801612615585327
train gradient:  0.34129005868693135
iteration : 4532
train acc:  0.875
train loss:  0.3411104083061218
train gradient:  0.21326649302423878
iteration : 4533
train acc:  0.8125
train loss:  0.40158355236053467
train gradient:  0.3599272155495597
iteration : 4534
train acc:  0.875
train loss:  0.2993992865085602
train gradient:  0.2423378668741807
iteration : 4535
train acc:  0.84375
train loss:  0.3670153021812439
train gradient:  0.31210240158857305
iteration : 4536
train acc:  0.8828125
train loss:  0.3221810460090637
train gradient:  0.345556055249114
iteration : 4537
train acc:  0.796875
train loss:  0.4012949466705322
train gradient:  0.48113014070018506
iteration : 4538
train acc:  0.859375
train loss:  0.347503662109375
train gradient:  0.24342463037573286
iteration : 4539
train acc:  0.859375
train loss:  0.308101087808609
train gradient:  0.2592954668083699
iteration : 4540
train acc:  0.8515625
train loss:  0.311786949634552
train gradient:  0.17073897957323403
iteration : 4541
train acc:  0.8046875
train loss:  0.47920382022857666
train gradient:  0.4956983676680565
iteration : 4542
train acc:  0.8359375
train loss:  0.39033177495002747
train gradient:  0.3058584330694651
iteration : 4543
train acc:  0.859375
train loss:  0.37502187490463257
train gradient:  0.29349685791232477
iteration : 4544
train acc:  0.859375
train loss:  0.3040999472141266
train gradient:  0.23820082604189677
iteration : 4545
train acc:  0.8984375
train loss:  0.27931421995162964
train gradient:  0.1779358511288333
iteration : 4546
train acc:  0.8359375
train loss:  0.4004574418067932
train gradient:  0.3488831714668577
iteration : 4547
train acc:  0.8515625
train loss:  0.3517657518386841
train gradient:  0.2869772844783666
iteration : 4548
train acc:  0.84375
train loss:  0.37755104899406433
train gradient:  0.29020286014835595
iteration : 4549
train acc:  0.828125
train loss:  0.40584152936935425
train gradient:  0.5232320215611761
iteration : 4550
train acc:  0.8359375
train loss:  0.4178488254547119
train gradient:  0.3439874018750078
iteration : 4551
train acc:  0.8046875
train loss:  0.3351982831954956
train gradient:  0.2983371389305056
iteration : 4552
train acc:  0.8203125
train loss:  0.40120580792427063
train gradient:  0.3082656438205463
iteration : 4553
train acc:  0.78125
train loss:  0.40482568740844727
train gradient:  0.3115973464499374
iteration : 4554
train acc:  0.8203125
train loss:  0.4006870687007904
train gradient:  0.29654138968809945
iteration : 4555
train acc:  0.8359375
train loss:  0.37813350558280945
train gradient:  0.3381558445431076
iteration : 4556
train acc:  0.8671875
train loss:  0.35812050104141235
train gradient:  0.38865644067387056
iteration : 4557
train acc:  0.8125
train loss:  0.467072457075119
train gradient:  0.48407283203681334
iteration : 4558
train acc:  0.8671875
train loss:  0.3176232576370239
train gradient:  0.31791603096231696
iteration : 4559
train acc:  0.8671875
train loss:  0.3733828067779541
train gradient:  0.28314441128240553
iteration : 4560
train acc:  0.828125
train loss:  0.34081849455833435
train gradient:  0.19712754358996692
iteration : 4561
train acc:  0.84375
train loss:  0.35793083906173706
train gradient:  0.2394917732367191
iteration : 4562
train acc:  0.84375
train loss:  0.33870068192481995
train gradient:  0.40988015676073847
iteration : 4563
train acc:  0.828125
train loss:  0.3541339635848999
train gradient:  0.4201946195752255
iteration : 4564
train acc:  0.890625
train loss:  0.3073686361312866
train gradient:  0.24109906586287383
iteration : 4565
train acc:  0.8671875
train loss:  0.3230239748954773
train gradient:  0.25212751064663724
iteration : 4566
train acc:  0.765625
train loss:  0.38879913091659546
train gradient:  0.20964660768892268
iteration : 4567
train acc:  0.859375
train loss:  0.37595513463020325
train gradient:  0.3450132898172233
iteration : 4568
train acc:  0.7734375
train loss:  0.3998369574546814
train gradient:  0.5149069797831155
iteration : 4569
train acc:  0.8359375
train loss:  0.33092600107192993
train gradient:  0.2296940998486478
iteration : 4570
train acc:  0.8125
train loss:  0.3701304793357849
train gradient:  0.27561324555357813
iteration : 4571
train acc:  0.859375
train loss:  0.3717668056488037
train gradient:  0.37723742847041414
iteration : 4572
train acc:  0.84375
train loss:  0.29838234186172485
train gradient:  0.21412145558414533
iteration : 4573
train acc:  0.8671875
train loss:  0.2979907989501953
train gradient:  0.27363630746248935
iteration : 4574
train acc:  0.828125
train loss:  0.3294275999069214
train gradient:  0.23793285666326924
iteration : 4575
train acc:  0.8203125
train loss:  0.40327751636505127
train gradient:  0.4890137029989983
iteration : 4576
train acc:  0.8359375
train loss:  0.41916024684906006
train gradient:  0.2798676711426776
iteration : 4577
train acc:  0.8203125
train loss:  0.3731245994567871
train gradient:  0.2668282929050187
iteration : 4578
train acc:  0.8359375
train loss:  0.3731822967529297
train gradient:  0.23463972231765717
iteration : 4579
train acc:  0.8515625
train loss:  0.33740419149398804
train gradient:  0.26725410371410124
iteration : 4580
train acc:  0.8125
train loss:  0.36417895555496216
train gradient:  0.3036653732860438
iteration : 4581
train acc:  0.8046875
train loss:  0.44407767057418823
train gradient:  0.3820278528901467
iteration : 4582
train acc:  0.78125
train loss:  0.5006688833236694
train gradient:  0.5256316058740687
iteration : 4583
train acc:  0.828125
train loss:  0.39292609691619873
train gradient:  0.35641669642140583
iteration : 4584
train acc:  0.75
train loss:  0.5161593556404114
train gradient:  0.4035089002934297
iteration : 4585
train acc:  0.8828125
train loss:  0.31682437658309937
train gradient:  0.3317838324745064
iteration : 4586
train acc:  0.859375
train loss:  0.4135398864746094
train gradient:  0.28948485039308935
iteration : 4587
train acc:  0.8046875
train loss:  0.4167940020561218
train gradient:  0.3497915621065467
iteration : 4588
train acc:  0.84375
train loss:  0.38271936774253845
train gradient:  0.2488148251665222
iteration : 4589
train acc:  0.8203125
train loss:  0.4341963529586792
train gradient:  0.4304379182299281
iteration : 4590
train acc:  0.8515625
train loss:  0.34163594245910645
train gradient:  0.307184995924128
iteration : 4591
train acc:  0.7421875
train loss:  0.4453012943267822
train gradient:  0.4924245554242697
iteration : 4592
train acc:  0.8203125
train loss:  0.3507559895515442
train gradient:  0.25464534853562076
iteration : 4593
train acc:  0.8359375
train loss:  0.3914906978607178
train gradient:  0.2634053344331051
iteration : 4594
train acc:  0.8828125
train loss:  0.29352718591690063
train gradient:  0.17737175194543592
iteration : 4595
train acc:  0.875
train loss:  0.2904309928417206
train gradient:  0.11759835828337564
iteration : 4596
train acc:  0.8125
train loss:  0.3558652400970459
train gradient:  0.29870945582015873
iteration : 4597
train acc:  0.8046875
train loss:  0.44617441296577454
train gradient:  0.31751256270281175
iteration : 4598
train acc:  0.890625
train loss:  0.35896939039230347
train gradient:  0.2095200504466878
iteration : 4599
train acc:  0.8203125
train loss:  0.3783528208732605
train gradient:  0.3178661395124032
iteration : 4600
train acc:  0.8828125
train loss:  0.3167703151702881
train gradient:  0.18487357092685566
iteration : 4601
train acc:  0.84375
train loss:  0.3593524098396301
train gradient:  0.28724474810321843
iteration : 4602
train acc:  0.859375
train loss:  0.2987653613090515
train gradient:  0.16808760307319695
iteration : 4603
train acc:  0.8359375
train loss:  0.34797102212905884
train gradient:  0.2542330520570194
iteration : 4604
train acc:  0.890625
train loss:  0.3461272418498993
train gradient:  0.23485759144879995
iteration : 4605
train acc:  0.8359375
train loss:  0.3655719459056854
train gradient:  0.2303362108850434
iteration : 4606
train acc:  0.8359375
train loss:  0.3295260965824127
train gradient:  0.21799588151379884
iteration : 4607
train acc:  0.8125
train loss:  0.3775845468044281
train gradient:  0.2895172406339176
iteration : 4608
train acc:  0.8515625
train loss:  0.3541731834411621
train gradient:  0.23378859360360316
iteration : 4609
train acc:  0.890625
train loss:  0.3071138858795166
train gradient:  0.24156600597322087
iteration : 4610
train acc:  0.8671875
train loss:  0.32097896933555603
train gradient:  0.2264468856932076
iteration : 4611
train acc:  0.8828125
train loss:  0.3288186490535736
train gradient:  0.25189204678097576
iteration : 4612
train acc:  0.78125
train loss:  0.46100741624832153
train gradient:  0.4020336632292206
iteration : 4613
train acc:  0.828125
train loss:  0.3734052777290344
train gradient:  0.2514278413706851
iteration : 4614
train acc:  0.828125
train loss:  0.3796498775482178
train gradient:  0.35712475778971486
iteration : 4615
train acc:  0.8828125
train loss:  0.30296146869659424
train gradient:  0.14443820582721528
iteration : 4616
train acc:  0.8203125
train loss:  0.38772669434547424
train gradient:  0.31521696350890976
iteration : 4617
train acc:  0.859375
train loss:  0.334738552570343
train gradient:  0.22808638394635364
iteration : 4618
train acc:  0.8359375
train loss:  0.38103604316711426
train gradient:  0.27390190484233373
iteration : 4619
train acc:  0.859375
train loss:  0.2984820604324341
train gradient:  0.2525212846706986
iteration : 4620
train acc:  0.828125
train loss:  0.3720056116580963
train gradient:  0.22302834349370584
iteration : 4621
train acc:  0.828125
train loss:  0.3443233370780945
train gradient:  0.20352807186223199
iteration : 4622
train acc:  0.890625
train loss:  0.29582345485687256
train gradient:  0.18708556647265578
iteration : 4623
train acc:  0.828125
train loss:  0.3733481764793396
train gradient:  0.28934977216689745
iteration : 4624
train acc:  0.8203125
train loss:  0.3848652243614197
train gradient:  0.2518177249831066
iteration : 4625
train acc:  0.8671875
train loss:  0.3207833766937256
train gradient:  0.25351258012878153
iteration : 4626
train acc:  0.859375
train loss:  0.32349270582199097
train gradient:  0.225781428831567
iteration : 4627
train acc:  0.875
train loss:  0.3838546872138977
train gradient:  0.27000836944228895
iteration : 4628
train acc:  0.8828125
train loss:  0.3196200132369995
train gradient:  0.26283915729165747
iteration : 4629
train acc:  0.875
train loss:  0.3208875358104706
train gradient:  0.17754501637238326
iteration : 4630
train acc:  0.8984375
train loss:  0.2526419162750244
train gradient:  0.16686358591956604
iteration : 4631
train acc:  0.859375
train loss:  0.35753917694091797
train gradient:  0.32207870688247675
iteration : 4632
train acc:  0.8046875
train loss:  0.34306639432907104
train gradient:  0.2038243628563415
iteration : 4633
train acc:  0.8671875
train loss:  0.30241501331329346
train gradient:  0.27583194994490007
iteration : 4634
train acc:  0.859375
train loss:  0.3343598544597626
train gradient:  0.16425034348502027
iteration : 4635
train acc:  0.890625
train loss:  0.3058091104030609
train gradient:  0.21267556940511448
iteration : 4636
train acc:  0.875
train loss:  0.27643895149230957
train gradient:  0.16514032220345987
iteration : 4637
train acc:  0.875
train loss:  0.3451436161994934
train gradient:  0.21257172429427812
iteration : 4638
train acc:  0.8046875
train loss:  0.4066264033317566
train gradient:  0.3396327372814722
iteration : 4639
train acc:  0.828125
train loss:  0.40599802136421204
train gradient:  0.4071827587351476
iteration : 4640
train acc:  0.859375
train loss:  0.3452311158180237
train gradient:  0.21875144031848942
iteration : 4641
train acc:  0.875
train loss:  0.3480075001716614
train gradient:  0.21220076092978585
iteration : 4642
train acc:  0.8671875
train loss:  0.3421708345413208
train gradient:  0.36005740771282413
iteration : 4643
train acc:  0.8203125
train loss:  0.37942975759506226
train gradient:  0.31882952791611135
iteration : 4644
train acc:  0.8203125
train loss:  0.35998284816741943
train gradient:  0.23482324277394623
iteration : 4645
train acc:  0.875
train loss:  0.2973906397819519
train gradient:  0.2312255186933629
iteration : 4646
train acc:  0.828125
train loss:  0.3868371546268463
train gradient:  0.32610703160984716
iteration : 4647
train acc:  0.828125
train loss:  0.340416818857193
train gradient:  0.19133259372477257
iteration : 4648
train acc:  0.84375
train loss:  0.32565146684646606
train gradient:  0.28597336311981497
iteration : 4649
train acc:  0.84375
train loss:  0.3846070170402527
train gradient:  0.31481878501992894
iteration : 4650
train acc:  0.8203125
train loss:  0.371346652507782
train gradient:  0.325269955811654
iteration : 4651
train acc:  0.796875
train loss:  0.4893534183502197
train gradient:  0.3963090391552868
iteration : 4652
train acc:  0.8828125
train loss:  0.29807448387145996
train gradient:  0.27871799565331445
iteration : 4653
train acc:  0.8515625
train loss:  0.3958972692489624
train gradient:  0.28402171534557646
iteration : 4654
train acc:  0.890625
train loss:  0.34661710262298584
train gradient:  0.27111709665525263
iteration : 4655
train acc:  0.8125
train loss:  0.3689814507961273
train gradient:  0.31355575222055126
iteration : 4656
train acc:  0.921875
train loss:  0.29767045378685
train gradient:  0.1662830838667403
iteration : 4657
train acc:  0.859375
train loss:  0.34343990683555603
train gradient:  0.26332057302381934
iteration : 4658
train acc:  0.8046875
train loss:  0.414307177066803
train gradient:  0.37004463324822173
iteration : 4659
train acc:  0.8671875
train loss:  0.31197860836982727
train gradient:  0.21875231275980905
iteration : 4660
train acc:  0.8125
train loss:  0.4049302637577057
train gradient:  0.3179866393524081
iteration : 4661
train acc:  0.875
train loss:  0.33923155069351196
train gradient:  0.2813336518045082
iteration : 4662
train acc:  0.8046875
train loss:  0.40229353308677673
train gradient:  0.39571709530631544
iteration : 4663
train acc:  0.8515625
train loss:  0.37057843804359436
train gradient:  0.28707663960564056
iteration : 4664
train acc:  0.8671875
train loss:  0.3475733995437622
train gradient:  0.21511117008788855
iteration : 4665
train acc:  0.8046875
train loss:  0.42702293395996094
train gradient:  0.38699671149375875
iteration : 4666
train acc:  0.8203125
train loss:  0.3518896996974945
train gradient:  0.2880884208463257
iteration : 4667
train acc:  0.7578125
train loss:  0.5152106285095215
train gradient:  0.3974856596615813
iteration : 4668
train acc:  0.84375
train loss:  0.36523234844207764
train gradient:  0.24219005416521291
iteration : 4669
train acc:  0.828125
train loss:  0.35128670930862427
train gradient:  0.27940090060188844
iteration : 4670
train acc:  0.8359375
train loss:  0.3761575520038605
train gradient:  0.26658964121290984
iteration : 4671
train acc:  0.8203125
train loss:  0.4030440151691437
train gradient:  0.32544557302095367
iteration : 4672
train acc:  0.828125
train loss:  0.38937416672706604
train gradient:  0.4046600660460556
iteration : 4673
train acc:  0.875
train loss:  0.34405678510665894
train gradient:  0.2216002732610921
iteration : 4674
train acc:  0.8515625
train loss:  0.34293532371520996
train gradient:  0.37229560104307774
iteration : 4675
train acc:  0.8671875
train loss:  0.40320050716400146
train gradient:  0.2206742044730797
iteration : 4676
train acc:  0.8359375
train loss:  0.37709885835647583
train gradient:  0.32745654443995226
iteration : 4677
train acc:  0.859375
train loss:  0.3653251826763153
train gradient:  0.24159339175627695
iteration : 4678
train acc:  0.8359375
train loss:  0.34741878509521484
train gradient:  0.27193660860130575
iteration : 4679
train acc:  0.8671875
train loss:  0.28399229049682617
train gradient:  0.15336659236376698
iteration : 4680
train acc:  0.8203125
train loss:  0.4036208391189575
train gradient:  0.3407039449316236
iteration : 4681
train acc:  0.921875
train loss:  0.24034619331359863
train gradient:  0.13702808581872541
iteration : 4682
train acc:  0.8515625
train loss:  0.3828999996185303
train gradient:  0.20718507474727038
iteration : 4683
train acc:  0.84375
train loss:  0.32914265990257263
train gradient:  0.22925517790722916
iteration : 4684
train acc:  0.90625
train loss:  0.3252091705799103
train gradient:  0.2249284808854799
iteration : 4685
train acc:  0.8046875
train loss:  0.406416654586792
train gradient:  0.2981061368850852
iteration : 4686
train acc:  0.828125
train loss:  0.34446197748184204
train gradient:  0.28000555925661325
iteration : 4687
train acc:  0.828125
train loss:  0.3734896183013916
train gradient:  0.23839035327822006
iteration : 4688
train acc:  0.84375
train loss:  0.38552480936050415
train gradient:  0.27132823864996414
iteration : 4689
train acc:  0.84375
train loss:  0.35977840423583984
train gradient:  0.2665773928273829
iteration : 4690
train acc:  0.8828125
train loss:  0.32826316356658936
train gradient:  0.19012694050581555
iteration : 4691
train acc:  0.8515625
train loss:  0.3057313561439514
train gradient:  0.2260053970937201
iteration : 4692
train acc:  0.8046875
train loss:  0.4186621904373169
train gradient:  0.3808444157136038
iteration : 4693
train acc:  0.7734375
train loss:  0.45130136609077454
train gradient:  0.3868272137010607
iteration : 4694
train acc:  0.8359375
train loss:  0.37334150075912476
train gradient:  0.4590386164950133
iteration : 4695
train acc:  0.796875
train loss:  0.39996108412742615
train gradient:  0.3212874710833517
iteration : 4696
train acc:  0.828125
train loss:  0.3581486940383911
train gradient:  0.2825926168979358
iteration : 4697
train acc:  0.875
train loss:  0.3871150612831116
train gradient:  0.2292215903884066
iteration : 4698
train acc:  0.8515625
train loss:  0.3492394983768463
train gradient:  0.31090299082258843
iteration : 4699
train acc:  0.8125
train loss:  0.3589761555194855
train gradient:  0.2289340915155199
iteration : 4700
train acc:  0.875
train loss:  0.30629992485046387
train gradient:  0.21850991227203373
iteration : 4701
train acc:  0.78125
train loss:  0.42252933979034424
train gradient:  0.32521802231216584
iteration : 4702
train acc:  0.8671875
train loss:  0.3359539806842804
train gradient:  0.20594156616316245
iteration : 4703
train acc:  0.796875
train loss:  0.40233051776885986
train gradient:  0.27975354799478036
iteration : 4704
train acc:  0.8359375
train loss:  0.3851449489593506
train gradient:  0.2714869169712912
iteration : 4705
train acc:  0.8671875
train loss:  0.30788615345954895
train gradient:  0.21062888441681388
iteration : 4706
train acc:  0.84375
train loss:  0.3764389157295227
train gradient:  0.1989810175156927
iteration : 4707
train acc:  0.859375
train loss:  0.3276596963405609
train gradient:  0.20027305966323838
iteration : 4708
train acc:  0.8515625
train loss:  0.3887782096862793
train gradient:  0.403070009200803
iteration : 4709
train acc:  0.8203125
train loss:  0.31040963530540466
train gradient:  0.22869403715201814
iteration : 4710
train acc:  0.8203125
train loss:  0.43481209874153137
train gradient:  0.291911377780682
iteration : 4711
train acc:  0.8828125
train loss:  0.27787959575653076
train gradient:  0.1802934129198639
iteration : 4712
train acc:  0.859375
train loss:  0.3714749217033386
train gradient:  0.2532381864165996
iteration : 4713
train acc:  0.8203125
train loss:  0.3558695912361145
train gradient:  0.23574497328421617
iteration : 4714
train acc:  0.8984375
train loss:  0.2929547131061554
train gradient:  0.15533066339272583
iteration : 4715
train acc:  0.828125
train loss:  0.3738235831260681
train gradient:  0.3045607370617261
iteration : 4716
train acc:  0.8125
train loss:  0.3885432779788971
train gradient:  0.22581276461813413
iteration : 4717
train acc:  0.8359375
train loss:  0.38445329666137695
train gradient:  0.273014196480139
iteration : 4718
train acc:  0.8203125
train loss:  0.35804295539855957
train gradient:  0.3301265437229577
iteration : 4719
train acc:  0.8515625
train loss:  0.3472965955734253
train gradient:  0.2566589697387602
iteration : 4720
train acc:  0.8828125
train loss:  0.3046661615371704
train gradient:  0.19314483612708555
iteration : 4721
train acc:  0.8515625
train loss:  0.31240367889404297
train gradient:  0.21003385861593246
iteration : 4722
train acc:  0.8359375
train loss:  0.3780793845653534
train gradient:  0.2903204415055642
iteration : 4723
train acc:  0.8515625
train loss:  0.2943843603134155
train gradient:  0.195982922299135
iteration : 4724
train acc:  0.8828125
train loss:  0.30599454045295715
train gradient:  0.1851564040213116
iteration : 4725
train acc:  0.8828125
train loss:  0.3571203649044037
train gradient:  0.3305243218190055
iteration : 4726
train acc:  0.875
train loss:  0.29339510202407837
train gradient:  0.1558889924035059
iteration : 4727
train acc:  0.859375
train loss:  0.3555644452571869
train gradient:  0.22180174666462169
iteration : 4728
train acc:  0.828125
train loss:  0.364917516708374
train gradient:  0.3876282528246652
iteration : 4729
train acc:  0.8046875
train loss:  0.3982478082180023
train gradient:  0.36564445463462447
iteration : 4730
train acc:  0.84375
train loss:  0.31200894713401794
train gradient:  0.16896841661813855
iteration : 4731
train acc:  0.890625
train loss:  0.28920412063598633
train gradient:  0.2141421445764923
iteration : 4732
train acc:  0.8125
train loss:  0.4202266335487366
train gradient:  0.4057359266664269
iteration : 4733
train acc:  0.8515625
train loss:  0.3514239192008972
train gradient:  0.24287157201458592
iteration : 4734
train acc:  0.8671875
train loss:  0.29032155871391296
train gradient:  0.16280998707750552
iteration : 4735
train acc:  0.8046875
train loss:  0.41892361640930176
train gradient:  0.34750688442309197
iteration : 4736
train acc:  0.828125
train loss:  0.32807034254074097
train gradient:  0.2404653783989102
iteration : 4737
train acc:  0.8828125
train loss:  0.33524399995803833
train gradient:  0.18073038091210036
iteration : 4738
train acc:  0.8125
train loss:  0.3503577411174774
train gradient:  0.23143020594303598
iteration : 4739
train acc:  0.8359375
train loss:  0.35454946756362915
train gradient:  0.23700105478403005
iteration : 4740
train acc:  0.8125
train loss:  0.3895941972732544
train gradient:  0.2317207838171971
iteration : 4741
train acc:  0.8671875
train loss:  0.36247938871383667
train gradient:  0.3078000157477225
iteration : 4742
train acc:  0.875
train loss:  0.3114711046218872
train gradient:  0.2218201996042361
iteration : 4743
train acc:  0.84375
train loss:  0.3470650613307953
train gradient:  0.15595004285042818
iteration : 4744
train acc:  0.8828125
train loss:  0.26577430963516235
train gradient:  0.18031748635203745
iteration : 4745
train acc:  0.875
train loss:  0.3153499662876129
train gradient:  0.22115958333595334
iteration : 4746
train acc:  0.8203125
train loss:  0.4268132150173187
train gradient:  0.3497533840044817
iteration : 4747
train acc:  0.8515625
train loss:  0.38407325744628906
train gradient:  0.268368815708027
iteration : 4748
train acc:  0.875
train loss:  0.32671821117401123
train gradient:  0.20592393230342848
iteration : 4749
train acc:  0.8203125
train loss:  0.37910428643226624
train gradient:  0.2485267120361921
iteration : 4750
train acc:  0.8828125
train loss:  0.28666144609451294
train gradient:  0.22991106371610395
iteration : 4751
train acc:  0.8671875
train loss:  0.310585618019104
train gradient:  0.19728975848614305
iteration : 4752
train acc:  0.828125
train loss:  0.42954546213150024
train gradient:  0.3072616622712942
iteration : 4753
train acc:  0.859375
train loss:  0.3222423791885376
train gradient:  0.21577475030131804
iteration : 4754
train acc:  0.8125
train loss:  0.4435267746448517
train gradient:  0.33658891344434627
iteration : 4755
train acc:  0.84375
train loss:  0.3181036114692688
train gradient:  0.3018663363217012
iteration : 4756
train acc:  0.859375
train loss:  0.35301077365875244
train gradient:  0.25871127818510103
iteration : 4757
train acc:  0.8046875
train loss:  0.40170642733573914
train gradient:  0.3342548389150641
iteration : 4758
train acc:  0.8046875
train loss:  0.38646256923675537
train gradient:  0.27068564898049613
iteration : 4759
train acc:  0.890625
train loss:  0.3160974979400635
train gradient:  0.18838744693026516
iteration : 4760
train acc:  0.8359375
train loss:  0.3456532955169678
train gradient:  0.30405423626219175
iteration : 4761
train acc:  0.8359375
train loss:  0.34792327880859375
train gradient:  0.33941958508878617
iteration : 4762
train acc:  0.84375
train loss:  0.34398114681243896
train gradient:  0.1893576749234044
iteration : 4763
train acc:  0.859375
train loss:  0.31335824728012085
train gradient:  0.20512496824563542
iteration : 4764
train acc:  0.828125
train loss:  0.34936049580574036
train gradient:  0.2052841235135875
iteration : 4765
train acc:  0.890625
train loss:  0.3300854563713074
train gradient:  0.2621746683363715
iteration : 4766
train acc:  0.90625
train loss:  0.29108726978302
train gradient:  0.1927652475910031
iteration : 4767
train acc:  0.8125
train loss:  0.38184744119644165
train gradient:  0.26638697591919663
iteration : 4768
train acc:  0.8671875
train loss:  0.3485719859600067
train gradient:  0.1751261084540568
iteration : 4769
train acc:  0.859375
train loss:  0.3632392883300781
train gradient:  0.36628697739598914
iteration : 4770
train acc:  0.8671875
train loss:  0.33173608779907227
train gradient:  0.24765758427342677
iteration : 4771
train acc:  0.859375
train loss:  0.3542490601539612
train gradient:  0.2359998377389801
iteration : 4772
train acc:  0.8359375
train loss:  0.32145437598228455
train gradient:  0.19491933568827097
iteration : 4773
train acc:  0.8671875
train loss:  0.34959888458251953
train gradient:  0.22199358813558234
iteration : 4774
train acc:  0.8671875
train loss:  0.321255624294281
train gradient:  0.20637136879995488
iteration : 4775
train acc:  0.8203125
train loss:  0.38460010290145874
train gradient:  0.2901775541719508
iteration : 4776
train acc:  0.90625
train loss:  0.3220592737197876
train gradient:  0.27497758830506125
iteration : 4777
train acc:  0.859375
train loss:  0.32142022252082825
train gradient:  0.21787827375276791
iteration : 4778
train acc:  0.8203125
train loss:  0.3841898441314697
train gradient:  0.23356932798751373
iteration : 4779
train acc:  0.8125
train loss:  0.41931790113449097
train gradient:  0.35052841930957684
iteration : 4780
train acc:  0.8359375
train loss:  0.3703117072582245
train gradient:  0.24776791477076215
iteration : 4781
train acc:  0.84375
train loss:  0.37980973720550537
train gradient:  0.290339524976839
iteration : 4782
train acc:  0.8203125
train loss:  0.34134501218795776
train gradient:  0.2666708273879787
iteration : 4783
train acc:  0.875
train loss:  0.278703510761261
train gradient:  0.17977844972947976
iteration : 4784
train acc:  0.8203125
train loss:  0.3644336462020874
train gradient:  0.22613721121677363
iteration : 4785
train acc:  0.7890625
train loss:  0.4182564616203308
train gradient:  0.4274703481760955
iteration : 4786
train acc:  0.8671875
train loss:  0.313554584980011
train gradient:  0.18351735258442958
iteration : 4787
train acc:  0.875
train loss:  0.331274151802063
train gradient:  0.20184608842597676
iteration : 4788
train acc:  0.828125
train loss:  0.37730202078819275
train gradient:  0.2601521886514685
iteration : 4789
train acc:  0.859375
train loss:  0.3018295168876648
train gradient:  0.19843166767029125
iteration : 4790
train acc:  0.8203125
train loss:  0.49622881412506104
train gradient:  1.0478438268558472
iteration : 4791
train acc:  0.8125
train loss:  0.40115657448768616
train gradient:  0.36438303512356357
iteration : 4792
train acc:  0.84375
train loss:  0.3502613306045532
train gradient:  0.17990106797721264
iteration : 4793
train acc:  0.828125
train loss:  0.38153529167175293
train gradient:  0.26843976398501335
iteration : 4794
train acc:  0.8359375
train loss:  0.36450064182281494
train gradient:  0.20438917135806794
iteration : 4795
train acc:  0.8359375
train loss:  0.36325815320014954
train gradient:  0.21535879947097392
iteration : 4796
train acc:  0.8125
train loss:  0.36907702684402466
train gradient:  0.2480210789255199
iteration : 4797
train acc:  0.8515625
train loss:  0.3253929018974304
train gradient:  0.24504930023386537
iteration : 4798
train acc:  0.828125
train loss:  0.3983731269836426
train gradient:  0.411068694006449
iteration : 4799
train acc:  0.859375
train loss:  0.35197195410728455
train gradient:  0.22593342033523744
iteration : 4800
train acc:  0.84375
train loss:  0.3331839442253113
train gradient:  0.2606842981237581
iteration : 4801
train acc:  0.8359375
train loss:  0.35778167843818665
train gradient:  0.2025058214996915
iteration : 4802
train acc:  0.921875
train loss:  0.273267924785614
train gradient:  0.15207954716896038
iteration : 4803
train acc:  0.8828125
train loss:  0.33546900749206543
train gradient:  0.13993413178397057
iteration : 4804
train acc:  0.875
train loss:  0.34849315881729126
train gradient:  0.2898735521590815
iteration : 4805
train acc:  0.8671875
train loss:  0.302634060382843
train gradient:  0.1356002924221404
iteration : 4806
train acc:  0.8046875
train loss:  0.43318772315979004
train gradient:  0.4149914610579895
iteration : 4807
train acc:  0.84375
train loss:  0.42555105686187744
train gradient:  0.35652818231148087
iteration : 4808
train acc:  0.9140625
train loss:  0.29086965322494507
train gradient:  0.20682743615137075
iteration : 4809
train acc:  0.84375
train loss:  0.3968462347984314
train gradient:  0.3310926779852578
iteration : 4810
train acc:  0.8359375
train loss:  0.39652493596076965
train gradient:  0.2373976110571008
iteration : 4811
train acc:  0.8515625
train loss:  0.314150333404541
train gradient:  0.20536255270525297
iteration : 4812
train acc:  0.859375
train loss:  0.3503517806529999
train gradient:  0.22205328921829665
iteration : 4813
train acc:  0.859375
train loss:  0.34021395444869995
train gradient:  0.2488660900331123
iteration : 4814
train acc:  0.890625
train loss:  0.301093190908432
train gradient:  0.1956748077479178
iteration : 4815
train acc:  0.828125
train loss:  0.4405096173286438
train gradient:  0.4587236010220979
iteration : 4816
train acc:  0.8515625
train loss:  0.3115426301956177
train gradient:  0.23919429691660818
iteration : 4817
train acc:  0.828125
train loss:  0.37929922342300415
train gradient:  0.2158323212895389
iteration : 4818
train acc:  0.8125
train loss:  0.4319984018802643
train gradient:  0.35619732490431605
iteration : 4819
train acc:  0.8359375
train loss:  0.3647445738315582
train gradient:  0.34519232573740133
iteration : 4820
train acc:  0.875
train loss:  0.3218979239463806
train gradient:  0.2160612684926245
iteration : 4821
train acc:  0.8359375
train loss:  0.3179994523525238
train gradient:  0.2487649383538352
iteration : 4822
train acc:  0.8203125
train loss:  0.429823100566864
train gradient:  0.3221816515440146
iteration : 4823
train acc:  0.84375
train loss:  0.3559802174568176
train gradient:  0.207231653927038
iteration : 4824
train acc:  0.8671875
train loss:  0.31878653168678284
train gradient:  0.19139150410719846
iteration : 4825
train acc:  0.890625
train loss:  0.32426849007606506
train gradient:  0.2509271708570708
iteration : 4826
train acc:  0.7890625
train loss:  0.4002361595630646
train gradient:  0.3112255273129921
iteration : 4827
train acc:  0.84375
train loss:  0.359022319316864
train gradient:  0.23059357858117852
iteration : 4828
train acc:  0.8515625
train loss:  0.3279615640640259
train gradient:  0.1861136177325893
iteration : 4829
train acc:  0.8125
train loss:  0.3708159029483795
train gradient:  0.22281105494906206
iteration : 4830
train acc:  0.84375
train loss:  0.3354065418243408
train gradient:  0.2289084695279366
iteration : 4831
train acc:  0.84375
train loss:  0.3446711599826813
train gradient:  0.14319101587495792
iteration : 4832
train acc:  0.8046875
train loss:  0.388670951128006
train gradient:  0.23764618561789244
iteration : 4833
train acc:  0.8515625
train loss:  0.34464308619499207
train gradient:  0.25273328408231543
iteration : 4834
train acc:  0.8203125
train loss:  0.32459115982055664
train gradient:  0.31643531611138465
iteration : 4835
train acc:  0.875
train loss:  0.30055999755859375
train gradient:  0.20617560098986368
iteration : 4836
train acc:  0.8671875
train loss:  0.37473201751708984
train gradient:  0.27023106743878145
iteration : 4837
train acc:  0.828125
train loss:  0.36004623770713806
train gradient:  0.22052528451481984
iteration : 4838
train acc:  0.84375
train loss:  0.3203144967556
train gradient:  0.17545899964454698
iteration : 4839
train acc:  0.8359375
train loss:  0.3866076171398163
train gradient:  0.23392240707120832
iteration : 4840
train acc:  0.875
train loss:  0.33049702644348145
train gradient:  0.23276040329153413
iteration : 4841
train acc:  0.875
train loss:  0.33504849672317505
train gradient:  0.20254765329679758
iteration : 4842
train acc:  0.8046875
train loss:  0.3865484595298767
train gradient:  0.37782354893810854
iteration : 4843
train acc:  0.8125
train loss:  0.4109559953212738
train gradient:  0.3375359053239424
iteration : 4844
train acc:  0.8125
train loss:  0.3938378691673279
train gradient:  0.3150227067619717
iteration : 4845
train acc:  0.796875
train loss:  0.41696739196777344
train gradient:  0.2831789000066288
iteration : 4846
train acc:  0.8046875
train loss:  0.41164863109588623
train gradient:  0.2729356856135415
iteration : 4847
train acc:  0.8125
train loss:  0.3969029188156128
train gradient:  0.27034334275046784
iteration : 4848
train acc:  0.8515625
train loss:  0.3588500916957855
train gradient:  0.2449067405152642
iteration : 4849
train acc:  0.8046875
train loss:  0.4116593301296234
train gradient:  0.5088386979026621
iteration : 4850
train acc:  0.859375
train loss:  0.3032253384590149
train gradient:  0.28880000015329677
iteration : 4851
train acc:  0.84375
train loss:  0.3619723320007324
train gradient:  0.18346455696251016
iteration : 4852
train acc:  0.828125
train loss:  0.35795220732688904
train gradient:  0.1845000453959601
iteration : 4853
train acc:  0.8359375
train loss:  0.4142279624938965
train gradient:  0.28493545747707416
iteration : 4854
train acc:  0.8359375
train loss:  0.36868399381637573
train gradient:  0.2233818092708973
iteration : 4855
train acc:  0.859375
train loss:  0.37702327966690063
train gradient:  0.25802176012286154
iteration : 4856
train acc:  0.84375
train loss:  0.358595609664917
train gradient:  0.3072847106945102
iteration : 4857
train acc:  0.8515625
train loss:  0.3800649046897888
train gradient:  0.3179778845050127
iteration : 4858
train acc:  0.8828125
train loss:  0.2860270142555237
train gradient:  0.25008015470790684
iteration : 4859
train acc:  0.8203125
train loss:  0.3733102083206177
train gradient:  0.32327148935898053
iteration : 4860
train acc:  0.8515625
train loss:  0.3485492765903473
train gradient:  0.20340626458699576
iteration : 4861
train acc:  0.8671875
train loss:  0.32805249094963074
train gradient:  0.20404631441824295
iteration : 4862
train acc:  0.875
train loss:  0.2667091190814972
train gradient:  0.17581880483371543
iteration : 4863
train acc:  0.78125
train loss:  0.4027847647666931
train gradient:  0.27477333910933827
iteration : 4864
train acc:  0.9140625
train loss:  0.2790467441082001
train gradient:  0.18172005744303799
iteration : 4865
train acc:  0.8203125
train loss:  0.3468111455440521
train gradient:  0.22963327388897503
iteration : 4866
train acc:  0.8125
train loss:  0.38514652848243713
train gradient:  0.20683463952389552
iteration : 4867
train acc:  0.84375
train loss:  0.34749698638916016
train gradient:  0.3185586413378865
iteration : 4868
train acc:  0.828125
train loss:  0.4054536819458008
train gradient:  0.3012646557238025
iteration : 4869
train acc:  0.8203125
train loss:  0.3499922752380371
train gradient:  0.25767966621101857
iteration : 4870
train acc:  0.84375
train loss:  0.31613391637802124
train gradient:  0.24941124705969572
iteration : 4871
train acc:  0.8359375
train loss:  0.3587265610694885
train gradient:  0.24495381967322208
iteration : 4872
train acc:  0.875
train loss:  0.32181715965270996
train gradient:  0.20061924438713946
iteration : 4873
train acc:  0.8125
train loss:  0.40649306774139404
train gradient:  0.3353597166359823
iteration : 4874
train acc:  0.859375
train loss:  0.30466073751449585
train gradient:  0.23698643023982363
iteration : 4875
train acc:  0.8046875
train loss:  0.4005136489868164
train gradient:  0.40349044730951994
iteration : 4876
train acc:  0.7734375
train loss:  0.44303056597709656
train gradient:  0.5408434661934767
iteration : 4877
train acc:  0.890625
train loss:  0.3101206123828888
train gradient:  0.19497956111295128
iteration : 4878
train acc:  0.8046875
train loss:  0.45046937465667725
train gradient:  0.4456975257781253
iteration : 4879
train acc:  0.828125
train loss:  0.41740626096725464
train gradient:  0.22090724115521082
iteration : 4880
train acc:  0.8359375
train loss:  0.35966402292251587
train gradient:  0.28587361429489
iteration : 4881
train acc:  0.84375
train loss:  0.34069985151290894
train gradient:  0.2589946841477092
iteration : 4882
train acc:  0.828125
train loss:  0.38948553800582886
train gradient:  0.29720278014775736
iteration : 4883
train acc:  0.859375
train loss:  0.32131803035736084
train gradient:  0.280597976047886
iteration : 4884
train acc:  0.8046875
train loss:  0.3759455382823944
train gradient:  0.3264428510891149
iteration : 4885
train acc:  0.7890625
train loss:  0.4262874722480774
train gradient:  0.2783517026248843
iteration : 4886
train acc:  0.8125
train loss:  0.38429245352745056
train gradient:  0.27862205389575784
iteration : 4887
train acc:  0.8515625
train loss:  0.3265478014945984
train gradient:  0.2564053191718149
iteration : 4888
train acc:  0.828125
train loss:  0.3836919069290161
train gradient:  0.4377283561736306
iteration : 4889
train acc:  0.90625
train loss:  0.251701295375824
train gradient:  0.1596193653926785
iteration : 4890
train acc:  0.78125
train loss:  0.41241464018821716
train gradient:  0.4071469209567117
iteration : 4891
train acc:  0.828125
train loss:  0.4083148241043091
train gradient:  0.29839703141857254
iteration : 4892
train acc:  0.75
train loss:  0.4783042073249817
train gradient:  0.37723998652809265
iteration : 4893
train acc:  0.7890625
train loss:  0.40882453322410583
train gradient:  0.29201973343966997
iteration : 4894
train acc:  0.90625
train loss:  0.28659582138061523
train gradient:  0.12483110086505925
iteration : 4895
train acc:  0.828125
train loss:  0.323952853679657
train gradient:  0.2599287183685498
iteration : 4896
train acc:  0.8828125
train loss:  0.31780868768692017
train gradient:  0.14866798240042767
iteration : 4897
train acc:  0.859375
train loss:  0.3319832682609558
train gradient:  0.15522704885463182
iteration : 4898
train acc:  0.8046875
train loss:  0.430980920791626
train gradient:  0.30489055991463065
iteration : 4899
train acc:  0.859375
train loss:  0.3391604423522949
train gradient:  0.18621563495643367
iteration : 4900
train acc:  0.8671875
train loss:  0.2985956072807312
train gradient:  0.1678828286603839
iteration : 4901
train acc:  0.8203125
train loss:  0.3882167637348175
train gradient:  0.2555173520896184
iteration : 4902
train acc:  0.8125
train loss:  0.42688003182411194
train gradient:  0.36402302544987636
iteration : 4903
train acc:  0.9140625
train loss:  0.29014843702316284
train gradient:  0.1960206029653847
iteration : 4904
train acc:  0.890625
train loss:  0.3288573622703552
train gradient:  0.2225095059366236
iteration : 4905
train acc:  0.8671875
train loss:  0.3316248059272766
train gradient:  0.2470863307192581
iteration : 4906
train acc:  0.84375
train loss:  0.32995209097862244
train gradient:  0.2135692637013903
iteration : 4907
train acc:  0.828125
train loss:  0.337763249874115
train gradient:  0.2059936997842704
iteration : 4908
train acc:  0.875
train loss:  0.3308942914009094
train gradient:  0.26609708966192963
iteration : 4909
train acc:  0.8046875
train loss:  0.4659897983074188
train gradient:  0.3732487350398002
iteration : 4910
train acc:  0.8515625
train loss:  0.31368178129196167
train gradient:  0.22601026315489028
iteration : 4911
train acc:  0.8359375
train loss:  0.332201212644577
train gradient:  0.26147541000103636
iteration : 4912
train acc:  0.8828125
train loss:  0.2934224009513855
train gradient:  0.17755923362117404
iteration : 4913
train acc:  0.8515625
train loss:  0.32519739866256714
train gradient:  0.2643960069916272
iteration : 4914
train acc:  0.875
train loss:  0.3123719096183777
train gradient:  0.17745485959985507
iteration : 4915
train acc:  0.8125
train loss:  0.4616275131702423
train gradient:  0.422753551472684
iteration : 4916
train acc:  0.8984375
train loss:  0.2516518235206604
train gradient:  0.17380266302457703
iteration : 4917
train acc:  0.8125
train loss:  0.4000512361526489
train gradient:  0.2761076387136301
iteration : 4918
train acc:  0.8359375
train loss:  0.33831608295440674
train gradient:  0.21595587151568618
iteration : 4919
train acc:  0.84375
train loss:  0.30569708347320557
train gradient:  0.19427030030453377
iteration : 4920
train acc:  0.8046875
train loss:  0.4052945673465729
train gradient:  0.25035201254485595
iteration : 4921
train acc:  0.828125
train loss:  0.36100780963897705
train gradient:  0.2323555795341467
iteration : 4922
train acc:  0.8203125
train loss:  0.38341131806373596
train gradient:  0.2659583160083474
iteration : 4923
train acc:  0.84375
train loss:  0.34682008624076843
train gradient:  0.22959752427179383
iteration : 4924
train acc:  0.8359375
train loss:  0.3820751905441284
train gradient:  0.31158030724593466
iteration : 4925
train acc:  0.84375
train loss:  0.3333160877227783
train gradient:  0.2959781351345343
iteration : 4926
train acc:  0.84375
train loss:  0.3519183397293091
train gradient:  0.19478238329177774
iteration : 4927
train acc:  0.8125
train loss:  0.3836297392845154
train gradient:  0.27231451011717966
iteration : 4928
train acc:  0.8359375
train loss:  0.3629493713378906
train gradient:  0.2548376933404041
iteration : 4929
train acc:  0.8359375
train loss:  0.37935671210289
train gradient:  0.21397126776125835
iteration : 4930
train acc:  0.8203125
train loss:  0.39755523204803467
train gradient:  0.3332263581657656
iteration : 4931
train acc:  0.8359375
train loss:  0.36325758695602417
train gradient:  0.22690504239281672
iteration : 4932
train acc:  0.859375
train loss:  0.31125420331954956
train gradient:  0.23456685671103122
iteration : 4933
train acc:  0.859375
train loss:  0.33961573243141174
train gradient:  0.15467416709247633
iteration : 4934
train acc:  0.8046875
train loss:  0.4307355284690857
train gradient:  0.36771541396068924
iteration : 4935
train acc:  0.7734375
train loss:  0.47663551568984985
train gradient:  0.379717415808763
iteration : 4936
train acc:  0.8828125
train loss:  0.28126099705696106
train gradient:  0.17954212130908714
iteration : 4937
train acc:  0.828125
train loss:  0.3879965841770172
train gradient:  0.3650688185437411
iteration : 4938
train acc:  0.84375
train loss:  0.36171311140060425
train gradient:  0.2479274353544853
iteration : 4939
train acc:  0.890625
train loss:  0.30175840854644775
train gradient:  0.1997950180631279
iteration : 4940
train acc:  0.828125
train loss:  0.3593962788581848
train gradient:  0.24180602625180309
iteration : 4941
train acc:  0.78125
train loss:  0.49625641107559204
train gradient:  0.4092917918595337
iteration : 4942
train acc:  0.8359375
train loss:  0.39597612619400024
train gradient:  0.33029697933372754
iteration : 4943
train acc:  0.828125
train loss:  0.383107453584671
train gradient:  0.2047409583989177
iteration : 4944
train acc:  0.8359375
train loss:  0.42876726388931274
train gradient:  0.27158160134903625
iteration : 4945
train acc:  0.8046875
train loss:  0.36851876974105835
train gradient:  0.22984512367613585
iteration : 4946
train acc:  0.875
train loss:  0.32543814182281494
train gradient:  0.15214927628290917
iteration : 4947
train acc:  0.8515625
train loss:  0.4359855651855469
train gradient:  0.32839494052202645
iteration : 4948
train acc:  0.9140625
train loss:  0.2834111750125885
train gradient:  0.18869405712792603
iteration : 4949
train acc:  0.90625
train loss:  0.26901113986968994
train gradient:  0.18480396466314808
iteration : 4950
train acc:  0.8671875
train loss:  0.3341303765773773
train gradient:  0.1450408622383091
iteration : 4951
train acc:  0.8125
train loss:  0.41955626010894775
train gradient:  0.2556223116521275
iteration : 4952
train acc:  0.8203125
train loss:  0.3863726854324341
train gradient:  0.25817212247423127
iteration : 4953
train acc:  0.8359375
train loss:  0.3942362070083618
train gradient:  0.27214979297882447
iteration : 4954
train acc:  0.8359375
train loss:  0.3704642355442047
train gradient:  0.24557780113652308
iteration : 4955
train acc:  0.875
train loss:  0.3895057141780853
train gradient:  0.3126717224002437
iteration : 4956
train acc:  0.8359375
train loss:  0.42361193895339966
train gradient:  0.31572317164588537
iteration : 4957
train acc:  0.859375
train loss:  0.3764514923095703
train gradient:  0.25856910473726813
iteration : 4958
train acc:  0.8203125
train loss:  0.4195749759674072
train gradient:  0.24150222745575128
iteration : 4959
train acc:  0.84375
train loss:  0.34105831384658813
train gradient:  0.33791442939631483
iteration : 4960
train acc:  0.8359375
train loss:  0.40931761264801025
train gradient:  0.31339759625297
iteration : 4961
train acc:  0.8671875
train loss:  0.39930760860443115
train gradient:  0.31775415038377297
iteration : 4962
train acc:  0.8671875
train loss:  0.33246153593063354
train gradient:  0.2379319166997274
iteration : 4963
train acc:  0.8125
train loss:  0.3982512354850769
train gradient:  0.3098879556162938
iteration : 4964
train acc:  0.890625
train loss:  0.26538532972335815
train gradient:  0.15633170531875584
iteration : 4965
train acc:  0.8515625
train loss:  0.3385198414325714
train gradient:  0.3007654949856039
iteration : 4966
train acc:  0.84375
train loss:  0.33724159002304077
train gradient:  0.30311262354479535
iteration : 4967
train acc:  0.8203125
train loss:  0.4292471408843994
train gradient:  0.27426486897771774
iteration : 4968
train acc:  0.8359375
train loss:  0.32647860050201416
train gradient:  0.2697175931793257
iteration : 4969
train acc:  0.859375
train loss:  0.3035426735877991
train gradient:  0.2073436163389939
iteration : 4970
train acc:  0.828125
train loss:  0.3610992729663849
train gradient:  0.24255835524978006
iteration : 4971
train acc:  0.8671875
train loss:  0.3769475221633911
train gradient:  0.25704714881059043
iteration : 4972
train acc:  0.8125
train loss:  0.3602192997932434
train gradient:  0.18104081237881833
iteration : 4973
train acc:  0.84375
train loss:  0.3230469822883606
train gradient:  0.23490816220714836
iteration : 4974
train acc:  0.8359375
train loss:  0.3408159911632538
train gradient:  0.205597340365937
iteration : 4975
train acc:  0.90625
train loss:  0.28413042426109314
train gradient:  0.1472833797521888
iteration : 4976
train acc:  0.8359375
train loss:  0.3596916198730469
train gradient:  0.23608143803785753
iteration : 4977
train acc:  0.90625
train loss:  0.2618444561958313
train gradient:  0.1374894108282339
iteration : 4978
train acc:  0.8515625
train loss:  0.34498053789138794
train gradient:  0.16252681365218713
iteration : 4979
train acc:  0.828125
train loss:  0.40590226650238037
train gradient:  0.31445459165977474
iteration : 4980
train acc:  0.8984375
train loss:  0.29719722270965576
train gradient:  0.1374456406079
iteration : 4981
train acc:  0.84375
train loss:  0.34739744663238525
train gradient:  0.28163128068023424
iteration : 4982
train acc:  0.875
train loss:  0.29852765798568726
train gradient:  0.35216927977151363
iteration : 4983
train acc:  0.8984375
train loss:  0.33086246252059937
train gradient:  0.2186276004806329
iteration : 4984
train acc:  0.8515625
train loss:  0.30739977955818176
train gradient:  0.15192046587951466
iteration : 4985
train acc:  0.84375
train loss:  0.35231712460517883
train gradient:  0.21466549526356654
iteration : 4986
train acc:  0.8515625
train loss:  0.3362470269203186
train gradient:  0.2529570443949199
iteration : 4987
train acc:  0.796875
train loss:  0.3689783215522766
train gradient:  0.33324023738020964
iteration : 4988
train acc:  0.8671875
train loss:  0.3293035626411438
train gradient:  0.2070786514099766
iteration : 4989
train acc:  0.8359375
train loss:  0.3736793100833893
train gradient:  0.28929632070187383
iteration : 4990
train acc:  0.828125
train loss:  0.3877326548099518
train gradient:  0.2479996278300979
iteration : 4991
train acc:  0.859375
train loss:  0.2983568608760834
train gradient:  0.15829436755131784
iteration : 4992
train acc:  0.8515625
train loss:  0.3398200571537018
train gradient:  0.2929283819065759
iteration : 4993
train acc:  0.8359375
train loss:  0.4049198627471924
train gradient:  0.24637564685878652
iteration : 4994
train acc:  0.828125
train loss:  0.36445242166519165
train gradient:  0.25737387400700446
iteration : 4995
train acc:  0.8984375
train loss:  0.23788964748382568
train gradient:  0.16801903851088584
iteration : 4996
train acc:  0.796875
train loss:  0.39593803882598877
train gradient:  0.2640661083216475
iteration : 4997
train acc:  0.8671875
train loss:  0.32758182287216187
train gradient:  0.23543680360629232
iteration : 4998
train acc:  0.890625
train loss:  0.2945478558540344
train gradient:  0.15236145157222442
iteration : 4999
train acc:  0.8515625
train loss:  0.36441341042518616
train gradient:  0.21122307479773866
iteration : 5000
train acc:  0.84375
train loss:  0.30813395977020264
train gradient:  0.19478302459170846
iteration : 5001
train acc:  0.875
train loss:  0.31290245056152344
train gradient:  0.24584893539352776
iteration : 5002
train acc:  0.84375
train loss:  0.3331970274448395
train gradient:  0.29981959252590606
iteration : 5003
train acc:  0.84375
train loss:  0.3739488422870636
train gradient:  0.2581717982301635
iteration : 5004
train acc:  0.890625
train loss:  0.27851176261901855
train gradient:  0.18995900289203096
iteration : 5005
train acc:  0.875
train loss:  0.299174964427948
train gradient:  0.16464155828644952
iteration : 5006
train acc:  0.9140625
train loss:  0.25213104486465454
train gradient:  0.18679943114396538
iteration : 5007
train acc:  0.796875
train loss:  0.3813580870628357
train gradient:  0.29900423660893655
iteration : 5008
train acc:  0.8046875
train loss:  0.4414394199848175
train gradient:  0.374529190365615
iteration : 5009
train acc:  0.8515625
train loss:  0.3793790936470032
train gradient:  0.3119678281184697
iteration : 5010
train acc:  0.796875
train loss:  0.4663434624671936
train gradient:  0.42833188048100845
iteration : 5011
train acc:  0.84375
train loss:  0.37951430678367615
train gradient:  0.20486940669056575
iteration : 5012
train acc:  0.8828125
train loss:  0.33483636379241943
train gradient:  0.18462329090583374
iteration : 5013
train acc:  0.8203125
train loss:  0.39666616916656494
train gradient:  0.36898206088275515
iteration : 5014
train acc:  0.8203125
train loss:  0.3736107349395752
train gradient:  0.2843987564106468
iteration : 5015
train acc:  0.8515625
train loss:  0.35825634002685547
train gradient:  0.28279015395907975
iteration : 5016
train acc:  0.8203125
train loss:  0.38425108790397644
train gradient:  0.25453095999291997
iteration : 5017
train acc:  0.8203125
train loss:  0.3465684652328491
train gradient:  0.26966929468871303
iteration : 5018
train acc:  0.859375
train loss:  0.3217774033546448
train gradient:  0.21731851149257764
iteration : 5019
train acc:  0.828125
train loss:  0.3737272620201111
train gradient:  0.26033930072319245
iteration : 5020
train acc:  0.84375
train loss:  0.31020623445510864
train gradient:  0.25732345732096545
iteration : 5021
train acc:  0.875
train loss:  0.3115519881248474
train gradient:  0.25448432357277356
iteration : 5022
train acc:  0.8671875
train loss:  0.3381792902946472
train gradient:  0.21520063548278756
iteration : 5023
train acc:  0.84375
train loss:  0.34728938341140747
train gradient:  0.322771056043126
iteration : 5024
train acc:  0.828125
train loss:  0.35649698972702026
train gradient:  0.32209573828402366
iteration : 5025
train acc:  0.84375
train loss:  0.34728243947029114
train gradient:  0.27598979272848095
iteration : 5026
train acc:  0.8125
train loss:  0.40244731307029724
train gradient:  0.2474821112238904
iteration : 5027
train acc:  0.84375
train loss:  0.3970751166343689
train gradient:  0.29239279630524695
iteration : 5028
train acc:  0.875
train loss:  0.2792325019836426
train gradient:  0.23125178178556988
iteration : 5029
train acc:  0.8671875
train loss:  0.3071666657924652
train gradient:  0.17127005935647233
iteration : 5030
train acc:  0.8203125
train loss:  0.42971163988113403
train gradient:  0.4262742387419543
iteration : 5031
train acc:  0.8828125
train loss:  0.2801631689071655
train gradient:  0.19258485984955936
iteration : 5032
train acc:  0.75
train loss:  0.5047442317008972
train gradient:  0.40673703448398274
iteration : 5033
train acc:  0.828125
train loss:  0.366055428981781
train gradient:  0.18513472328393343
iteration : 5034
train acc:  0.8046875
train loss:  0.38104772567749023
train gradient:  0.2664854361616626
iteration : 5035
train acc:  0.84375
train loss:  0.3935402035713196
train gradient:  0.36268095379727666
iteration : 5036
train acc:  0.8359375
train loss:  0.3357045650482178
train gradient:  0.221735168954308
iteration : 5037
train acc:  0.8359375
train loss:  0.39593586325645447
train gradient:  0.26376900369907635
iteration : 5038
train acc:  0.8515625
train loss:  0.3297607898712158
train gradient:  0.2686732771950331
iteration : 5039
train acc:  0.8828125
train loss:  0.30869799852371216
train gradient:  0.17112628376580538
iteration : 5040
train acc:  0.84375
train loss:  0.33471614122390747
train gradient:  0.1386241842779805
iteration : 5041
train acc:  0.9140625
train loss:  0.2678358554840088
train gradient:  0.16196274116181097
iteration : 5042
train acc:  0.875
train loss:  0.2939620018005371
train gradient:  0.19508860269816164
iteration : 5043
train acc:  0.8828125
train loss:  0.345816433429718
train gradient:  0.21979955059158945
iteration : 5044
train acc:  0.8515625
train loss:  0.4187455177307129
train gradient:  0.34863981587404796
iteration : 5045
train acc:  0.84375
train loss:  0.29912397265434265
train gradient:  0.25755362812604354
iteration : 5046
train acc:  0.890625
train loss:  0.28565657138824463
train gradient:  0.3434602821432499
iteration : 5047
train acc:  0.859375
train loss:  0.31585198640823364
train gradient:  0.2666935652103222
iteration : 5048
train acc:  0.8359375
train loss:  0.3109617233276367
train gradient:  0.28576264554322267
iteration : 5049
train acc:  0.8203125
train loss:  0.449351966381073
train gradient:  0.4162753325647885
iteration : 5050
train acc:  0.859375
train loss:  0.3233581781387329
train gradient:  0.2995549214397236
iteration : 5051
train acc:  0.8125
train loss:  0.4080505967140198
train gradient:  0.4300238154286279
iteration : 5052
train acc:  0.8125
train loss:  0.3538004159927368
train gradient:  0.22461465096582228
iteration : 5053
train acc:  0.8359375
train loss:  0.36076265573501587
train gradient:  0.24544761629691642
iteration : 5054
train acc:  0.8515625
train loss:  0.32638972997665405
train gradient:  0.22556260206635248
iteration : 5055
train acc:  0.8671875
train loss:  0.30589187145233154
train gradient:  0.31749061014029123
iteration : 5056
train acc:  0.890625
train loss:  0.3548968732357025
train gradient:  0.3147841932988468
iteration : 5057
train acc:  0.875
train loss:  0.3059268593788147
train gradient:  0.21302369302177054
iteration : 5058
train acc:  0.84375
train loss:  0.38914090394973755
train gradient:  0.3020290058528413
iteration : 5059
train acc:  0.8515625
train loss:  0.31959545612335205
train gradient:  0.2536150782484288
iteration : 5060
train acc:  0.8203125
train loss:  0.35580313205718994
train gradient:  0.27064156898321845
iteration : 5061
train acc:  0.8515625
train loss:  0.3188382685184479
train gradient:  0.3689745617918986
iteration : 5062
train acc:  0.7890625
train loss:  0.3909008502960205
train gradient:  0.3138995827688329
iteration : 5063
train acc:  0.875
train loss:  0.3359326124191284
train gradient:  0.3191747529320107
iteration : 5064
train acc:  0.859375
train loss:  0.3713309168815613
train gradient:  0.20481552676794545
iteration : 5065
train acc:  0.859375
train loss:  0.3370349407196045
train gradient:  0.2543575297425111
iteration : 5066
train acc:  0.859375
train loss:  0.2936765253543854
train gradient:  0.17342351906778447
iteration : 5067
train acc:  0.828125
train loss:  0.35603004693984985
train gradient:  0.25699903343459213
iteration : 5068
train acc:  0.8984375
train loss:  0.2985466718673706
train gradient:  0.2566237847263609
iteration : 5069
train acc:  0.8828125
train loss:  0.30221259593963623
train gradient:  0.2499099406531051
iteration : 5070
train acc:  0.84375
train loss:  0.3853303790092468
train gradient:  0.32792878080342874
iteration : 5071
train acc:  0.8828125
train loss:  0.34742119908332825
train gradient:  0.3320612969736579
iteration : 5072
train acc:  0.859375
train loss:  0.31251662969589233
train gradient:  0.30943338731096803
iteration : 5073
train acc:  0.8125
train loss:  0.41098731756210327
train gradient:  0.3169101771238615
iteration : 5074
train acc:  0.8359375
train loss:  0.4295811653137207
train gradient:  0.33245825997566325
iteration : 5075
train acc:  0.859375
train loss:  0.33447110652923584
train gradient:  0.23715028557183349
iteration : 5076
train acc:  0.875
train loss:  0.3006259799003601
train gradient:  0.20790773233853596
iteration : 5077
train acc:  0.8515625
train loss:  0.3558051884174347
train gradient:  0.36977622575570984
iteration : 5078
train acc:  0.796875
train loss:  0.4433659613132477
train gradient:  0.3594274368045685
iteration : 5079
train acc:  0.84375
train loss:  0.36161917448043823
train gradient:  0.25777814119828524
iteration : 5080
train acc:  0.828125
train loss:  0.4037435054779053
train gradient:  0.40643248102212304
iteration : 5081
train acc:  0.90625
train loss:  0.2823336124420166
train gradient:  0.21679404844004746
iteration : 5082
train acc:  0.8515625
train loss:  0.31275829672813416
train gradient:  0.30736026665353633
iteration : 5083
train acc:  0.8828125
train loss:  0.29847198724746704
train gradient:  0.22022683163778814
iteration : 5084
train acc:  0.8359375
train loss:  0.3311666250228882
train gradient:  0.18560638045632868
iteration : 5085
train acc:  0.8671875
train loss:  0.31143325567245483
train gradient:  0.26016652336317164
iteration : 5086
train acc:  0.90625
train loss:  0.24712786078453064
train gradient:  0.1736526136136834
iteration : 5087
train acc:  0.90625
train loss:  0.3745364844799042
train gradient:  0.26912069251750553
iteration : 5088
train acc:  0.7734375
train loss:  0.4303646385669708
train gradient:  0.3884523597983446
iteration : 5089
train acc:  0.8515625
train loss:  0.34641990065574646
train gradient:  0.21449172071081737
iteration : 5090
train acc:  0.8671875
train loss:  0.318773090839386
train gradient:  0.2931760882642931
iteration : 5091
train acc:  0.859375
train loss:  0.35904139280319214
train gradient:  0.19889107610345702
iteration : 5092
train acc:  0.890625
train loss:  0.3243190050125122
train gradient:  0.18512979911192992
iteration : 5093
train acc:  0.8515625
train loss:  0.39469724893569946
train gradient:  0.33604462037991967
iteration : 5094
train acc:  0.8203125
train loss:  0.3295772075653076
train gradient:  0.26754646570292506
iteration : 5095
train acc:  0.8671875
train loss:  0.3162393867969513
train gradient:  0.2247857650243784
iteration : 5096
train acc:  0.84375
train loss:  0.3684966564178467
train gradient:  0.34670590895313763
iteration : 5097
train acc:  0.8984375
train loss:  0.3010997176170349
train gradient:  0.17795420172284804
iteration : 5098
train acc:  0.8125
train loss:  0.45189613103866577
train gradient:  0.4089270268337251
iteration : 5099
train acc:  0.84375
train loss:  0.32694530487060547
train gradient:  0.263060126292338
iteration : 5100
train acc:  0.8203125
train loss:  0.3463461995124817
train gradient:  0.2543270350927171
iteration : 5101
train acc:  0.8359375
train loss:  0.3454028069972992
train gradient:  0.24349521569390045
iteration : 5102
train acc:  0.8828125
train loss:  0.27855581045150757
train gradient:  0.16507884878165252
iteration : 5103
train acc:  0.75
train loss:  0.49196743965148926
train gradient:  0.4060840353667232
iteration : 5104
train acc:  0.7890625
train loss:  0.4601493179798126
train gradient:  0.3349599418018432
iteration : 5105
train acc:  0.8359375
train loss:  0.37341636419296265
train gradient:  0.3302344486441449
iteration : 5106
train acc:  0.8046875
train loss:  0.41661444306373596
train gradient:  0.3283177809784166
iteration : 5107
train acc:  0.7890625
train loss:  0.4654092490673065
train gradient:  0.4392569777070018
iteration : 5108
train acc:  0.8828125
train loss:  0.2685856223106384
train gradient:  0.17668831878740482
iteration : 5109
train acc:  0.8671875
train loss:  0.3147209584712982
train gradient:  0.19080993450271444
iteration : 5110
train acc:  0.875
train loss:  0.31733736395835876
train gradient:  0.1779201697246567
iteration : 5111
train acc:  0.8515625
train loss:  0.338585764169693
train gradient:  0.20645534999073437
iteration : 5112
train acc:  0.8828125
train loss:  0.33909428119659424
train gradient:  0.2588513677090725
iteration : 5113
train acc:  0.8671875
train loss:  0.3101102411746979
train gradient:  0.1946627577080307
iteration : 5114
train acc:  0.8984375
train loss:  0.29327142238616943
train gradient:  0.19122619610498473
iteration : 5115
train acc:  0.8671875
train loss:  0.29599103331565857
train gradient:  0.17201813128042925
iteration : 5116
train acc:  0.8515625
train loss:  0.433109849691391
train gradient:  0.39090159489107956
iteration : 5117
train acc:  0.8359375
train loss:  0.37373724579811096
train gradient:  0.24437067268595103
iteration : 5118
train acc:  0.828125
train loss:  0.35140374302864075
train gradient:  0.22249148855297574
iteration : 5119
train acc:  0.84375
train loss:  0.368624746799469
train gradient:  0.3295777744291764
iteration : 5120
train acc:  0.8359375
train loss:  0.34130191802978516
train gradient:  0.33253787068254176
iteration : 5121
train acc:  0.875
train loss:  0.292153000831604
train gradient:  0.245251934002548
iteration : 5122
train acc:  0.875
train loss:  0.29755377769470215
train gradient:  0.1842373634656483
iteration : 5123
train acc:  0.828125
train loss:  0.3878125846385956
train gradient:  0.29664534119389496
iteration : 5124
train acc:  0.828125
train loss:  0.3590013086795807
train gradient:  0.24070529598955598
iteration : 5125
train acc:  0.8828125
train loss:  0.34354695677757263
train gradient:  0.25179763762359886
iteration : 5126
train acc:  0.8046875
train loss:  0.4096384644508362
train gradient:  0.2819677310801911
iteration : 5127
train acc:  0.8671875
train loss:  0.3523678779602051
train gradient:  0.24887101775810638
iteration : 5128
train acc:  0.8203125
train loss:  0.4198048412799835
train gradient:  0.2663702376158271
iteration : 5129
train acc:  0.8203125
train loss:  0.36080804467201233
train gradient:  0.32834057432513075
iteration : 5130
train acc:  0.7578125
train loss:  0.4547688961029053
train gradient:  0.3352292905179126
iteration : 5131
train acc:  0.7265625
train loss:  0.5324687957763672
train gradient:  0.5106231191801002
iteration : 5132
train acc:  0.8203125
train loss:  0.4273982346057892
train gradient:  0.3297288637000181
iteration : 5133
train acc:  0.8515625
train loss:  0.38700228929519653
train gradient:  0.28994693961268453
iteration : 5134
train acc:  0.8359375
train loss:  0.4549057185649872
train gradient:  0.5572724212642943
iteration : 5135
train acc:  0.9140625
train loss:  0.28420203924179077
train gradient:  0.23892798686701766
iteration : 5136
train acc:  0.859375
train loss:  0.37804561853408813
train gradient:  0.26888299470250837
iteration : 5137
train acc:  0.84375
train loss:  0.31815850734710693
train gradient:  0.2210410579831355
iteration : 5138
train acc:  0.84375
train loss:  0.3872441053390503
train gradient:  0.2967979875841312
iteration : 5139
train acc:  0.8984375
train loss:  0.2740742564201355
train gradient:  0.16446432890614004
iteration : 5140
train acc:  0.8125
train loss:  0.3759687542915344
train gradient:  0.2484073573009864
iteration : 5141
train acc:  0.8671875
train loss:  0.3072051405906677
train gradient:  0.17511999977810966
iteration : 5142
train acc:  0.796875
train loss:  0.37771332263946533
train gradient:  0.24412572618413314
iteration : 5143
train acc:  0.8125
train loss:  0.347208708524704
train gradient:  0.20729848127972897
iteration : 5144
train acc:  0.84375
train loss:  0.3416302800178528
train gradient:  0.24293116042923574
iteration : 5145
train acc:  0.8515625
train loss:  0.32395046949386597
train gradient:  0.17419361941023243
iteration : 5146
train acc:  0.78125
train loss:  0.5305629968643188
train gradient:  0.41183223614410536
iteration : 5147
train acc:  0.84375
train loss:  0.34318792819976807
train gradient:  0.20234795216823828
iteration : 5148
train acc:  0.8125
train loss:  0.39740103483200073
train gradient:  0.37116072880324535
iteration : 5149
train acc:  0.8828125
train loss:  0.3289353847503662
train gradient:  0.18854952957885768
iteration : 5150
train acc:  0.8125
train loss:  0.38309919834136963
train gradient:  0.2438333009749093
iteration : 5151
train acc:  0.8828125
train loss:  0.32834509015083313
train gradient:  0.17847449937068421
iteration : 5152
train acc:  0.890625
train loss:  0.29365822672843933
train gradient:  0.1876413256945278
iteration : 5153
train acc:  0.8359375
train loss:  0.36713266372680664
train gradient:  0.28139358941728404
iteration : 5154
train acc:  0.84375
train loss:  0.3550083637237549
train gradient:  0.1704657417909347
iteration : 5155
train acc:  0.875
train loss:  0.337694376707077
train gradient:  0.19856688298775285
iteration : 5156
train acc:  0.9140625
train loss:  0.24564293026924133
train gradient:  0.14745356747363608
iteration : 5157
train acc:  0.8359375
train loss:  0.33575326204299927
train gradient:  0.23452699525853316
iteration : 5158
train acc:  0.8515625
train loss:  0.3710147738456726
train gradient:  0.29455536045056196
iteration : 5159
train acc:  0.8203125
train loss:  0.38186800479888916
train gradient:  0.3682435920793952
iteration : 5160
train acc:  0.859375
train loss:  0.3293758034706116
train gradient:  0.2159200271132091
iteration : 5161
train acc:  0.8359375
train loss:  0.34972891211509705
train gradient:  0.2255800244629412
iteration : 5162
train acc:  0.8203125
train loss:  0.3894829750061035
train gradient:  0.22928400252435532
iteration : 5163
train acc:  0.8359375
train loss:  0.331394761800766
train gradient:  0.26600330097532043
iteration : 5164
train acc:  0.84375
train loss:  0.32965540885925293
train gradient:  0.19645648262465226
iteration : 5165
train acc:  0.890625
train loss:  0.2839844822883606
train gradient:  0.22992630308004822
iteration : 5166
train acc:  0.859375
train loss:  0.29949039220809937
train gradient:  0.21155181790194721
iteration : 5167
train acc:  0.828125
train loss:  0.3486420512199402
train gradient:  0.2685642879765369
iteration : 5168
train acc:  0.8203125
train loss:  0.3708004951477051
train gradient:  0.28613510998959374
iteration : 5169
train acc:  0.8515625
train loss:  0.32033640146255493
train gradient:  0.22168271964812608
iteration : 5170
train acc:  0.8046875
train loss:  0.38208726048469543
train gradient:  0.41980990612578634
iteration : 5171
train acc:  0.8203125
train loss:  0.37878739833831787
train gradient:  0.34127145399279507
iteration : 5172
train acc:  0.8203125
train loss:  0.4110476076602936
train gradient:  0.3686073556679888
iteration : 5173
train acc:  0.8828125
train loss:  0.29205775260925293
train gradient:  0.17779451728168838
iteration : 5174
train acc:  0.84375
train loss:  0.35929369926452637
train gradient:  0.20247862780946943
iteration : 5175
train acc:  0.8984375
train loss:  0.31000185012817383
train gradient:  0.17894023807586235
iteration : 5176
train acc:  0.8125
train loss:  0.4030224680900574
train gradient:  0.20277273731294948
iteration : 5177
train acc:  0.8671875
train loss:  0.27965110540390015
train gradient:  0.18393376059558225
iteration : 5178
train acc:  0.859375
train loss:  0.30810344219207764
train gradient:  0.1805866610833458
iteration : 5179
train acc:  0.8671875
train loss:  0.3193323016166687
train gradient:  0.2266977794540891
iteration : 5180
train acc:  0.890625
train loss:  0.295612096786499
train gradient:  0.1857605103992777
iteration : 5181
train acc:  0.8125
train loss:  0.4515179395675659
train gradient:  0.33635767162717645
iteration : 5182
train acc:  0.84375
train loss:  0.35596710443496704
train gradient:  0.2323894759218503
iteration : 5183
train acc:  0.828125
train loss:  0.4014224112033844
train gradient:  0.3264262854487118
iteration : 5184
train acc:  0.859375
train loss:  0.29979169368743896
train gradient:  0.2370957438165085
iteration : 5185
train acc:  0.8515625
train loss:  0.349963515996933
train gradient:  0.2971704629843402
iteration : 5186
train acc:  0.796875
train loss:  0.4377753436565399
train gradient:  0.2859009891401678
iteration : 5187
train acc:  0.828125
train loss:  0.3556542694568634
train gradient:  0.2420370875545541
iteration : 5188
train acc:  0.8515625
train loss:  0.3527514934539795
train gradient:  0.2807913961412464
iteration : 5189
train acc:  0.796875
train loss:  0.37831735610961914
train gradient:  0.32338603951180905
iteration : 5190
train acc:  0.8125
train loss:  0.41988301277160645
train gradient:  0.23133246480204228
iteration : 5191
train acc:  0.8125
train loss:  0.4314126670360565
train gradient:  0.4998155426433174
iteration : 5192
train acc:  0.890625
train loss:  0.3432936668395996
train gradient:  0.29849814697726257
iteration : 5193
train acc:  0.8515625
train loss:  0.3337036371231079
train gradient:  0.2052246444416647
iteration : 5194
train acc:  0.890625
train loss:  0.32461434602737427
train gradient:  0.1721453531974212
iteration : 5195
train acc:  0.828125
train loss:  0.3689519762992859
train gradient:  0.2644577136851965
iteration : 5196
train acc:  0.890625
train loss:  0.28006020188331604
train gradient:  0.16459012489048666
iteration : 5197
train acc:  0.8203125
train loss:  0.3853415250778198
train gradient:  0.2745848839697205
iteration : 5198
train acc:  0.8515625
train loss:  0.3517642617225647
train gradient:  0.19345266533519845
iteration : 5199
train acc:  0.8203125
train loss:  0.3852878510951996
train gradient:  0.20178636703141686
iteration : 5200
train acc:  0.84375
train loss:  0.30865123867988586
train gradient:  0.1592928359648004
iteration : 5201
train acc:  0.8046875
train loss:  0.42397212982177734
train gradient:  0.3704312532230458
iteration : 5202
train acc:  0.859375
train loss:  0.33152493834495544
train gradient:  0.2501927685880919
iteration : 5203
train acc:  0.8515625
train loss:  0.33263713121414185
train gradient:  0.16538724251168482
iteration : 5204
train acc:  0.8046875
train loss:  0.3770333528518677
train gradient:  0.31910122846031175
iteration : 5205
train acc:  0.875
train loss:  0.31993767619132996
train gradient:  0.22906815912170791
iteration : 5206
train acc:  0.84375
train loss:  0.3216257095336914
train gradient:  0.21900847071961266
iteration : 5207
train acc:  0.8046875
train loss:  0.43184924125671387
train gradient:  0.2756184430335095
iteration : 5208
train acc:  0.7578125
train loss:  0.47934114933013916
train gradient:  0.36816601860304865
iteration : 5209
train acc:  0.859375
train loss:  0.35454005002975464
train gradient:  0.232148650452657
iteration : 5210
train acc:  0.8515625
train loss:  0.45111173391342163
train gradient:  0.3334047079088385
iteration : 5211
train acc:  0.859375
train loss:  0.3100249767303467
train gradient:  0.20659480098965016
iteration : 5212
train acc:  0.8359375
train loss:  0.31118637323379517
train gradient:  0.2181896730967571
iteration : 5213
train acc:  0.8515625
train loss:  0.3497687578201294
train gradient:  0.19297716284751126
iteration : 5214
train acc:  0.859375
train loss:  0.3271159529685974
train gradient:  0.19361764970972764
iteration : 5215
train acc:  0.9140625
train loss:  0.2442210167646408
train gradient:  0.22205950941247746
iteration : 5216
train acc:  0.859375
train loss:  0.37641891837120056
train gradient:  0.27782467702104585
iteration : 5217
train acc:  0.8046875
train loss:  0.34340864419937134
train gradient:  0.258254172855784
iteration : 5218
train acc:  0.8125
train loss:  0.42320188879966736
train gradient:  0.3284362475437779
iteration : 5219
train acc:  0.828125
train loss:  0.30207353830337524
train gradient:  0.1673584956096158
iteration : 5220
train acc:  0.796875
train loss:  0.4606536030769348
train gradient:  0.539942118712897
iteration : 5221
train acc:  0.8203125
train loss:  0.3935820460319519
train gradient:  0.28311649690149127
iteration : 5222
train acc:  0.8515625
train loss:  0.3609514832496643
train gradient:  0.16094154193679522
iteration : 5223
train acc:  0.84375
train loss:  0.3587391972541809
train gradient:  0.2926461949818036
iteration : 5224
train acc:  0.875
train loss:  0.3628564178943634
train gradient:  0.38398558461789306
iteration : 5225
train acc:  0.8515625
train loss:  0.31302008032798767
train gradient:  0.14665427264253458
iteration : 5226
train acc:  0.8828125
train loss:  0.3154505789279938
train gradient:  0.22655184691735664
iteration : 5227
train acc:  0.796875
train loss:  0.4166889190673828
train gradient:  0.3352684131604971
iteration : 5228
train acc:  0.875
train loss:  0.29000869393348694
train gradient:  0.26325595240224725
iteration : 5229
train acc:  0.8671875
train loss:  0.336045503616333
train gradient:  0.17156987272085217
iteration : 5230
train acc:  0.8515625
train loss:  0.3776458501815796
train gradient:  0.2735254888612599
iteration : 5231
train acc:  0.8125
train loss:  0.36031243205070496
train gradient:  0.277666283166062
iteration : 5232
train acc:  0.875
train loss:  0.2786823511123657
train gradient:  0.10567360782109667
iteration : 5233
train acc:  0.828125
train loss:  0.3636716604232788
train gradient:  0.24121196887171312
iteration : 5234
train acc:  0.8359375
train loss:  0.4059462547302246
train gradient:  0.1962313352605477
iteration : 5235
train acc:  0.84375
train loss:  0.3552664816379547
train gradient:  0.19937777729251202
iteration : 5236
train acc:  0.8671875
train loss:  0.3391149640083313
train gradient:  0.22432755274151603
iteration : 5237
train acc:  0.8515625
train loss:  0.3399800956249237
train gradient:  0.20730056962652152
iteration : 5238
train acc:  0.859375
train loss:  0.4098873734474182
train gradient:  0.33182703036525263
iteration : 5239
train acc:  0.8515625
train loss:  0.3020787835121155
train gradient:  0.16127719684547848
iteration : 5240
train acc:  0.8046875
train loss:  0.3874800205230713
train gradient:  0.2768341848968283
iteration : 5241
train acc:  0.84375
train loss:  0.32517197728157043
train gradient:  0.20023740758433234
iteration : 5242
train acc:  0.8515625
train loss:  0.350022554397583
train gradient:  0.25225431561735584
iteration : 5243
train acc:  0.8359375
train loss:  0.3394162058830261
train gradient:  0.2195240233634823
iteration : 5244
train acc:  0.7734375
train loss:  0.5348814129829407
train gradient:  0.4847751623406053
iteration : 5245
train acc:  0.8203125
train loss:  0.3942885100841522
train gradient:  0.28952619330669244
iteration : 5246
train acc:  0.890625
train loss:  0.27022913098335266
train gradient:  0.21755272978430706
iteration : 5247
train acc:  0.8828125
train loss:  0.29254448413848877
train gradient:  0.17791540695273422
iteration : 5248
train acc:  0.875
train loss:  0.3692399859428406
train gradient:  0.2462215034662073
iteration : 5249
train acc:  0.8203125
train loss:  0.35088109970092773
train gradient:  0.2146332369032179
iteration : 5250
train acc:  0.8125
train loss:  0.41195571422576904
train gradient:  0.3555808220382799
iteration : 5251
train acc:  0.8671875
train loss:  0.3032672107219696
train gradient:  0.16858535670434463
iteration : 5252
train acc:  0.8515625
train loss:  0.3074425160884857
train gradient:  0.16919255627448396
iteration : 5253
train acc:  0.84375
train loss:  0.3987843990325928
train gradient:  0.25116841636140097
iteration : 5254
train acc:  0.9140625
train loss:  0.26892536878585815
train gradient:  0.11917079700699706
iteration : 5255
train acc:  0.8046875
train loss:  0.40003377199172974
train gradient:  0.33270556672422824
iteration : 5256
train acc:  0.90625
train loss:  0.26840925216674805
train gradient:  0.14109780322653515
iteration : 5257
train acc:  0.8515625
train loss:  0.3774968385696411
train gradient:  0.2709902787163775
iteration : 5258
train acc:  0.859375
train loss:  0.2762758135795593
train gradient:  0.18933969282232144
iteration : 5259
train acc:  0.8359375
train loss:  0.40627264976501465
train gradient:  0.39488499043538616
iteration : 5260
train acc:  0.8671875
train loss:  0.3361482620239258
train gradient:  0.3462519155951012
iteration : 5261
train acc:  0.8671875
train loss:  0.30599480867385864
train gradient:  0.3185646082699216
iteration : 5262
train acc:  0.8359375
train loss:  0.3865484595298767
train gradient:  0.31520028003933215
iteration : 5263
train acc:  0.859375
train loss:  0.32226166129112244
train gradient:  0.15559586415450938
iteration : 5264
train acc:  0.84375
train loss:  0.38569605350494385
train gradient:  0.2204458815282565
iteration : 5265
train acc:  0.84375
train loss:  0.40004128217697144
train gradient:  0.2683194231567857
iteration : 5266
train acc:  0.84375
train loss:  0.4237731695175171
train gradient:  0.36202006346878274
iteration : 5267
train acc:  0.8515625
train loss:  0.3379426896572113
train gradient:  0.19504913460675438
iteration : 5268
train acc:  0.859375
train loss:  0.326532244682312
train gradient:  0.13764207980224769
iteration : 5269
train acc:  0.8828125
train loss:  0.2834506630897522
train gradient:  0.16277597936052493
iteration : 5270
train acc:  0.859375
train loss:  0.3037896156311035
train gradient:  0.17383352172599778
iteration : 5271
train acc:  0.7890625
train loss:  0.4147202968597412
train gradient:  0.34373721826467935
iteration : 5272
train acc:  0.8359375
train loss:  0.405735045671463
train gradient:  0.2423717200140237
iteration : 5273
train acc:  0.8828125
train loss:  0.27186083793640137
train gradient:  0.2048462035895238
iteration : 5274
train acc:  0.8515625
train loss:  0.35315799713134766
train gradient:  0.367601228567701
iteration : 5275
train acc:  0.8671875
train loss:  0.3480977416038513
train gradient:  0.2098202436439215
iteration : 5276
train acc:  0.875
train loss:  0.3024703860282898
train gradient:  0.18848897473442833
iteration : 5277
train acc:  0.84375
train loss:  0.3420907258987427
train gradient:  0.24420258782503523
iteration : 5278
train acc:  0.890625
train loss:  0.23907767236232758
train gradient:  0.13585101808449945
iteration : 5279
train acc:  0.8203125
train loss:  0.4451722502708435
train gradient:  0.34124955489243763
iteration : 5280
train acc:  0.8203125
train loss:  0.37408462166786194
train gradient:  0.22053781628582775
iteration : 5281
train acc:  0.796875
train loss:  0.46339672803878784
train gradient:  0.38106379355757897
iteration : 5282
train acc:  0.84375
train loss:  0.3345795273780823
train gradient:  0.15372782145976568
iteration : 5283
train acc:  0.8203125
train loss:  0.4233577251434326
train gradient:  0.36921943731699536
iteration : 5284
train acc:  0.8125
train loss:  0.35784712433815
train gradient:  0.2302535741722545
iteration : 5285
train acc:  0.8671875
train loss:  0.3307967185974121
train gradient:  0.18182782178135246
iteration : 5286
train acc:  0.8671875
train loss:  0.29382646083831787
train gradient:  0.23205557533316912
iteration : 5287
train acc:  0.8828125
train loss:  0.32641589641571045
train gradient:  0.20024839235322936
iteration : 5288
train acc:  0.8984375
train loss:  0.2502230405807495
train gradient:  0.134413830118525
iteration : 5289
train acc:  0.859375
train loss:  0.3769618272781372
train gradient:  0.23727884660959359
iteration : 5290
train acc:  0.84375
train loss:  0.38416939973831177
train gradient:  0.26057312092173296
iteration : 5291
train acc:  0.875
train loss:  0.3444129228591919
train gradient:  0.16240452142235398
iteration : 5292
train acc:  0.828125
train loss:  0.3441145420074463
train gradient:  0.22068493000116052
iteration : 5293
train acc:  0.828125
train loss:  0.38819777965545654
train gradient:  0.22858010951057076
iteration : 5294
train acc:  0.875
train loss:  0.2934413552284241
train gradient:  0.23340575606453096
iteration : 5295
train acc:  0.8046875
train loss:  0.33089905977249146
train gradient:  0.3130840351188786
iteration : 5296
train acc:  0.84375
train loss:  0.373430073261261
train gradient:  0.24513611697274373
iteration : 5297
train acc:  0.7890625
train loss:  0.41967517137527466
train gradient:  0.3145671206221081
iteration : 5298
train acc:  0.8203125
train loss:  0.4283367395401001
train gradient:  0.2389519280895227
iteration : 5299
train acc:  0.7890625
train loss:  0.3961373567581177
train gradient:  0.27935326386641085
iteration : 5300
train acc:  0.8515625
train loss:  0.3526272177696228
train gradient:  0.2886476024564783
iteration : 5301
train acc:  0.8515625
train loss:  0.3493892550468445
train gradient:  0.34992123731572816
iteration : 5302
train acc:  0.828125
train loss:  0.3799237906932831
train gradient:  0.3097656597977369
iteration : 5303
train acc:  0.828125
train loss:  0.416146844625473
train gradient:  0.26145684459467616
iteration : 5304
train acc:  0.8515625
train loss:  0.3408074378967285
train gradient:  0.1782194233568271
iteration : 5305
train acc:  0.796875
train loss:  0.4005686044692993
train gradient:  0.2928743926747542
iteration : 5306
train acc:  0.796875
train loss:  0.4153174161911011
train gradient:  0.2216994437732065
iteration : 5307
train acc:  0.8671875
train loss:  0.3190259337425232
train gradient:  0.24643883783692455
iteration : 5308
train acc:  0.796875
train loss:  0.38572388887405396
train gradient:  0.22655288134748197
iteration : 5309
train acc:  0.84375
train loss:  0.44741153717041016
train gradient:  0.3103563996096826
iteration : 5310
train acc:  0.84375
train loss:  0.3778496980667114
train gradient:  0.20694553018754158
iteration : 5311
train acc:  0.828125
train loss:  0.42127150297164917
train gradient:  0.31397511795391886
iteration : 5312
train acc:  0.859375
train loss:  0.3354085683822632
train gradient:  0.24951272648059367
iteration : 5313
train acc:  0.8203125
train loss:  0.48268768191337585
train gradient:  0.43580779753527304
iteration : 5314
train acc:  0.8359375
train loss:  0.3594824969768524
train gradient:  0.19088216429152965
iteration : 5315
train acc:  0.84375
train loss:  0.3533897399902344
train gradient:  0.23228481936194806
iteration : 5316
train acc:  0.8125
train loss:  0.4026179909706116
train gradient:  0.3144639902825649
iteration : 5317
train acc:  0.859375
train loss:  0.3039523959159851
train gradient:  0.22349861468895765
iteration : 5318
train acc:  0.90625
train loss:  0.29194921255111694
train gradient:  0.17464710074065293
iteration : 5319
train acc:  0.8671875
train loss:  0.391090452671051
train gradient:  0.341749012292954
iteration : 5320
train acc:  0.8671875
train loss:  0.3063144385814667
train gradient:  0.2626315728844138
iteration : 5321
train acc:  0.8203125
train loss:  0.3668791651725769
train gradient:  0.25632505898390795
iteration : 5322
train acc:  0.8671875
train loss:  0.31747156381607056
train gradient:  0.18142649439967054
iteration : 5323
train acc:  0.828125
train loss:  0.40914976596832275
train gradient:  0.3237995114353178
iteration : 5324
train acc:  0.8515625
train loss:  0.31653493642807007
train gradient:  0.159773849081882
iteration : 5325
train acc:  0.8671875
train loss:  0.3231947422027588
train gradient:  0.18038650686300695
iteration : 5326
train acc:  0.875
train loss:  0.34470194578170776
train gradient:  0.20896374901261397
iteration : 5327
train acc:  0.8046875
train loss:  0.3917878270149231
train gradient:  0.2163154208988572
iteration : 5328
train acc:  0.8515625
train loss:  0.371751993894577
train gradient:  0.23387278297979563
iteration : 5329
train acc:  0.875
train loss:  0.3083476424217224
train gradient:  0.23908541710618547
iteration : 5330
train acc:  0.8515625
train loss:  0.2944837510585785
train gradient:  0.14368201368458244
iteration : 5331
train acc:  0.8046875
train loss:  0.48916083574295044
train gradient:  0.43947197940194593
iteration : 5332
train acc:  0.890625
train loss:  0.33517181873321533
train gradient:  0.40235433281455507
iteration : 5333
train acc:  0.8125
train loss:  0.40534496307373047
train gradient:  0.377245126405185
iteration : 5334
train acc:  0.890625
train loss:  0.27191320061683655
train gradient:  0.17581485373208638
iteration : 5335
train acc:  0.828125
train loss:  0.4018740653991699
train gradient:  0.27159820023998893
iteration : 5336
train acc:  0.8125
train loss:  0.35585999488830566
train gradient:  0.2357362032811075
iteration : 5337
train acc:  0.8359375
train loss:  0.30762872099876404
train gradient:  0.21699073084974835
iteration : 5338
train acc:  0.7578125
train loss:  0.4647682011127472
train gradient:  0.3719866938950403
iteration : 5339
train acc:  0.8671875
train loss:  0.3138701319694519
train gradient:  0.3076732388297482
iteration : 5340
train acc:  0.859375
train loss:  0.3593965172767639
train gradient:  0.391775828713892
iteration : 5341
train acc:  0.84375
train loss:  0.2985183000564575
train gradient:  0.2043974963009651
iteration : 5342
train acc:  0.8671875
train loss:  0.3076726198196411
train gradient:  0.1845225208637882
iteration : 5343
train acc:  0.8515625
train loss:  0.32924389839172363
train gradient:  0.3164728902467551
iteration : 5344
train acc:  0.8125
train loss:  0.3585761785507202
train gradient:  0.2399009878044307
iteration : 5345
train acc:  0.8359375
train loss:  0.3689671754837036
train gradient:  0.2693615608346743
iteration : 5346
train acc:  0.8203125
train loss:  0.32006049156188965
train gradient:  0.17659040724089853
iteration : 5347
train acc:  0.8359375
train loss:  0.3455561101436615
train gradient:  0.20286464480083713
iteration : 5348
train acc:  0.859375
train loss:  0.2963531017303467
train gradient:  0.20394009762626836
iteration : 5349
train acc:  0.828125
train loss:  0.43044912815093994
train gradient:  0.26839536590530605
iteration : 5350
train acc:  0.828125
train loss:  0.41515809297561646
train gradient:  0.3128877075454804
iteration : 5351
train acc:  0.8203125
train loss:  0.35916948318481445
train gradient:  0.22250144813628037
iteration : 5352
train acc:  0.8359375
train loss:  0.33620497584342957
train gradient:  0.23429050376846366
iteration : 5353
train acc:  0.8515625
train loss:  0.31707966327667236
train gradient:  0.19099506611424927
iteration : 5354
train acc:  0.8515625
train loss:  0.3441237807273865
train gradient:  0.41656113885541324
iteration : 5355
train acc:  0.8828125
train loss:  0.3167891204357147
train gradient:  0.17886542376246192
iteration : 5356
train acc:  0.8359375
train loss:  0.37519800662994385
train gradient:  0.30776581387432694
iteration : 5357
train acc:  0.84375
train loss:  0.3404461741447449
train gradient:  0.27938155031029704
iteration : 5358
train acc:  0.8828125
train loss:  0.26626884937286377
train gradient:  0.17442650720692882
iteration : 5359
train acc:  0.8359375
train loss:  0.3973737955093384
train gradient:  0.3276020422737215
iteration : 5360
train acc:  0.859375
train loss:  0.33689770102500916
train gradient:  0.18963001301200227
iteration : 5361
train acc:  0.8203125
train loss:  0.3241840600967407
train gradient:  0.17385415316995576
iteration : 5362
train acc:  0.875
train loss:  0.2495446503162384
train gradient:  0.1555913191181289
iteration : 5363
train acc:  0.8828125
train loss:  0.3063388764858246
train gradient:  0.22353127207377962
iteration : 5364
train acc:  0.8125
train loss:  0.3838171064853668
train gradient:  0.28315914798596953
iteration : 5365
train acc:  0.828125
train loss:  0.39230096340179443
train gradient:  0.37075552723984095
iteration : 5366
train acc:  0.859375
train loss:  0.3507113456726074
train gradient:  0.23436423164905654
iteration : 5367
train acc:  0.8515625
train loss:  0.3729051649570465
train gradient:  0.27499482382424456
iteration : 5368
train acc:  0.828125
train loss:  0.3237611651420593
train gradient:  0.24086343236111313
iteration : 5369
train acc:  0.84375
train loss:  0.3313734233379364
train gradient:  0.26176904600083306
iteration : 5370
train acc:  0.8359375
train loss:  0.3268205523490906
train gradient:  0.15347714839053284
iteration : 5371
train acc:  0.8359375
train loss:  0.4171369671821594
train gradient:  0.3049408353100534
iteration : 5372
train acc:  0.796875
train loss:  0.36325863003730774
train gradient:  0.3007842463983646
iteration : 5373
train acc:  0.8203125
train loss:  0.37351834774017334
train gradient:  0.2712865255705808
iteration : 5374
train acc:  0.8515625
train loss:  0.35292309522628784
train gradient:  0.21152752564978738
iteration : 5375
train acc:  0.8671875
train loss:  0.39334210753440857
train gradient:  0.2819899178590605
iteration : 5376
train acc:  0.875
train loss:  0.31836995482444763
train gradient:  0.2431459786477482
iteration : 5377
train acc:  0.8125
train loss:  0.3934035301208496
train gradient:  0.24579719167129505
iteration : 5378
train acc:  0.859375
train loss:  0.3760192394256592
train gradient:  0.3050931483862925
iteration : 5379
train acc:  0.875
train loss:  0.2978780269622803
train gradient:  0.15714739970751868
iteration : 5380
train acc:  0.8671875
train loss:  0.33667582273483276
train gradient:  0.16063162724501062
iteration : 5381
train acc:  0.8203125
train loss:  0.3669414520263672
train gradient:  0.21758646394046305
iteration : 5382
train acc:  0.8125
train loss:  0.4151647090911865
train gradient:  0.28008233670213606
iteration : 5383
train acc:  0.8671875
train loss:  0.3019518256187439
train gradient:  0.16659234021390973
iteration : 5384
train acc:  0.8359375
train loss:  0.2998615503311157
train gradient:  0.1858120400337949
iteration : 5385
train acc:  0.8515625
train loss:  0.3512956500053406
train gradient:  0.31334829869935993
iteration : 5386
train acc:  0.8828125
train loss:  0.3271428048610687
train gradient:  0.15555507630063775
iteration : 5387
train acc:  0.8046875
train loss:  0.48467540740966797
train gradient:  0.3744572863425428
iteration : 5388
train acc:  0.8671875
train loss:  0.3030667304992676
train gradient:  0.228629603633335
iteration : 5389
train acc:  0.828125
train loss:  0.41619187593460083
train gradient:  0.25927945292840415
iteration : 5390
train acc:  0.8359375
train loss:  0.34102755784988403
train gradient:  0.21006482086333603
iteration : 5391
train acc:  0.8828125
train loss:  0.32559430599212646
train gradient:  0.19476603510018775
iteration : 5392
train acc:  0.84375
train loss:  0.34978342056274414
train gradient:  0.2717743573776777
iteration : 5393
train acc:  0.875
train loss:  0.3264327049255371
train gradient:  0.1423417060928926
iteration : 5394
train acc:  0.890625
train loss:  0.29386475682258606
train gradient:  0.26773784604243717
iteration : 5395
train acc:  0.8203125
train loss:  0.3704332709312439
train gradient:  0.3115587388262161
iteration : 5396
train acc:  0.859375
train loss:  0.33189570903778076
train gradient:  0.25542042450313235
iteration : 5397
train acc:  0.75
train loss:  0.43895864486694336
train gradient:  0.25891529224632315
iteration : 5398
train acc:  0.859375
train loss:  0.3189643621444702
train gradient:  0.23228900990601278
iteration : 5399
train acc:  0.7734375
train loss:  0.4930645227432251
train gradient:  0.452784460469656
iteration : 5400
train acc:  0.8125
train loss:  0.370547890663147
train gradient:  0.2691023118245843
iteration : 5401
train acc:  0.8828125
train loss:  0.32803189754486084
train gradient:  0.2503089319741157
iteration : 5402
train acc:  0.8515625
train loss:  0.3529665172100067
train gradient:  0.1747427496561995
iteration : 5403
train acc:  0.828125
train loss:  0.4357760548591614
train gradient:  0.25810244661554127
iteration : 5404
train acc:  0.8125
train loss:  0.3840538263320923
train gradient:  0.3689845000496806
iteration : 5405
train acc:  0.8515625
train loss:  0.3404639959335327
train gradient:  0.19711863303506277
iteration : 5406
train acc:  0.7890625
train loss:  0.45340028405189514
train gradient:  0.28157137283424183
iteration : 5407
train acc:  0.765625
train loss:  0.43402937054634094
train gradient:  0.29490685812902007
iteration : 5408
train acc:  0.8984375
train loss:  0.26529622077941895
train gradient:  0.1600516256708471
iteration : 5409
train acc:  0.8203125
train loss:  0.35356849431991577
train gradient:  0.23957461083082549
iteration : 5410
train acc:  0.8046875
train loss:  0.3692154586315155
train gradient:  0.20481615719961555
iteration : 5411
train acc:  0.8515625
train loss:  0.2981087565422058
train gradient:  0.20619032294873904
iteration : 5412
train acc:  0.84375
train loss:  0.3657090961933136
train gradient:  0.22262174982074784
iteration : 5413
train acc:  0.7890625
train loss:  0.44234585762023926
train gradient:  0.39830547598496957
iteration : 5414
train acc:  0.8125
train loss:  0.3628121316432953
train gradient:  0.20788298125916027
iteration : 5415
train acc:  0.84375
train loss:  0.35087913274765015
train gradient:  0.2174914996011168
iteration : 5416
train acc:  0.84375
train loss:  0.32006344199180603
train gradient:  0.21997677718906483
iteration : 5417
train acc:  0.8125
train loss:  0.4324101507663727
train gradient:  0.2207642707607746
iteration : 5418
train acc:  0.875
train loss:  0.35338759422302246
train gradient:  0.1991528534500867
iteration : 5419
train acc:  0.8359375
train loss:  0.3336629867553711
train gradient:  0.15912754851721955
iteration : 5420
train acc:  0.890625
train loss:  0.3530269265174866
train gradient:  0.2708749201541698
iteration : 5421
train acc:  0.8671875
train loss:  0.3448244631290436
train gradient:  0.19213567124179867
iteration : 5422
train acc:  0.90625
train loss:  0.2869489789009094
train gradient:  0.1487753831314494
iteration : 5423
train acc:  0.8984375
train loss:  0.2676279544830322
train gradient:  0.11416674070956463
iteration : 5424
train acc:  0.859375
train loss:  0.37906497716903687
train gradient:  0.2097184135835402
iteration : 5425
train acc:  0.8984375
train loss:  0.2680222988128662
train gradient:  0.13090588380384682
iteration : 5426
train acc:  0.828125
train loss:  0.36751818656921387
train gradient:  0.20036113203399647
iteration : 5427
train acc:  0.859375
train loss:  0.35782134532928467
train gradient:  0.2168329555079601
iteration : 5428
train acc:  0.796875
train loss:  0.3970649242401123
train gradient:  0.30513761058046435
iteration : 5429
train acc:  0.8046875
train loss:  0.4172630310058594
train gradient:  0.2785767518258096
iteration : 5430
train acc:  0.8828125
train loss:  0.3153098523616791
train gradient:  0.16147389524151062
iteration : 5431
train acc:  0.828125
train loss:  0.4036843180656433
train gradient:  0.25886343600665845
iteration : 5432
train acc:  0.8828125
train loss:  0.2564387917518616
train gradient:  0.16946570354380341
iteration : 5433
train acc:  0.8203125
train loss:  0.35811182856559753
train gradient:  0.22643960026667356
iteration : 5434
train acc:  0.8203125
train loss:  0.3651069700717926
train gradient:  0.2291674964274759
iteration : 5435
train acc:  0.8671875
train loss:  0.41346168518066406
train gradient:  0.3704724518693104
iteration : 5436
train acc:  0.8125
train loss:  0.43184852600097656
train gradient:  0.43156255578757197
iteration : 5437
train acc:  0.84375
train loss:  0.3508336544036865
train gradient:  0.17910463695424228
iteration : 5438
train acc:  0.828125
train loss:  0.3628860116004944
train gradient:  0.18399351291300037
iteration : 5439
train acc:  0.8671875
train loss:  0.3044797480106354
train gradient:  0.17587122017762732
iteration : 5440
train acc:  0.84375
train loss:  0.32124847173690796
train gradient:  0.22893322666556493
iteration : 5441
train acc:  0.8203125
train loss:  0.38283997774124146
train gradient:  0.27087062003546764
iteration : 5442
train acc:  0.890625
train loss:  0.26387184858322144
train gradient:  0.12688617085056494
iteration : 5443
train acc:  0.84375
train loss:  0.34016847610473633
train gradient:  0.2904777828752747
iteration : 5444
train acc:  0.8671875
train loss:  0.287137508392334
train gradient:  0.16932544406509972
iteration : 5445
train acc:  0.8203125
train loss:  0.46152132749557495
train gradient:  0.25473422461908846
iteration : 5446
train acc:  0.8984375
train loss:  0.2945006489753723
train gradient:  0.2966379139260083
iteration : 5447
train acc:  0.8359375
train loss:  0.36357447504997253
train gradient:  0.2226051058202126
iteration : 5448
train acc:  0.8203125
train loss:  0.3277643918991089
train gradient:  0.18676390934726794
iteration : 5449
train acc:  0.7890625
train loss:  0.4413195252418518
train gradient:  0.3671584710225675
iteration : 5450
train acc:  0.796875
train loss:  0.4367358088493347
train gradient:  0.356447624959435
iteration : 5451
train acc:  0.8671875
train loss:  0.3509822189807892
train gradient:  0.17921174382803082
iteration : 5452
train acc:  0.8515625
train loss:  0.3268781304359436
train gradient:  0.15885456325867756
iteration : 5453
train acc:  0.875
train loss:  0.29289865493774414
train gradient:  0.16927765424910868
iteration : 5454
train acc:  0.8046875
train loss:  0.5159100890159607
train gradient:  0.4506446467343275
iteration : 5455
train acc:  0.8046875
train loss:  0.3277013301849365
train gradient:  0.21092747558376862
iteration : 5456
train acc:  0.859375
train loss:  0.3681491017341614
train gradient:  0.19400026157524128
iteration : 5457
train acc:  0.78125
train loss:  0.39168769121170044
train gradient:  0.42701403531576165
iteration : 5458
train acc:  0.8515625
train loss:  0.3605386018753052
train gradient:  0.29725372374064546
iteration : 5459
train acc:  0.8984375
train loss:  0.294499009847641
train gradient:  0.2444333018901017
iteration : 5460
train acc:  0.875
train loss:  0.2866895794868469
train gradient:  0.12150919552633546
iteration : 5461
train acc:  0.828125
train loss:  0.37207046151161194
train gradient:  0.2713673313517475
iteration : 5462
train acc:  0.875
train loss:  0.29384154081344604
train gradient:  0.23539617983040823
iteration : 5463
train acc:  0.859375
train loss:  0.34151414036750793
train gradient:  0.20274144719247955
iteration : 5464
train acc:  0.828125
train loss:  0.32537412643432617
train gradient:  0.28704417824411443
iteration : 5465
train acc:  0.8203125
train loss:  0.4634159207344055
train gradient:  0.34379564269096047
iteration : 5466
train acc:  0.828125
train loss:  0.3583396077156067
train gradient:  0.17619260895429123
iteration : 5467
train acc:  0.84375
train loss:  0.32085901498794556
train gradient:  0.2859235123827576
iteration : 5468
train acc:  0.8203125
train loss:  0.3667881488800049
train gradient:  0.24781158370902534
iteration : 5469
train acc:  0.796875
train loss:  0.3670356869697571
train gradient:  0.21677978023891892
iteration : 5470
train acc:  0.84375
train loss:  0.32432299852371216
train gradient:  0.23215905708527365
iteration : 5471
train acc:  0.8515625
train loss:  0.28948014974594116
train gradient:  0.14117965517871417
iteration : 5472
train acc:  0.84375
train loss:  0.3282940089702606
train gradient:  0.23333139813441728
iteration : 5473
train acc:  0.828125
train loss:  0.38474971055984497
train gradient:  0.20271172559526407
iteration : 5474
train acc:  0.796875
train loss:  0.456903874874115
train gradient:  0.4907173507652687
iteration : 5475
train acc:  0.875
train loss:  0.29488858580589294
train gradient:  0.26428780989229445
iteration : 5476
train acc:  0.8515625
train loss:  0.3435278534889221
train gradient:  0.20091099382583996
iteration : 5477
train acc:  0.8515625
train loss:  0.3035981357097626
train gradient:  0.12001724212487731
iteration : 5478
train acc:  0.9140625
train loss:  0.263498455286026
train gradient:  0.1324010439873166
iteration : 5479
train acc:  0.8203125
train loss:  0.37338733673095703
train gradient:  0.29166240821140665
iteration : 5480
train acc:  0.8203125
train loss:  0.3802306652069092
train gradient:  0.25473437480630395
iteration : 5481
train acc:  0.859375
train loss:  0.31103187799453735
train gradient:  0.14897837094043448
iteration : 5482
train acc:  0.859375
train loss:  0.39248859882354736
train gradient:  0.25853339741016146
iteration : 5483
train acc:  0.84375
train loss:  0.3314545452594757
train gradient:  0.15216897996880668
iteration : 5484
train acc:  0.859375
train loss:  0.31216317415237427
train gradient:  0.1867355944903869
iteration : 5485
train acc:  0.8125
train loss:  0.41554850339889526
train gradient:  0.30329057126940134
iteration : 5486
train acc:  0.859375
train loss:  0.3048933148384094
train gradient:  0.25710912361422145
iteration : 5487
train acc:  0.84375
train loss:  0.4086315929889679
train gradient:  0.33606369658906715
iteration : 5488
train acc:  0.8203125
train loss:  0.3799620270729065
train gradient:  0.22388589655502414
iteration : 5489
train acc:  0.8359375
train loss:  0.38344606757164
train gradient:  0.33034746469265336
iteration : 5490
train acc:  0.8125
train loss:  0.3750379681587219
train gradient:  0.22808897507792053
iteration : 5491
train acc:  0.8671875
train loss:  0.3220067024230957
train gradient:  0.19189621596720247
iteration : 5492
train acc:  0.8203125
train loss:  0.34567680954933167
train gradient:  0.23647751710383258
iteration : 5493
train acc:  0.7890625
train loss:  0.4422001838684082
train gradient:  0.3558322787392788
iteration : 5494
train acc:  0.859375
train loss:  0.32653507590293884
train gradient:  0.177469067528913
iteration : 5495
train acc:  0.828125
train loss:  0.3572651147842407
train gradient:  0.27601130501799453
iteration : 5496
train acc:  0.8828125
train loss:  0.2998724579811096
train gradient:  0.13985575607855122
iteration : 5497
train acc:  0.8359375
train loss:  0.3339604139328003
train gradient:  0.23492015484698264
iteration : 5498
train acc:  0.8984375
train loss:  0.3311777412891388
train gradient:  0.2250113350722868
iteration : 5499
train acc:  0.8046875
train loss:  0.43201562762260437
train gradient:  0.3793785907990481
iteration : 5500
train acc:  0.8671875
train loss:  0.32833176851272583
train gradient:  0.16130906638336298
iteration : 5501
train acc:  0.8046875
train loss:  0.3944711685180664
train gradient:  0.32842847418758386
iteration : 5502
train acc:  0.8515625
train loss:  0.3518988788127899
train gradient:  0.20607252978299342
iteration : 5503
train acc:  0.890625
train loss:  0.3110902011394501
train gradient:  0.1664313928404531
iteration : 5504
train acc:  0.828125
train loss:  0.34953129291534424
train gradient:  0.23276870319180137
iteration : 5505
train acc:  0.8984375
train loss:  0.29536759853363037
train gradient:  0.1484233388307836
iteration : 5506
train acc:  0.859375
train loss:  0.2961733937263489
train gradient:  0.17592353020951032
iteration : 5507
train acc:  0.8984375
train loss:  0.25914567708969116
train gradient:  0.11826696711306366
iteration : 5508
train acc:  0.8203125
train loss:  0.40769410133361816
train gradient:  0.23897398871821793
iteration : 5509
train acc:  0.8515625
train loss:  0.3594592809677124
train gradient:  0.22333246283587804
iteration : 5510
train acc:  0.90625
train loss:  0.27342426776885986
train gradient:  0.16223207729205313
iteration : 5511
train acc:  0.84375
train loss:  0.33919578790664673
train gradient:  0.1744936858862558
iteration : 5512
train acc:  0.8515625
train loss:  0.34545618295669556
train gradient:  0.14695738874650605
iteration : 5513
train acc:  0.8671875
train loss:  0.3107771873474121
train gradient:  0.2315224900006109
iteration : 5514
train acc:  0.78125
train loss:  0.38266438245773315
train gradient:  0.2796992622479665
iteration : 5515
train acc:  0.8125
train loss:  0.34733423590660095
train gradient:  0.2281071505802596
iteration : 5516
train acc:  0.890625
train loss:  0.2946878969669342
train gradient:  0.15926598945258688
iteration : 5517
train acc:  0.8828125
train loss:  0.30275392532348633
train gradient:  0.1598988745162988
iteration : 5518
train acc:  0.8984375
train loss:  0.2929701805114746
train gradient:  0.1498882614526708
iteration : 5519
train acc:  0.765625
train loss:  0.42982587218284607
train gradient:  0.2742854210067011
iteration : 5520
train acc:  0.8359375
train loss:  0.3405013978481293
train gradient:  0.20991055094462027
iteration : 5521
train acc:  0.828125
train loss:  0.3701648712158203
train gradient:  0.24280059129437298
iteration : 5522
train acc:  0.828125
train loss:  0.34925705194473267
train gradient:  0.30478902080815645
iteration : 5523
train acc:  0.8359375
train loss:  0.3299303352832794
train gradient:  0.2408280887356017
iteration : 5524
train acc:  0.875
train loss:  0.303336501121521
train gradient:  0.18974814841211454
iteration : 5525
train acc:  0.90625
train loss:  0.268283486366272
train gradient:  0.20706567208244697
iteration : 5526
train acc:  0.8984375
train loss:  0.3155933618545532
train gradient:  0.3216175679878926
iteration : 5527
train acc:  0.8828125
train loss:  0.2900696098804474
train gradient:  0.13548188174500242
iteration : 5528
train acc:  0.859375
train loss:  0.387200266122818
train gradient:  0.30154855096923516
iteration : 5529
train acc:  0.8828125
train loss:  0.31031689047813416
train gradient:  0.2865713551811381
iteration : 5530
train acc:  0.8125
train loss:  0.39925113320350647
train gradient:  0.27986347859707095
iteration : 5531
train acc:  0.8203125
train loss:  0.3832956552505493
train gradient:  0.26876979617090463
iteration : 5532
train acc:  0.8203125
train loss:  0.3912069797515869
train gradient:  0.2767037716781955
iteration : 5533
train acc:  0.8828125
train loss:  0.29122376441955566
train gradient:  0.1196605150253014
iteration : 5534
train acc:  0.859375
train loss:  0.3336504101753235
train gradient:  0.22860332475607875
iteration : 5535
train acc:  0.890625
train loss:  0.3369050621986389
train gradient:  0.18497997696823595
iteration : 5536
train acc:  0.875
train loss:  0.38150855898857117
train gradient:  0.2213591304205159
iteration : 5537
train acc:  0.8515625
train loss:  0.3202897310256958
train gradient:  0.31082717148385725
iteration : 5538
train acc:  0.8203125
train loss:  0.376290887594223
train gradient:  0.23246266310168534
iteration : 5539
train acc:  0.7890625
train loss:  0.40335166454315186
train gradient:  0.3356155609222761
iteration : 5540
train acc:  0.8515625
train loss:  0.30522745847702026
train gradient:  0.33220509236373236
iteration : 5541
train acc:  0.875
train loss:  0.2634398341178894
train gradient:  0.14225725387233235
iteration : 5542
train acc:  0.828125
train loss:  0.4072301983833313
train gradient:  0.3725878770939585
iteration : 5543
train acc:  0.78125
train loss:  0.4449115991592407
train gradient:  0.25919637480876445
iteration : 5544
train acc:  0.859375
train loss:  0.3263775408267975
train gradient:  0.3195204893207638
iteration : 5545
train acc:  0.8984375
train loss:  0.25589966773986816
train gradient:  0.3383544421799211
iteration : 5546
train acc:  0.859375
train loss:  0.3032451868057251
train gradient:  0.22222167638491963
iteration : 5547
train acc:  0.921875
train loss:  0.23570913076400757
train gradient:  0.13892439279087423
iteration : 5548
train acc:  0.8125
train loss:  0.3693382740020752
train gradient:  0.24324725426864774
iteration : 5549
train acc:  0.796875
train loss:  0.4233020544052124
train gradient:  0.49343511374181204
iteration : 5550
train acc:  0.9140625
train loss:  0.2675841450691223
train gradient:  0.13831362469754205
iteration : 5551
train acc:  0.8515625
train loss:  0.32570376992225647
train gradient:  0.20409879043675466
iteration : 5552
train acc:  0.859375
train loss:  0.32581499218940735
train gradient:  0.24694111805409094
iteration : 5553
train acc:  0.8515625
train loss:  0.357965886592865
train gradient:  0.21529140493652882
iteration : 5554
train acc:  0.859375
train loss:  0.33552980422973633
train gradient:  0.3622791644819025
iteration : 5555
train acc:  0.875
train loss:  0.2746089696884155
train gradient:  0.17261754697390916
iteration : 5556
train acc:  0.828125
train loss:  0.37069177627563477
train gradient:  0.22889353875136914
iteration : 5557
train acc:  0.8515625
train loss:  0.27780580520629883
train gradient:  0.16059192194734165
iteration : 5558
train acc:  0.8203125
train loss:  0.40698397159576416
train gradient:  0.36850123117401334
iteration : 5559
train acc:  0.875
train loss:  0.30341261625289917
train gradient:  0.2881255728071291
iteration : 5560
train acc:  0.828125
train loss:  0.40162384510040283
train gradient:  0.33805830838926665
iteration : 5561
train acc:  0.875
train loss:  0.3360576331615448
train gradient:  0.22564095655623825
iteration : 5562
train acc:  0.875
train loss:  0.31976068019866943
train gradient:  0.22035103946371254
iteration : 5563
train acc:  0.8359375
train loss:  0.39844077825546265
train gradient:  0.37088007628232117
iteration : 5564
train acc:  0.8203125
train loss:  0.3169618248939514
train gradient:  0.2131707731682812
iteration : 5565
train acc:  0.859375
train loss:  0.3768816590309143
train gradient:  0.17764307334032636
iteration : 5566
train acc:  0.7890625
train loss:  0.41755858063697815
train gradient:  0.3050911846281889
iteration : 5567
train acc:  0.8515625
train loss:  0.34626099467277527
train gradient:  0.30514570014843173
iteration : 5568
train acc:  0.8515625
train loss:  0.3368075489997864
train gradient:  0.23947232189154194
iteration : 5569
train acc:  0.8125
train loss:  0.40081655979156494
train gradient:  0.2921390537958364
iteration : 5570
train acc:  0.8515625
train loss:  0.32369571924209595
train gradient:  0.1797079220210224
iteration : 5571
train acc:  0.8515625
train loss:  0.3236866593360901
train gradient:  0.2978414366418792
iteration : 5572
train acc:  0.84375
train loss:  0.36871129274368286
train gradient:  0.2852431626338915
iteration : 5573
train acc:  0.875
train loss:  0.285650372505188
train gradient:  0.1628987850366887
iteration : 5574
train acc:  0.8515625
train loss:  0.33062052726745605
train gradient:  0.1440313862974813
iteration : 5575
train acc:  0.8671875
train loss:  0.33056068420410156
train gradient:  0.21958427482730458
iteration : 5576
train acc:  0.8125
train loss:  0.42836856842041016
train gradient:  0.2881775463065246
iteration : 5577
train acc:  0.8359375
train loss:  0.4036211669445038
train gradient:  0.32392389837074714
iteration : 5578
train acc:  0.8671875
train loss:  0.3500613868236542
train gradient:  0.2637780882161453
iteration : 5579
train acc:  0.8828125
train loss:  0.27316686511039734
train gradient:  0.1724449604230492
iteration : 5580
train acc:  0.828125
train loss:  0.3852684497833252
train gradient:  0.2528192459509978
iteration : 5581
train acc:  0.8515625
train loss:  0.3336830139160156
train gradient:  0.16943235754222824
iteration : 5582
train acc:  0.8515625
train loss:  0.2809344530105591
train gradient:  0.13793692367295726
iteration : 5583
train acc:  0.8515625
train loss:  0.32633012533187866
train gradient:  0.224135285490943
iteration : 5584
train acc:  0.8984375
train loss:  0.2757989764213562
train gradient:  0.15445323573910033
iteration : 5585
train acc:  0.828125
train loss:  0.3684001564979553
train gradient:  0.26584503664416265
iteration : 5586
train acc:  0.8671875
train loss:  0.28945887088775635
train gradient:  0.19196156951946694
iteration : 5587
train acc:  0.796875
train loss:  0.41675809025764465
train gradient:  0.3487227591332645
iteration : 5588
train acc:  0.8046875
train loss:  0.43325871229171753
train gradient:  0.421163544591805
iteration : 5589
train acc:  0.78125
train loss:  0.44083940982818604
train gradient:  0.32068916161984623
iteration : 5590
train acc:  0.796875
train loss:  0.42665404081344604
train gradient:  0.3953401428493121
iteration : 5591
train acc:  0.859375
train loss:  0.3344487249851227
train gradient:  0.2051810982928381
iteration : 5592
train acc:  0.859375
train loss:  0.34298938512802124
train gradient:  0.2416784474243623
iteration : 5593
train acc:  0.8203125
train loss:  0.4042196273803711
train gradient:  0.2543773883603473
iteration : 5594
train acc:  0.8203125
train loss:  0.42432811856269836
train gradient:  0.30188572381432777
iteration : 5595
train acc:  0.859375
train loss:  0.37481069564819336
train gradient:  0.2511334156006127
iteration : 5596
train acc:  0.890625
train loss:  0.3108929991722107
train gradient:  0.1785306040167066
iteration : 5597
train acc:  0.7890625
train loss:  0.39402300119400024
train gradient:  0.3344512933443207
iteration : 5598
train acc:  0.8828125
train loss:  0.30215680599212646
train gradient:  0.20628468714813875
iteration : 5599
train acc:  0.8359375
train loss:  0.3322104811668396
train gradient:  0.1987379992378941
iteration : 5600
train acc:  0.8828125
train loss:  0.31464678049087524
train gradient:  0.17515496875278042
iteration : 5601
train acc:  0.8828125
train loss:  0.29558610916137695
train gradient:  0.15264064125343568
iteration : 5602
train acc:  0.859375
train loss:  0.330106258392334
train gradient:  0.18700441342894858
iteration : 5603
train acc:  0.8125
train loss:  0.40231549739837646
train gradient:  0.28946754529379437
iteration : 5604
train acc:  0.796875
train loss:  0.3904004395008087
train gradient:  0.2215614509838807
iteration : 5605
train acc:  0.8671875
train loss:  0.3042141795158386
train gradient:  0.13860427492434252
iteration : 5606
train acc:  0.875
train loss:  0.3064354956150055
train gradient:  0.15470421445006224
iteration : 5607
train acc:  0.8828125
train loss:  0.3089471459388733
train gradient:  0.15780345847294552
iteration : 5608
train acc:  0.8359375
train loss:  0.33753716945648193
train gradient:  0.24740741710645542
iteration : 5609
train acc:  0.890625
train loss:  0.291930228471756
train gradient:  0.13486702023384795
iteration : 5610
train acc:  0.796875
train loss:  0.4242324233055115
train gradient:  0.36530983813531714
iteration : 5611
train acc:  0.859375
train loss:  0.3216399848461151
train gradient:  0.1927571353905987
iteration : 5612
train acc:  0.90625
train loss:  0.2802630066871643
train gradient:  0.16795819844660137
iteration : 5613
train acc:  0.84375
train loss:  0.3146006166934967
train gradient:  0.19063002481455876
iteration : 5614
train acc:  0.8125
train loss:  0.4001728594303131
train gradient:  0.25686548868878134
iteration : 5615
train acc:  0.890625
train loss:  0.3025340139865875
train gradient:  0.2542338983464603
iteration : 5616
train acc:  0.8359375
train loss:  0.3480299115180969
train gradient:  0.18791651924887817
iteration : 5617
train acc:  0.8828125
train loss:  0.289064884185791
train gradient:  0.1597472620633013
iteration : 5618
train acc:  0.8515625
train loss:  0.35049301385879517
train gradient:  0.2537047860235622
iteration : 5619
train acc:  0.8046875
train loss:  0.45237523317337036
train gradient:  0.29967901551258697
iteration : 5620
train acc:  0.8671875
train loss:  0.3428518772125244
train gradient:  0.18784737848308686
iteration : 5621
train acc:  0.859375
train loss:  0.32876306772232056
train gradient:  0.19437407204651197
iteration : 5622
train acc:  0.875
train loss:  0.28859683871269226
train gradient:  0.19477009558549316
iteration : 5623
train acc:  0.8984375
train loss:  0.29147711396217346
train gradient:  0.2146538084573147
iteration : 5624
train acc:  0.84375
train loss:  0.32593193650245667
train gradient:  0.15316216246383105
iteration : 5625
train acc:  0.8515625
train loss:  0.28370028734207153
train gradient:  0.17276289327851846
iteration : 5626
train acc:  0.875
train loss:  0.295382559299469
train gradient:  0.21439849512962048
iteration : 5627
train acc:  0.84375
train loss:  0.3616306185722351
train gradient:  0.3087078257426385
iteration : 5628
train acc:  0.8046875
train loss:  0.43497219681739807
train gradient:  0.30522476802428017
iteration : 5629
train acc:  0.84375
train loss:  0.3186696171760559
train gradient:  0.20857798077107786
iteration : 5630
train acc:  0.8671875
train loss:  0.2775943875312805
train gradient:  0.19556368552564596
iteration : 5631
train acc:  0.7890625
train loss:  0.3749396800994873
train gradient:  0.26338533124387103
iteration : 5632
train acc:  0.84375
train loss:  0.3236764669418335
train gradient:  0.18038381045237772
iteration : 5633
train acc:  0.84375
train loss:  0.33599168062210083
train gradient:  0.24480178768502886
iteration : 5634
train acc:  0.78125
train loss:  0.47619789838790894
train gradient:  0.4294212827248044
iteration : 5635
train acc:  0.90625
train loss:  0.2649153769016266
train gradient:  0.1358006324586813
iteration : 5636
train acc:  0.84375
train loss:  0.3406379818916321
train gradient:  0.23826888164269405
iteration : 5637
train acc:  0.8671875
train loss:  0.31938636302948
train gradient:  0.18203812815363127
iteration : 5638
train acc:  0.8515625
train loss:  0.3289688527584076
train gradient:  0.24654114692150253
iteration : 5639
train acc:  0.890625
train loss:  0.26398950815200806
train gradient:  0.17361858472972913
iteration : 5640
train acc:  0.859375
train loss:  0.2867710590362549
train gradient:  0.2032461191222648
iteration : 5641
train acc:  0.84375
train loss:  0.3458702564239502
train gradient:  0.2312026268032671
iteration : 5642
train acc:  0.921875
train loss:  0.25878825783729553
train gradient:  0.20892467399997522
iteration : 5643
train acc:  0.8515625
train loss:  0.3460270166397095
train gradient:  0.19342663549026362
iteration : 5644
train acc:  0.859375
train loss:  0.3250521421432495
train gradient:  0.17959506664542702
iteration : 5645
train acc:  0.921875
train loss:  0.25376400351524353
train gradient:  0.16844955622112448
iteration : 5646
train acc:  0.828125
train loss:  0.3971288800239563
train gradient:  0.30368924811942793
iteration : 5647
train acc:  0.953125
train loss:  0.24511770904064178
train gradient:  0.3158669537022474
iteration : 5648
train acc:  0.859375
train loss:  0.30769431591033936
train gradient:  0.16670277713237086
iteration : 5649
train acc:  0.859375
train loss:  0.36144134402275085
train gradient:  0.24405685133089794
iteration : 5650
train acc:  0.8203125
train loss:  0.40904176235198975
train gradient:  0.3123416248912849
iteration : 5651
train acc:  0.8828125
train loss:  0.25746607780456543
train gradient:  0.1391844266385016
iteration : 5652
train acc:  0.890625
train loss:  0.2812458872795105
train gradient:  0.23041963227953294
iteration : 5653
train acc:  0.84375
train loss:  0.31515568494796753
train gradient:  0.28574252152282714
iteration : 5654
train acc:  0.828125
train loss:  0.3553629219532013
train gradient:  0.40518792413766114
iteration : 5655
train acc:  0.859375
train loss:  0.32057541608810425
train gradient:  0.22152425130289685
iteration : 5656
train acc:  0.8359375
train loss:  0.43682682514190674
train gradient:  0.3140484014416634
iteration : 5657
train acc:  0.859375
train loss:  0.33027493953704834
train gradient:  0.1743177204513864
iteration : 5658
train acc:  0.8671875
train loss:  0.31358492374420166
train gradient:  0.23381523036775387
iteration : 5659
train acc:  0.859375
train loss:  0.2875044047832489
train gradient:  0.17124324301376115
iteration : 5660
train acc:  0.859375
train loss:  0.3538675308227539
train gradient:  0.27045968169358836
iteration : 5661
train acc:  0.7578125
train loss:  0.4538153111934662
train gradient:  0.29023584206182285
iteration : 5662
train acc:  0.84375
train loss:  0.32577693462371826
train gradient:  0.3533174654134598
iteration : 5663
train acc:  0.8515625
train loss:  0.27998805046081543
train gradient:  0.1552986115134489
iteration : 5664
train acc:  0.7578125
train loss:  0.47846919298171997
train gradient:  0.4753983021805701
iteration : 5665
train acc:  0.8515625
train loss:  0.4172423183917999
train gradient:  0.38898104222071556
iteration : 5666
train acc:  0.8671875
train loss:  0.34940046072006226
train gradient:  0.27301920915892985
iteration : 5667
train acc:  0.8515625
train loss:  0.302842378616333
train gradient:  0.1944574154325406
iteration : 5668
train acc:  0.8515625
train loss:  0.37212690711021423
train gradient:  0.36965473826417833
iteration : 5669
train acc:  0.8828125
train loss:  0.2754042148590088
train gradient:  0.25870050569819275
iteration : 5670
train acc:  0.828125
train loss:  0.3845614194869995
train gradient:  0.2738054312883286
iteration : 5671
train acc:  0.859375
train loss:  0.29656344652175903
train gradient:  0.15185756526828167
iteration : 5672
train acc:  0.828125
train loss:  0.3424113094806671
train gradient:  0.25031944983182003
iteration : 5673
train acc:  0.75
train loss:  0.5177919864654541
train gradient:  0.5207721531867848
iteration : 5674
train acc:  0.8828125
train loss:  0.2728946805000305
train gradient:  0.15343234234136577
iteration : 5675
train acc:  0.8828125
train loss:  0.2890830636024475
train gradient:  0.2505962765006074
iteration : 5676
train acc:  0.8671875
train loss:  0.34268346428871155
train gradient:  0.2728679949249827
iteration : 5677
train acc:  0.859375
train loss:  0.37233448028564453
train gradient:  0.3036415196122643
iteration : 5678
train acc:  0.8984375
train loss:  0.24628371000289917
train gradient:  0.17483187514121123
iteration : 5679
train acc:  0.875
train loss:  0.31964111328125
train gradient:  0.2779068456758295
iteration : 5680
train acc:  0.875
train loss:  0.2849580645561218
train gradient:  0.2267972753499941
iteration : 5681
train acc:  0.875
train loss:  0.3153176009654999
train gradient:  0.27251601882778864
iteration : 5682
train acc:  0.8203125
train loss:  0.3932383060455322
train gradient:  0.32782479997612146
iteration : 5683
train acc:  0.875
train loss:  0.29503124952316284
train gradient:  0.18556740122360377
iteration : 5684
train acc:  0.8671875
train loss:  0.3054315447807312
train gradient:  0.2491446588234411
iteration : 5685
train acc:  0.8203125
train loss:  0.42235249280929565
train gradient:  0.2944046212530271
iteration : 5686
train acc:  0.9140625
train loss:  0.27925756573677063
train gradient:  0.1427011655270526
iteration : 5687
train acc:  0.875
train loss:  0.3650656044483185
train gradient:  0.20923142599831213
iteration : 5688
train acc:  0.890625
train loss:  0.3482283353805542
train gradient:  0.22951834332536553
iteration : 5689
train acc:  0.8828125
train loss:  0.3315775990486145
train gradient:  0.22213516521695256
iteration : 5690
train acc:  0.8515625
train loss:  0.37637585401535034
train gradient:  0.30113245793366084
iteration : 5691
train acc:  0.875
train loss:  0.3053763508796692
train gradient:  0.2075605585315073
iteration : 5692
train acc:  0.8359375
train loss:  0.36838531494140625
train gradient:  0.4926640899184439
iteration : 5693
train acc:  0.796875
train loss:  0.4309704899787903
train gradient:  0.30427701136150764
iteration : 5694
train acc:  0.8203125
train loss:  0.38653564453125
train gradient:  0.21819881815313266
iteration : 5695
train acc:  0.8046875
train loss:  0.43391284346580505
train gradient:  0.2718524502121398
iteration : 5696
train acc:  0.890625
train loss:  0.3072422742843628
train gradient:  0.1626599097798034
iteration : 5697
train acc:  0.8828125
train loss:  0.27284306287765503
train gradient:  0.19219881742800052
iteration : 5698
train acc:  0.890625
train loss:  0.3627973794937134
train gradient:  0.2861341521295492
iteration : 5699
train acc:  0.8828125
train loss:  0.33621349930763245
train gradient:  0.23811358839609126
iteration : 5700
train acc:  0.8671875
train loss:  0.40588200092315674
train gradient:  0.37739773940832905
iteration : 5701
train acc:  0.859375
train loss:  0.30920350551605225
train gradient:  0.2308637183872464
iteration : 5702
train acc:  0.8515625
train loss:  0.35537078976631165
train gradient:  0.20834552450097443
iteration : 5703
train acc:  0.8125
train loss:  0.3589511513710022
train gradient:  0.22105879212695265
iteration : 5704
train acc:  0.890625
train loss:  0.2855607271194458
train gradient:  0.2231469592441968
iteration : 5705
train acc:  0.8515625
train loss:  0.3690563440322876
train gradient:  0.322251281872661
iteration : 5706
train acc:  0.8515625
train loss:  0.3285890221595764
train gradient:  0.2223549490346957
iteration : 5707
train acc:  0.828125
train loss:  0.36492079496383667
train gradient:  0.1930741186400814
iteration : 5708
train acc:  0.8515625
train loss:  0.3505311608314514
train gradient:  0.16255893019484285
iteration : 5709
train acc:  0.8125
train loss:  0.3394359052181244
train gradient:  0.15797887259606913
iteration : 5710
train acc:  0.84375
train loss:  0.35655349493026733
train gradient:  0.32110028653259437
iteration : 5711
train acc:  0.8984375
train loss:  0.25688090920448303
train gradient:  0.1746332103458729
iteration : 5712
train acc:  0.8515625
train loss:  0.2835034132003784
train gradient:  0.16888520864042778
iteration : 5713
train acc:  0.84375
train loss:  0.30332595109939575
train gradient:  0.22247225306622423
iteration : 5714
train acc:  0.890625
train loss:  0.3256209194660187
train gradient:  0.24556645678562083
iteration : 5715
train acc:  0.796875
train loss:  0.43404489755630493
train gradient:  0.3561970216882969
iteration : 5716
train acc:  0.8515625
train loss:  0.30661043524742126
train gradient:  0.1755894497365734
iteration : 5717
train acc:  0.8125
train loss:  0.3838920593261719
train gradient:  0.26389971619945807
iteration : 5718
train acc:  0.8203125
train loss:  0.3291699290275574
train gradient:  0.2639458293280819
iteration : 5719
train acc:  0.8046875
train loss:  0.37797877192497253
train gradient:  0.22895070170882584
iteration : 5720
train acc:  0.859375
train loss:  0.33554840087890625
train gradient:  0.1859905461039204
iteration : 5721
train acc:  0.84375
train loss:  0.38117027282714844
train gradient:  0.20971869043222502
iteration : 5722
train acc:  0.8671875
train loss:  0.34117305278778076
train gradient:  0.20996175833387826
iteration : 5723
train acc:  0.8671875
train loss:  0.3205675482749939
train gradient:  0.23170355320849245
iteration : 5724
train acc:  0.8046875
train loss:  0.40321969985961914
train gradient:  0.2511043360624934
iteration : 5725
train acc:  0.875
train loss:  0.3041754961013794
train gradient:  0.2941928238861477
iteration : 5726
train acc:  0.8671875
train loss:  0.31553563475608826
train gradient:  0.2992023957496851
iteration : 5727
train acc:  0.890625
train loss:  0.2702726423740387
train gradient:  0.15621289185305418
iteration : 5728
train acc:  0.84375
train loss:  0.3549934923648834
train gradient:  0.18922157180592009
iteration : 5729
train acc:  0.8671875
train loss:  0.357056200504303
train gradient:  0.22753653155965714
iteration : 5730
train acc:  0.828125
train loss:  0.35872960090637207
train gradient:  0.2737625831615161
iteration : 5731
train acc:  0.8828125
train loss:  0.27199992537498474
train gradient:  0.19319878878006894
iteration : 5732
train acc:  0.8671875
train loss:  0.3369365930557251
train gradient:  0.171115446180282
iteration : 5733
train acc:  0.8671875
train loss:  0.34933656454086304
train gradient:  0.3329370201813718
iteration : 5734
train acc:  0.9140625
train loss:  0.2534400224685669
train gradient:  0.22001579009298747
iteration : 5735
train acc:  0.84375
train loss:  0.3663397431373596
train gradient:  0.23846800175839694
iteration : 5736
train acc:  0.8515625
train loss:  0.3373088836669922
train gradient:  0.1971357031500099
iteration : 5737
train acc:  0.8125
train loss:  0.388317734003067
train gradient:  0.28234267950426906
iteration : 5738
train acc:  0.84375
train loss:  0.31051498651504517
train gradient:  0.20317979385302282
iteration : 5739
train acc:  0.8046875
train loss:  0.37036368250846863
train gradient:  0.3939378870476153
iteration : 5740
train acc:  0.8359375
train loss:  0.40774452686309814
train gradient:  0.2294703413822874
iteration : 5741
train acc:  0.8515625
train loss:  0.34723660349845886
train gradient:  0.4341458455497718
iteration : 5742
train acc:  0.8046875
train loss:  0.4400273561477661
train gradient:  0.5447137461072405
iteration : 5743
train acc:  0.8671875
train loss:  0.37571752071380615
train gradient:  0.40630019458782524
iteration : 5744
train acc:  0.8671875
train loss:  0.30680620670318604
train gradient:  0.2722918283574468
iteration : 5745
train acc:  0.859375
train loss:  0.32037103176116943
train gradient:  0.23049673099218135
iteration : 5746
train acc:  0.7734375
train loss:  0.5255067348480225
train gradient:  0.4336009947841184
iteration : 5747
train acc:  0.8203125
train loss:  0.4026859402656555
train gradient:  0.22633249874193379
iteration : 5748
train acc:  0.8359375
train loss:  0.3985576629638672
train gradient:  0.3717149116897899
iteration : 5749
train acc:  0.890625
train loss:  0.3005249500274658
train gradient:  0.2151415838876261
iteration : 5750
train acc:  0.8984375
train loss:  0.27861207723617554
train gradient:  0.14751593283916736
iteration : 5751
train acc:  0.796875
train loss:  0.3882913589477539
train gradient:  0.3051445854186801
iteration : 5752
train acc:  0.875
train loss:  0.33359578251838684
train gradient:  0.15487890287760822
iteration : 5753
train acc:  0.8203125
train loss:  0.322009801864624
train gradient:  0.16281649999050324
iteration : 5754
train acc:  0.8515625
train loss:  0.3441597521305084
train gradient:  0.19839184936234322
iteration : 5755
train acc:  0.921875
train loss:  0.24167120456695557
train gradient:  0.15185562909317718
iteration : 5756
train acc:  0.84375
train loss:  0.3419642448425293
train gradient:  0.24601316312865418
iteration : 5757
train acc:  0.796875
train loss:  0.49311304092407227
train gradient:  0.4027834596068649
iteration : 5758
train acc:  0.8671875
train loss:  0.36149924993515015
train gradient:  0.23510706467002782
iteration : 5759
train acc:  0.875
train loss:  0.319032222032547
train gradient:  0.1900447292577649
iteration : 5760
train acc:  0.8671875
train loss:  0.3177793025970459
train gradient:  0.18554759100953705
iteration : 5761
train acc:  0.78125
train loss:  0.40851667523384094
train gradient:  0.2909002803698417
iteration : 5762
train acc:  0.8359375
train loss:  0.40107348561286926
train gradient:  0.26803481078243196
iteration : 5763
train acc:  0.84375
train loss:  0.3204054832458496
train gradient:  0.1730157209958012
iteration : 5764
train acc:  0.8515625
train loss:  0.3335596024990082
train gradient:  0.28974158546781575
iteration : 5765
train acc:  0.796875
train loss:  0.43035194277763367
train gradient:  0.3243950150486855
iteration : 5766
train acc:  0.8046875
train loss:  0.39242151379585266
train gradient:  0.262974922509973
iteration : 5767
train acc:  0.828125
train loss:  0.3532094955444336
train gradient:  0.16330306491815028
iteration : 5768
train acc:  0.8125
train loss:  0.3324648141860962
train gradient:  0.18708253134876204
iteration : 5769
train acc:  0.84375
train loss:  0.32475852966308594
train gradient:  0.20337038127043217
iteration : 5770
train acc:  0.8515625
train loss:  0.3540019690990448
train gradient:  0.32321506783899734
iteration : 5771
train acc:  0.8515625
train loss:  0.3337903618812561
train gradient:  0.17945896874481393
iteration : 5772
train acc:  0.8046875
train loss:  0.3860527276992798
train gradient:  0.21767025011130764
iteration : 5773
train acc:  0.8125
train loss:  0.33881789445877075
train gradient:  0.19622227274549414
iteration : 5774
train acc:  0.8984375
train loss:  0.23474079370498657
train gradient:  0.2043565633706996
iteration : 5775
train acc:  0.9296875
train loss:  0.23891903460025787
train gradient:  0.09839454652729106
iteration : 5776
train acc:  0.8046875
train loss:  0.37681376934051514
train gradient:  0.3554812516749151
iteration : 5777
train acc:  0.875
train loss:  0.325228214263916
train gradient:  0.2856259803066374
iteration : 5778
train acc:  0.8515625
train loss:  0.383132666349411
train gradient:  0.22099930418608157
iteration : 5779
train acc:  0.8203125
train loss:  0.44239944219589233
train gradient:  0.3649304516127779
iteration : 5780
train acc:  0.84375
train loss:  0.36136138439178467
train gradient:  0.3034742586568217
iteration : 5781
train acc:  0.84375
train loss:  0.34363389015197754
train gradient:  0.17973769563529782
iteration : 5782
train acc:  0.8203125
train loss:  0.33947956562042236
train gradient:  0.17878855053812598
iteration : 5783
train acc:  0.859375
train loss:  0.39907336235046387
train gradient:  0.268395882522329
iteration : 5784
train acc:  0.875
train loss:  0.3487151265144348
train gradient:  0.17562981023689578
iteration : 5785
train acc:  0.8515625
train loss:  0.42139023542404175
train gradient:  0.3296392637338835
iteration : 5786
train acc:  0.9140625
train loss:  0.257314532995224
train gradient:  0.15954270608980664
iteration : 5787
train acc:  0.84375
train loss:  0.3543489873409271
train gradient:  0.20848674051804708
iteration : 5788
train acc:  0.8359375
train loss:  0.3340926766395569
train gradient:  0.1777262348199411
iteration : 5789
train acc:  0.84375
train loss:  0.33878305554389954
train gradient:  0.22559305065832908
iteration : 5790
train acc:  0.7265625
train loss:  0.5269003510475159
train gradient:  0.4468716627434222
iteration : 5791
train acc:  0.8359375
train loss:  0.3612495958805084
train gradient:  0.20270492676099333
iteration : 5792
train acc:  0.828125
train loss:  0.44756728410720825
train gradient:  0.31351162361140095
iteration : 5793
train acc:  0.8125
train loss:  0.4358072578907013
train gradient:  0.3390973664466582
iteration : 5794
train acc:  0.859375
train loss:  0.33533138036727905
train gradient:  0.21702845351439753
iteration : 5795
train acc:  0.8125
train loss:  0.43457165360450745
train gradient:  0.2680315200274174
iteration : 5796
train acc:  0.8203125
train loss:  0.42314356565475464
train gradient:  0.30559276239168376
iteration : 5797
train acc:  0.828125
train loss:  0.3438194990158081
train gradient:  0.21055306131963347
iteration : 5798
train acc:  0.8828125
train loss:  0.28110432624816895
train gradient:  0.12437804891844642
iteration : 5799
train acc:  0.78125
train loss:  0.4309462010860443
train gradient:  0.3841199006569679
iteration : 5800
train acc:  0.8828125
train loss:  0.3357662558555603
train gradient:  0.3163387774916701
iteration : 5801
train acc:  0.84375
train loss:  0.3929222822189331
train gradient:  0.30254612299682604
iteration : 5802
train acc:  0.828125
train loss:  0.3291288912296295
train gradient:  0.2600145923301836
iteration : 5803
train acc:  0.8828125
train loss:  0.27142757177352905
train gradient:  0.17064733070448476
iteration : 5804
train acc:  0.859375
train loss:  0.3015827536582947
train gradient:  0.22565543293707485
iteration : 5805
train acc:  0.8359375
train loss:  0.3708752989768982
train gradient:  0.18712586485786803
iteration : 5806
train acc:  0.7890625
train loss:  0.42580318450927734
train gradient:  0.3409753138223555
iteration : 5807
train acc:  0.921875
train loss:  0.28266599774360657
train gradient:  0.1614119846329069
iteration : 5808
train acc:  0.828125
train loss:  0.38704565167427063
train gradient:  0.28603151037061675
iteration : 5809
train acc:  0.859375
train loss:  0.3246085047721863
train gradient:  0.2540325896023238
iteration : 5810
train acc:  0.8125
train loss:  0.4097837507724762
train gradient:  0.2285156537747713
iteration : 5811
train acc:  0.828125
train loss:  0.4174986481666565
train gradient:  0.23597590749755615
iteration : 5812
train acc:  0.859375
train loss:  0.35142943263053894
train gradient:  0.23043423566190385
iteration : 5813
train acc:  0.8046875
train loss:  0.3995009660720825
train gradient:  0.2248080211783059
iteration : 5814
train acc:  0.8515625
train loss:  0.35342246294021606
train gradient:  0.3371721192346675
iteration : 5815
train acc:  0.8359375
train loss:  0.4085215628147125
train gradient:  0.2757056148328267
iteration : 5816
train acc:  0.84375
train loss:  0.32711124420166016
train gradient:  0.26609409257817873
iteration : 5817
train acc:  0.875
train loss:  0.34948545694351196
train gradient:  0.18988687982864222
iteration : 5818
train acc:  0.84375
train loss:  0.36854472756385803
train gradient:  0.2544153732598715
iteration : 5819
train acc:  0.9140625
train loss:  0.29010123014450073
train gradient:  0.15354924682207513
iteration : 5820
train acc:  0.765625
train loss:  0.4618889391422272
train gradient:  0.23747136607475505
iteration : 5821
train acc:  0.859375
train loss:  0.3660092353820801
train gradient:  0.2203427257531021
iteration : 5822
train acc:  0.78125
train loss:  0.5086296796798706
train gradient:  0.3979059776157433
iteration : 5823
train acc:  0.8359375
train loss:  0.32827672362327576
train gradient:  0.20257364788210025
iteration : 5824
train acc:  0.8125
train loss:  0.4241645336151123
train gradient:  0.35936008483966647
iteration : 5825
train acc:  0.8671875
train loss:  0.3202163279056549
train gradient:  0.15097782511071678
iteration : 5826
train acc:  0.796875
train loss:  0.3813527524471283
train gradient:  0.30637387249086073
iteration : 5827
train acc:  0.8046875
train loss:  0.369851678609848
train gradient:  0.234409764374323
iteration : 5828
train acc:  0.8046875
train loss:  0.3577284514904022
train gradient:  0.1752101265724453
iteration : 5829
train acc:  0.859375
train loss:  0.3143461048603058
train gradient:  0.18581458477176613
iteration : 5830
train acc:  0.8671875
train loss:  0.37135908007621765
train gradient:  0.22582419577006596
iteration : 5831
train acc:  0.859375
train loss:  0.30090147256851196
train gradient:  0.13362823099195398
iteration : 5832
train acc:  0.8203125
train loss:  0.38803401589393616
train gradient:  0.2459188429476365
iteration : 5833
train acc:  0.8203125
train loss:  0.407766729593277
train gradient:  0.2510601077187895
iteration : 5834
train acc:  0.84375
train loss:  0.36370328068733215
train gradient:  0.2822208364109192
iteration : 5835
train acc:  0.890625
train loss:  0.305438756942749
train gradient:  0.1470110286927438
iteration : 5836
train acc:  0.8828125
train loss:  0.27525830268859863
train gradient:  0.22839934491008657
iteration : 5837
train acc:  0.8984375
train loss:  0.26909512281417847
train gradient:  0.13397429040781475
iteration : 5838
train acc:  0.8515625
train loss:  0.31170332431793213
train gradient:  0.1942824838577334
iteration : 5839
train acc:  0.78125
train loss:  0.40750637650489807
train gradient:  0.31440380356750847
iteration : 5840
train acc:  0.8046875
train loss:  0.39843595027923584
train gradient:  0.2733051163472871
iteration : 5841
train acc:  0.84375
train loss:  0.29342788457870483
train gradient:  0.1620694480708204
iteration : 5842
train acc:  0.8203125
train loss:  0.3809034526348114
train gradient:  0.31273757200617713
iteration : 5843
train acc:  0.8671875
train loss:  0.34925687313079834
train gradient:  0.15953135024807122
iteration : 5844
train acc:  0.796875
train loss:  0.39854681491851807
train gradient:  0.3239848363493681
iteration : 5845
train acc:  0.875
train loss:  0.28855180740356445
train gradient:  0.2222556740929529
iteration : 5846
train acc:  0.8359375
train loss:  0.4029318690299988
train gradient:  0.26518747826590494
iteration : 5847
train acc:  0.8984375
train loss:  0.26279979944229126
train gradient:  0.15386473935460093
iteration : 5848
train acc:  0.828125
train loss:  0.35425588488578796
train gradient:  0.20717967122406772
iteration : 5849
train acc:  0.8671875
train loss:  0.30446505546569824
train gradient:  0.2505969489104106
iteration : 5850
train acc:  0.84375
train loss:  0.36792686581611633
train gradient:  0.2485330192155727
iteration : 5851
train acc:  0.78125
train loss:  0.39939916133880615
train gradient:  0.2571587397221299
iteration : 5852
train acc:  0.8046875
train loss:  0.41206058859825134
train gradient:  0.2652779742400839
iteration : 5853
train acc:  0.84375
train loss:  0.36442819237709045
train gradient:  0.19709366746233753
iteration : 5854
train acc:  0.8203125
train loss:  0.37513744831085205
train gradient:  0.22939098080288917
iteration : 5855
train acc:  0.8125
train loss:  0.37377870082855225
train gradient:  0.22702380953961898
iteration : 5856
train acc:  0.8515625
train loss:  0.38321226835250854
train gradient:  0.20560445052364956
iteration : 5857
train acc:  0.875
train loss:  0.31230080127716064
train gradient:  0.23431075834469292
iteration : 5858
train acc:  0.8515625
train loss:  0.3716544806957245
train gradient:  0.2608195643954558
iteration : 5859
train acc:  0.8515625
train loss:  0.37393081188201904
train gradient:  0.2811932491001245
iteration : 5860
train acc:  0.7890625
train loss:  0.47807610034942627
train gradient:  0.3298799435151334
iteration : 5861
train acc:  0.890625
train loss:  0.3357906937599182
train gradient:  0.13836379714196495
iteration : 5862
train acc:  0.921875
train loss:  0.239431232213974
train gradient:  0.12981158742702786
iteration : 5863
train acc:  0.84375
train loss:  0.3621065616607666
train gradient:  0.20587827677848805
iteration : 5864
train acc:  0.8984375
train loss:  0.24939227104187012
train gradient:  0.15467762474040386
iteration : 5865
train acc:  0.890625
train loss:  0.2508593797683716
train gradient:  0.13063918812686226
iteration : 5866
train acc:  0.84375
train loss:  0.4050024747848511
train gradient:  0.2784376772949425
iteration : 5867
train acc:  0.8359375
train loss:  0.4578202962875366
train gradient:  0.29623024383576757
iteration : 5868
train acc:  0.859375
train loss:  0.34118586778640747
train gradient:  0.26372694834098104
iteration : 5869
train acc:  0.8203125
train loss:  0.38688501715660095
train gradient:  0.2568316257818707
iteration : 5870
train acc:  0.84375
train loss:  0.367920845746994
train gradient:  0.2562325820014448
iteration : 5871
train acc:  0.796875
train loss:  0.4781211018562317
train gradient:  0.41869451248858225
iteration : 5872
train acc:  0.8359375
train loss:  0.34105342626571655
train gradient:  0.2027520927684538
iteration : 5873
train acc:  0.84375
train loss:  0.3635479509830475
train gradient:  0.18806423864549937
iteration : 5874
train acc:  0.828125
train loss:  0.3805878162384033
train gradient:  0.2308863499714413
iteration : 5875
train acc:  0.8046875
train loss:  0.38827750086784363
train gradient:  0.26366787338954956
iteration : 5876
train acc:  0.8203125
train loss:  0.38995152711868286
train gradient:  0.2213733687013481
iteration : 5877
train acc:  0.8671875
train loss:  0.33709657192230225
train gradient:  0.3038973800684588
iteration : 5878
train acc:  0.8828125
train loss:  0.32009008526802063
train gradient:  0.18265440068416045
iteration : 5879
train acc:  0.890625
train loss:  0.3203824758529663
train gradient:  0.1477357354207351
iteration : 5880
train acc:  0.8671875
train loss:  0.3108890950679779
train gradient:  0.17153434444029306
iteration : 5881
train acc:  0.828125
train loss:  0.3463417887687683
train gradient:  0.18919206184730675
iteration : 5882
train acc:  0.859375
train loss:  0.35236650705337524
train gradient:  0.19000961912079703
iteration : 5883
train acc:  0.8671875
train loss:  0.37622731924057007
train gradient:  0.2451985382426125
iteration : 5884
train acc:  0.8359375
train loss:  0.3941890299320221
train gradient:  0.25221234413074667
iteration : 5885
train acc:  0.859375
train loss:  0.3060414791107178
train gradient:  0.20344943244393676
iteration : 5886
train acc:  0.8203125
train loss:  0.40045106410980225
train gradient:  0.2188973695394058
iteration : 5887
train acc:  0.8828125
train loss:  0.3184601664543152
train gradient:  0.1544089258473532
iteration : 5888
train acc:  0.8671875
train loss:  0.3051619529724121
train gradient:  0.173858121189001
iteration : 5889
train acc:  0.90625
train loss:  0.2409612536430359
train gradient:  0.12293191549989892
iteration : 5890
train acc:  0.8515625
train loss:  0.3788391351699829
train gradient:  0.23649462997826104
iteration : 5891
train acc:  0.8984375
train loss:  0.31871283054351807
train gradient:  0.17053147224140996
iteration : 5892
train acc:  0.84375
train loss:  0.3326842188835144
train gradient:  0.2108060002786527
iteration : 5893
train acc:  0.84375
train loss:  0.4064876139163971
train gradient:  0.36196816715627
iteration : 5894
train acc:  0.8359375
train loss:  0.3805885314941406
train gradient:  0.2223536680292948
iteration : 5895
train acc:  0.828125
train loss:  0.3361614942550659
train gradient:  0.2458092558069216
iteration : 5896
train acc:  0.890625
train loss:  0.2850005030632019
train gradient:  0.12821526308795245
iteration : 5897
train acc:  0.8515625
train loss:  0.337710440158844
train gradient:  0.3584235624780543
iteration : 5898
train acc:  0.828125
train loss:  0.3902987241744995
train gradient:  0.16840530323886324
iteration : 5899
train acc:  0.84375
train loss:  0.35704872012138367
train gradient:  0.17797690890811024
iteration : 5900
train acc:  0.8359375
train loss:  0.35044246912002563
train gradient:  0.1970686314416076
iteration : 5901
train acc:  0.796875
train loss:  0.3847505450248718
train gradient:  0.21909060680354586
iteration : 5902
train acc:  0.9375
train loss:  0.26246535778045654
train gradient:  0.1339961464174213
iteration : 5903
train acc:  0.890625
train loss:  0.30965656042099
train gradient:  0.17215716466551062
iteration : 5904
train acc:  0.84375
train loss:  0.316085547208786
train gradient:  0.18558233726411189
iteration : 5905
train acc:  0.8671875
train loss:  0.2896803319454193
train gradient:  0.13194216884375753
iteration : 5906
train acc:  0.8125
train loss:  0.34926217794418335
train gradient:  0.25763942235919823
iteration : 5907
train acc:  0.828125
train loss:  0.42069917917251587
train gradient:  0.2588347213900819
iteration : 5908
train acc:  0.9140625
train loss:  0.32027047872543335
train gradient:  0.13070469206153304
iteration : 5909
train acc:  0.8359375
train loss:  0.358676016330719
train gradient:  0.1821852551572822
iteration : 5910
train acc:  0.859375
train loss:  0.3043423891067505
train gradient:  0.19377552244555607
iteration : 5911
train acc:  0.828125
train loss:  0.36933326721191406
train gradient:  0.23923638379378376
iteration : 5912
train acc:  0.890625
train loss:  0.3173403739929199
train gradient:  0.13253159804922265
iteration : 5913
train acc:  0.859375
train loss:  0.30942803621292114
train gradient:  0.1307421740960252
iteration : 5914
train acc:  0.7734375
train loss:  0.4837307929992676
train gradient:  0.39439674762377813
iteration : 5915
train acc:  0.8515625
train loss:  0.3406488299369812
train gradient:  0.21429588923109746
iteration : 5916
train acc:  0.78125
train loss:  0.40709441900253296
train gradient:  0.2732011383712576
iteration : 5917
train acc:  0.8359375
train loss:  0.3336048126220703
train gradient:  0.1799360472446304
iteration : 5918
train acc:  0.859375
train loss:  0.35082411766052246
train gradient:  0.22954350376801463
iteration : 5919
train acc:  0.84375
train loss:  0.3649282455444336
train gradient:  0.19260624348045274
iteration : 5920
train acc:  0.859375
train loss:  0.364357054233551
train gradient:  0.24481225818020258
iteration : 5921
train acc:  0.828125
train loss:  0.3272467851638794
train gradient:  0.21040473445167554
iteration : 5922
train acc:  0.8203125
train loss:  0.4456862211227417
train gradient:  0.36699909777502643
iteration : 5923
train acc:  0.859375
train loss:  0.35406699776649475
train gradient:  0.16954947479227966
iteration : 5924
train acc:  0.8203125
train loss:  0.3713168799877167
train gradient:  0.20598931139344187
iteration : 5925
train acc:  0.84375
train loss:  0.33215510845184326
train gradient:  0.2088770803011364
iteration : 5926
train acc:  0.859375
train loss:  0.33212029933929443
train gradient:  0.20785795182211486
iteration : 5927
train acc:  0.8046875
train loss:  0.3843404948711395
train gradient:  0.20591322531027187
iteration : 5928
train acc:  0.828125
train loss:  0.36433741450309753
train gradient:  0.2304779780484452
iteration : 5929
train acc:  0.875
train loss:  0.28780102729797363
train gradient:  0.1839017932427343
iteration : 5930
train acc:  0.84375
train loss:  0.35179129242897034
train gradient:  0.2857034071092872
iteration : 5931
train acc:  0.84375
train loss:  0.3628586530685425
train gradient:  0.19622326523158284
iteration : 5932
train acc:  0.859375
train loss:  0.33877143263816833
train gradient:  0.1926260308985109
iteration : 5933
train acc:  0.8125
train loss:  0.36739301681518555
train gradient:  0.2794534994371382
iteration : 5934
train acc:  0.8828125
train loss:  0.37140554189682007
train gradient:  0.1599571630004258
iteration : 5935
train acc:  0.8828125
train loss:  0.3049294352531433
train gradient:  0.1564844073925251
iteration : 5936
train acc:  0.875
train loss:  0.32005560398101807
train gradient:  0.172000261904294
iteration : 5937
train acc:  0.859375
train loss:  0.3146688938140869
train gradient:  0.20850022181229116
iteration : 5938
train acc:  0.84375
train loss:  0.38848742842674255
train gradient:  0.2855863603720854
iteration : 5939
train acc:  0.78125
train loss:  0.4885594844818115
train gradient:  0.43094634133930376
iteration : 5940
train acc:  0.84375
train loss:  0.3053058981895447
train gradient:  0.17771629697845803
iteration : 5941
train acc:  0.84375
train loss:  0.3643682599067688
train gradient:  0.2098796339125964
iteration : 5942
train acc:  0.875
train loss:  0.30399519205093384
train gradient:  0.1841046684512481
iteration : 5943
train acc:  0.8203125
train loss:  0.4503880441188812
train gradient:  0.3264091959711212
iteration : 5944
train acc:  0.8828125
train loss:  0.3527477979660034
train gradient:  0.19014507891357688
iteration : 5945
train acc:  0.828125
train loss:  0.37562429904937744
train gradient:  0.2400525048215464
iteration : 5946
train acc:  0.8359375
train loss:  0.3535882234573364
train gradient:  0.19174859007924533
iteration : 5947
train acc:  0.828125
train loss:  0.3553785979747772
train gradient:  0.17312057459106994
iteration : 5948
train acc:  0.828125
train loss:  0.31800490617752075
train gradient:  0.2529708485547734
iteration : 5949
train acc:  0.765625
train loss:  0.46539026498794556
train gradient:  0.5068765393128878
iteration : 5950
train acc:  0.8828125
train loss:  0.2833004891872406
train gradient:  0.16182034955562008
iteration : 5951
train acc:  0.859375
train loss:  0.3351937234401703
train gradient:  0.1946266762877752
iteration : 5952
train acc:  0.8125
train loss:  0.45202016830444336
train gradient:  0.3298811853793679
iteration : 5953
train acc:  0.8515625
train loss:  0.33317095041275024
train gradient:  0.14461843721829334
iteration : 5954
train acc:  0.78125
train loss:  0.4630301892757416
train gradient:  0.4040352917154158
iteration : 5955
train acc:  0.8359375
train loss:  0.37369686365127563
train gradient:  0.17277013285431114
iteration : 5956
train acc:  0.8515625
train loss:  0.3381257653236389
train gradient:  0.1823766749006271
iteration : 5957
train acc:  0.84375
train loss:  0.3321077823638916
train gradient:  0.1878727834748926
iteration : 5958
train acc:  0.890625
train loss:  0.2696077227592468
train gradient:  0.11571771917448888
iteration : 5959
train acc:  0.875
train loss:  0.33714592456817627
train gradient:  0.21423518463563035
iteration : 5960
train acc:  0.8125
train loss:  0.3705557584762573
train gradient:  0.16581628003037802
iteration : 5961
train acc:  0.859375
train loss:  0.30102360248565674
train gradient:  0.12671021984260292
iteration : 5962
train acc:  0.8671875
train loss:  0.3377535343170166
train gradient:  0.21477196794679654
iteration : 5963
train acc:  0.8671875
train loss:  0.3779219388961792
train gradient:  0.2539327718555884
iteration : 5964
train acc:  0.8671875
train loss:  0.32778334617614746
train gradient:  0.14824052726858916
iteration : 5965
train acc:  0.8203125
train loss:  0.37492650747299194
train gradient:  0.2773766187462043
iteration : 5966
train acc:  0.8828125
train loss:  0.33146023750305176
train gradient:  0.1247309968786483
iteration : 5967
train acc:  0.859375
train loss:  0.27035102248191833
train gradient:  0.16273774308288064
iteration : 5968
train acc:  0.8515625
train loss:  0.3833324909210205
train gradient:  0.2765768555701767
iteration : 5969
train acc:  0.8359375
train loss:  0.3737879991531372
train gradient:  0.18902234494637174
iteration : 5970
train acc:  0.8671875
train loss:  0.3066421151161194
train gradient:  0.1627429665284036
iteration : 5971
train acc:  0.8828125
train loss:  0.29384446144104004
train gradient:  0.16284503652756324
iteration : 5972
train acc:  0.8984375
train loss:  0.28334343433380127
train gradient:  0.1425359164757038
iteration : 5973
train acc:  0.859375
train loss:  0.32879191637039185
train gradient:  0.19338202881120342
iteration : 5974
train acc:  0.8203125
train loss:  0.4021035432815552
train gradient:  0.23495857764944347
iteration : 5975
train acc:  0.828125
train loss:  0.4452883005142212
train gradient:  0.38407728194961943
iteration : 5976
train acc:  0.8671875
train loss:  0.3270000219345093
train gradient:  0.15764365704386027
iteration : 5977
train acc:  0.90625
train loss:  0.2567110061645508
train gradient:  0.1177040825156103
iteration : 5978
train acc:  0.890625
train loss:  0.29741957783699036
train gradient:  0.13522876150565394
iteration : 5979
train acc:  0.8828125
train loss:  0.34592559933662415
train gradient:  0.178654478572058
iteration : 5980
train acc:  0.8359375
train loss:  0.4205018877983093
train gradient:  0.35946089092162065
iteration : 5981
train acc:  0.8828125
train loss:  0.29899322986602783
train gradient:  0.1695764725328457
iteration : 5982
train acc:  0.8984375
train loss:  0.2851848900318146
train gradient:  0.14513868561371987
iteration : 5983
train acc:  0.8125
train loss:  0.4095850884914398
train gradient:  0.332930540291122
iteration : 5984
train acc:  0.796875
train loss:  0.3669638931751251
train gradient:  0.20138860549781204
iteration : 5985
train acc:  0.90625
train loss:  0.28022170066833496
train gradient:  0.1895105970122627
iteration : 5986
train acc:  0.8203125
train loss:  0.3630806505680084
train gradient:  0.27059496613652434
iteration : 5987
train acc:  0.8203125
train loss:  0.3751262426376343
train gradient:  0.21780498419779
iteration : 5988
train acc:  0.8671875
train loss:  0.30809542536735535
train gradient:  0.13273915683387474
iteration : 5989
train acc:  0.859375
train loss:  0.2919136881828308
train gradient:  0.18547364021132975
iteration : 5990
train acc:  0.8125
train loss:  0.4825090765953064
train gradient:  0.3985271719123408
iteration : 5991
train acc:  0.8359375
train loss:  0.37868183851242065
train gradient:  0.29994922049737266
iteration : 5992
train acc:  0.8515625
train loss:  0.34171634912490845
train gradient:  0.19604584858734408
iteration : 5993
train acc:  0.8515625
train loss:  0.3350808620452881
train gradient:  0.21628173171429532
iteration : 5994
train acc:  0.8984375
train loss:  0.31298115849494934
train gradient:  0.17025825486472906
iteration : 5995
train acc:  0.8203125
train loss:  0.3525698781013489
train gradient:  0.302185285525117
iteration : 5996
train acc:  0.8671875
train loss:  0.3472819924354553
train gradient:  0.19079257663284577
iteration : 5997
train acc:  0.8203125
train loss:  0.3775686025619507
train gradient:  0.2629564157039197
iteration : 5998
train acc:  0.7578125
train loss:  0.48189279437065125
train gradient:  0.33591256341145515
iteration : 5999
train acc:  0.8828125
train loss:  0.3448501527309418
train gradient:  0.20608501512783708
iteration : 6000
train acc:  0.7890625
train loss:  0.4008020758628845
train gradient:  0.21762675368915604
iteration : 6001
train acc:  0.9140625
train loss:  0.23812490701675415
train gradient:  0.12507748709892105
iteration : 6002
train acc:  0.84375
train loss:  0.331447035074234
train gradient:  0.18388058908836025
iteration : 6003
train acc:  0.875
train loss:  0.3407056927680969
train gradient:  0.18895350953975965
iteration : 6004
train acc:  0.8046875
train loss:  0.34933269023895264
train gradient:  0.19050487444489012
iteration : 6005
train acc:  0.828125
train loss:  0.3319751024246216
train gradient:  0.1723837745488211
iteration : 6006
train acc:  0.875
train loss:  0.32401445508003235
train gradient:  0.1879595242705011
iteration : 6007
train acc:  0.8203125
train loss:  0.4029512405395508
train gradient:  0.26190662130184916
iteration : 6008
train acc:  0.8046875
train loss:  0.3886837363243103
train gradient:  0.24550219957496913
iteration : 6009
train acc:  0.796875
train loss:  0.3893033266067505
train gradient:  0.28756268787756584
iteration : 6010
train acc:  0.890625
train loss:  0.26742732524871826
train gradient:  0.18496684065154734
iteration : 6011
train acc:  0.859375
train loss:  0.33370494842529297
train gradient:  0.22514280176398507
iteration : 6012
train acc:  0.8671875
train loss:  0.3521658182144165
train gradient:  0.2806940162157469
iteration : 6013
train acc:  0.84375
train loss:  0.3962590992450714
train gradient:  0.2848010464714095
iteration : 6014
train acc:  0.875
train loss:  0.3164759874343872
train gradient:  0.19417095432800782
iteration : 6015
train acc:  0.859375
train loss:  0.33812156319618225
train gradient:  0.2023820655637684
iteration : 6016
train acc:  0.828125
train loss:  0.390245258808136
train gradient:  0.31348551172072614
iteration : 6017
train acc:  0.8515625
train loss:  0.39077454805374146
train gradient:  0.27514393723780506
iteration : 6018
train acc:  0.8359375
train loss:  0.32972654700279236
train gradient:  0.17678212311038738
iteration : 6019
train acc:  0.84375
train loss:  0.3481767773628235
train gradient:  0.21659158041263354
iteration : 6020
train acc:  0.8515625
train loss:  0.37216684222221375
train gradient:  0.23903607829484594
iteration : 6021
train acc:  0.796875
train loss:  0.44105398654937744
train gradient:  0.30615761531952385
iteration : 6022
train acc:  0.8046875
train loss:  0.3545866012573242
train gradient:  0.372236392659253
iteration : 6023
train acc:  0.8359375
train loss:  0.32603079080581665
train gradient:  0.15759290613700236
iteration : 6024
train acc:  0.8046875
train loss:  0.4147706925868988
train gradient:  0.33808923745738856
iteration : 6025
train acc:  0.84375
train loss:  0.2998199462890625
train gradient:  0.13542177778607797
iteration : 6026
train acc:  0.8671875
train loss:  0.3168790340423584
train gradient:  0.15060282321919835
iteration : 6027
train acc:  0.828125
train loss:  0.3343912363052368
train gradient:  0.18273754072508042
iteration : 6028
train acc:  0.8671875
train loss:  0.3395117521286011
train gradient:  0.18218307601756628
iteration : 6029
train acc:  0.8515625
train loss:  0.3405311405658722
train gradient:  0.26634113663974124
iteration : 6030
train acc:  0.8125
train loss:  0.3347979187965393
train gradient:  0.1734433903994338
iteration : 6031
train acc:  0.84375
train loss:  0.3260497748851776
train gradient:  0.16124801297770835
iteration : 6032
train acc:  0.8125
train loss:  0.3689291775226593
train gradient:  0.3672353811407836
iteration : 6033
train acc:  0.8671875
train loss:  0.27972155809402466
train gradient:  0.19286399268341559
iteration : 6034
train acc:  0.84375
train loss:  0.3326107859611511
train gradient:  0.20268595914991316
iteration : 6035
train acc:  0.875
train loss:  0.28381702303886414
train gradient:  0.11629535789625206
iteration : 6036
train acc:  0.8203125
train loss:  0.41107845306396484
train gradient:  0.35430925375420746
iteration : 6037
train acc:  0.8203125
train loss:  0.35786592960357666
train gradient:  0.20085802347358822
iteration : 6038
train acc:  0.859375
train loss:  0.33802056312561035
train gradient:  0.22019183220927874
iteration : 6039
train acc:  0.828125
train loss:  0.37652838230133057
train gradient:  0.3042818537493174
iteration : 6040
train acc:  0.7890625
train loss:  0.3964995741844177
train gradient:  0.2221808664453817
iteration : 6041
train acc:  0.8828125
train loss:  0.3367302417755127
train gradient:  0.20629113629890017
iteration : 6042
train acc:  0.8984375
train loss:  0.29145026206970215
train gradient:  0.22939060048332807
iteration : 6043
train acc:  0.859375
train loss:  0.31096112728118896
train gradient:  0.1554243533970247
iteration : 6044
train acc:  0.796875
train loss:  0.4133636951446533
train gradient:  0.37412501706960194
iteration : 6045
train acc:  0.8046875
train loss:  0.31130343675613403
train gradient:  0.15420976153265714
iteration : 6046
train acc:  0.8515625
train loss:  0.3183760643005371
train gradient:  0.19241473581736088
iteration : 6047
train acc:  0.859375
train loss:  0.32401400804519653
train gradient:  0.18849311729550033
iteration : 6048
train acc:  0.828125
train loss:  0.30461543798446655
train gradient:  0.1409391879272402
iteration : 6049
train acc:  0.78125
train loss:  0.46563202142715454
train gradient:  0.39646167954695277
iteration : 6050
train acc:  0.84375
train loss:  0.33131852746009827
train gradient:  0.1492418253656448
iteration : 6051
train acc:  0.796875
train loss:  0.38131922483444214
train gradient:  0.22303928988284583
iteration : 6052
train acc:  0.8828125
train loss:  0.29767507314682007
train gradient:  0.15595337050118774
iteration : 6053
train acc:  0.828125
train loss:  0.3950023055076599
train gradient:  0.2478469004383678
iteration : 6054
train acc:  0.8515625
train loss:  0.33814454078674316
train gradient:  0.20890253281588755
iteration : 6055
train acc:  0.9140625
train loss:  0.26483064889907837
train gradient:  0.21044809029865152
iteration : 6056
train acc:  0.8359375
train loss:  0.42334580421447754
train gradient:  0.2693099120728549
iteration : 6057
train acc:  0.8828125
train loss:  0.29367944598197937
train gradient:  0.19175359112712814
iteration : 6058
train acc:  0.890625
train loss:  0.27505186200141907
train gradient:  0.12916957569224896
iteration : 6059
train acc:  0.921875
train loss:  0.2461088001728058
train gradient:  0.10486911224663743
iteration : 6060
train acc:  0.828125
train loss:  0.37217867374420166
train gradient:  0.16983526987630226
iteration : 6061
train acc:  0.8359375
train loss:  0.33671170473098755
train gradient:  0.23105540635223268
iteration : 6062
train acc:  0.8359375
train loss:  0.37308308482170105
train gradient:  0.2256456896727908
iteration : 6063
train acc:  0.859375
train loss:  0.4319761395454407
train gradient:  0.41333019206508054
iteration : 6064
train acc:  0.8125
train loss:  0.41623616218566895
train gradient:  0.38194449217341553
iteration : 6065
train acc:  0.828125
train loss:  0.3452759087085724
train gradient:  0.17901680550975493
iteration : 6066
train acc:  0.796875
train loss:  0.442640483379364
train gradient:  0.37042231564572387
iteration : 6067
train acc:  0.8125
train loss:  0.4334171712398529
train gradient:  0.4635693996990652
iteration : 6068
train acc:  0.859375
train loss:  0.34685376286506653
train gradient:  0.24025481419422784
iteration : 6069
train acc:  0.8046875
train loss:  0.37466710805892944
train gradient:  0.25507527702474225
iteration : 6070
train acc:  0.828125
train loss:  0.32166144251823425
train gradient:  0.23820042640968003
iteration : 6071
train acc:  0.875
train loss:  0.2515071928501129
train gradient:  0.1705444567131979
iteration : 6072
train acc:  0.828125
train loss:  0.35588717460632324
train gradient:  0.17064592256726624
iteration : 6073
train acc:  0.8828125
train loss:  0.29365259408950806
train gradient:  0.15333675447536713
iteration : 6074
train acc:  0.8125
train loss:  0.3389745354652405
train gradient:  0.28435511311348644
iteration : 6075
train acc:  0.8671875
train loss:  0.35153916478157043
train gradient:  0.19963209570830917
iteration : 6076
train acc:  0.8359375
train loss:  0.369318425655365
train gradient:  0.24184765463167265
iteration : 6077
train acc:  0.890625
train loss:  0.29821664094924927
train gradient:  0.13652821015366012
iteration : 6078
train acc:  0.84375
train loss:  0.3757588267326355
train gradient:  0.21761434508438027
iteration : 6079
train acc:  0.8359375
train loss:  0.3273816406726837
train gradient:  0.20751690937111264
iteration : 6080
train acc:  0.796875
train loss:  0.4341340661048889
train gradient:  0.29574388961095366
iteration : 6081
train acc:  0.8515625
train loss:  0.26104119420051575
train gradient:  0.16199185571694227
iteration : 6082
train acc:  0.8515625
train loss:  0.36077627539634705
train gradient:  0.18625264290082658
iteration : 6083
train acc:  0.875
train loss:  0.31244930624961853
train gradient:  0.1820948517145759
iteration : 6084
train acc:  0.8125
train loss:  0.4578036069869995
train gradient:  0.2531296330109247
iteration : 6085
train acc:  0.828125
train loss:  0.3489585220813751
train gradient:  0.2895877268748219
iteration : 6086
train acc:  0.9140625
train loss:  0.23443372547626495
train gradient:  0.17603569073844938
iteration : 6087
train acc:  0.890625
train loss:  0.24523469805717468
train gradient:  0.09599055512118836
iteration : 6088
train acc:  0.859375
train loss:  0.35287511348724365
train gradient:  0.28926329907973175
iteration : 6089
train acc:  0.8984375
train loss:  0.27841493487358093
train gradient:  0.1372576138344291
iteration : 6090
train acc:  0.796875
train loss:  0.45525944232940674
train gradient:  0.37127620833623715
iteration : 6091
train acc:  0.8828125
train loss:  0.2992076277732849
train gradient:  0.17319423261181935
iteration : 6092
train acc:  0.8828125
train loss:  0.25488442182540894
train gradient:  0.10962705742599639
iteration : 6093
train acc:  0.8671875
train loss:  0.32472357153892517
train gradient:  0.14872838244862072
iteration : 6094
train acc:  0.828125
train loss:  0.34759318828582764
train gradient:  0.23393103187581296
iteration : 6095
train acc:  0.875
train loss:  0.2902275323867798
train gradient:  0.14700768816869977
iteration : 6096
train acc:  0.890625
train loss:  0.2963828146457672
train gradient:  0.14858280612007163
iteration : 6097
train acc:  0.828125
train loss:  0.39711886644363403
train gradient:  0.32224417701031877
iteration : 6098
train acc:  0.7890625
train loss:  0.4001697301864624
train gradient:  0.23233829987744425
iteration : 6099
train acc:  0.828125
train loss:  0.33476996421813965
train gradient:  0.16231345045608414
iteration : 6100
train acc:  0.8671875
train loss:  0.2740006744861603
train gradient:  0.14052807000958933
iteration : 6101
train acc:  0.75
train loss:  0.4914858937263489
train gradient:  0.4135474227401057
iteration : 6102
train acc:  0.8203125
train loss:  0.3891599178314209
train gradient:  0.30199453825958095
iteration : 6103
train acc:  0.7734375
train loss:  0.463362455368042
train gradient:  0.34358978464430767
iteration : 6104
train acc:  0.8828125
train loss:  0.30669623613357544
train gradient:  0.16485787342146463
iteration : 6105
train acc:  0.7890625
train loss:  0.41106924414634705
train gradient:  0.3800173942793196
iteration : 6106
train acc:  0.875
train loss:  0.332802414894104
train gradient:  0.165210903654534
iteration : 6107
train acc:  0.8515625
train loss:  0.36511144042015076
train gradient:  0.2668180361601763
iteration : 6108
train acc:  0.7734375
train loss:  0.44203394651412964
train gradient:  0.30798366842127184
iteration : 6109
train acc:  0.8125
train loss:  0.3631517291069031
train gradient:  0.17581466852442937
iteration : 6110
train acc:  0.8203125
train loss:  0.38459259271621704
train gradient:  0.22324355725979067
iteration : 6111
train acc:  0.8515625
train loss:  0.3784279227256775
train gradient:  0.25127095887201384
iteration : 6112
train acc:  0.8515625
train loss:  0.34136807918548584
train gradient:  0.19676779619023418
iteration : 6113
train acc:  0.8515625
train loss:  0.3887162208557129
train gradient:  0.189960674542838
iteration : 6114
train acc:  0.828125
train loss:  0.37037187814712524
train gradient:  0.28244269466834787
iteration : 6115
train acc:  0.8203125
train loss:  0.3629017472267151
train gradient:  0.19780502512752182
iteration : 6116
train acc:  0.8359375
train loss:  0.38838914036750793
train gradient:  0.29524761341155853
iteration : 6117
train acc:  0.84375
train loss:  0.3479824662208557
train gradient:  0.27550338144823083
iteration : 6118
train acc:  0.8125
train loss:  0.3710050582885742
train gradient:  0.21658020291192592
iteration : 6119
train acc:  0.828125
train loss:  0.36253273487091064
train gradient:  0.17303329793813485
iteration : 6120
train acc:  0.8515625
train loss:  0.37763047218322754
train gradient:  0.1857338935072851
iteration : 6121
train acc:  0.796875
train loss:  0.39965754747390747
train gradient:  0.24186996950575637
iteration : 6122
train acc:  0.8046875
train loss:  0.4078533947467804
train gradient:  0.23058726230233995
iteration : 6123
train acc:  0.8671875
train loss:  0.3564479649066925
train gradient:  0.23722351814613596
iteration : 6124
train acc:  0.84375
train loss:  0.3463886082172394
train gradient:  0.1540191020952425
iteration : 6125
train acc:  0.8359375
train loss:  0.3881264925003052
train gradient:  0.2955853522391306
iteration : 6126
train acc:  0.8515625
train loss:  0.333211749792099
train gradient:  0.2559825061876455
iteration : 6127
train acc:  0.8203125
train loss:  0.3591879606246948
train gradient:  0.2236557036296665
iteration : 6128
train acc:  0.8359375
train loss:  0.34177082777023315
train gradient:  0.24346323280932142
iteration : 6129
train acc:  0.859375
train loss:  0.30435991287231445
train gradient:  0.19713738728166366
iteration : 6130
train acc:  0.8046875
train loss:  0.3263292908668518
train gradient:  0.17372020532073287
iteration : 6131
train acc:  0.8359375
train loss:  0.33590415120124817
train gradient:  0.19704690904345135
iteration : 6132
train acc:  0.828125
train loss:  0.3676171600818634
train gradient:  0.18790748866027696
iteration : 6133
train acc:  0.8125
train loss:  0.3930765688419342
train gradient:  0.33966789867942193
iteration : 6134
train acc:  0.90625
train loss:  0.2922007441520691
train gradient:  0.14143092096214388
iteration : 6135
train acc:  0.875
train loss:  0.30262279510498047
train gradient:  0.18920123886567994
iteration : 6136
train acc:  0.8515625
train loss:  0.3369511365890503
train gradient:  0.20165087621443573
iteration : 6137
train acc:  0.875
train loss:  0.3085724115371704
train gradient:  0.1892705669992643
iteration : 6138
train acc:  0.7890625
train loss:  0.46219974756240845
train gradient:  0.32736902927886286
iteration : 6139
train acc:  0.8046875
train loss:  0.4375744163990021
train gradient:  0.23871804894143295
iteration : 6140
train acc:  0.8828125
train loss:  0.2752336263656616
train gradient:  0.1344549952859318
iteration : 6141
train acc:  0.8671875
train loss:  0.354293555021286
train gradient:  0.23821638372414097
iteration : 6142
train acc:  0.828125
train loss:  0.3321710228919983
train gradient:  0.17477368039819347
iteration : 6143
train acc:  0.8515625
train loss:  0.3310666084289551
train gradient:  0.2293518776183701
iteration : 6144
train acc:  0.859375
train loss:  0.3888220191001892
train gradient:  0.2618265979575793
iteration : 6145
train acc:  0.859375
train loss:  0.3310672640800476
train gradient:  0.18005826032206812
iteration : 6146
train acc:  0.8515625
train loss:  0.30996230244636536
train gradient:  0.18910890769230992
iteration : 6147
train acc:  0.8203125
train loss:  0.34876614809036255
train gradient:  0.2699934247252084
iteration : 6148
train acc:  0.8515625
train loss:  0.36882078647613525
train gradient:  0.2566315277411842
iteration : 6149
train acc:  0.8203125
train loss:  0.34333741664886475
train gradient:  0.18365873070568878
iteration : 6150
train acc:  0.84375
train loss:  0.3597647249698639
train gradient:  0.2584680199286433
iteration : 6151
train acc:  0.8203125
train loss:  0.3419170379638672
train gradient:  0.23715433391481267
iteration : 6152
train acc:  0.8359375
train loss:  0.31869685649871826
train gradient:  0.22334363991121114
iteration : 6153
train acc:  0.84375
train loss:  0.3199261724948883
train gradient:  0.22043667822833923
iteration : 6154
train acc:  0.78125
train loss:  0.4079420268535614
train gradient:  0.3484834550861178
iteration : 6155
train acc:  0.8828125
train loss:  0.27505549788475037
train gradient:  0.1859560858090241
iteration : 6156
train acc:  0.765625
train loss:  0.470045804977417
train gradient:  0.3460464483499126
iteration : 6157
train acc:  0.8046875
train loss:  0.4175005555152893
train gradient:  0.2822615339152188
iteration : 6158
train acc:  0.84375
train loss:  0.3217015862464905
train gradient:  0.22182896007371333
iteration : 6159
train acc:  0.8828125
train loss:  0.2619889974594116
train gradient:  0.16053170600964364
iteration : 6160
train acc:  0.8828125
train loss:  0.2932246923446655
train gradient:  0.2636061476540847
iteration : 6161
train acc:  0.859375
train loss:  0.37041041254997253
train gradient:  0.20945445994001863
iteration : 6162
train acc:  0.8359375
train loss:  0.330802321434021
train gradient:  0.1729238878615305
iteration : 6163
train acc:  0.828125
train loss:  0.3569105863571167
train gradient:  0.22230809153292957
iteration : 6164
train acc:  0.8671875
train loss:  0.3396052122116089
train gradient:  0.15796292114170854
iteration : 6165
train acc:  0.78125
train loss:  0.3809090554714203
train gradient:  0.2835097263230518
iteration : 6166
train acc:  0.8515625
train loss:  0.33818289637565613
train gradient:  0.2460853724971656
iteration : 6167
train acc:  0.828125
train loss:  0.3713608384132385
train gradient:  0.20143935071996444
iteration : 6168
train acc:  0.7890625
train loss:  0.4120174050331116
train gradient:  0.3417521988178638
iteration : 6169
train acc:  0.8359375
train loss:  0.3179835081100464
train gradient:  0.1769801233509468
iteration : 6170
train acc:  0.8671875
train loss:  0.27699726819992065
train gradient:  0.15012125683448024
iteration : 6171
train acc:  0.859375
train loss:  0.2960982322692871
train gradient:  0.12717252149269503
iteration : 6172
train acc:  0.84375
train loss:  0.38061749935150146
train gradient:  0.21616729060624412
iteration : 6173
train acc:  0.8046875
train loss:  0.3745267987251282
train gradient:  0.30303267199361067
iteration : 6174
train acc:  0.828125
train loss:  0.375416100025177
train gradient:  0.23705717645602323
iteration : 6175
train acc:  0.8671875
train loss:  0.3176596164703369
train gradient:  0.15958653955598928
iteration : 6176
train acc:  0.8515625
train loss:  0.3363966941833496
train gradient:  0.17555578236429786
iteration : 6177
train acc:  0.8828125
train loss:  0.2715691924095154
train gradient:  0.20878138825312623
iteration : 6178
train acc:  0.8046875
train loss:  0.3969523310661316
train gradient:  0.267724988174418
iteration : 6179
train acc:  0.8046875
train loss:  0.4261183738708496
train gradient:  0.34911991687086563
iteration : 6180
train acc:  0.78125
train loss:  0.4142402112483978
train gradient:  0.35860797497162855
iteration : 6181
train acc:  0.828125
train loss:  0.3531855344772339
train gradient:  0.20930000762233641
iteration : 6182
train acc:  0.8984375
train loss:  0.315337598323822
train gradient:  0.23076605348926774
iteration : 6183
train acc:  0.8359375
train loss:  0.3551384508609772
train gradient:  0.16838454730192076
iteration : 6184
train acc:  0.84375
train loss:  0.33912235498428345
train gradient:  0.22831904775734818
iteration : 6185
train acc:  0.8515625
train loss:  0.393484890460968
train gradient:  0.2602610477688304
iteration : 6186
train acc:  0.8828125
train loss:  0.33410972356796265
train gradient:  0.21867645453162673
iteration : 6187
train acc:  0.859375
train loss:  0.34784573316574097
train gradient:  0.24494651587344446
iteration : 6188
train acc:  0.875
train loss:  0.308249294757843
train gradient:  0.1731190282651595
iteration : 6189
train acc:  0.84375
train loss:  0.38049042224884033
train gradient:  0.2505410356181068
iteration : 6190
train acc:  0.8828125
train loss:  0.2925223708152771
train gradient:  0.1653814005851712
iteration : 6191
train acc:  0.828125
train loss:  0.34286195039749146
train gradient:  0.21779195047362385
iteration : 6192
train acc:  0.8828125
train loss:  0.29711610078811646
train gradient:  0.1823213792833981
iteration : 6193
train acc:  0.8828125
train loss:  0.3787890374660492
train gradient:  0.22007544357171946
iteration : 6194
train acc:  0.8125
train loss:  0.3450160622596741
train gradient:  0.21240945407966064
iteration : 6195
train acc:  0.890625
train loss:  0.28183358907699585
train gradient:  0.1503675544288136
iteration : 6196
train acc:  0.84375
train loss:  0.3275989890098572
train gradient:  0.3313621420393239
iteration : 6197
train acc:  0.828125
train loss:  0.4338744878768921
train gradient:  0.2618350859116084
iteration : 6198
train acc:  0.8984375
train loss:  0.30374211072921753
train gradient:  0.1605449183599361
iteration : 6199
train acc:  0.8671875
train loss:  0.31245720386505127
train gradient:  0.1704177720557662
iteration : 6200
train acc:  0.8125
train loss:  0.38633424043655396
train gradient:  0.2883210093006103
iteration : 6201
train acc:  0.8203125
train loss:  0.39683762192726135
train gradient:  0.19297886688364804
iteration : 6202
train acc:  0.8515625
train loss:  0.35484856367111206
train gradient:  0.2067465065230238
iteration : 6203
train acc:  0.875
train loss:  0.3179417848587036
train gradient:  0.1926473477454563
iteration : 6204
train acc:  0.9140625
train loss:  0.2827140688896179
train gradient:  0.13692894081547372
iteration : 6205
train acc:  0.7578125
train loss:  0.4549112915992737
train gradient:  0.2715920751623114
iteration : 6206
train acc:  0.9140625
train loss:  0.29675352573394775
train gradient:  0.16280124444923513
iteration : 6207
train acc:  0.875
train loss:  0.2920692563056946
train gradient:  0.15752538745101477
iteration : 6208
train acc:  0.84375
train loss:  0.3522936999797821
train gradient:  0.25466071335243423
iteration : 6209
train acc:  0.8828125
train loss:  0.3022806644439697
train gradient:  0.13002256267948684
iteration : 6210
train acc:  0.8671875
train loss:  0.3340088129043579
train gradient:  0.17377610395203208
iteration : 6211
train acc:  0.875
train loss:  0.35717201232910156
train gradient:  0.2662846398360478
iteration : 6212
train acc:  0.890625
train loss:  0.32021164894104004
train gradient:  0.1545611648307849
iteration : 6213
train acc:  0.8046875
train loss:  0.4015006721019745
train gradient:  0.32818292720191844
iteration : 6214
train acc:  0.8984375
train loss:  0.31273722648620605
train gradient:  0.21561530368260545
iteration : 6215
train acc:  0.8359375
train loss:  0.3979395031929016
train gradient:  0.23356707143026767
iteration : 6216
train acc:  0.8671875
train loss:  0.36366498470306396
train gradient:  0.34903909816282647
iteration : 6217
train acc:  0.8203125
train loss:  0.40003663301467896
train gradient:  0.2896062673887489
iteration : 6218
train acc:  0.890625
train loss:  0.2625664472579956
train gradient:  0.11871847235736958
iteration : 6219
train acc:  0.796875
train loss:  0.37043899297714233
train gradient:  0.19789369853061392
iteration : 6220
train acc:  0.8671875
train loss:  0.29411038756370544
train gradient:  0.17677964079872882
iteration : 6221
train acc:  0.7890625
train loss:  0.4195144772529602
train gradient:  0.24121739782572146
iteration : 6222
train acc:  0.8671875
train loss:  0.35167187452316284
train gradient:  0.232270519796564
iteration : 6223
train acc:  0.9296875
train loss:  0.21234142780303955
train gradient:  0.12112369655413271
iteration : 6224
train acc:  0.7734375
train loss:  0.5279952883720398
train gradient:  0.38985636244293104
iteration : 6225
train acc:  0.8828125
train loss:  0.3231046199798584
train gradient:  0.16020474583186656
iteration : 6226
train acc:  0.8828125
train loss:  0.3177996873855591
train gradient:  0.1767325032509895
iteration : 6227
train acc:  0.7890625
train loss:  0.37018489837646484
train gradient:  0.20572225388519372
iteration : 6228
train acc:  0.8515625
train loss:  0.3030717372894287
train gradient:  0.17950013684701965
iteration : 6229
train acc:  0.8125
train loss:  0.32310011982917786
train gradient:  0.215517355504535
iteration : 6230
train acc:  0.859375
train loss:  0.31541505455970764
train gradient:  0.20598097789923558
iteration : 6231
train acc:  0.84375
train loss:  0.32987523078918457
train gradient:  0.16940967823660574
iteration : 6232
train acc:  0.859375
train loss:  0.31279128789901733
train gradient:  0.25700540240727654
iteration : 6233
train acc:  0.875
train loss:  0.36099618673324585
train gradient:  0.26512279357249474
iteration : 6234
train acc:  0.828125
train loss:  0.3735561668872833
train gradient:  0.22749583346038244
iteration : 6235
train acc:  0.84375
train loss:  0.35506683588027954
train gradient:  0.2524763384440741
iteration : 6236
train acc:  0.8359375
train loss:  0.33289873600006104
train gradient:  0.20430923785622473
iteration : 6237
train acc:  0.8671875
train loss:  0.38240447640419006
train gradient:  0.3105554367211585
iteration : 6238
train acc:  0.8203125
train loss:  0.3608327805995941
train gradient:  0.21009242967606798
iteration : 6239
train acc:  0.8828125
train loss:  0.3216191530227661
train gradient:  0.14740154278411666
iteration : 6240
train acc:  0.890625
train loss:  0.27506333589553833
train gradient:  0.15414841818907135
iteration : 6241
train acc:  0.796875
train loss:  0.48687782883644104
train gradient:  0.3069517638743128
iteration : 6242
train acc:  0.875
train loss:  0.32220929861068726
train gradient:  0.2056724555780482
iteration : 6243
train acc:  0.859375
train loss:  0.3147042393684387
train gradient:  0.15785321482281972
iteration : 6244
train acc:  0.90625
train loss:  0.3044183850288391
train gradient:  0.18504653935224605
iteration : 6245
train acc:  0.8984375
train loss:  0.3048228621482849
train gradient:  0.1454795497434162
iteration : 6246
train acc:  0.859375
train loss:  0.31558459997177124
train gradient:  0.21684541949946168
iteration : 6247
train acc:  0.8671875
train loss:  0.2477109134197235
train gradient:  0.1723432957699198
iteration : 6248
train acc:  0.828125
train loss:  0.37246954441070557
train gradient:  0.21578308527339124
iteration : 6249
train acc:  0.796875
train loss:  0.42289620637893677
train gradient:  0.2530219713134114
iteration : 6250
train acc:  0.8515625
train loss:  0.31009379029273987
train gradient:  0.20959290362809974
iteration : 6251
train acc:  0.828125
train loss:  0.34513378143310547
train gradient:  0.20863346483698195
iteration : 6252
train acc:  0.8046875
train loss:  0.5018110275268555
train gradient:  0.5161511275290344
iteration : 6253
train acc:  0.890625
train loss:  0.2868279218673706
train gradient:  0.17686739577944474
iteration : 6254
train acc:  0.84375
train loss:  0.3541107177734375
train gradient:  0.28432414344825474
iteration : 6255
train acc:  0.84375
train loss:  0.341607928276062
train gradient:  0.16021941859783367
iteration : 6256
train acc:  0.8359375
train loss:  0.3356430232524872
train gradient:  0.17303967928794317
iteration : 6257
train acc:  0.84375
train loss:  0.3190463185310364
train gradient:  0.265143399596753
iteration : 6258
train acc:  0.84375
train loss:  0.3274739384651184
train gradient:  0.1903728594141491
iteration : 6259
train acc:  0.8515625
train loss:  0.32339590787887573
train gradient:  0.17050566818145566
iteration : 6260
train acc:  0.921875
train loss:  0.3059021234512329
train gradient:  0.1458940030858427
iteration : 6261
train acc:  0.8828125
train loss:  0.2721814513206482
train gradient:  0.1378276603941313
iteration : 6262
train acc:  0.828125
train loss:  0.42196542024612427
train gradient:  0.24896695245113332
iteration : 6263
train acc:  0.859375
train loss:  0.32122063636779785
train gradient:  0.19085970341790123
iteration : 6264
train acc:  0.828125
train loss:  0.38713952898979187
train gradient:  0.3786593983246365
iteration : 6265
train acc:  0.859375
train loss:  0.32985544204711914
train gradient:  0.31924601350723425
iteration : 6266
train acc:  0.875
train loss:  0.288784921169281
train gradient:  0.149775667148351
iteration : 6267
train acc:  0.828125
train loss:  0.4306795597076416
train gradient:  0.3193947359475338
iteration : 6268
train acc:  0.8515625
train loss:  0.3449820876121521
train gradient:  0.20660277707774438
iteration : 6269
train acc:  0.84375
train loss:  0.30662453174591064
train gradient:  0.1902868899599114
iteration : 6270
train acc:  0.796875
train loss:  0.38691842555999756
train gradient:  0.242539506088621
iteration : 6271
train acc:  0.8359375
train loss:  0.3742884695529938
train gradient:  0.21293184384696318
iteration : 6272
train acc:  0.8203125
train loss:  0.3748810589313507
train gradient:  0.16639758717314287
iteration : 6273
train acc:  0.859375
train loss:  0.34623879194259644
train gradient:  0.24648134583679684
iteration : 6274
train acc:  0.84375
train loss:  0.3167697787284851
train gradient:  0.18998971150091998
iteration : 6275
train acc:  0.8828125
train loss:  0.2900340259075165
train gradient:  0.16503591978357807
iteration : 6276
train acc:  0.890625
train loss:  0.26750150322914124
train gradient:  0.12645027862975117
iteration : 6277
train acc:  0.8203125
train loss:  0.3734334707260132
train gradient:  0.29943273819684607
iteration : 6278
train acc:  0.78125
train loss:  0.48365625739097595
train gradient:  0.47623549189883296
iteration : 6279
train acc:  0.84375
train loss:  0.39626118540763855
train gradient:  0.261213939626637
iteration : 6280
train acc:  0.8671875
train loss:  0.31688621640205383
train gradient:  0.1548826727447491
iteration : 6281
train acc:  0.828125
train loss:  0.32610613107681274
train gradient:  0.2659606301786267
iteration : 6282
train acc:  0.8359375
train loss:  0.33150315284729004
train gradient:  0.17998500408901258
iteration : 6283
train acc:  0.8046875
train loss:  0.3636016547679901
train gradient:  0.2729424750151589
iteration : 6284
train acc:  0.84375
train loss:  0.37049126625061035
train gradient:  0.25313287906040055
iteration : 6285
train acc:  0.8359375
train loss:  0.4076136350631714
train gradient:  0.31837043304237816
iteration : 6286
train acc:  0.8203125
train loss:  0.35945016145706177
train gradient:  0.29202928247450227
iteration : 6287
train acc:  0.875
train loss:  0.2822611331939697
train gradient:  0.15474368931794785
iteration : 6288
train acc:  0.875
train loss:  0.2793213725090027
train gradient:  0.1494888017581544
iteration : 6289
train acc:  0.8671875
train loss:  0.3263689875602722
train gradient:  0.19059000835896084
iteration : 6290
train acc:  0.875
train loss:  0.3346123695373535
train gradient:  0.3241717880539696
iteration : 6291
train acc:  0.859375
train loss:  0.3243042826652527
train gradient:  0.19925830231774583
iteration : 6292
train acc:  0.8203125
train loss:  0.34934282302856445
train gradient:  0.1933008840019726
iteration : 6293
train acc:  0.84375
train loss:  0.36838337779045105
train gradient:  0.20846108886112455
iteration : 6294
train acc:  0.8359375
train loss:  0.38548165559768677
train gradient:  0.2476241939012489
iteration : 6295
train acc:  0.8671875
train loss:  0.27328914403915405
train gradient:  0.15149911786807918
iteration : 6296
train acc:  0.875
train loss:  0.3083140552043915
train gradient:  0.16749543349189921
iteration : 6297
train acc:  0.84375
train loss:  0.3371010422706604
train gradient:  0.18481302242078146
iteration : 6298
train acc:  0.84375
train loss:  0.38356196880340576
train gradient:  0.254352150893437
iteration : 6299
train acc:  0.828125
train loss:  0.32906246185302734
train gradient:  0.18089179202716998
iteration : 6300
train acc:  0.8515625
train loss:  0.31804242730140686
train gradient:  0.19142500649775207
iteration : 6301
train acc:  0.8359375
train loss:  0.39487239718437195
train gradient:  0.2301776372526405
iteration : 6302
train acc:  0.8671875
train loss:  0.26200923323631287
train gradient:  0.1267458165491373
iteration : 6303
train acc:  0.84375
train loss:  0.3393329381942749
train gradient:  0.26036596304258136
iteration : 6304
train acc:  0.8359375
train loss:  0.36539706587791443
train gradient:  0.21519272494867825
iteration : 6305
train acc:  0.8828125
train loss:  0.289922297000885
train gradient:  0.1780966188930711
iteration : 6306
train acc:  0.8203125
train loss:  0.3772081136703491
train gradient:  0.24414974350238916
iteration : 6307
train acc:  0.765625
train loss:  0.48253414034843445
train gradient:  0.45406198198446057
iteration : 6308
train acc:  0.796875
train loss:  0.3997578024864197
train gradient:  0.2624496013228777
iteration : 6309
train acc:  0.828125
train loss:  0.42722997069358826
train gradient:  0.23302896194193223
iteration : 6310
train acc:  0.8515625
train loss:  0.356449156999588
train gradient:  0.2539292233696467
iteration : 6311
train acc:  0.875
train loss:  0.35147541761398315
train gradient:  0.2022882407414764
iteration : 6312
train acc:  0.8203125
train loss:  0.42760828137397766
train gradient:  0.3029545065156587
iteration : 6313
train acc:  0.859375
train loss:  0.2831449508666992
train gradient:  0.17450367122613658
iteration : 6314
train acc:  0.875
train loss:  0.3093319535255432
train gradient:  0.20095638365236726
iteration : 6315
train acc:  0.8046875
train loss:  0.40939050912857056
train gradient:  0.28677276353316283
iteration : 6316
train acc:  0.8515625
train loss:  0.332042396068573
train gradient:  0.24640200070992482
iteration : 6317
train acc:  0.7890625
train loss:  0.4463580250740051
train gradient:  0.3050934010688474
iteration : 6318
train acc:  0.8359375
train loss:  0.3547334671020508
train gradient:  0.24731809891313786
iteration : 6319
train acc:  0.8828125
train loss:  0.34120649099349976
train gradient:  0.34871292909364054
iteration : 6320
train acc:  0.875
train loss:  0.29502397775650024
train gradient:  0.1337249269179835
iteration : 6321
train acc:  0.8125
train loss:  0.33809980750083923
train gradient:  0.2560757905450213
iteration : 6322
train acc:  0.8515625
train loss:  0.3755374550819397
train gradient:  0.22062244527727448
iteration : 6323
train acc:  0.8828125
train loss:  0.29744940996170044
train gradient:  0.13133379018904795
iteration : 6324
train acc:  0.8671875
train loss:  0.3381196856498718
train gradient:  0.19121905090451785
iteration : 6325
train acc:  0.84375
train loss:  0.35206419229507446
train gradient:  0.21198567667035947
iteration : 6326
train acc:  0.7890625
train loss:  0.3911571502685547
train gradient:  0.2792442223203961
iteration : 6327
train acc:  0.859375
train loss:  0.3025965094566345
train gradient:  0.14339606464402943
iteration : 6328
train acc:  0.828125
train loss:  0.396232545375824
train gradient:  0.23895126482473097
iteration : 6329
train acc:  0.9296875
train loss:  0.2572104334831238
train gradient:  0.1206281605277871
iteration : 6330
train acc:  0.828125
train loss:  0.3635241389274597
train gradient:  0.16881447799724766
iteration : 6331
train acc:  0.828125
train loss:  0.34574198722839355
train gradient:  0.2679567677508302
iteration : 6332
train acc:  0.8828125
train loss:  0.34834107756614685
train gradient:  0.18105476276574003
iteration : 6333
train acc:  0.921875
train loss:  0.22899843752384186
train gradient:  0.24210397929337757
iteration : 6334
train acc:  0.78125
train loss:  0.4583306908607483
train gradient:  0.31285639813484517
iteration : 6335
train acc:  0.828125
train loss:  0.36376234889030457
train gradient:  0.2558117380382836
iteration : 6336
train acc:  0.875
train loss:  0.3029134273529053
train gradient:  0.16433006344858586
iteration : 6337
train acc:  0.84375
train loss:  0.3706413209438324
train gradient:  0.25182976844381016
iteration : 6338
train acc:  0.890625
train loss:  0.30905041098594666
train gradient:  0.1860314261496421
iteration : 6339
train acc:  0.8515625
train loss:  0.34818506240844727
train gradient:  0.29251524824657793
iteration : 6340
train acc:  0.8359375
train loss:  0.357097864151001
train gradient:  0.24462499530206916
iteration : 6341
train acc:  0.859375
train loss:  0.39750364422798157
train gradient:  0.1635673376275848
iteration : 6342
train acc:  0.875
train loss:  0.30493471026420593
train gradient:  0.17908372752071133
iteration : 6343
train acc:  0.8359375
train loss:  0.4256739318370819
train gradient:  0.33838382836944525
iteration : 6344
train acc:  0.8671875
train loss:  0.29588913917541504
train gradient:  0.17314457278437206
iteration : 6345
train acc:  0.875
train loss:  0.30815935134887695
train gradient:  0.1787472263247915
iteration : 6346
train acc:  0.8671875
train loss:  0.3501642346382141
train gradient:  0.2307548273745508
iteration : 6347
train acc:  0.8359375
train loss:  0.39984434843063354
train gradient:  0.26534514957803357
iteration : 6348
train acc:  0.8203125
train loss:  0.3616074323654175
train gradient:  0.1998033919887907
iteration : 6349
train acc:  0.7734375
train loss:  0.45624783635139465
train gradient:  0.28112567575584124
iteration : 6350
train acc:  0.8984375
train loss:  0.2995113730430603
train gradient:  0.1426600372825345
iteration : 6351
train acc:  0.8671875
train loss:  0.3347008228302002
train gradient:  0.13824812557246965
iteration : 6352
train acc:  0.8125
train loss:  0.33347582817077637
train gradient:  0.2935503964181708
iteration : 6353
train acc:  0.84375
train loss:  0.3076428174972534
train gradient:  0.21000302173617913
iteration : 6354
train acc:  0.8359375
train loss:  0.307029128074646
train gradient:  0.15999626151241925
iteration : 6355
train acc:  0.828125
train loss:  0.4271038770675659
train gradient:  0.42766643384831
iteration : 6356
train acc:  0.8203125
train loss:  0.35347068309783936
train gradient:  0.23789362504515252
iteration : 6357
train acc:  0.8671875
train loss:  0.33568689227104187
train gradient:  0.24445518364100793
iteration : 6358
train acc:  0.8671875
train loss:  0.3353317975997925
train gradient:  0.18292171002774615
iteration : 6359
train acc:  0.828125
train loss:  0.377352774143219
train gradient:  0.2899480687802701
iteration : 6360
train acc:  0.921875
train loss:  0.28078728914260864
train gradient:  0.2331533794526705
iteration : 6361
train acc:  0.8671875
train loss:  0.30523326992988586
train gradient:  0.2349929102435434
iteration : 6362
train acc:  0.875
train loss:  0.30945703387260437
train gradient:  0.15296190794820413
iteration : 6363
train acc:  0.796875
train loss:  0.3880348205566406
train gradient:  0.3320174683901142
iteration : 6364
train acc:  0.8515625
train loss:  0.3467394709587097
train gradient:  0.2827241721056808
iteration : 6365
train acc:  0.828125
train loss:  0.3555702567100525
train gradient:  0.21451570821714366
iteration : 6366
train acc:  0.8671875
train loss:  0.3295082151889801
train gradient:  0.17879589606798524
iteration : 6367
train acc:  0.875
train loss:  0.2711796164512634
train gradient:  0.22116198499764037
iteration : 6368
train acc:  0.875
train loss:  0.25951826572418213
train gradient:  0.21006665959076173
iteration : 6369
train acc:  0.8203125
train loss:  0.3877753019332886
train gradient:  0.26978494527914243
iteration : 6370
train acc:  0.8359375
train loss:  0.3335608243942261
train gradient:  0.2671567513401122
iteration : 6371
train acc:  0.90625
train loss:  0.2521662712097168
train gradient:  0.162086581743556
iteration : 6372
train acc:  0.8359375
train loss:  0.4228088855743408
train gradient:  0.2918083371098003
iteration : 6373
train acc:  0.7734375
train loss:  0.49084919691085815
train gradient:  0.4315993119064597
iteration : 6374
train acc:  0.84375
train loss:  0.34793028235435486
train gradient:  0.23353253745272973
iteration : 6375
train acc:  0.8515625
train loss:  0.3219109773635864
train gradient:  0.22578500159351034
iteration : 6376
train acc:  0.8515625
train loss:  0.3382943272590637
train gradient:  0.26894274263190177
iteration : 6377
train acc:  0.8203125
train loss:  0.3968948721885681
train gradient:  0.3054786757899205
iteration : 6378
train acc:  0.828125
train loss:  0.3742713928222656
train gradient:  0.3059953101964893
iteration : 6379
train acc:  0.859375
train loss:  0.2856435775756836
train gradient:  0.1417432424667215
iteration : 6380
train acc:  0.828125
train loss:  0.3856070041656494
train gradient:  0.19810527416780424
iteration : 6381
train acc:  0.84375
train loss:  0.3691415786743164
train gradient:  0.1924396100345784
iteration : 6382
train acc:  0.8515625
train loss:  0.40748125314712524
train gradient:  0.32480206817398916
iteration : 6383
train acc:  0.828125
train loss:  0.38017573952674866
train gradient:  0.2917840415206316
iteration : 6384
train acc:  0.8359375
train loss:  0.35635820031166077
train gradient:  0.2407564650097746
iteration : 6385
train acc:  0.8046875
train loss:  0.37370502948760986
train gradient:  0.28466725956541294
iteration : 6386
train acc:  0.84375
train loss:  0.3511729836463928
train gradient:  0.26729914673401
iteration : 6387
train acc:  0.8125
train loss:  0.3646777868270874
train gradient:  0.24739408171026228
iteration : 6388
train acc:  0.796875
train loss:  0.4347267150878906
train gradient:  0.2732688478728438
iteration : 6389
train acc:  0.859375
train loss:  0.2845476567745209
train gradient:  0.14167014218407567
iteration : 6390
train acc:  0.859375
train loss:  0.37372058629989624
train gradient:  0.17708654409712768
iteration : 6391
train acc:  0.859375
train loss:  0.3626469373703003
train gradient:  0.280848975635724
iteration : 6392
train acc:  0.8046875
train loss:  0.4361853003501892
train gradient:  0.23518223347955003
iteration : 6393
train acc:  0.8125
train loss:  0.44068682193756104
train gradient:  0.3431007741630082
iteration : 6394
train acc:  0.8359375
train loss:  0.38848739862442017
train gradient:  0.25530820242001667
iteration : 6395
train acc:  0.8359375
train loss:  0.34687870740890503
train gradient:  0.1771458402412382
iteration : 6396
train acc:  0.8515625
train loss:  0.3594472408294678
train gradient:  0.19073836049587506
iteration : 6397
train acc:  0.8203125
train loss:  0.42601823806762695
train gradient:  0.24265493521593098
iteration : 6398
train acc:  0.875
train loss:  0.3312975764274597
train gradient:  0.19947800111990877
iteration : 6399
train acc:  0.8671875
train loss:  0.3569035530090332
train gradient:  0.18179188747163416
iteration : 6400
train acc:  0.8515625
train loss:  0.33676114678382874
train gradient:  0.21429635102268357
iteration : 6401
train acc:  0.8046875
train loss:  0.4035910367965698
train gradient:  0.23092055126966765
iteration : 6402
train acc:  0.84375
train loss:  0.36605125665664673
train gradient:  0.23579157839005227
iteration : 6403
train acc:  0.859375
train loss:  0.4009942412376404
train gradient:  0.22617402397276137
iteration : 6404
train acc:  0.8984375
train loss:  0.2858726978302002
train gradient:  0.1832194449464745
iteration : 6405
train acc:  0.828125
train loss:  0.37518012523651123
train gradient:  0.2551873173490905
iteration : 6406
train acc:  0.859375
train loss:  0.3381395936012268
train gradient:  0.19779124532477332
iteration : 6407
train acc:  0.8046875
train loss:  0.36191752552986145
train gradient:  0.24230139167745557
iteration : 6408
train acc:  0.8828125
train loss:  0.330674946308136
train gradient:  0.12632798518496874
iteration : 6409
train acc:  0.8359375
train loss:  0.39363864064216614
train gradient:  0.26678446874217865
iteration : 6410
train acc:  0.8125
train loss:  0.44469568133354187
train gradient:  0.5637710523011783
iteration : 6411
train acc:  0.828125
train loss:  0.3832717537879944
train gradient:  0.29407793995795994
iteration : 6412
train acc:  0.828125
train loss:  0.3750356435775757
train gradient:  0.17975617394142301
iteration : 6413
train acc:  0.8671875
train loss:  0.30935782194137573
train gradient:  0.16858018593750215
iteration : 6414
train acc:  0.8828125
train loss:  0.3235948383808136
train gradient:  0.14385672233649216
iteration : 6415
train acc:  0.875
train loss:  0.287193238735199
train gradient:  0.10784452306938379
iteration : 6416
train acc:  0.7890625
train loss:  0.3968998193740845
train gradient:  0.31970744108490934
iteration : 6417
train acc:  0.84375
train loss:  0.3274396061897278
train gradient:  0.2037622306048565
iteration : 6418
train acc:  0.859375
train loss:  0.3043370246887207
train gradient:  0.1513813792295006
iteration : 6419
train acc:  0.78125
train loss:  0.3849306106567383
train gradient:  0.19802588847401034
iteration : 6420
train acc:  0.8046875
train loss:  0.4009680151939392
train gradient:  0.30178765130449164
iteration : 6421
train acc:  0.8828125
train loss:  0.3019646406173706
train gradient:  0.126563548821288
iteration : 6422
train acc:  0.8671875
train loss:  0.2884907126426697
train gradient:  0.13750817236061585
iteration : 6423
train acc:  0.8984375
train loss:  0.2885007858276367
train gradient:  0.14920570211698192
iteration : 6424
train acc:  0.8515625
train loss:  0.3399912118911743
train gradient:  0.2224411102219031
iteration : 6425
train acc:  0.8203125
train loss:  0.32277145981788635
train gradient:  0.17993595924315814
iteration : 6426
train acc:  0.8671875
train loss:  0.29769742488861084
train gradient:  0.12656445872038757
iteration : 6427
train acc:  0.875
train loss:  0.32146427035331726
train gradient:  0.1367104054221688
iteration : 6428
train acc:  0.8125
train loss:  0.4170480966567993
train gradient:  0.218492569336226
iteration : 6429
train acc:  0.828125
train loss:  0.3604866862297058
train gradient:  0.25384888811180406
iteration : 6430
train acc:  0.8515625
train loss:  0.3198796510696411
train gradient:  0.19974037715565124
iteration : 6431
train acc:  0.8125
train loss:  0.3680762052536011
train gradient:  0.17219528294299408
iteration : 6432
train acc:  0.828125
train loss:  0.4395297169685364
train gradient:  0.3439454747286918
iteration : 6433
train acc:  0.8515625
train loss:  0.328075110912323
train gradient:  0.2733707806765321
iteration : 6434
train acc:  0.859375
train loss:  0.34064584970474243
train gradient:  0.13547131875262924
iteration : 6435
train acc:  0.8359375
train loss:  0.37147343158721924
train gradient:  0.2070175952560581
iteration : 6436
train acc:  0.8203125
train loss:  0.3929162919521332
train gradient:  0.2842454197009458
iteration : 6437
train acc:  0.8984375
train loss:  0.2822827100753784
train gradient:  0.12456161228090036
iteration : 6438
train acc:  0.859375
train loss:  0.35011768341064453
train gradient:  0.20346830522782636
iteration : 6439
train acc:  0.859375
train loss:  0.33808982372283936
train gradient:  0.17887419731873905
iteration : 6440
train acc:  0.8515625
train loss:  0.3523310124874115
train gradient:  0.19278992202242312
iteration : 6441
train acc:  0.8828125
train loss:  0.3238573968410492
train gradient:  0.20156462852859502
iteration : 6442
train acc:  0.8515625
train loss:  0.30887502431869507
train gradient:  0.16763388177143693
iteration : 6443
train acc:  0.828125
train loss:  0.38155001401901245
train gradient:  0.19899368483361335
iteration : 6444
train acc:  0.828125
train loss:  0.3187173306941986
train gradient:  0.19165189347644124
iteration : 6445
train acc:  0.8671875
train loss:  0.3194942772388458
train gradient:  0.2036728541706644
iteration : 6446
train acc:  0.859375
train loss:  0.4114946126937866
train gradient:  0.2824026547741736
iteration : 6447
train acc:  0.859375
train loss:  0.36578842997550964
train gradient:  0.20468490754142493
iteration : 6448
train acc:  0.890625
train loss:  0.29560405015945435
train gradient:  0.13128179788460412
iteration : 6449
train acc:  0.859375
train loss:  0.38155972957611084
train gradient:  0.19102209002198756
iteration : 6450
train acc:  0.8203125
train loss:  0.3594461977481842
train gradient:  0.2763038114258564
iteration : 6451
train acc:  0.8359375
train loss:  0.35459744930267334
train gradient:  0.2735054217182572
iteration : 6452
train acc:  0.859375
train loss:  0.31724613904953003
train gradient:  0.19858103045775607
iteration : 6453
train acc:  0.84375
train loss:  0.3393930494785309
train gradient:  0.15267510405016066
iteration : 6454
train acc:  0.875
train loss:  0.34126317501068115
train gradient:  0.1804904639736466
iteration : 6455
train acc:  0.8515625
train loss:  0.29776379466056824
train gradient:  0.13481722222106363
iteration : 6456
train acc:  0.90625
train loss:  0.2801118493080139
train gradient:  0.12760182670753484
iteration : 6457
train acc:  0.9375
train loss:  0.24648815393447876
train gradient:  0.18761288510533497
iteration : 6458
train acc:  0.9140625
train loss:  0.2650696039199829
train gradient:  0.17753053554323103
iteration : 6459
train acc:  0.9140625
train loss:  0.2258986383676529
train gradient:  0.1314138205886498
iteration : 6460
train acc:  0.8828125
train loss:  0.2596646547317505
train gradient:  0.13028082786676992
iteration : 6461
train acc:  0.8046875
train loss:  0.3927977979183197
train gradient:  0.3377475635059362
iteration : 6462
train acc:  0.8671875
train loss:  0.30042973160743713
train gradient:  0.15631153755753996
iteration : 6463
train acc:  0.875
train loss:  0.27938929200172424
train gradient:  0.12151739042150846
iteration : 6464
train acc:  0.84375
train loss:  0.30617183446884155
train gradient:  0.23617573376656564
iteration : 6465
train acc:  0.84375
train loss:  0.3768048882484436
train gradient:  0.30872604487369426
iteration : 6466
train acc:  0.8203125
train loss:  0.4294983148574829
train gradient:  0.2659646855448577
iteration : 6467
train acc:  0.8125
train loss:  0.37300753593444824
train gradient:  0.19497408942606512
iteration : 6468
train acc:  0.8515625
train loss:  0.36068642139434814
train gradient:  0.3117356288820367
iteration : 6469
train acc:  0.84375
train loss:  0.33987271785736084
train gradient:  0.18546572937375788
iteration : 6470
train acc:  0.8203125
train loss:  0.3728807270526886
train gradient:  0.32948185315892986
iteration : 6471
train acc:  0.859375
train loss:  0.3156541883945465
train gradient:  0.18314690288762353
iteration : 6472
train acc:  0.8671875
train loss:  0.28737619519233704
train gradient:  0.12816164663737895
iteration : 6473
train acc:  0.78125
train loss:  0.45518434047698975
train gradient:  0.3836694840263138
iteration : 6474
train acc:  0.828125
train loss:  0.3397938311100006
train gradient:  0.16659494782980963
iteration : 6475
train acc:  0.8359375
train loss:  0.3511946201324463
train gradient:  0.24469846254576755
iteration : 6476
train acc:  0.875
train loss:  0.29936468601226807
train gradient:  0.22208660531473934
iteration : 6477
train acc:  0.8515625
train loss:  0.29425153136253357
train gradient:  0.23561543923664335
iteration : 6478
train acc:  0.890625
train loss:  0.2902396321296692
train gradient:  0.11486234958721836
iteration : 6479
train acc:  0.8125
train loss:  0.36746901273727417
train gradient:  0.2742623275406007
iteration : 6480
train acc:  0.8671875
train loss:  0.34257036447525024
train gradient:  0.23023812829948295
iteration : 6481
train acc:  0.84375
train loss:  0.34642669558525085
train gradient:  0.18754803864128303
iteration : 6482
train acc:  0.828125
train loss:  0.3163050413131714
train gradient:  0.1939721848799156
iteration : 6483
train acc:  0.859375
train loss:  0.35633566975593567
train gradient:  0.17877914746188173
iteration : 6484
train acc:  0.8515625
train loss:  0.33322399854660034
train gradient:  0.19413346503809792
iteration : 6485
train acc:  0.90625
train loss:  0.24391940236091614
train gradient:  0.08945687725710683
iteration : 6486
train acc:  0.90625
train loss:  0.2508898973464966
train gradient:  0.10891330579455519
iteration : 6487
train acc:  0.8671875
train loss:  0.3195546269416809
train gradient:  0.2330605592312894
iteration : 6488
train acc:  0.8203125
train loss:  0.3933010995388031
train gradient:  0.4134596821430137
iteration : 6489
train acc:  0.8515625
train loss:  0.3516506552696228
train gradient:  0.17671840154051488
iteration : 6490
train acc:  0.921875
train loss:  0.28655803203582764
train gradient:  0.1640049134037297
iteration : 6491
train acc:  0.8046875
train loss:  0.44373950362205505
train gradient:  0.36960621169129837
iteration : 6492
train acc:  0.8828125
train loss:  0.29867446422576904
train gradient:  0.1398150960808759
iteration : 6493
train acc:  0.8515625
train loss:  0.32922980189323425
train gradient:  0.1953727474057812
iteration : 6494
train acc:  0.828125
train loss:  0.4466124475002289
train gradient:  0.3590707428469883
iteration : 6495
train acc:  0.8359375
train loss:  0.33260610699653625
train gradient:  0.1720327177718985
iteration : 6496
train acc:  0.8515625
train loss:  0.3230801224708557
train gradient:  0.3237511081904738
iteration : 6497
train acc:  0.84375
train loss:  0.3504895567893982
train gradient:  0.2859319928026065
iteration : 6498
train acc:  0.828125
train loss:  0.37941908836364746
train gradient:  0.3082221728234185
iteration : 6499
train acc:  0.8515625
train loss:  0.38846147060394287
train gradient:  0.19040443895582548
iteration : 6500
train acc:  0.859375
train loss:  0.33753591775894165
train gradient:  0.1954617387307202
iteration : 6501
train acc:  0.859375
train loss:  0.2964109182357788
train gradient:  0.11397453980232626
iteration : 6502
train acc:  0.8359375
train loss:  0.35397326946258545
train gradient:  0.20793780656009705
iteration : 6503
train acc:  0.828125
train loss:  0.3973480463027954
train gradient:  0.24421742936992036
iteration : 6504
train acc:  0.8359375
train loss:  0.3762035667896271
train gradient:  0.24849270923568623
iteration : 6505
train acc:  0.859375
train loss:  0.3146701455116272
train gradient:  0.2093733294604564
iteration : 6506
train acc:  0.828125
train loss:  0.38396644592285156
train gradient:  0.3102921507311861
iteration : 6507
train acc:  0.828125
train loss:  0.33275485038757324
train gradient:  0.2487525186891767
iteration : 6508
train acc:  0.859375
train loss:  0.3221426010131836
train gradient:  0.19209653758197442
iteration : 6509
train acc:  0.84375
train loss:  0.31914907693862915
train gradient:  0.2897381627263322
iteration : 6510
train acc:  0.8671875
train loss:  0.3276585042476654
train gradient:  0.23428853761697932
iteration : 6511
train acc:  0.828125
train loss:  0.3655056953430176
train gradient:  0.2578414627296064
iteration : 6512
train acc:  0.8125
train loss:  0.3825420141220093
train gradient:  0.25082495386514064
iteration : 6513
train acc:  0.859375
train loss:  0.29490846395492554
train gradient:  0.13474526724581293
iteration : 6514
train acc:  0.8125
train loss:  0.4101139307022095
train gradient:  0.2914432645199031
iteration : 6515
train acc:  0.8671875
train loss:  0.3383404314517975
train gradient:  0.21631703850648165
iteration : 6516
train acc:  0.90625
train loss:  0.28012335300445557
train gradient:  0.11939578760478727
iteration : 6517
train acc:  0.8359375
train loss:  0.32277464866638184
train gradient:  0.21976697623552213
iteration : 6518
train acc:  0.8203125
train loss:  0.3330835700035095
train gradient:  0.2122846925292421
iteration : 6519
train acc:  0.890625
train loss:  0.24525931477546692
train gradient:  0.11822017039344657
iteration : 6520
train acc:  0.8203125
train loss:  0.41667473316192627
train gradient:  0.32088741888737016
iteration : 6521
train acc:  0.7890625
train loss:  0.3945668339729309
train gradient:  0.262009876338839
iteration : 6522
train acc:  0.8359375
train loss:  0.3384256362915039
train gradient:  0.24269942229725305
iteration : 6523
train acc:  0.8515625
train loss:  0.3132765293121338
train gradient:  0.21321652769443653
iteration : 6524
train acc:  0.8828125
train loss:  0.2886052131652832
train gradient:  0.20785511011429048
iteration : 6525
train acc:  0.890625
train loss:  0.2849043607711792
train gradient:  0.11034508483660735
iteration : 6526
train acc:  0.875
train loss:  0.2957081198692322
train gradient:  0.18624355096546596
iteration : 6527
train acc:  0.8828125
train loss:  0.2709324061870575
train gradient:  0.19976546365274486
iteration : 6528
train acc:  0.8515625
train loss:  0.3587382435798645
train gradient:  0.23977361140835102
iteration : 6529
train acc:  0.875
train loss:  0.3174325227737427
train gradient:  0.2004496392390454
iteration : 6530
train acc:  0.8046875
train loss:  0.38354772329330444
train gradient:  0.2403244150749865
iteration : 6531
train acc:  0.859375
train loss:  0.31270653009414673
train gradient:  0.21405817174068598
iteration : 6532
train acc:  0.8359375
train loss:  0.388397216796875
train gradient:  0.18955083618260454
iteration : 6533
train acc:  0.875
train loss:  0.3606983423233032
train gradient:  0.19301532141881672
iteration : 6534
train acc:  0.8125
train loss:  0.44352906942367554
train gradient:  0.40756313684011525
iteration : 6535
train acc:  0.859375
train loss:  0.3189866542816162
train gradient:  0.17702674851336184
iteration : 6536
train acc:  0.890625
train loss:  0.30660486221313477
train gradient:  0.22839545957727825
iteration : 6537
train acc:  0.78125
train loss:  0.4178609848022461
train gradient:  0.34273803652048573
iteration : 6538
train acc:  0.859375
train loss:  0.3549661636352539
train gradient:  0.2436574969182294
iteration : 6539
train acc:  0.8515625
train loss:  0.3196597397327423
train gradient:  0.12385287327285473
iteration : 6540
train acc:  0.828125
train loss:  0.31582164764404297
train gradient:  0.20340014626522626
iteration : 6541
train acc:  0.859375
train loss:  0.3367679715156555
train gradient:  0.15677674050694435
iteration : 6542
train acc:  0.828125
train loss:  0.35008135437965393
train gradient:  0.18862838869391832
iteration : 6543
train acc:  0.8359375
train loss:  0.34769678115844727
train gradient:  0.24729770365635487
iteration : 6544
train acc:  0.875
train loss:  0.31199735403060913
train gradient:  0.15430675456024445
iteration : 6545
train acc:  0.8125
train loss:  0.421588659286499
train gradient:  0.2918856324621839
iteration : 6546
train acc:  0.8046875
train loss:  0.38473451137542725
train gradient:  0.22631812275752705
iteration : 6547
train acc:  0.8125
train loss:  0.34958136081695557
train gradient:  0.1950217208511837
iteration : 6548
train acc:  0.84375
train loss:  0.3418678641319275
train gradient:  0.28466490134555966
iteration : 6549
train acc:  0.8515625
train loss:  0.34694457054138184
train gradient:  0.23013114540806484
iteration : 6550
train acc:  0.8515625
train loss:  0.35235196352005005
train gradient:  0.19660841784674168
iteration : 6551
train acc:  0.8515625
train loss:  0.34606221318244934
train gradient:  0.3140209426791284
iteration : 6552
train acc:  0.859375
train loss:  0.30781909823417664
train gradient:  0.16498942921623708
iteration : 6553
train acc:  0.8515625
train loss:  0.3964185416698456
train gradient:  0.2769333971346656
iteration : 6554
train acc:  0.875
train loss:  0.35463911294937134
train gradient:  0.2214832350853258
iteration : 6555
train acc:  0.8515625
train loss:  0.29144346714019775
train gradient:  0.1374432725271848
iteration : 6556
train acc:  0.84375
train loss:  0.3260534107685089
train gradient:  0.2354635678259544
iteration : 6557
train acc:  0.875
train loss:  0.3141040503978729
train gradient:  0.1909533234574593
iteration : 6558
train acc:  0.8203125
train loss:  0.36823874711990356
train gradient:  0.24512660834382705
iteration : 6559
train acc:  0.8046875
train loss:  0.38885605335235596
train gradient:  0.3658843699651806
iteration : 6560
train acc:  0.859375
train loss:  0.37670040130615234
train gradient:  0.2713003975360456
iteration : 6561
train acc:  0.84375
train loss:  0.37766027450561523
train gradient:  0.25839705067176155
iteration : 6562
train acc:  0.828125
train loss:  0.33402347564697266
train gradient:  0.2436839757196916
iteration : 6563
train acc:  0.8671875
train loss:  0.3517322540283203
train gradient:  0.29702498317219034
iteration : 6564
train acc:  0.8671875
train loss:  0.3237483501434326
train gradient:  0.21219266018190763
iteration : 6565
train acc:  0.8671875
train loss:  0.3555040955543518
train gradient:  0.18334242392467925
iteration : 6566
train acc:  0.8359375
train loss:  0.3239525854587555
train gradient:  0.15908103362351397
iteration : 6567
train acc:  0.90625
train loss:  0.2797717750072479
train gradient:  0.17568116057243593
iteration : 6568
train acc:  0.8359375
train loss:  0.3793906569480896
train gradient:  0.20646854734260112
iteration : 6569
train acc:  0.8671875
train loss:  0.3345007002353668
train gradient:  0.1637083141044276
iteration : 6570
train acc:  0.8828125
train loss:  0.2975212335586548
train gradient:  0.15061277094727632
iteration : 6571
train acc:  0.890625
train loss:  0.2963259816169739
train gradient:  0.11453348950910601
iteration : 6572
train acc:  0.84375
train loss:  0.38968518376350403
train gradient:  0.4205703787057934
iteration : 6573
train acc:  0.8125
train loss:  0.404288649559021
train gradient:  0.25373618913151075
iteration : 6574
train acc:  0.8203125
train loss:  0.383466899394989
train gradient:  0.1687676626711026
iteration : 6575
train acc:  0.8125
train loss:  0.37375372648239136
train gradient:  0.39135294528408077
iteration : 6576
train acc:  0.8125
train loss:  0.3968392014503479
train gradient:  0.254585753318059
iteration : 6577
train acc:  0.8828125
train loss:  0.2993919253349304
train gradient:  0.17268788747563074
iteration : 6578
train acc:  0.84375
train loss:  0.3484872579574585
train gradient:  0.23607652900950232
iteration : 6579
train acc:  0.8359375
train loss:  0.3888433575630188
train gradient:  0.1726691521788548
iteration : 6580
train acc:  0.8828125
train loss:  0.29163476824760437
train gradient:  0.1831300070594704
iteration : 6581
train acc:  0.84375
train loss:  0.3198641836643219
train gradient:  0.19407069825040754
iteration : 6582
train acc:  0.859375
train loss:  0.32996866106987
train gradient:  0.16771490374092407
iteration : 6583
train acc:  0.8828125
train loss:  0.2931685447692871
train gradient:  0.17293793141034391
iteration : 6584
train acc:  0.8671875
train loss:  0.29313522577285767
train gradient:  0.16924660652476714
iteration : 6585
train acc:  0.8671875
train loss:  0.3250581622123718
train gradient:  0.17714631210307616
iteration : 6586
train acc:  0.7890625
train loss:  0.44978004693984985
train gradient:  0.3252983772980774
iteration : 6587
train acc:  0.875
train loss:  0.3056264817714691
train gradient:  0.18816353175644948
iteration : 6588
train acc:  0.859375
train loss:  0.31896480917930603
train gradient:  0.13424132123909754
iteration : 6589
train acc:  0.8359375
train loss:  0.3334369659423828
train gradient:  0.2773076652927413
iteration : 6590
train acc:  0.8671875
train loss:  0.2970402240753174
train gradient:  0.16108117714782672
iteration : 6591
train acc:  0.859375
train loss:  0.3014336824417114
train gradient:  0.19289772829720314
iteration : 6592
train acc:  0.8359375
train loss:  0.44080600142478943
train gradient:  0.3172544772600096
iteration : 6593
train acc:  0.8515625
train loss:  0.31649619340896606
train gradient:  0.17014262266284158
iteration : 6594
train acc:  0.9140625
train loss:  0.27174776792526245
train gradient:  0.13091889786382255
iteration : 6595
train acc:  0.8671875
train loss:  0.2872231602668762
train gradient:  0.17111516826067563
iteration : 6596
train acc:  0.8203125
train loss:  0.356781005859375
train gradient:  0.18633487152658298
iteration : 6597
train acc:  0.8515625
train loss:  0.3447633981704712
train gradient:  0.2727984859738615
iteration : 6598
train acc:  0.8359375
train loss:  0.3227279782295227
train gradient:  0.2424348725816146
iteration : 6599
train acc:  0.875
train loss:  0.30944693088531494
train gradient:  0.2390738699683684
iteration : 6600
train acc:  0.875
train loss:  0.3064795732498169
train gradient:  0.15529205901578705
iteration : 6601
train acc:  0.8359375
train loss:  0.37352579832077026
train gradient:  0.28209252696774173
iteration : 6602
train acc:  0.8828125
train loss:  0.35171711444854736
train gradient:  0.2357397388947871
iteration : 6603
train acc:  0.828125
train loss:  0.31180769205093384
train gradient:  0.1461949944420786
iteration : 6604
train acc:  0.859375
train loss:  0.3356841802597046
train gradient:  0.2623601522485346
iteration : 6605
train acc:  0.84375
train loss:  0.4192151725292206
train gradient:  0.263663270506631
iteration : 6606
train acc:  0.8671875
train loss:  0.3022083044052124
train gradient:  0.1870582050216955
iteration : 6607
train acc:  0.765625
train loss:  0.47071030735969543
train gradient:  0.38567320505177427
iteration : 6608
train acc:  0.859375
train loss:  0.3202740550041199
train gradient:  0.17081505836073796
iteration : 6609
train acc:  0.796875
train loss:  0.36816442012786865
train gradient:  0.18458456344649676
iteration : 6610
train acc:  0.8828125
train loss:  0.28682762384414673
train gradient:  0.19571723076871375
iteration : 6611
train acc:  0.890625
train loss:  0.2751208543777466
train gradient:  0.12633664980267942
iteration : 6612
train acc:  0.8359375
train loss:  0.35464802384376526
train gradient:  0.23998762450440675
iteration : 6613
train acc:  0.859375
train loss:  0.3107669949531555
train gradient:  0.21234759332247205
iteration : 6614
train acc:  0.8671875
train loss:  0.400320827960968
train gradient:  0.2359632837179259
iteration : 6615
train acc:  0.8359375
train loss:  0.4013344645500183
train gradient:  0.24738394364659388
iteration : 6616
train acc:  0.8203125
train loss:  0.41681337356567383
train gradient:  0.28092793620432077
iteration : 6617
train acc:  0.859375
train loss:  0.29189106822013855
train gradient:  0.12769985214279606
iteration : 6618
train acc:  0.8828125
train loss:  0.2879418730735779
train gradient:  0.16095952455857176
iteration : 6619
train acc:  0.84375
train loss:  0.36396074295043945
train gradient:  0.25885120596458605
iteration : 6620
train acc:  0.8828125
train loss:  0.2808074355125427
train gradient:  0.1371376850196754
iteration : 6621
train acc:  0.828125
train loss:  0.3852020800113678
train gradient:  0.2827406364866211
iteration : 6622
train acc:  0.8515625
train loss:  0.33266040682792664
train gradient:  0.20457621009591442
iteration : 6623
train acc:  0.8203125
train loss:  0.42384034395217896
train gradient:  0.28165454715323934
iteration : 6624
train acc:  0.890625
train loss:  0.2745581269264221
train gradient:  0.21414442269946948
iteration : 6625
train acc:  0.875
train loss:  0.28685447573661804
train gradient:  0.14627788812552572
iteration : 6626
train acc:  0.8671875
train loss:  0.2839980125427246
train gradient:  0.12213445363038912
iteration : 6627
train acc:  0.90625
train loss:  0.2605096697807312
train gradient:  0.150434559785303
iteration : 6628
train acc:  0.890625
train loss:  0.2449113428592682
train gradient:  0.11444839698075254
iteration : 6629
train acc:  0.8828125
train loss:  0.34875187277793884
train gradient:  0.17382492722025594
iteration : 6630
train acc:  0.8359375
train loss:  0.366937518119812
train gradient:  0.32037652384184223
iteration : 6631
train acc:  0.859375
train loss:  0.2814011573791504
train gradient:  0.10727528767302202
iteration : 6632
train acc:  0.8203125
train loss:  0.3357464075088501
train gradient:  0.22530264962436045
iteration : 6633
train acc:  0.8203125
train loss:  0.3675227761268616
train gradient:  0.3312079019635698
iteration : 6634
train acc:  0.890625
train loss:  0.2908291816711426
train gradient:  0.13195888238684836
iteration : 6635
train acc:  0.8203125
train loss:  0.34840911626815796
train gradient:  0.23070677326548175
iteration : 6636
train acc:  0.8828125
train loss:  0.3445371985435486
train gradient:  0.1975887126916192
iteration : 6637
train acc:  0.8671875
train loss:  0.38108590245246887
train gradient:  0.34566207127510035
iteration : 6638
train acc:  0.7734375
train loss:  0.47178468108177185
train gradient:  0.4853404514743162
iteration : 6639
train acc:  0.8984375
train loss:  0.2849663496017456
train gradient:  0.18409133805391462
iteration : 6640
train acc:  0.8515625
train loss:  0.36293160915374756
train gradient:  0.21020894596395667
iteration : 6641
train acc:  0.828125
train loss:  0.3712660074234009
train gradient:  0.25156212944573936
iteration : 6642
train acc:  0.796875
train loss:  0.3589930832386017
train gradient:  0.23449050279289343
iteration : 6643
train acc:  0.8359375
train loss:  0.3487444818019867
train gradient:  0.2376861827937666
iteration : 6644
train acc:  0.7890625
train loss:  0.44155484437942505
train gradient:  0.4195748815270543
iteration : 6645
train acc:  0.84375
train loss:  0.3152851462364197
train gradient:  0.190129982998538
iteration : 6646
train acc:  0.84375
train loss:  0.32145756483078003
train gradient:  0.19563818902734445
iteration : 6647
train acc:  0.90625
train loss:  0.2781709134578705
train gradient:  0.18288487374978024
iteration : 6648
train acc:  0.828125
train loss:  0.3469548225402832
train gradient:  0.2595274466507215
iteration : 6649
train acc:  0.8515625
train loss:  0.2979752719402313
train gradient:  0.16264259964859745
iteration : 6650
train acc:  0.84375
train loss:  0.41186267137527466
train gradient:  0.3368102070568995
iteration : 6651
train acc:  0.875
train loss:  0.32490217685699463
train gradient:  0.16948516827999038
iteration : 6652
train acc:  0.8359375
train loss:  0.33359360694885254
train gradient:  0.18137426971402387
iteration : 6653
train acc:  0.8671875
train loss:  0.33305227756500244
train gradient:  0.2536064231847819
iteration : 6654
train acc:  0.8046875
train loss:  0.44431352615356445
train gradient:  0.2955178996284998
iteration : 6655
train acc:  0.90625
train loss:  0.2929898202419281
train gradient:  0.1444363359890536
iteration : 6656
train acc:  0.8828125
train loss:  0.27734535932540894
train gradient:  0.1699915462036165
iteration : 6657
train acc:  0.78125
train loss:  0.4769284725189209
train gradient:  0.2516570818678051
iteration : 6658
train acc:  0.8671875
train loss:  0.28700971603393555
train gradient:  0.14440542451758664
iteration : 6659
train acc:  0.859375
train loss:  0.3186817765235901
train gradient:  0.24870722281708937
iteration : 6660
train acc:  0.8828125
train loss:  0.2785537838935852
train gradient:  0.12142209765186318
iteration : 6661
train acc:  0.8828125
train loss:  0.3081526756286621
train gradient:  0.14971485984291727
iteration : 6662
train acc:  0.890625
train loss:  0.2814190685749054
train gradient:  0.20234593931616948
iteration : 6663
train acc:  0.84375
train loss:  0.3226010799407959
train gradient:  0.17416504714762704
iteration : 6664
train acc:  0.8671875
train loss:  0.3220455050468445
train gradient:  0.14250446569894565
iteration : 6665
train acc:  0.8046875
train loss:  0.4376061260700226
train gradient:  0.2952203097198939
iteration : 6666
train acc:  0.8828125
train loss:  0.29195570945739746
train gradient:  0.13172931755259
iteration : 6667
train acc:  0.8125
train loss:  0.3700811266899109
train gradient:  0.26066971990443094
iteration : 6668
train acc:  0.84375
train loss:  0.3409719467163086
train gradient:  0.22764321947723182
iteration : 6669
train acc:  0.875
train loss:  0.3568345308303833
train gradient:  0.29160298516333166
iteration : 6670
train acc:  0.8203125
train loss:  0.40551501512527466
train gradient:  0.37264367301482837
iteration : 6671
train acc:  0.8515625
train loss:  0.3135056495666504
train gradient:  0.15709337420927008
iteration : 6672
train acc:  0.875
train loss:  0.2929982542991638
train gradient:  0.1415771905220477
iteration : 6673
train acc:  0.8671875
train loss:  0.29765671491622925
train gradient:  0.16996846008975097
iteration : 6674
train acc:  0.796875
train loss:  0.41475820541381836
train gradient:  0.35139661520754534
iteration : 6675
train acc:  0.8203125
train loss:  0.41022396087646484
train gradient:  0.30820412250485874
iteration : 6676
train acc:  0.828125
train loss:  0.3776874840259552
train gradient:  0.225870546688313
iteration : 6677
train acc:  0.875
train loss:  0.31595975160598755
train gradient:  0.20538482749899006
iteration : 6678
train acc:  0.859375
train loss:  0.36031675338745117
train gradient:  0.2256530870233509
iteration : 6679
train acc:  0.8203125
train loss:  0.3887447714805603
train gradient:  0.20348114376345283
iteration : 6680
train acc:  0.8359375
train loss:  0.3851465582847595
train gradient:  0.18021825129798325
iteration : 6681
train acc:  0.7734375
train loss:  0.48200222849845886
train gradient:  0.43364644677843867
iteration : 6682
train acc:  0.8671875
train loss:  0.3436945676803589
train gradient:  0.24237364120455723
iteration : 6683
train acc:  0.8515625
train loss:  0.3389461636543274
train gradient:  0.15399996537189392
iteration : 6684
train acc:  0.8828125
train loss:  0.3099031448364258
train gradient:  0.19390017490869166
iteration : 6685
train acc:  0.8671875
train loss:  0.3250468671321869
train gradient:  0.14357711723534786
iteration : 6686
train acc:  0.8515625
train loss:  0.386756032705307
train gradient:  0.23029248857054357
iteration : 6687
train acc:  0.875
train loss:  0.3088706135749817
train gradient:  0.14919436455288543
iteration : 6688
train acc:  0.8203125
train loss:  0.36203819513320923
train gradient:  0.2666640893080848
iteration : 6689
train acc:  0.8671875
train loss:  0.31795865297317505
train gradient:  0.22597297663011257
iteration : 6690
train acc:  0.828125
train loss:  0.3356350064277649
train gradient:  0.20112722903590866
iteration : 6691
train acc:  0.8671875
train loss:  0.3534930944442749
train gradient:  0.2946913202728067
iteration : 6692
train acc:  0.859375
train loss:  0.32021355628967285
train gradient:  0.1700626871332815
iteration : 6693
train acc:  0.8359375
train loss:  0.3785979747772217
train gradient:  0.22562151643904899
iteration : 6694
train acc:  0.859375
train loss:  0.344241738319397
train gradient:  0.20683169051885547
iteration : 6695
train acc:  0.8515625
train loss:  0.40170228481292725
train gradient:  0.2441475641789313
iteration : 6696
train acc:  0.8359375
train loss:  0.33770573139190674
train gradient:  0.18415607308894727
iteration : 6697
train acc:  0.890625
train loss:  0.2745874226093292
train gradient:  0.19898789622016375
iteration : 6698
train acc:  0.8515625
train loss:  0.3719392716884613
train gradient:  0.22624237017975218
iteration : 6699
train acc:  0.8125
train loss:  0.3271486163139343
train gradient:  0.20629106358570656
iteration : 6700
train acc:  0.8515625
train loss:  0.338998943567276
train gradient:  0.22726681784804256
iteration : 6701
train acc:  0.84375
train loss:  0.3358536660671234
train gradient:  0.19275948066424303
iteration : 6702
train acc:  0.8203125
train loss:  0.37286069989204407
train gradient:  0.2575005017285209
iteration : 6703
train acc:  0.8671875
train loss:  0.36993181705474854
train gradient:  0.26255794067098165
iteration : 6704
train acc:  0.8828125
train loss:  0.3068373203277588
train gradient:  0.16395467215842893
iteration : 6705
train acc:  0.8359375
train loss:  0.3336326479911804
train gradient:  0.14788049388304436
iteration : 6706
train acc:  0.8984375
train loss:  0.33140385150909424
train gradient:  0.18370339938469488
iteration : 6707
train acc:  0.8125
train loss:  0.35888224840164185
train gradient:  0.2509740379984645
iteration : 6708
train acc:  0.859375
train loss:  0.3424559533596039
train gradient:  0.1852549271570412
iteration : 6709
train acc:  0.875
train loss:  0.26026618480682373
train gradient:  0.12181084961685815
iteration : 6710
train acc:  0.8125
train loss:  0.33219051361083984
train gradient:  0.2599667452137106
iteration : 6711
train acc:  0.859375
train loss:  0.32924291491508484
train gradient:  0.1692097545642916
iteration : 6712
train acc:  0.8515625
train loss:  0.3631686568260193
train gradient:  0.2792552740567141
iteration : 6713
train acc:  0.8203125
train loss:  0.3846553862094879
train gradient:  0.2474343691794491
iteration : 6714
train acc:  0.84375
train loss:  0.35257360339164734
train gradient:  0.19108580719018511
iteration : 6715
train acc:  0.8515625
train loss:  0.3506239056587219
train gradient:  0.20660159822578816
iteration : 6716
train acc:  0.84375
train loss:  0.3299183249473572
train gradient:  0.18917516668250056
iteration : 6717
train acc:  0.875
train loss:  0.2749088406562805
train gradient:  0.14990596137886605
iteration : 6718
train acc:  0.9140625
train loss:  0.22328074276447296
train gradient:  0.10522447653863244
iteration : 6719
train acc:  0.828125
train loss:  0.3388972580432892
train gradient:  0.17922849930828116
iteration : 6720
train acc:  0.8359375
train loss:  0.4282476305961609
train gradient:  0.2138275552621225
iteration : 6721
train acc:  0.8515625
train loss:  0.34227851033210754
train gradient:  0.17933559005022146
iteration : 6722
train acc:  0.8359375
train loss:  0.3415389657020569
train gradient:  0.1852898156385366
iteration : 6723
train acc:  0.8671875
train loss:  0.31198036670684814
train gradient:  0.1862414459225478
iteration : 6724
train acc:  0.875
train loss:  0.25502800941467285
train gradient:  0.09435383995801794
iteration : 6725
train acc:  0.84375
train loss:  0.3494430184364319
train gradient:  0.18770002195629093
iteration : 6726
train acc:  0.8125
train loss:  0.34092941880226135
train gradient:  0.1975640155390534
iteration : 6727
train acc:  0.8203125
train loss:  0.3941488564014435
train gradient:  0.28447433167642977
iteration : 6728
train acc:  0.859375
train loss:  0.31785836815834045
train gradient:  0.18962993922744548
iteration : 6729
train acc:  0.828125
train loss:  0.3771345913410187
train gradient:  0.26092664640391394
iteration : 6730
train acc:  0.8671875
train loss:  0.32416489720344543
train gradient:  0.1879647674260244
iteration : 6731
train acc:  0.84375
train loss:  0.358151912689209
train gradient:  0.2200998332915438
iteration : 6732
train acc:  0.7734375
train loss:  0.43731528520584106
train gradient:  0.30108536512116096
iteration : 6733
train acc:  0.8515625
train loss:  0.28053274750709534
train gradient:  0.2241906669286255
iteration : 6734
train acc:  0.875
train loss:  0.29405373334884644
train gradient:  0.20322076174702935
iteration : 6735
train acc:  0.8203125
train loss:  0.37056636810302734
train gradient:  0.2596227964427787
iteration : 6736
train acc:  0.859375
train loss:  0.36112260818481445
train gradient:  0.28852848480758053
iteration : 6737
train acc:  0.859375
train loss:  0.3578304648399353
train gradient:  0.19459148521541642
iteration : 6738
train acc:  0.875
train loss:  0.354375958442688
train gradient:  0.16975486703884962
iteration : 6739
train acc:  0.890625
train loss:  0.2606022357940674
train gradient:  0.14013626775654128
iteration : 6740
train acc:  0.859375
train loss:  0.32038167119026184
train gradient:  0.22316966628206208
iteration : 6741
train acc:  0.84375
train loss:  0.37608659267425537
train gradient:  0.2781593818211127
iteration : 6742
train acc:  0.84375
train loss:  0.3622271418571472
train gradient:  0.20871650993019128
iteration : 6743
train acc:  0.84375
train loss:  0.36339879035949707
train gradient:  0.4121362780561358
iteration : 6744
train acc:  0.8515625
train loss:  0.3548852801322937
train gradient:  0.1857034657793883
iteration : 6745
train acc:  0.8671875
train loss:  0.30556559562683105
train gradient:  0.1684509814275903
iteration : 6746
train acc:  0.8828125
train loss:  0.2939643859863281
train gradient:  0.14675832430111135
iteration : 6747
train acc:  0.8046875
train loss:  0.41073793172836304
train gradient:  0.28861131894945263
iteration : 6748
train acc:  0.8828125
train loss:  0.35632240772247314
train gradient:  0.26276261674394236
iteration : 6749
train acc:  0.8671875
train loss:  0.32143449783325195
train gradient:  0.2112369144140931
iteration : 6750
train acc:  0.796875
train loss:  0.38413503766059875
train gradient:  0.28072471032398394
iteration : 6751
train acc:  0.8203125
train loss:  0.4078567922115326
train gradient:  0.3338391185466349
iteration : 6752
train acc:  0.8359375
train loss:  0.3411399722099304
train gradient:  0.1915984489769723
iteration : 6753
train acc:  0.8203125
train loss:  0.4105897545814514
train gradient:  0.2771749353019817
iteration : 6754
train acc:  0.7734375
train loss:  0.42016899585723877
train gradient:  0.24786420468293052
iteration : 6755
train acc:  0.8359375
train loss:  0.3638681471347809
train gradient:  0.21821270955449362
iteration : 6756
train acc:  0.8984375
train loss:  0.27209311723709106
train gradient:  0.14717812933848123
iteration : 6757
train acc:  0.8828125
train loss:  0.3275986611843109
train gradient:  0.19682475635178362
iteration : 6758
train acc:  0.9140625
train loss:  0.26610296964645386
train gradient:  0.16059567883279258
iteration : 6759
train acc:  0.8984375
train loss:  0.2812729477882385
train gradient:  0.19449832086900049
iteration : 6760
train acc:  0.8203125
train loss:  0.3692821264266968
train gradient:  0.45962478349632935
iteration : 6761
train acc:  0.8046875
train loss:  0.38506418466567993
train gradient:  0.23355986098469536
iteration : 6762
train acc:  0.8828125
train loss:  0.32304343581199646
train gradient:  0.1668998016200709
iteration : 6763
train acc:  0.8671875
train loss:  0.30218204855918884
train gradient:  0.1447071513945698
iteration : 6764
train acc:  0.8515625
train loss:  0.37020325660705566
train gradient:  0.5692068075992818
iteration : 6765
train acc:  0.8515625
train loss:  0.3519614338874817
train gradient:  0.20558591075465518
iteration : 6766
train acc:  0.8359375
train loss:  0.34882259368896484
train gradient:  0.1622850981882641
iteration : 6767
train acc:  0.84375
train loss:  0.3579978048801422
train gradient:  0.3179221603100214
iteration : 6768
train acc:  0.875
train loss:  0.29565417766571045
train gradient:  0.1807177052845929
iteration : 6769
train acc:  0.890625
train loss:  0.2683522701263428
train gradient:  0.13524228937399851
iteration : 6770
train acc:  0.828125
train loss:  0.41862165927886963
train gradient:  0.38890228319681447
iteration : 6771
train acc:  0.8359375
train loss:  0.3930090665817261
train gradient:  0.224315421324908
iteration : 6772
train acc:  0.8984375
train loss:  0.25997886061668396
train gradient:  0.22539415846958488
iteration : 6773
train acc:  0.828125
train loss:  0.37315046787261963
train gradient:  0.2156896015319978
iteration : 6774
train acc:  0.84375
train loss:  0.3254541754722595
train gradient:  0.19799751860209036
iteration : 6775
train acc:  0.8515625
train loss:  0.36743295192718506
train gradient:  0.23403617507218394
iteration : 6776
train acc:  0.8046875
train loss:  0.37768083810806274
train gradient:  0.30984305801205286
iteration : 6777
train acc:  0.8046875
train loss:  0.41564008593559265
train gradient:  0.3150104361345609
iteration : 6778
train acc:  0.8671875
train loss:  0.3385564684867859
train gradient:  0.2264450839965526
iteration : 6779
train acc:  0.859375
train loss:  0.3630531430244446
train gradient:  0.208438820614559
iteration : 6780
train acc:  0.84375
train loss:  0.36958232522010803
train gradient:  0.2387430984420612
iteration : 6781
train acc:  0.8671875
train loss:  0.33427131175994873
train gradient:  0.27865841892664245
iteration : 6782
train acc:  0.8984375
train loss:  0.31224215030670166
train gradient:  0.19487139936561537
iteration : 6783
train acc:  0.8828125
train loss:  0.2771141827106476
train gradient:  0.14579459909296955
iteration : 6784
train acc:  0.8984375
train loss:  0.26150891184806824
train gradient:  0.14983959419620438
iteration : 6785
train acc:  0.84375
train loss:  0.36616113781929016
train gradient:  0.3321839319146097
iteration : 6786
train acc:  0.8203125
train loss:  0.4278780519962311
train gradient:  0.2906760503197327
iteration : 6787
train acc:  0.859375
train loss:  0.34792226552963257
train gradient:  0.18172851427743153
iteration : 6788
train acc:  0.8671875
train loss:  0.252709299325943
train gradient:  0.2235430829407974
iteration : 6789
train acc:  0.8515625
train loss:  0.3403944969177246
train gradient:  0.22129582059790576
iteration : 6790
train acc:  0.90625
train loss:  0.2938283085823059
train gradient:  0.18165738377114754
iteration : 6791
train acc:  0.859375
train loss:  0.3151223659515381
train gradient:  0.13358551386435075
iteration : 6792
train acc:  0.8515625
train loss:  0.4106377363204956
train gradient:  0.23954005572338616
iteration : 6793
train acc:  0.8515625
train loss:  0.28489425778388977
train gradient:  0.19697640235760544
iteration : 6794
train acc:  0.859375
train loss:  0.3686712384223938
train gradient:  0.2226715772368737
iteration : 6795
train acc:  0.8046875
train loss:  0.3707427382469177
train gradient:  0.33682107485703455
iteration : 6796
train acc:  0.8515625
train loss:  0.3320509195327759
train gradient:  0.1909299751353191
iteration : 6797
train acc:  0.859375
train loss:  0.27057379484176636
train gradient:  0.13113492158887596
iteration : 6798
train acc:  0.84375
train loss:  0.36472880840301514
train gradient:  0.34976780431092447
iteration : 6799
train acc:  0.84375
train loss:  0.3463119864463806
train gradient:  0.17436899227004077
iteration : 6800
train acc:  0.8359375
train loss:  0.31642746925354004
train gradient:  0.20017786393829753
iteration : 6801
train acc:  0.9140625
train loss:  0.27927035093307495
train gradient:  0.24162113561455426
iteration : 6802
train acc:  0.90625
train loss:  0.2785085439682007
train gradient:  0.10484575413640551
iteration : 6803
train acc:  0.828125
train loss:  0.4120607376098633
train gradient:  0.36142913960011724
iteration : 6804
train acc:  0.84375
train loss:  0.283501535654068
train gradient:  0.19029743214758468
iteration : 6805
train acc:  0.890625
train loss:  0.27697449922561646
train gradient:  0.12376562837935011
iteration : 6806
train acc:  0.8515625
train loss:  0.34819164872169495
train gradient:  0.23554240780421842
iteration : 6807
train acc:  0.8125
train loss:  0.4620975852012634
train gradient:  0.32686178690886947
iteration : 6808
train acc:  0.8671875
train loss:  0.30270808935165405
train gradient:  0.20446774305290444
iteration : 6809
train acc:  0.8203125
train loss:  0.31719404458999634
train gradient:  0.1821557407791684
iteration : 6810
train acc:  0.796875
train loss:  0.3621976971626282
train gradient:  0.23203959298555318
iteration : 6811
train acc:  0.84375
train loss:  0.3589697480201721
train gradient:  0.25634274973994964
iteration : 6812
train acc:  0.8359375
train loss:  0.29263681173324585
train gradient:  0.2162708509215428
iteration : 6813
train acc:  0.875
train loss:  0.2670077383518219
train gradient:  0.130954609932563
iteration : 6814
train acc:  0.84375
train loss:  0.3432833254337311
train gradient:  0.23715616268638864
iteration : 6815
train acc:  0.859375
train loss:  0.33179110288619995
train gradient:  0.14456383818641025
iteration : 6816
train acc:  0.8359375
train loss:  0.3506224453449249
train gradient:  0.21395217010135106
iteration : 6817
train acc:  0.8125
train loss:  0.3940138518810272
train gradient:  0.27590508298773514
iteration : 6818
train acc:  0.875
train loss:  0.3128054141998291
train gradient:  0.177312506402097
iteration : 6819
train acc:  0.8515625
train loss:  0.36938923597335815
train gradient:  0.23211604875428973
iteration : 6820
train acc:  0.84375
train loss:  0.31466081738471985
train gradient:  0.2192746048127622
iteration : 6821
train acc:  0.8203125
train loss:  0.35000163316726685
train gradient:  0.22518947213873103
iteration : 6822
train acc:  0.8984375
train loss:  0.3088840842247009
train gradient:  0.1648332157877199
iteration : 6823
train acc:  0.8515625
train loss:  0.30262839794158936
train gradient:  0.20459427111849793
iteration : 6824
train acc:  0.8046875
train loss:  0.42737430334091187
train gradient:  0.28264237150858335
iteration : 6825
train acc:  0.8828125
train loss:  0.31911835074424744
train gradient:  0.16688967621335093
iteration : 6826
train acc:  0.921875
train loss:  0.21273010969161987
train gradient:  0.11411531590612031
iteration : 6827
train acc:  0.8828125
train loss:  0.256504625082016
train gradient:  0.11878123115165357
iteration : 6828
train acc:  0.8359375
train loss:  0.34492039680480957
train gradient:  0.23031300914050185
iteration : 6829
train acc:  0.796875
train loss:  0.3935391306877136
train gradient:  0.24220173287321403
iteration : 6830
train acc:  0.8515625
train loss:  0.3598865866661072
train gradient:  0.18423146431423293
iteration : 6831
train acc:  0.8671875
train loss:  0.27032673358917236
train gradient:  0.13579235452599553
iteration : 6832
train acc:  0.8046875
train loss:  0.37225887179374695
train gradient:  0.31092529874848385
iteration : 6833
train acc:  0.8828125
train loss:  0.33382028341293335
train gradient:  0.18868972157137553
iteration : 6834
train acc:  0.8125
train loss:  0.37710168957710266
train gradient:  0.460219302105535
iteration : 6835
train acc:  0.828125
train loss:  0.38709405064582825
train gradient:  0.22160929448590588
iteration : 6836
train acc:  0.890625
train loss:  0.30757737159729004
train gradient:  0.1067761782971852
iteration : 6837
train acc:  0.8046875
train loss:  0.33778101205825806
train gradient:  0.19672650763692245
iteration : 6838
train acc:  0.859375
train loss:  0.3384249210357666
train gradient:  0.24401866371496245
iteration : 6839
train acc:  0.84375
train loss:  0.32834213972091675
train gradient:  0.17843395012868013
iteration : 6840
train acc:  0.875
train loss:  0.3135012090206146
train gradient:  0.16978392498800554
iteration : 6841
train acc:  0.828125
train loss:  0.3577229976654053
train gradient:  0.2820006050764255
iteration : 6842
train acc:  0.859375
train loss:  0.30297961831092834
train gradient:  0.13651106767331236
iteration : 6843
train acc:  0.8671875
train loss:  0.273914635181427
train gradient:  0.13878717223840067
iteration : 6844
train acc:  0.84375
train loss:  0.3153906464576721
train gradient:  0.17428596047672837
iteration : 6845
train acc:  0.8515625
train loss:  0.30785495042800903
train gradient:  0.29314447548513334
iteration : 6846
train acc:  0.8671875
train loss:  0.3273423910140991
train gradient:  0.24800398150681324
iteration : 6847
train acc:  0.8984375
train loss:  0.26444894075393677
train gradient:  0.13215021007819763
iteration : 6848
train acc:  0.828125
train loss:  0.3617858290672302
train gradient:  0.2623498686281555
iteration : 6849
train acc:  0.8984375
train loss:  0.27571648359298706
train gradient:  0.11319311036362148
iteration : 6850
train acc:  0.8203125
train loss:  0.4087209701538086
train gradient:  0.2729085351802673
iteration : 6851
train acc:  0.828125
train loss:  0.44439440965652466
train gradient:  0.2602551481998003
iteration : 6852
train acc:  0.8046875
train loss:  0.48527270555496216
train gradient:  0.32999073665185524
iteration : 6853
train acc:  0.7578125
train loss:  0.45081397891044617
train gradient:  0.23412981502943414
iteration : 6854
train acc:  0.84375
train loss:  0.3698023557662964
train gradient:  0.2004182133218832
iteration : 6855
train acc:  0.875
train loss:  0.3068379759788513
train gradient:  0.1472291051739642
iteration : 6856
train acc:  0.890625
train loss:  0.3265480697154999
train gradient:  0.22057645482097513
iteration : 6857
train acc:  0.8515625
train loss:  0.3272517919540405
train gradient:  0.17895864017410257
iteration : 6858
train acc:  0.890625
train loss:  0.30692318081855774
train gradient:  0.1089099020164926
iteration : 6859
train acc:  0.8203125
train loss:  0.35589975118637085
train gradient:  0.24006157463851455
iteration : 6860
train acc:  0.8359375
train loss:  0.30376529693603516
train gradient:  0.1782869764926073
iteration : 6861
train acc:  0.8515625
train loss:  0.3473775386810303
train gradient:  0.15953734447344226
iteration : 6862
train acc:  0.8515625
train loss:  0.36161714792251587
train gradient:  0.24806081122674473
iteration : 6863
train acc:  0.8203125
train loss:  0.387334942817688
train gradient:  0.16464478916730188
iteration : 6864
train acc:  0.8125
train loss:  0.40118682384490967
train gradient:  0.30802270575125074
iteration : 6865
train acc:  0.8046875
train loss:  0.3845042586326599
train gradient:  0.2626477832955811
iteration : 6866
train acc:  0.8671875
train loss:  0.30875450372695923
train gradient:  0.2904429187305405
iteration : 6867
train acc:  0.8125
train loss:  0.41014614701271057
train gradient:  0.2867186415308143
iteration : 6868
train acc:  0.8359375
train loss:  0.31190210580825806
train gradient:  0.1364717841676142
iteration : 6869
train acc:  0.84375
train loss:  0.3449745178222656
train gradient:  0.25825533425596087
iteration : 6870
train acc:  0.859375
train loss:  0.34831103682518005
train gradient:  0.2471723085179222
iteration : 6871
train acc:  0.8125
train loss:  0.39647233486175537
train gradient:  0.27299985534101423
iteration : 6872
train acc:  0.8203125
train loss:  0.3503165543079376
train gradient:  0.184056667727637
iteration : 6873
train acc:  0.8828125
train loss:  0.329262375831604
train gradient:  0.2082775640509252
iteration : 6874
train acc:  0.78125
train loss:  0.5025730133056641
train gradient:  0.3728064029566106
iteration : 6875
train acc:  0.8671875
train loss:  0.29408979415893555
train gradient:  0.16680886534753042
iteration : 6876
train acc:  0.828125
train loss:  0.37729912996292114
train gradient:  0.3168536068868161
iteration : 6877
train acc:  0.828125
train loss:  0.4028782248497009
train gradient:  0.2657804232223997
iteration : 6878
train acc:  0.859375
train loss:  0.31221991777420044
train gradient:  0.14377667795054427
iteration : 6879
train acc:  0.8515625
train loss:  0.3653550148010254
train gradient:  0.18029321688540192
iteration : 6880
train acc:  0.8359375
train loss:  0.41321924328804016
train gradient:  0.32011562612461203
iteration : 6881
train acc:  0.8671875
train loss:  0.3181361258029938
train gradient:  0.14606171552718428
iteration : 6882
train acc:  0.8984375
train loss:  0.23105260729789734
train gradient:  0.13142450825165955
iteration : 6883
train acc:  0.796875
train loss:  0.38109931349754333
train gradient:  0.30753445899588977
iteration : 6884
train acc:  0.8515625
train loss:  0.3437637686729431
train gradient:  0.1727980282121373
iteration : 6885
train acc:  0.859375
train loss:  0.39512118697166443
train gradient:  0.2571775982058468
iteration : 6886
train acc:  0.8125
train loss:  0.34377336502075195
train gradient:  0.2245438324017455
iteration : 6887
train acc:  0.8359375
train loss:  0.34534594416618347
train gradient:  0.1825604209328087
iteration : 6888
train acc:  0.8203125
train loss:  0.317574679851532
train gradient:  0.16056368098464893
iteration : 6889
train acc:  0.8203125
train loss:  0.3491116762161255
train gradient:  0.18557135160107177
iteration : 6890
train acc:  0.8046875
train loss:  0.43208202719688416
train gradient:  0.3048523941638299
iteration : 6891
train acc:  0.8359375
train loss:  0.37200504541397095
train gradient:  0.2501261673238649
iteration : 6892
train acc:  0.84375
train loss:  0.33318209648132324
train gradient:  0.15508481120206621
iteration : 6893
train acc:  0.875
train loss:  0.3422210216522217
train gradient:  0.15261964684079446
iteration : 6894
train acc:  0.8046875
train loss:  0.45591726899147034
train gradient:  0.39736215401241864
iteration : 6895
train acc:  0.828125
train loss:  0.37624484300613403
train gradient:  0.20595929272345354
iteration : 6896
train acc:  0.875
train loss:  0.3013858199119568
train gradient:  0.17261121981575034
iteration : 6897
train acc:  0.8125
train loss:  0.3358747065067291
train gradient:  0.24085212510407117
iteration : 6898
train acc:  0.8203125
train loss:  0.40406399965286255
train gradient:  0.30658530182459853
iteration : 6899
train acc:  0.8515625
train loss:  0.333954781293869
train gradient:  0.2351154053403251
iteration : 6900
train acc:  0.8515625
train loss:  0.3190224766731262
train gradient:  0.2744956772341253
iteration : 6901
train acc:  0.8359375
train loss:  0.34325122833251953
train gradient:  0.19806647026212085
iteration : 6902
train acc:  0.796875
train loss:  0.3979777693748474
train gradient:  0.24984666045578158
iteration : 6903
train acc:  0.828125
train loss:  0.3997056186199188
train gradient:  0.21789624796176552
iteration : 6904
train acc:  0.8515625
train loss:  0.3071816563606262
train gradient:  0.21523729922662316
iteration : 6905
train acc:  0.8359375
train loss:  0.3434726893901825
train gradient:  0.16157208545400992
iteration : 6906
train acc:  0.8515625
train loss:  0.3588367700576782
train gradient:  0.16659551982255266
iteration : 6907
train acc:  0.84375
train loss:  0.3629878759384155
train gradient:  0.15393345283691917
iteration : 6908
train acc:  0.8671875
train loss:  0.3334909677505493
train gradient:  0.16098518841418635
iteration : 6909
train acc:  0.8671875
train loss:  0.31729382276535034
train gradient:  0.1302723956949046
iteration : 6910
train acc:  0.7890625
train loss:  0.41099637746810913
train gradient:  0.3547825334373558
iteration : 6911
train acc:  0.8984375
train loss:  0.3313691020011902
train gradient:  0.15788523897711637
iteration : 6912
train acc:  0.8359375
train loss:  0.35206279158592224
train gradient:  0.22187321752465344
iteration : 6913
train acc:  0.84375
train loss:  0.3094623386859894
train gradient:  0.16405262191150347
iteration : 6914
train acc:  0.890625
train loss:  0.26379597187042236
train gradient:  0.1246440500988186
iteration : 6915
train acc:  0.828125
train loss:  0.3628707230091095
train gradient:  0.20470487693193704
iteration : 6916
train acc:  0.8515625
train loss:  0.3572271764278412
train gradient:  0.19578029517750795
iteration : 6917
train acc:  0.875
train loss:  0.3222990930080414
train gradient:  0.15277108009034768
iteration : 6918
train acc:  0.90625
train loss:  0.23856186866760254
train gradient:  0.1281118370853897
iteration : 6919
train acc:  0.875
train loss:  0.2994164824485779
train gradient:  0.17378441552815
iteration : 6920
train acc:  0.84375
train loss:  0.3396543860435486
train gradient:  0.3774286341558716
iteration : 6921
train acc:  0.7265625
train loss:  0.42690014839172363
train gradient:  0.2575644306904941
iteration : 6922
train acc:  0.8046875
train loss:  0.4105481207370758
train gradient:  0.24782982153582037
iteration : 6923
train acc:  0.8359375
train loss:  0.37134850025177
train gradient:  0.24642668788564334
iteration : 6924
train acc:  0.7734375
train loss:  0.43276795744895935
train gradient:  0.2503713141084862
iteration : 6925
train acc:  0.7890625
train loss:  0.44390231370925903
train gradient:  0.259615551810058
iteration : 6926
train acc:  0.8828125
train loss:  0.2885529100894928
train gradient:  0.156323315248305
iteration : 6927
train acc:  0.8828125
train loss:  0.26023709774017334
train gradient:  0.09515067973784777
iteration : 6928
train acc:  0.8828125
train loss:  0.3063947558403015
train gradient:  0.14638716094900894
iteration : 6929
train acc:  0.8125
train loss:  0.3605296015739441
train gradient:  0.24993396867444573
iteration : 6930
train acc:  0.84375
train loss:  0.3883671760559082
train gradient:  0.28959697617002783
iteration : 6931
train acc:  0.8203125
train loss:  0.3845688998699188
train gradient:  0.25823730655372734
iteration : 6932
train acc:  0.84375
train loss:  0.34777557849884033
train gradient:  0.2211911278596883
iteration : 6933
train acc:  0.7890625
train loss:  0.4427739083766937
train gradient:  0.36266631906320457
iteration : 6934
train acc:  0.859375
train loss:  0.3309114873409271
train gradient:  0.2596588389375695
iteration : 6935
train acc:  0.8203125
train loss:  0.36963096261024475
train gradient:  0.22541933739532483
iteration : 6936
train acc:  0.8359375
train loss:  0.3345573842525482
train gradient:  0.18163918200671547
iteration : 6937
train acc:  0.84375
train loss:  0.40002259612083435
train gradient:  0.2660336883591303
iteration : 6938
train acc:  0.84375
train loss:  0.37886467576026917
train gradient:  0.19086076834085902
iteration : 6939
train acc:  0.875
train loss:  0.3179982900619507
train gradient:  0.15603785298505252
iteration : 6940
train acc:  0.8046875
train loss:  0.42162325978279114
train gradient:  0.31477707320424164
iteration : 6941
train acc:  0.8359375
train loss:  0.38249820470809937
train gradient:  0.273898008085021
iteration : 6942
train acc:  0.8359375
train loss:  0.4500492215156555
train gradient:  0.3937730894754124
iteration : 6943
train acc:  0.859375
train loss:  0.3049783706665039
train gradient:  0.15009524032420585
iteration : 6944
train acc:  0.8828125
train loss:  0.2866370975971222
train gradient:  0.09569647325790587
iteration : 6945
train acc:  0.8359375
train loss:  0.3420153856277466
train gradient:  0.20120553874082964
iteration : 6946
train acc:  0.890625
train loss:  0.24485176801681519
train gradient:  0.1293713923731643
iteration : 6947
train acc:  0.8828125
train loss:  0.3150979280471802
train gradient:  0.20663470233504283
iteration : 6948
train acc:  0.828125
train loss:  0.39652857184410095
train gradient:  0.17011148466215173
iteration : 6949
train acc:  0.8828125
train loss:  0.3544871211051941
train gradient:  0.22098278537993793
iteration : 6950
train acc:  0.8671875
train loss:  0.33170855045318604
train gradient:  0.20241762785516443
iteration : 6951
train acc:  0.8984375
train loss:  0.32413068413734436
train gradient:  0.22870324720579813
iteration : 6952
train acc:  0.859375
train loss:  0.36268091201782227
train gradient:  0.2096337201946904
iteration : 6953
train acc:  0.8515625
train loss:  0.3197529911994934
train gradient:  0.12761788317054218
iteration : 6954
train acc:  0.8671875
train loss:  0.2845161557197571
train gradient:  0.13649194705384257
iteration : 6955
train acc:  0.8984375
train loss:  0.3382151126861572
train gradient:  0.25751316585839573
iteration : 6956
train acc:  0.8359375
train loss:  0.41351941227912903
train gradient:  0.3526886510199703
iteration : 6957
train acc:  0.8671875
train loss:  0.30687659978866577
train gradient:  0.12335972363232166
iteration : 6958
train acc:  0.8671875
train loss:  0.27828261256217957
train gradient:  0.18574414894654034
iteration : 6959
train acc:  0.8203125
train loss:  0.3946795165538788
train gradient:  0.27194760344194646
iteration : 6960
train acc:  0.875
train loss:  0.32521578669548035
train gradient:  0.3117699854524889
iteration : 6961
train acc:  0.890625
train loss:  0.26078319549560547
train gradient:  0.1319982995380843
iteration : 6962
train acc:  0.90625
train loss:  0.24936291575431824
train gradient:  0.11467660359108066
iteration : 6963
train acc:  0.859375
train loss:  0.2856696844100952
train gradient:  0.12409373686504367
iteration : 6964
train acc:  0.8359375
train loss:  0.33646732568740845
train gradient:  0.19925334834635192
iteration : 6965
train acc:  0.859375
train loss:  0.31534212827682495
train gradient:  0.17803386152435968
iteration : 6966
train acc:  0.8828125
train loss:  0.2975568473339081
train gradient:  0.1659215695845866
iteration : 6967
train acc:  0.859375
train loss:  0.3605705499649048
train gradient:  0.23027510270939971
iteration : 6968
train acc:  0.859375
train loss:  0.3483860492706299
train gradient:  0.16125569585072158
iteration : 6969
train acc:  0.8515625
train loss:  0.41495639085769653
train gradient:  0.25080834216844017
iteration : 6970
train acc:  0.8359375
train loss:  0.3508050739765167
train gradient:  0.1583489126343904
iteration : 6971
train acc:  0.8828125
train loss:  0.2517777681350708
train gradient:  0.10077552211726028
iteration : 6972
train acc:  0.8359375
train loss:  0.3507381081581116
train gradient:  0.20466981222174413
iteration : 6973
train acc:  0.8671875
train loss:  0.36580151319503784
train gradient:  0.20153147490951012
iteration : 6974
train acc:  0.8515625
train loss:  0.28663116693496704
train gradient:  0.13181107105737444
iteration : 6975
train acc:  0.875
train loss:  0.29621684551239014
train gradient:  0.1857320836030793
iteration : 6976
train acc:  0.875
train loss:  0.2642831802368164
train gradient:  0.13030800859812514
iteration : 6977
train acc:  0.875
train loss:  0.27561384439468384
train gradient:  0.16078805935729495
iteration : 6978
train acc:  0.8046875
train loss:  0.4230808615684509
train gradient:  0.3071413345179455
iteration : 6979
train acc:  0.859375
train loss:  0.32911115884780884
train gradient:  0.12853276096092203
iteration : 6980
train acc:  0.8671875
train loss:  0.29577505588531494
train gradient:  0.14977612329933937
iteration : 6981
train acc:  0.8203125
train loss:  0.3941380977630615
train gradient:  0.2242578840848875
iteration : 6982
train acc:  0.8359375
train loss:  0.3504530191421509
train gradient:  0.23081108856671143
iteration : 6983
train acc:  0.875
train loss:  0.34409576654434204
train gradient:  0.16137882365808937
iteration : 6984
train acc:  0.875
train loss:  0.3044334053993225
train gradient:  0.1785819718940475
iteration : 6985
train acc:  0.8671875
train loss:  0.34853217005729675
train gradient:  0.1787818043079647
iteration : 6986
train acc:  0.8984375
train loss:  0.32096803188323975
train gradient:  0.12852674474449033
iteration : 6987
train acc:  0.9375
train loss:  0.19420111179351807
train gradient:  0.0892394205246614
iteration : 6988
train acc:  0.8515625
train loss:  0.3226103186607361
train gradient:  0.14435772893115326
iteration : 6989
train acc:  0.8125
train loss:  0.3420138359069824
train gradient:  0.24871054839918277
iteration : 6990
train acc:  0.875
train loss:  0.2831239104270935
train gradient:  0.12490020279334355
iteration : 6991
train acc:  0.8046875
train loss:  0.3633863925933838
train gradient:  0.17561160785493354
iteration : 6992
train acc:  0.84375
train loss:  0.3485158681869507
train gradient:  0.14361138522300065
iteration : 6993
train acc:  0.859375
train loss:  0.32475924491882324
train gradient:  0.20522597890648114
iteration : 6994
train acc:  0.859375
train loss:  0.32295286655426025
train gradient:  0.22799408659152892
iteration : 6995
train acc:  0.8515625
train loss:  0.3822367191314697
train gradient:  0.18100821256158023
iteration : 6996
train acc:  0.859375
train loss:  0.3387763500213623
train gradient:  0.20000372085775417
iteration : 6997
train acc:  0.8203125
train loss:  0.38254889845848083
train gradient:  0.23836503408055135
iteration : 6998
train acc:  0.9140625
train loss:  0.22317183017730713
train gradient:  0.1067533912614148
iteration : 6999
train acc:  0.8671875
train loss:  0.26961031556129456
train gradient:  0.1772901910097775
iteration : 7000
train acc:  0.875
train loss:  0.3119657039642334
train gradient:  0.10296893606400598
iteration : 7001
train acc:  0.8515625
train loss:  0.34556978940963745
train gradient:  0.2162009176043423
iteration : 7002
train acc:  0.875
train loss:  0.3096303939819336
train gradient:  0.14676088697503273
iteration : 7003
train acc:  0.8125
train loss:  0.4332689642906189
train gradient:  0.2444230887126447
iteration : 7004
train acc:  0.8515625
train loss:  0.35931506752967834
train gradient:  0.2701282982580774
iteration : 7005
train acc:  0.78125
train loss:  0.3917543292045593
train gradient:  0.2624164427919734
iteration : 7006
train acc:  0.8046875
train loss:  0.4088330864906311
train gradient:  0.27209480791400353
iteration : 7007
train acc:  0.890625
train loss:  0.28369656205177307
train gradient:  0.16370235952474604
iteration : 7008
train acc:  0.796875
train loss:  0.3981347382068634
train gradient:  0.2726777043789958
iteration : 7009
train acc:  0.828125
train loss:  0.3620449900627136
train gradient:  0.2308262875010499
iteration : 7010
train acc:  0.734375
train loss:  0.5997021198272705
train gradient:  0.5999413655977865
iteration : 7011
train acc:  0.8125
train loss:  0.3874775171279907
train gradient:  0.25099035730960295
iteration : 7012
train acc:  0.84375
train loss:  0.31186535954475403
train gradient:  0.09294188465453243
iteration : 7013
train acc:  0.8515625
train loss:  0.2814064621925354
train gradient:  0.14421637228652248
iteration : 7014
train acc:  0.796875
train loss:  0.4289180040359497
train gradient:  0.20535891257344047
iteration : 7015
train acc:  0.8671875
train loss:  0.3105902075767517
train gradient:  0.13430446625311282
iteration : 7016
train acc:  0.875
train loss:  0.29920899868011475
train gradient:  0.1574956868175805
iteration : 7017
train acc:  0.828125
train loss:  0.3334345817565918
train gradient:  0.17559184026991542
iteration : 7018
train acc:  0.859375
train loss:  0.3368377089500427
train gradient:  0.16403743318944913
iteration : 7019
train acc:  0.8125
train loss:  0.39428508281707764
train gradient:  0.32566250892782106
iteration : 7020
train acc:  0.875
train loss:  0.3022313117980957
train gradient:  0.13924279862213362
iteration : 7021
train acc:  0.7890625
train loss:  0.43140822649002075
train gradient:  0.27969046259287156
iteration : 7022
train acc:  0.8359375
train loss:  0.29049041867256165
train gradient:  0.09900653087052703
iteration : 7023
train acc:  0.8671875
train loss:  0.3569929003715515
train gradient:  0.19065058284303985
iteration : 7024
train acc:  0.8515625
train loss:  0.37604016065597534
train gradient:  0.19212381433803982
iteration : 7025
train acc:  0.828125
train loss:  0.3555813431739807
train gradient:  0.20331004894723897
iteration : 7026
train acc:  0.890625
train loss:  0.31976884603500366
train gradient:  0.19561787557774252
iteration : 7027
train acc:  0.8359375
train loss:  0.369233101606369
train gradient:  0.19351783703032877
iteration : 7028
train acc:  0.8828125
train loss:  0.2775096595287323
train gradient:  0.1668361581074024
iteration : 7029
train acc:  0.875
train loss:  0.3134913444519043
train gradient:  0.1771495733044468
iteration : 7030
train acc:  0.84375
train loss:  0.32530200481414795
train gradient:  0.1597585095064683
iteration : 7031
train acc:  0.8515625
train loss:  0.2992871403694153
train gradient:  0.23644338521955666
iteration : 7032
train acc:  0.890625
train loss:  0.32763242721557617
train gradient:  0.13703212145832044
iteration : 7033
train acc:  0.84375
train loss:  0.36028796434402466
train gradient:  0.24516365321735226
iteration : 7034
train acc:  0.8359375
train loss:  0.3861910104751587
train gradient:  0.3239153862889132
iteration : 7035
train acc:  0.8515625
train loss:  0.3531595468521118
train gradient:  0.18415548910707213
iteration : 7036
train acc:  0.859375
train loss:  0.3942365050315857
train gradient:  0.16838154320512186
iteration : 7037
train acc:  0.8125
train loss:  0.3809119462966919
train gradient:  0.2831329496413881
iteration : 7038
train acc:  0.8984375
train loss:  0.323819100856781
train gradient:  0.22308270914341852
iteration : 7039
train acc:  0.8359375
train loss:  0.3736060559749603
train gradient:  0.24428328962182427
iteration : 7040
train acc:  0.84375
train loss:  0.3667565882205963
train gradient:  0.20031500939405059
iteration : 7041
train acc:  0.9296875
train loss:  0.23411113023757935
train gradient:  0.1581997314996147
iteration : 7042
train acc:  0.875
train loss:  0.31951481103897095
train gradient:  0.16356666778095463
iteration : 7043
train acc:  0.828125
train loss:  0.40742987394332886
train gradient:  0.27466991580970074
iteration : 7044
train acc:  0.859375
train loss:  0.30430352687835693
train gradient:  0.13617703167916667
iteration : 7045
train acc:  0.828125
train loss:  0.3697769045829773
train gradient:  0.1903913773411448
iteration : 7046
train acc:  0.890625
train loss:  0.2571214437484741
train gradient:  0.13901271593798553
iteration : 7047
train acc:  0.765625
train loss:  0.49978798627853394
train gradient:  0.3443655826458167
iteration : 7048
train acc:  0.8671875
train loss:  0.32241290807724
train gradient:  0.21686058371924793
iteration : 7049
train acc:  0.78125
train loss:  0.39329999685287476
train gradient:  0.19043984175256262
iteration : 7050
train acc:  0.8671875
train loss:  0.25229111313819885
train gradient:  0.12655020253875926
iteration : 7051
train acc:  0.8359375
train loss:  0.45984509587287903
train gradient:  0.3121873968686413
iteration : 7052
train acc:  0.8203125
train loss:  0.35736173391342163
train gradient:  0.2037592568350986
iteration : 7053
train acc:  0.828125
train loss:  0.3805862069129944
train gradient:  0.25029729838175935
iteration : 7054
train acc:  0.8671875
train loss:  0.2915652394294739
train gradient:  0.2019718589631534
iteration : 7055
train acc:  0.8203125
train loss:  0.3821435570716858
train gradient:  0.2024633376102472
iteration : 7056
train acc:  0.859375
train loss:  0.3464201092720032
train gradient:  0.18047231670529085
iteration : 7057
train acc:  0.8515625
train loss:  0.3843541145324707
train gradient:  0.26917022454970363
iteration : 7058
train acc:  0.859375
train loss:  0.34878653287887573
train gradient:  0.23179154737094132
iteration : 7059
train acc:  0.8828125
train loss:  0.3253377676010132
train gradient:  0.14830808464945278
iteration : 7060
train acc:  0.84375
train loss:  0.38020527362823486
train gradient:  0.23316725432035365
iteration : 7061
train acc:  0.8515625
train loss:  0.35015639662742615
train gradient:  0.12096473572486097
iteration : 7062
train acc:  0.875
train loss:  0.3187791109085083
train gradient:  0.12200586784499158
iteration : 7063
train acc:  0.8125
train loss:  0.3827148675918579
train gradient:  0.19759579526162302
iteration : 7064
train acc:  0.890625
train loss:  0.25810909271240234
train gradient:  0.13102311913988884
iteration : 7065
train acc:  0.84375
train loss:  0.34524238109588623
train gradient:  0.1893039910448012
iteration : 7066
train acc:  0.859375
train loss:  0.32025963068008423
train gradient:  0.15982277058434535
iteration : 7067
train acc:  0.8359375
train loss:  0.37488996982574463
train gradient:  0.21110784097130608
iteration : 7068
train acc:  0.78125
train loss:  0.4117865562438965
train gradient:  0.29581085469633295
iteration : 7069
train acc:  0.8515625
train loss:  0.35515379905700684
train gradient:  0.19277432588498927
iteration : 7070
train acc:  0.8359375
train loss:  0.3398068845272064
train gradient:  0.19266547345696144
iteration : 7071
train acc:  0.8828125
train loss:  0.30987727642059326
train gradient:  0.18346113041552115
iteration : 7072
train acc:  0.90625
train loss:  0.2674480080604553
train gradient:  0.08888528526500489
iteration : 7073
train acc:  0.8671875
train loss:  0.3244667053222656
train gradient:  0.1609566791105405
iteration : 7074
train acc:  0.8984375
train loss:  0.2890164852142334
train gradient:  0.12965969556985318
iteration : 7075
train acc:  0.84375
train loss:  0.39840781688690186
train gradient:  0.2673864397553223
iteration : 7076
train acc:  0.84375
train loss:  0.34883779287338257
train gradient:  0.24366060132627057
iteration : 7077
train acc:  0.7734375
train loss:  0.44909489154815674
train gradient:  0.3540011603386359
iteration : 7078
train acc:  0.90625
train loss:  0.23754416406154633
train gradient:  0.1294110938764685
iteration : 7079
train acc:  0.875
train loss:  0.2933436632156372
train gradient:  0.16513776332457966
iteration : 7080
train acc:  0.8515625
train loss:  0.3287453353404999
train gradient:  0.15344505132160463
iteration : 7081
train acc:  0.8515625
train loss:  0.4031451344490051
train gradient:  0.3640402361797633
iteration : 7082
train acc:  0.8359375
train loss:  0.3420824408531189
train gradient:  0.18086822533728916
iteration : 7083
train acc:  0.8515625
train loss:  0.3159010112285614
train gradient:  0.16229382832783734
iteration : 7084
train acc:  0.8984375
train loss:  0.3115415871143341
train gradient:  0.18562501985492663
iteration : 7085
train acc:  0.8203125
train loss:  0.34574031829833984
train gradient:  0.2322466050289291
iteration : 7086
train acc:  0.8515625
train loss:  0.3247968256473541
train gradient:  0.19985907966916872
iteration : 7087
train acc:  0.8046875
train loss:  0.381173700094223
train gradient:  0.18621327309983152
iteration : 7088
train acc:  0.8203125
train loss:  0.3403950035572052
train gradient:  0.21368446108666173
iteration : 7089
train acc:  0.84375
train loss:  0.3038007616996765
train gradient:  0.1636688893945606
iteration : 7090
train acc:  0.859375
train loss:  0.2577715814113617
train gradient:  0.10981774510472857
iteration : 7091
train acc:  0.9140625
train loss:  0.31998151540756226
train gradient:  0.1564787607754625
iteration : 7092
train acc:  0.7578125
train loss:  0.43395668268203735
train gradient:  0.22246629270163548
iteration : 7093
train acc:  0.8125
train loss:  0.37158727645874023
train gradient:  0.6685083175974371
iteration : 7094
train acc:  0.84375
train loss:  0.3070438504219055
train gradient:  0.10770602550216465
iteration : 7095
train acc:  0.84375
train loss:  0.3927869200706482
train gradient:  0.2140375258426713
iteration : 7096
train acc:  0.84375
train loss:  0.33470532298088074
train gradient:  0.1788371238024557
iteration : 7097
train acc:  0.8828125
train loss:  0.29507654905319214
train gradient:  0.602087733779189
iteration : 7098
train acc:  0.8515625
train loss:  0.35579487681388855
train gradient:  0.18950809718792616
iteration : 7099
train acc:  0.78125
train loss:  0.3870369791984558
train gradient:  0.25083892567333693
iteration : 7100
train acc:  0.8359375
train loss:  0.38308393955230713
train gradient:  0.2435235740509536
iteration : 7101
train acc:  0.859375
train loss:  0.26364266872406006
train gradient:  0.16378872148418688
iteration : 7102
train acc:  0.8125
train loss:  0.37933290004730225
train gradient:  0.36102917782967425
iteration : 7103
train acc:  0.8359375
train loss:  0.3711097240447998
train gradient:  0.17496670588323598
iteration : 7104
train acc:  0.859375
train loss:  0.3737488389015198
train gradient:  0.20796142252267225
iteration : 7105
train acc:  0.8359375
train loss:  0.3893764019012451
train gradient:  0.18774573729783467
iteration : 7106
train acc:  0.875
train loss:  0.3043259382247925
train gradient:  0.127501980239025
iteration : 7107
train acc:  0.828125
train loss:  0.36359381675720215
train gradient:  0.14394246446942793
iteration : 7108
train acc:  0.875
train loss:  0.2710643708705902
train gradient:  0.11696241824121352
iteration : 7109
train acc:  0.8359375
train loss:  0.35531410574913025
train gradient:  0.1470868797231703
iteration : 7110
train acc:  0.7734375
train loss:  0.40368321537971497
train gradient:  0.29782085250266493
iteration : 7111
train acc:  0.8203125
train loss:  0.3489949107170105
train gradient:  0.1654694037524519
iteration : 7112
train acc:  0.8046875
train loss:  0.4153617024421692
train gradient:  0.2770574780648706
iteration : 7113
train acc:  0.890625
train loss:  0.3246345818042755
train gradient:  0.15560232177024724
iteration : 7114
train acc:  0.8515625
train loss:  0.3294440507888794
train gradient:  0.21122182424924535
iteration : 7115
train acc:  0.875
train loss:  0.35422149300575256
train gradient:  0.21942739251682813
iteration : 7116
train acc:  0.84375
train loss:  0.36549270153045654
train gradient:  0.1636225927590209
iteration : 7117
train acc:  0.859375
train loss:  0.2898256480693817
train gradient:  0.11122535589258
iteration : 7118
train acc:  0.8828125
train loss:  0.3063103258609772
train gradient:  0.19330814669487037
iteration : 7119
train acc:  0.828125
train loss:  0.3507236838340759
train gradient:  0.19619705698149523
iteration : 7120
train acc:  0.8125
train loss:  0.33782076835632324
train gradient:  0.1307647456088279
iteration : 7121
train acc:  0.84375
train loss:  0.30554842948913574
train gradient:  0.18523546769388097
iteration : 7122
train acc:  0.84375
train loss:  0.2845342755317688
train gradient:  0.1383909252042802
iteration : 7123
train acc:  0.8671875
train loss:  0.32383543252944946
train gradient:  0.23950168007720765
iteration : 7124
train acc:  0.859375
train loss:  0.29735857248306274
train gradient:  0.19913696663157804
iteration : 7125
train acc:  0.8671875
train loss:  0.3095322847366333
train gradient:  0.12737780053339634
iteration : 7126
train acc:  0.859375
train loss:  0.280721515417099
train gradient:  0.18149389030439078
iteration : 7127
train acc:  0.8515625
train loss:  0.29149919748306274
train gradient:  0.20813057038688498
iteration : 7128
train acc:  0.78125
train loss:  0.3946482241153717
train gradient:  0.28992441375385397
iteration : 7129
train acc:  0.796875
train loss:  0.4146444797515869
train gradient:  0.20413707422879618
iteration : 7130
train acc:  0.8828125
train loss:  0.31258225440979004
train gradient:  0.19005335640564752
iteration : 7131
train acc:  0.8125
train loss:  0.35596752166748047
train gradient:  0.2207465114920215
iteration : 7132
train acc:  0.828125
train loss:  0.3553823232650757
train gradient:  0.1956227241507541
iteration : 7133
train acc:  0.8359375
train loss:  0.37650445103645325
train gradient:  0.19805231222628072
iteration : 7134
train acc:  0.8359375
train loss:  0.37691205739974976
train gradient:  0.35244289815407737
iteration : 7135
train acc:  0.8828125
train loss:  0.3200398087501526
train gradient:  0.1791217270436194
iteration : 7136
train acc:  0.828125
train loss:  0.3832792639732361
train gradient:  0.24324967445259094
iteration : 7137
train acc:  0.84375
train loss:  0.37656137347221375
train gradient:  0.27044349049363015
iteration : 7138
train acc:  0.8671875
train loss:  0.283439040184021
train gradient:  0.111913681003148
iteration : 7139
train acc:  0.8125
train loss:  0.40088552236557007
train gradient:  0.17969474493972878
iteration : 7140
train acc:  0.8359375
train loss:  0.281038761138916
train gradient:  0.12426968456405443
iteration : 7141
train acc:  0.90625
train loss:  0.27082425355911255
train gradient:  0.11884232620708347
iteration : 7142
train acc:  0.8828125
train loss:  0.3355019688606262
train gradient:  0.14074416173611232
iteration : 7143
train acc:  0.859375
train loss:  0.3748127520084381
train gradient:  0.24019747574886668
iteration : 7144
train acc:  0.859375
train loss:  0.3025111258029938
train gradient:  0.18804613574020085
iteration : 7145
train acc:  0.84375
train loss:  0.29744768142700195
train gradient:  0.11559411608859578
iteration : 7146
train acc:  0.8515625
train loss:  0.34342435002326965
train gradient:  0.17946429035517694
iteration : 7147
train acc:  0.828125
train loss:  0.3993871510028839
train gradient:  0.2212712610313594
iteration : 7148
train acc:  0.84375
train loss:  0.3256518542766571
train gradient:  0.20153479873366714
iteration : 7149
train acc:  0.84375
train loss:  0.3244408369064331
train gradient:  0.1402151380463467
iteration : 7150
train acc:  0.8359375
train loss:  0.3419138789176941
train gradient:  0.2060236826090369
iteration : 7151
train acc:  0.8984375
train loss:  0.3032627999782562
train gradient:  0.20200408638857764
iteration : 7152
train acc:  0.8125
train loss:  0.39452412724494934
train gradient:  0.24495032078356127
iteration : 7153
train acc:  0.8359375
train loss:  0.3408298194408417
train gradient:  0.15747681085989176
iteration : 7154
train acc:  0.8671875
train loss:  0.36276769638061523
train gradient:  0.1676671642989482
iteration : 7155
train acc:  0.8515625
train loss:  0.4098127484321594
train gradient:  0.2113866518153868
iteration : 7156
train acc:  0.8515625
train loss:  0.35619354248046875
train gradient:  0.2692434868163592
iteration : 7157
train acc:  0.7890625
train loss:  0.4259440302848816
train gradient:  0.2471374200943212
iteration : 7158
train acc:  0.84375
train loss:  0.328336238861084
train gradient:  0.15878801939166703
iteration : 7159
train acc:  0.9140625
train loss:  0.244439035654068
train gradient:  0.0959474596055958
iteration : 7160
train acc:  0.8984375
train loss:  0.3119874596595764
train gradient:  0.13064987170419623
iteration : 7161
train acc:  0.8203125
train loss:  0.36260417103767395
train gradient:  0.18404497272597198
iteration : 7162
train acc:  0.8515625
train loss:  0.32737332582473755
train gradient:  0.22102014289743627
iteration : 7163
train acc:  0.8671875
train loss:  0.2951112389564514
train gradient:  0.1499239930694688
iteration : 7164
train acc:  0.875
train loss:  0.24850864708423615
train gradient:  0.14207035790207306
iteration : 7165
train acc:  0.859375
train loss:  0.3403380215167999
train gradient:  0.23012370331273735
iteration : 7166
train acc:  0.875
train loss:  0.3108900785446167
train gradient:  0.10634014213474732
iteration : 7167
train acc:  0.828125
train loss:  0.3257487714290619
train gradient:  0.16936776767634304
iteration : 7168
train acc:  0.8359375
train loss:  0.39013999700546265
train gradient:  0.2477748396847988
iteration : 7169
train acc:  0.875
train loss:  0.3450653553009033
train gradient:  0.17444553071895924
iteration : 7170
train acc:  0.8515625
train loss:  0.34332162141799927
train gradient:  0.15992047599600734
iteration : 7171
train acc:  0.8125
train loss:  0.4037175476551056
train gradient:  0.2851975335102526
iteration : 7172
train acc:  0.828125
train loss:  0.3801557421684265
train gradient:  0.3014901760372863
iteration : 7173
train acc:  0.875
train loss:  0.2897752523422241
train gradient:  0.11826078246052539
iteration : 7174
train acc:  0.84375
train loss:  0.3996470272541046
train gradient:  0.2781358172449627
iteration : 7175
train acc:  0.8828125
train loss:  0.2762044668197632
train gradient:  0.11283139292494705
iteration : 7176
train acc:  0.859375
train loss:  0.3064157962799072
train gradient:  0.18599815392616292
iteration : 7177
train acc:  0.8515625
train loss:  0.3774851858615875
train gradient:  0.1668611663772508
iteration : 7178
train acc:  0.828125
train loss:  0.33131277561187744
train gradient:  0.20000232326765052
iteration : 7179
train acc:  0.8828125
train loss:  0.2956066131591797
train gradient:  0.21145922154228425
iteration : 7180
train acc:  0.8125
train loss:  0.4278774559497833
train gradient:  0.3114266025297194
iteration : 7181
train acc:  0.828125
train loss:  0.34237080812454224
train gradient:  0.1953921434676705
iteration : 7182
train acc:  0.8125
train loss:  0.37176403403282166
train gradient:  0.2253622881044225
iteration : 7183
train acc:  0.8515625
train loss:  0.37294241786003113
train gradient:  0.2629020540309836
iteration : 7184
train acc:  0.7890625
train loss:  0.3877590596675873
train gradient:  0.309864575315659
iteration : 7185
train acc:  0.8671875
train loss:  0.3610549569129944
train gradient:  0.13653708917018068
iteration : 7186
train acc:  0.859375
train loss:  0.30383309721946716
train gradient:  0.2698971352657234
iteration : 7187
train acc:  0.859375
train loss:  0.34631186723709106
train gradient:  0.20600119680089604
iteration : 7188
train acc:  0.875
train loss:  0.31896471977233887
train gradient:  0.18453552995392586
iteration : 7189
train acc:  0.859375
train loss:  0.36202043294906616
train gradient:  0.19340643198287283
iteration : 7190
train acc:  0.84375
train loss:  0.3599591553211212
train gradient:  0.23205935620233872
iteration : 7191
train acc:  0.8515625
train loss:  0.34155234694480896
train gradient:  0.16133995523788536
iteration : 7192
train acc:  0.859375
train loss:  0.341496080160141
train gradient:  0.20189045645148448
iteration : 7193
train acc:  0.8671875
train loss:  0.362216591835022
train gradient:  0.19971808990200457
iteration : 7194
train acc:  0.875
train loss:  0.31211328506469727
train gradient:  0.23385970605594053
iteration : 7195
train acc:  0.8203125
train loss:  0.3254860043525696
train gradient:  0.15435181599647305
iteration : 7196
train acc:  0.859375
train loss:  0.31603941321372986
train gradient:  0.20115998321242184
iteration : 7197
train acc:  0.84375
train loss:  0.39146631956100464
train gradient:  0.23866478714967693
iteration : 7198
train acc:  0.8359375
train loss:  0.3304097056388855
train gradient:  0.23377505469641305
iteration : 7199
train acc:  0.75
train loss:  0.47985541820526123
train gradient:  0.4354879722103145
iteration : 7200
train acc:  0.859375
train loss:  0.3383623957633972
train gradient:  0.1738138777026222
iteration : 7201
train acc:  0.859375
train loss:  0.3256406784057617
train gradient:  0.23523410334429906
iteration : 7202
train acc:  0.8515625
train loss:  0.3188285827636719
train gradient:  0.13591084887817623
iteration : 7203
train acc:  0.84375
train loss:  0.4021330177783966
train gradient:  0.2051282140690795
iteration : 7204
train acc:  0.8125
train loss:  0.3988642692565918
train gradient:  0.27186335467215655
iteration : 7205
train acc:  0.8671875
train loss:  0.385381281375885
train gradient:  0.29125300278722865
iteration : 7206
train acc:  0.859375
train loss:  0.3765745162963867
train gradient:  0.2671095332547867
iteration : 7207
train acc:  0.8359375
train loss:  0.3715307116508484
train gradient:  0.1943854587973209
iteration : 7208
train acc:  0.8515625
train loss:  0.3153616189956665
train gradient:  0.1587054656950213
iteration : 7209
train acc:  0.8125
train loss:  0.42886877059936523
train gradient:  0.24347814143059066
iteration : 7210
train acc:  0.8359375
train loss:  0.4061722755432129
train gradient:  0.3823665522774334
iteration : 7211
train acc:  0.8125
train loss:  0.4107092618942261
train gradient:  0.18544150454917607
iteration : 7212
train acc:  0.8359375
train loss:  0.39618346095085144
train gradient:  0.29963585618713795
iteration : 7213
train acc:  0.828125
train loss:  0.413194477558136
train gradient:  0.2643478809148523
iteration : 7214
train acc:  0.875
train loss:  0.2879617214202881
train gradient:  0.13385438364347738
iteration : 7215
train acc:  0.796875
train loss:  0.4256057143211365
train gradient:  0.2580290089695163
iteration : 7216
train acc:  0.8359375
train loss:  0.33949804306030273
train gradient:  0.25738564156492577
iteration : 7217
train acc:  0.859375
train loss:  0.31773829460144043
train gradient:  0.13558474451987587
iteration : 7218
train acc:  0.890625
train loss:  0.2874782383441925
train gradient:  0.10012914161051815
iteration : 7219
train acc:  0.828125
train loss:  0.3557817339897156
train gradient:  0.20080060824257884
iteration : 7220
train acc:  0.8203125
train loss:  0.3677675724029541
train gradient:  0.17399205549409721
iteration : 7221
train acc:  0.8515625
train loss:  0.3441099226474762
train gradient:  0.14600512907847385
iteration : 7222
train acc:  0.8984375
train loss:  0.2851024568080902
train gradient:  0.10585288175204166
iteration : 7223
train acc:  0.8515625
train loss:  0.37286505103111267
train gradient:  0.23965995106376758
iteration : 7224
train acc:  0.8828125
train loss:  0.29950135946273804
train gradient:  0.16220825611379264
iteration : 7225
train acc:  0.8671875
train loss:  0.30528339743614197
train gradient:  0.13064734668095285
iteration : 7226
train acc:  0.8515625
train loss:  0.2880418300628662
train gradient:  0.13550705091447407
iteration : 7227
train acc:  0.84375
train loss:  0.38427746295928955
train gradient:  0.28060612658317763
iteration : 7228
train acc:  0.84375
train loss:  0.32788217067718506
train gradient:  0.19452298959851178
iteration : 7229
train acc:  0.859375
train loss:  0.3606106638908386
train gradient:  0.24853793146876152
iteration : 7230
train acc:  0.84375
train loss:  0.36806541681289673
train gradient:  0.24024457075695743
iteration : 7231
train acc:  0.84375
train loss:  0.334201842546463
train gradient:  0.13044441837590579
iteration : 7232
train acc:  0.84375
train loss:  0.3662591576576233
train gradient:  0.25520565813568935
iteration : 7233
train acc:  0.8515625
train loss:  0.3539341688156128
train gradient:  0.21227577979558887
iteration : 7234
train acc:  0.8515625
train loss:  0.3616552948951721
train gradient:  0.16942017676782212
iteration : 7235
train acc:  0.7734375
train loss:  0.4504726529121399
train gradient:  0.264607376583895
iteration : 7236
train acc:  0.8046875
train loss:  0.3813287019729614
train gradient:  0.2502442714302557
iteration : 7237
train acc:  0.8671875
train loss:  0.3270547688007355
train gradient:  0.18479527046861832
iteration : 7238
train acc:  0.828125
train loss:  0.3684111535549164
train gradient:  0.25851519890639335
iteration : 7239
train acc:  0.859375
train loss:  0.38781484961509705
train gradient:  0.2839671840117404
iteration : 7240
train acc:  0.890625
train loss:  0.34608107805252075
train gradient:  0.21354135926032242
iteration : 7241
train acc:  0.8828125
train loss:  0.35038402676582336
train gradient:  0.22119079884293885
iteration : 7242
train acc:  0.8125
train loss:  0.376426637172699
train gradient:  0.19762981917280692
iteration : 7243
train acc:  0.8359375
train loss:  0.36381101608276367
train gradient:  0.20799438266586756
iteration : 7244
train acc:  0.8515625
train loss:  0.342384934425354
train gradient:  0.18389058532420438
iteration : 7245
train acc:  0.8515625
train loss:  0.36923426389694214
train gradient:  0.21099041300452587
iteration : 7246
train acc:  0.78125
train loss:  0.43360042572021484
train gradient:  0.250246971775029
iteration : 7247
train acc:  0.7890625
train loss:  0.4070836007595062
train gradient:  0.23514984362829003
iteration : 7248
train acc:  0.8828125
train loss:  0.24375391006469727
train gradient:  0.10598963372391672
iteration : 7249
train acc:  0.90625
train loss:  0.2457590252161026
train gradient:  0.10654232948162856
iteration : 7250
train acc:  0.8203125
train loss:  0.2923903465270996
train gradient:  0.21245523920642273
iteration : 7251
train acc:  0.8359375
train loss:  0.39609047770500183
train gradient:  0.22128067130780776
iteration : 7252
train acc:  0.8359375
train loss:  0.3419564366340637
train gradient:  0.14584360946262473
iteration : 7253
train acc:  0.84375
train loss:  0.37272608280181885
train gradient:  0.2685459100247067
iteration : 7254
train acc:  0.828125
train loss:  0.35484811663627625
train gradient:  0.15420166962136084
iteration : 7255
train acc:  0.8359375
train loss:  0.35355043411254883
train gradient:  0.22061791991329066
iteration : 7256
train acc:  0.859375
train loss:  0.3522225022315979
train gradient:  0.16218231844567854
iteration : 7257
train acc:  0.859375
train loss:  0.326194703578949
train gradient:  0.17610128946590375
iteration : 7258
train acc:  0.828125
train loss:  0.3884723484516144
train gradient:  0.19893525961191605
iteration : 7259
train acc:  0.8125
train loss:  0.3723510503768921
train gradient:  0.1824914962461884
iteration : 7260
train acc:  0.8359375
train loss:  0.3612516522407532
train gradient:  0.19475533091026978
iteration : 7261
train acc:  0.8125
train loss:  0.3721904158592224
train gradient:  0.18510460640530269
iteration : 7262
train acc:  0.8671875
train loss:  0.3344299793243408
train gradient:  0.13151679729516685
iteration : 7263
train acc:  0.8203125
train loss:  0.35219287872314453
train gradient:  0.21559965124750818
iteration : 7264
train acc:  0.890625
train loss:  0.2705458998680115
train gradient:  0.12240014583781877
iteration : 7265
train acc:  0.890625
train loss:  0.3003796637058258
train gradient:  0.1711287604747095
iteration : 7266
train acc:  0.8671875
train loss:  0.3315664529800415
train gradient:  0.17736368279217166
iteration : 7267
train acc:  0.796875
train loss:  0.42009487748146057
train gradient:  0.23108941325758836
iteration : 7268
train acc:  0.8359375
train loss:  0.34051835536956787
train gradient:  0.23171898011220798
iteration : 7269
train acc:  0.8515625
train loss:  0.30461767315864563
train gradient:  0.1566880528196623
iteration : 7270
train acc:  0.8203125
train loss:  0.38681918382644653
train gradient:  0.20234589267537711
iteration : 7271
train acc:  0.9140625
train loss:  0.24657410383224487
train gradient:  0.15674312519361563
iteration : 7272
train acc:  0.84375
train loss:  0.3628315329551697
train gradient:  0.19630182578294875
iteration : 7273
train acc:  0.890625
train loss:  0.3010022044181824
train gradient:  0.20702640255016075
iteration : 7274
train acc:  0.8984375
train loss:  0.27532958984375
train gradient:  0.11865469551238306
iteration : 7275
train acc:  0.875
train loss:  0.29746824502944946
train gradient:  0.1377203369855053
iteration : 7276
train acc:  0.875
train loss:  0.31977084279060364
train gradient:  0.14018031312975193
iteration : 7277
train acc:  0.8515625
train loss:  0.35492658615112305
train gradient:  0.161546163203945
iteration : 7278
train acc:  0.765625
train loss:  0.4141294062137604
train gradient:  0.21827651251455787
iteration : 7279
train acc:  0.890625
train loss:  0.28714412450790405
train gradient:  0.1313063823289629
iteration : 7280
train acc:  0.8671875
train loss:  0.31034961342811584
train gradient:  0.19292166797853535
iteration : 7281
train acc:  0.875
train loss:  0.2889968752861023
train gradient:  0.17828308368240145
iteration : 7282
train acc:  0.8515625
train loss:  0.32998237013816833
train gradient:  0.1589582756182144
iteration : 7283
train acc:  0.8515625
train loss:  0.3091847002506256
train gradient:  0.12539483261957685
iteration : 7284
train acc:  0.84375
train loss:  0.37467771768569946
train gradient:  0.21712015678921
iteration : 7285
train acc:  0.8359375
train loss:  0.3215043544769287
train gradient:  0.17535661692186427
iteration : 7286
train acc:  0.78125
train loss:  0.454621821641922
train gradient:  0.320775951814842
iteration : 7287
train acc:  0.8359375
train loss:  0.3608364462852478
train gradient:  0.2077297868846493
iteration : 7288
train acc:  0.8984375
train loss:  0.30243945121765137
train gradient:  0.1363889885927661
iteration : 7289
train acc:  0.875
train loss:  0.2793619632720947
train gradient:  0.0894344599873373
iteration : 7290
train acc:  0.8359375
train loss:  0.3476410508155823
train gradient:  0.2502808152776124
iteration : 7291
train acc:  0.859375
train loss:  0.3987022638320923
train gradient:  0.23854892629442448
iteration : 7292
train acc:  0.859375
train loss:  0.3659310042858124
train gradient:  0.17994389809049738
iteration : 7293
train acc:  0.8671875
train loss:  0.3597572445869446
train gradient:  0.3015233560280443
iteration : 7294
train acc:  0.8828125
train loss:  0.28169822692871094
train gradient:  0.13008568074846466
iteration : 7295
train acc:  0.8125
train loss:  0.3871183395385742
train gradient:  0.25727246471109094
iteration : 7296
train acc:  0.8984375
train loss:  0.28379344940185547
train gradient:  0.13484426179245176
iteration : 7297
train acc:  0.7890625
train loss:  0.380570650100708
train gradient:  0.25037677324537305
iteration : 7298
train acc:  0.8671875
train loss:  0.3230847120285034
train gradient:  0.2467348840404558
iteration : 7299
train acc:  0.90625
train loss:  0.30322182178497314
train gradient:  0.1814271631557551
iteration : 7300
train acc:  0.8203125
train loss:  0.4169403910636902
train gradient:  0.3742862702578022
iteration : 7301
train acc:  0.859375
train loss:  0.2969198226928711
train gradient:  0.2208294896122376
iteration : 7302
train acc:  0.875
train loss:  0.3284699320793152
train gradient:  0.24605839064815677
iteration : 7303
train acc:  0.859375
train loss:  0.3630261719226837
train gradient:  0.24104399319750305
iteration : 7304
train acc:  0.8125
train loss:  0.4189646542072296
train gradient:  0.21493778558119683
iteration : 7305
train acc:  0.796875
train loss:  0.43133068084716797
train gradient:  0.2254337732041358
iteration : 7306
train acc:  0.8515625
train loss:  0.308937668800354
train gradient:  0.13194816861226105
iteration : 7307
train acc:  0.828125
train loss:  0.36556416749954224
train gradient:  0.22043457808504005
iteration : 7308
train acc:  0.859375
train loss:  0.3382452726364136
train gradient:  0.13420698147931015
iteration : 7309
train acc:  0.859375
train loss:  0.37108054757118225
train gradient:  0.2144304120481537
iteration : 7310
train acc:  0.8671875
train loss:  0.3185693025588989
train gradient:  0.18678637176903864
iteration : 7311
train acc:  0.8828125
train loss:  0.285489022731781
train gradient:  0.17782921738001495
iteration : 7312
train acc:  0.890625
train loss:  0.32591670751571655
train gradient:  0.16278823933793357
iteration : 7313
train acc:  0.9140625
train loss:  0.2660003900527954
train gradient:  0.13471616742922438
iteration : 7314
train acc:  0.84375
train loss:  0.3337154984474182
train gradient:  0.23784831254240568
iteration : 7315
train acc:  0.8046875
train loss:  0.37071844935417175
train gradient:  0.18587309931471335
iteration : 7316
train acc:  0.84375
train loss:  0.2998703122138977
train gradient:  0.19585618556903872
iteration : 7317
train acc:  0.890625
train loss:  0.2932894825935364
train gradient:  0.13700449521255126
iteration : 7318
train acc:  0.859375
train loss:  0.37670576572418213
train gradient:  0.19770799995445432
iteration : 7319
train acc:  0.8359375
train loss:  0.36484670639038086
train gradient:  0.24488519300346734
iteration : 7320
train acc:  0.84375
train loss:  0.3646104335784912
train gradient:  0.1648537712235839
iteration : 7321
train acc:  0.9296875
train loss:  0.21229952573776245
train gradient:  0.1356951020952682
iteration : 7322
train acc:  0.84375
train loss:  0.2972075343132019
train gradient:  0.1453862560220348
iteration : 7323
train acc:  0.8515625
train loss:  0.3287581205368042
train gradient:  0.25369244465332347
iteration : 7324
train acc:  0.875
train loss:  0.28249645233154297
train gradient:  0.17687427023651298
iteration : 7325
train acc:  0.8125
train loss:  0.3591487407684326
train gradient:  0.17835285365445902
iteration : 7326
train acc:  0.8984375
train loss:  0.2905016839504242
train gradient:  0.11429958377972457
iteration : 7327
train acc:  0.8125
train loss:  0.38719475269317627
train gradient:  0.20378706521334977
iteration : 7328
train acc:  0.8828125
train loss:  0.2709202468395233
train gradient:  0.1085444070436685
iteration : 7329
train acc:  0.8359375
train loss:  0.31488606333732605
train gradient:  0.1385312613471003
iteration : 7330
train acc:  0.8203125
train loss:  0.3595888316631317
train gradient:  0.2619205188207209
iteration : 7331
train acc:  0.84375
train loss:  0.337637722492218
train gradient:  0.17146744395297278
iteration : 7332
train acc:  0.8125
train loss:  0.3855953514575958
train gradient:  0.20696749703095724
iteration : 7333
train acc:  0.8203125
train loss:  0.3382185697555542
train gradient:  0.17938445012860024
iteration : 7334
train acc:  0.859375
train loss:  0.395297110080719
train gradient:  0.25675476123395036
iteration : 7335
train acc:  0.8671875
train loss:  0.302948921918869
train gradient:  0.20818271620381745
iteration : 7336
train acc:  0.8203125
train loss:  0.4111795425415039
train gradient:  0.2636218252673627
iteration : 7337
train acc:  0.8359375
train loss:  0.3373730480670929
train gradient:  0.20566056682715106
iteration : 7338
train acc:  0.8671875
train loss:  0.2932780086994171
train gradient:  0.15642805834969972
iteration : 7339
train acc:  0.8515625
train loss:  0.28556954860687256
train gradient:  0.13067878063099317
iteration : 7340
train acc:  0.890625
train loss:  0.2506161630153656
train gradient:  0.11099075313645798
iteration : 7341
train acc:  0.8671875
train loss:  0.3019106984138489
train gradient:  0.16403213040359932
iteration : 7342
train acc:  0.859375
train loss:  0.31629273295402527
train gradient:  0.2544842422168837
iteration : 7343
train acc:  0.859375
train loss:  0.3398090600967407
train gradient:  0.20390334953102043
iteration : 7344
train acc:  0.8359375
train loss:  0.39860740303993225
train gradient:  0.2975959160872744
iteration : 7345
train acc:  0.859375
train loss:  0.38764554262161255
train gradient:  0.2238231153910083
iteration : 7346
train acc:  0.8828125
train loss:  0.2874007821083069
train gradient:  0.14040786850726594
iteration : 7347
train acc:  0.84375
train loss:  0.3297187089920044
train gradient:  0.20542685982702327
iteration : 7348
train acc:  0.796875
train loss:  0.4598056972026825
train gradient:  0.41225130625329237
iteration : 7349
train acc:  0.84375
train loss:  0.31821754574775696
train gradient:  0.18068644286079508
iteration : 7350
train acc:  0.8359375
train loss:  0.3548462390899658
train gradient:  0.18256138550275633
iteration : 7351
train acc:  0.84375
train loss:  0.3444080948829651
train gradient:  0.3866765269945209
iteration : 7352
train acc:  0.8359375
train loss:  0.40294331312179565
train gradient:  0.3000391241605074
iteration : 7353
train acc:  0.8515625
train loss:  0.40529000759124756
train gradient:  0.22139427089076705
iteration : 7354
train acc:  0.8515625
train loss:  0.35443806648254395
train gradient:  0.20526159575867864
iteration : 7355
train acc:  0.84375
train loss:  0.34292834997177124
train gradient:  0.1830081119619475
iteration : 7356
train acc:  0.84375
train loss:  0.2941667437553406
train gradient:  0.11309329580353299
iteration : 7357
train acc:  0.890625
train loss:  0.29494038224220276
train gradient:  0.17619334785891905
iteration : 7358
train acc:  0.9296875
train loss:  0.2600667476654053
train gradient:  0.14834344227132457
iteration : 7359
train acc:  0.828125
train loss:  0.37514835596084595
train gradient:  0.29124670108691547
iteration : 7360
train acc:  0.7890625
train loss:  0.4388580024242401
train gradient:  0.4419485932828294
iteration : 7361
train acc:  0.7734375
train loss:  0.46231305599212646
train gradient:  0.35391964598536046
iteration : 7362
train acc:  0.875
train loss:  0.3042273223400116
train gradient:  0.23982902628334152
iteration : 7363
train acc:  0.859375
train loss:  0.2834322452545166
train gradient:  0.15434567308098252
iteration : 7364
train acc:  0.8515625
train loss:  0.38850122690200806
train gradient:  0.2279375237291112
iteration : 7365
train acc:  0.8203125
train loss:  0.32336533069610596
train gradient:  0.11742947585119179
iteration : 7366
train acc:  0.796875
train loss:  0.3747016489505768
train gradient:  0.31658170667793994
iteration : 7367
train acc:  0.8125
train loss:  0.45989856123924255
train gradient:  0.31095384067249743
iteration : 7368
train acc:  0.859375
train loss:  0.31710678339004517
train gradient:  0.152326292769116
iteration : 7369
train acc:  0.8671875
train loss:  0.28487682342529297
train gradient:  0.08932633642719394
iteration : 7370
train acc:  0.828125
train loss:  0.36793792247772217
train gradient:  0.3340174087961664
iteration : 7371
train acc:  0.8125
train loss:  0.403605580329895
train gradient:  0.21392966960427479
iteration : 7372
train acc:  0.8671875
train loss:  0.30864566564559937
train gradient:  0.3055658904923497
iteration : 7373
train acc:  0.84375
train loss:  0.37845829129219055
train gradient:  0.2460247368577989
iteration : 7374
train acc:  0.859375
train loss:  0.30103015899658203
train gradient:  0.17365390786006837
iteration : 7375
train acc:  0.8828125
train loss:  0.2976672649383545
train gradient:  0.16958559946231497
iteration : 7376
train acc:  0.84375
train loss:  0.3406962752342224
train gradient:  0.3035997191087556
iteration : 7377
train acc:  0.859375
train loss:  0.3528438210487366
train gradient:  0.1825953395942174
iteration : 7378
train acc:  0.8671875
train loss:  0.36393818259239197
train gradient:  0.2039805782290708
iteration : 7379
train acc:  0.859375
train loss:  0.30760520696640015
train gradient:  0.13864769738299543
iteration : 7380
train acc:  0.875
train loss:  0.26839935779571533
train gradient:  0.11709248896666266
iteration : 7381
train acc:  0.8671875
train loss:  0.3159409165382385
train gradient:  0.15444749853240444
iteration : 7382
train acc:  0.8203125
train loss:  0.36768344044685364
train gradient:  0.18686166404322677
iteration : 7383
train acc:  0.8515625
train loss:  0.37707847356796265
train gradient:  0.28181195377230955
iteration : 7384
train acc:  0.8203125
train loss:  0.36187219619750977
train gradient:  0.17835384659468714
iteration : 7385
train acc:  0.7890625
train loss:  0.42479151487350464
train gradient:  0.34564328460269794
iteration : 7386
train acc:  0.8515625
train loss:  0.2965174913406372
train gradient:  0.22515048666190116
iteration : 7387
train acc:  0.921875
train loss:  0.24147498607635498
train gradient:  0.12872446348435113
iteration : 7388
train acc:  0.8671875
train loss:  0.3343053162097931
train gradient:  0.2431568616540327
iteration : 7389
train acc:  0.84375
train loss:  0.36475473642349243
train gradient:  0.24087856005302014
iteration : 7390
train acc:  0.84375
train loss:  0.32211917638778687
train gradient:  0.18507902674309867
iteration : 7391
train acc:  0.7890625
train loss:  0.39127007126808167
train gradient:  0.22846557171982323
iteration : 7392
train acc:  0.8359375
train loss:  0.3449794054031372
train gradient:  0.189185117859509
iteration : 7393
train acc:  0.9140625
train loss:  0.27170246839523315
train gradient:  0.11234474064520529
iteration : 7394
train acc:  0.8828125
train loss:  0.3198120594024658
train gradient:  0.1600268182788843
iteration : 7395
train acc:  0.875
train loss:  0.2974117696285248
train gradient:  0.14330674478272876
iteration : 7396
train acc:  0.8671875
train loss:  0.3395448923110962
train gradient:  0.15637630384940887
iteration : 7397
train acc:  0.8125
train loss:  0.37048932909965515
train gradient:  0.32265844605544747
iteration : 7398
train acc:  0.84375
train loss:  0.3374648094177246
train gradient:  0.20689427430153812
iteration : 7399
train acc:  0.859375
train loss:  0.32337939739227295
train gradient:  0.26371179816667656
iteration : 7400
train acc:  0.859375
train loss:  0.3090118169784546
train gradient:  0.1709419375089627
iteration : 7401
train acc:  0.8203125
train loss:  0.36423182487487793
train gradient:  0.17534759079494192
iteration : 7402
train acc:  0.828125
train loss:  0.3755612075328827
train gradient:  0.2376025183783366
iteration : 7403
train acc:  0.8828125
train loss:  0.2941087782382965
train gradient:  0.13159183044634237
iteration : 7404
train acc:  0.796875
train loss:  0.3556632399559021
train gradient:  0.2249393286828419
iteration : 7405
train acc:  0.828125
train loss:  0.3339785933494568
train gradient:  0.20627665878560328
iteration : 7406
train acc:  0.8203125
train loss:  0.3577931523323059
train gradient:  0.24704531984118183
iteration : 7407
train acc:  0.8671875
train loss:  0.3139033615589142
train gradient:  0.21320837567158962
iteration : 7408
train acc:  0.859375
train loss:  0.38146060705184937
train gradient:  0.21120144395585894
iteration : 7409
train acc:  0.8515625
train loss:  0.3323509693145752
train gradient:  0.15158001672979146
iteration : 7410
train acc:  0.8671875
train loss:  0.3272220492362976
train gradient:  0.1903242044093264
iteration : 7411
train acc:  0.8359375
train loss:  0.35620635747909546
train gradient:  0.19752002924598985
iteration : 7412
train acc:  0.875
train loss:  0.27983662486076355
train gradient:  0.09662852035339901
iteration : 7413
train acc:  0.859375
train loss:  0.2844967246055603
train gradient:  0.09828148541581497
iteration : 7414
train acc:  0.890625
train loss:  0.2767483592033386
train gradient:  0.13751103239294182
iteration : 7415
train acc:  0.859375
train loss:  0.3162303566932678
train gradient:  0.1466725435128725
iteration : 7416
train acc:  0.8046875
train loss:  0.38234618306159973
train gradient:  0.2752133759671162
iteration : 7417
train acc:  0.8359375
train loss:  0.3692471981048584
train gradient:  0.2112488767988521
iteration : 7418
train acc:  0.8671875
train loss:  0.3291826844215393
train gradient:  0.19758993347856346
iteration : 7419
train acc:  0.8984375
train loss:  0.26704373955726624
train gradient:  0.14696173773310067
iteration : 7420
train acc:  0.8515625
train loss:  0.3684801161289215
train gradient:  0.198490279067985
iteration : 7421
train acc:  0.796875
train loss:  0.4263780415058136
train gradient:  0.2456179880542803
iteration : 7422
train acc:  0.875
train loss:  0.3158305883407593
train gradient:  0.1334293793274886
iteration : 7423
train acc:  0.828125
train loss:  0.3883608281612396
train gradient:  0.23059106682851505
iteration : 7424
train acc:  0.84375
train loss:  0.3333386480808258
train gradient:  0.18805298997390765
iteration : 7425
train acc:  0.8515625
train loss:  0.31067895889282227
train gradient:  0.15075831598794415
iteration : 7426
train acc:  0.8046875
train loss:  0.375451922416687
train gradient:  0.3480753904491034
iteration : 7427
train acc:  0.8203125
train loss:  0.392637699842453
train gradient:  0.3378490966634958
iteration : 7428
train acc:  0.8359375
train loss:  0.34931668639183044
train gradient:  0.16587452069556977
iteration : 7429
train acc:  0.8515625
train loss:  0.2975756824016571
train gradient:  0.16996477817687355
iteration : 7430
train acc:  0.921875
train loss:  0.23976869881153107
train gradient:  0.13505457904299562
iteration : 7431
train acc:  0.8359375
train loss:  0.3726649880409241
train gradient:  0.27286062236138514
iteration : 7432
train acc:  0.890625
train loss:  0.3604559600353241
train gradient:  0.2553333482881813
iteration : 7433
train acc:  0.9140625
train loss:  0.23471516370773315
train gradient:  0.11771575748339234
iteration : 7434
train acc:  0.875
train loss:  0.2680044174194336
train gradient:  0.12815326187564643
iteration : 7435
train acc:  0.8125
train loss:  0.38241592049598694
train gradient:  0.2100972885724752
iteration : 7436
train acc:  0.8515625
train loss:  0.28568509221076965
train gradient:  0.14584250562788548
iteration : 7437
train acc:  0.890625
train loss:  0.27775299549102783
train gradient:  0.14881767965399478
iteration : 7438
train acc:  0.7890625
train loss:  0.3530084192752838
train gradient:  0.22337502233071405
iteration : 7439
train acc:  0.890625
train loss:  0.30680930614471436
train gradient:  0.18329774645390384
iteration : 7440
train acc:  0.8515625
train loss:  0.3460349142551422
train gradient:  0.20475253398523718
iteration : 7441
train acc:  0.84375
train loss:  0.3420182168483734
train gradient:  0.1759750184065273
iteration : 7442
train acc:  0.8515625
train loss:  0.37320977449417114
train gradient:  0.14881592123670112
iteration : 7443
train acc:  0.8359375
train loss:  0.37964725494384766
train gradient:  0.22421701336449137
iteration : 7444
train acc:  0.875
train loss:  0.2612486481666565
train gradient:  0.10937730271156076
iteration : 7445
train acc:  0.8515625
train loss:  0.3561631739139557
train gradient:  0.18217449618684456
iteration : 7446
train acc:  0.8828125
train loss:  0.25067001581192017
train gradient:  0.12006929788400902
iteration : 7447
train acc:  0.8984375
train loss:  0.24617725610733032
train gradient:  0.10668934289830147
iteration : 7448
train acc:  0.84375
train loss:  0.415071964263916
train gradient:  0.2391540901744604
iteration : 7449
train acc:  0.875
train loss:  0.23343423008918762
train gradient:  0.10632068848884076
iteration : 7450
train acc:  0.859375
train loss:  0.3273113965988159
train gradient:  0.1572794160545708
iteration : 7451
train acc:  0.8828125
train loss:  0.3364802300930023
train gradient:  0.17619241339926967
iteration : 7452
train acc:  0.8359375
train loss:  0.41034606099128723
train gradient:  0.296118199748658
iteration : 7453
train acc:  0.8515625
train loss:  0.3856929540634155
train gradient:  0.29075929878706713
iteration : 7454
train acc:  0.8203125
train loss:  0.3696814179420471
train gradient:  0.31849346417946006
iteration : 7455
train acc:  0.859375
train loss:  0.34696635603904724
train gradient:  0.20372343213546826
iteration : 7456
train acc:  0.875
train loss:  0.2885845899581909
train gradient:  0.1491331243571567
iteration : 7457
train acc:  0.84375
train loss:  0.4335825741291046
train gradient:  0.38164912010323
iteration : 7458
train acc:  0.8046875
train loss:  0.3811860680580139
train gradient:  0.2865185452202582
iteration : 7459
train acc:  0.859375
train loss:  0.34063223004341125
train gradient:  0.17124132363303002
iteration : 7460
train acc:  0.859375
train loss:  0.29290899634361267
train gradient:  0.17563981954918653
iteration : 7461
train acc:  0.7890625
train loss:  0.4572426378726959
train gradient:  0.2856868587797118
iteration : 7462
train acc:  0.8671875
train loss:  0.34610703587532043
train gradient:  0.13563561514823813
iteration : 7463
train acc:  0.875
train loss:  0.3109324276447296
train gradient:  0.2153186242069656
iteration : 7464
train acc:  0.859375
train loss:  0.37733179330825806
train gradient:  0.293507147810228
iteration : 7465
train acc:  0.8671875
train loss:  0.3278419077396393
train gradient:  0.17021174091795482
iteration : 7466
train acc:  0.90625
train loss:  0.2705947160720825
train gradient:  0.11301924510623017
iteration : 7467
train acc:  0.8671875
train loss:  0.33033305406570435
train gradient:  0.17357687251362586
iteration : 7468
train acc:  0.84375
train loss:  0.36285701394081116
train gradient:  0.2522082220278344
iteration : 7469
train acc:  0.8671875
train loss:  0.2781682312488556
train gradient:  0.13419861364409474
iteration : 7470
train acc:  0.8828125
train loss:  0.2677958607673645
train gradient:  0.16593996470843242
iteration : 7471
train acc:  0.859375
train loss:  0.2897942066192627
train gradient:  0.13675437063534074
iteration : 7472
train acc:  0.875
train loss:  0.3085511028766632
train gradient:  0.11899439392696783
iteration : 7473
train acc:  0.8125
train loss:  0.40354156494140625
train gradient:  0.2813747989592436
iteration : 7474
train acc:  0.8125
train loss:  0.38164326548576355
train gradient:  0.22424671722512202
iteration : 7475
train acc:  0.859375
train loss:  0.33678489923477173
train gradient:  0.20423400066835717
iteration : 7476
train acc:  0.8828125
train loss:  0.3302431106567383
train gradient:  0.20738689681584882
iteration : 7477
train acc:  0.8984375
train loss:  0.28530484437942505
train gradient:  0.12445634345605236
iteration : 7478
train acc:  0.8359375
train loss:  0.37572744488716125
train gradient:  0.2188762528605965
iteration : 7479
train acc:  0.859375
train loss:  0.3137611448764801
train gradient:  0.15270525429191578
iteration : 7480
train acc:  0.828125
train loss:  0.4258483052253723
train gradient:  0.3050434095100288
iteration : 7481
train acc:  0.8515625
train loss:  0.4360554814338684
train gradient:  0.2954232499527827
iteration : 7482
train acc:  0.8203125
train loss:  0.38226521015167236
train gradient:  0.2484748501209902
iteration : 7483
train acc:  0.828125
train loss:  0.3760502338409424
train gradient:  0.19237563998922064
iteration : 7484
train acc:  0.8515625
train loss:  0.36597633361816406
train gradient:  0.17738728056479064
iteration : 7485
train acc:  0.8203125
train loss:  0.3677600026130676
train gradient:  0.17759387830577517
iteration : 7486
train acc:  0.8203125
train loss:  0.422465980052948
train gradient:  0.26574639578471315
iteration : 7487
train acc:  0.90625
train loss:  0.2583244740962982
train gradient:  0.10787881886328465
iteration : 7488
train acc:  0.8984375
train loss:  0.3018180727958679
train gradient:  0.13811837755474232
iteration : 7489
train acc:  0.8359375
train loss:  0.4187413156032562
train gradient:  0.23230344282623955
iteration : 7490
train acc:  0.8515625
train loss:  0.3330051898956299
train gradient:  0.2004313047366159
iteration : 7491
train acc:  0.828125
train loss:  0.36350271105766296
train gradient:  0.21516523273031024
iteration : 7492
train acc:  0.8359375
train loss:  0.3356015384197235
train gradient:  0.20460453271098972
iteration : 7493
train acc:  0.8046875
train loss:  0.4208011031150818
train gradient:  0.23102632221830183
iteration : 7494
train acc:  0.875
train loss:  0.30964481830596924
train gradient:  0.1089168453283816
iteration : 7495
train acc:  0.78125
train loss:  0.3593585789203644
train gradient:  0.2584409953388519
iteration : 7496
train acc:  0.8984375
train loss:  0.2672663927078247
train gradient:  0.10091069457496031
iteration : 7497
train acc:  0.8515625
train loss:  0.3685033321380615
train gradient:  0.1430180288782678
iteration : 7498
train acc:  0.8671875
train loss:  0.3185872733592987
train gradient:  0.1747988072628327
iteration : 7499
train acc:  0.84375
train loss:  0.3443182706832886
train gradient:  0.16744361845797207
iteration : 7500
train acc:  0.8125
train loss:  0.36780449748039246
train gradient:  0.32024716584581736
iteration : 7501
train acc:  0.859375
train loss:  0.29745039343833923
train gradient:  0.18663085670130491
iteration : 7502
train acc:  0.875
train loss:  0.28348496556282043
train gradient:  0.15471453446280964
iteration : 7503
train acc:  0.859375
train loss:  0.2996223270893097
train gradient:  0.14050952423618546
iteration : 7504
train acc:  0.84375
train loss:  0.30608049035072327
train gradient:  0.19093687208303084
iteration : 7505
train acc:  0.921875
train loss:  0.2573516368865967
train gradient:  0.1680111866070323
iteration : 7506
train acc:  0.84375
train loss:  0.35433340072631836
train gradient:  0.1438872900947463
iteration : 7507
train acc:  0.828125
train loss:  0.39896371960639954
train gradient:  0.29281087656115173
iteration : 7508
train acc:  0.828125
train loss:  0.3838039040565491
train gradient:  0.326249373528721
iteration : 7509
train acc:  0.8828125
train loss:  0.2870420217514038
train gradient:  0.17205284351170777
iteration : 7510
train acc:  0.8359375
train loss:  0.3363600969314575
train gradient:  0.25580997549525003
iteration : 7511
train acc:  0.890625
train loss:  0.29821333289146423
train gradient:  0.15515412404774642
iteration : 7512
train acc:  0.890625
train loss:  0.32030636072158813
train gradient:  0.1648356661149641
iteration : 7513
train acc:  0.84375
train loss:  0.32648545503616333
train gradient:  0.21532006901567052
iteration : 7514
train acc:  0.859375
train loss:  0.32160085439682007
train gradient:  0.1999948883386266
iteration : 7515
train acc:  0.8046875
train loss:  0.3488750457763672
train gradient:  0.15833219236477214
iteration : 7516
train acc:  0.84375
train loss:  0.3663984537124634
train gradient:  0.19422870934313574
iteration : 7517
train acc:  0.8515625
train loss:  0.33378565311431885
train gradient:  0.15550953975602988
iteration : 7518
train acc:  0.8359375
train loss:  0.3402225971221924
train gradient:  0.19214851038752737
iteration : 7519
train acc:  0.8359375
train loss:  0.42608559131622314
train gradient:  0.2700083414464811
iteration : 7520
train acc:  0.890625
train loss:  0.32444143295288086
train gradient:  0.16449661961451267
iteration : 7521
train acc:  0.8359375
train loss:  0.3259833753108978
train gradient:  0.20717232000911145
iteration : 7522
train acc:  0.8359375
train loss:  0.32993102073669434
train gradient:  0.14994647539826622
iteration : 7523
train acc:  0.890625
train loss:  0.34813040494918823
train gradient:  0.20572186282249477
iteration : 7524
train acc:  0.828125
train loss:  0.4017799496650696
train gradient:  0.3064873337895599
iteration : 7525
train acc:  0.875
train loss:  0.26098787784576416
train gradient:  0.11587408673439584
iteration : 7526
train acc:  0.8359375
train loss:  0.31461766362190247
train gradient:  0.10688478614496155
iteration : 7527
train acc:  0.84375
train loss:  0.3740328252315521
train gradient:  0.2410252649801422
iteration : 7528
train acc:  0.84375
train loss:  0.33756357431411743
train gradient:  0.1512244275341728
iteration : 7529
train acc:  0.8359375
train loss:  0.3908655643463135
train gradient:  0.2498327258699225
iteration : 7530
train acc:  0.8359375
train loss:  0.3207648694515228
train gradient:  0.2420798070468971
iteration : 7531
train acc:  0.8203125
train loss:  0.35989412665367126
train gradient:  0.1982797906992504
iteration : 7532
train acc:  0.8359375
train loss:  0.34055089950561523
train gradient:  0.10839936309633189
iteration : 7533
train acc:  0.828125
train loss:  0.3305729627609253
train gradient:  0.14808785815635642
iteration : 7534
train acc:  0.8828125
train loss:  0.30752259492874146
train gradient:  0.15726471612354695
iteration : 7535
train acc:  0.8046875
train loss:  0.4161188304424286
train gradient:  0.20485126127961806
iteration : 7536
train acc:  0.8828125
train loss:  0.311539888381958
train gradient:  0.14060462231452975
iteration : 7537
train acc:  0.8515625
train loss:  0.3491987884044647
train gradient:  0.18699907634198676
iteration : 7538
train acc:  0.8203125
train loss:  0.3114791214466095
train gradient:  0.17728518375360272
iteration : 7539
train acc:  0.8515625
train loss:  0.3106960356235504
train gradient:  0.19505701225900285
iteration : 7540
train acc:  0.78125
train loss:  0.3973436951637268
train gradient:  0.28449080108155234
iteration : 7541
train acc:  0.796875
train loss:  0.38109979033470154
train gradient:  0.20542989311453658
iteration : 7542
train acc:  0.8828125
train loss:  0.3336005210876465
train gradient:  0.1606782890684055
iteration : 7543
train acc:  0.875
train loss:  0.2854738235473633
train gradient:  0.14759909267735316
iteration : 7544
train acc:  0.84375
train loss:  0.3469986915588379
train gradient:  0.21782491702459517
iteration : 7545
train acc:  0.8671875
train loss:  0.2950885593891144
train gradient:  0.1684550207554971
iteration : 7546
train acc:  0.84375
train loss:  0.3056029975414276
train gradient:  0.18493099616322345
iteration : 7547
train acc:  0.8359375
train loss:  0.3341255187988281
train gradient:  0.2059322091343215
iteration : 7548
train acc:  0.765625
train loss:  0.46733880043029785
train gradient:  0.28835182122454517
iteration : 7549
train acc:  0.84375
train loss:  0.3405109643936157
train gradient:  0.23476537311684872
iteration : 7550
train acc:  0.859375
train loss:  0.3455156683921814
train gradient:  0.18725019950806085
iteration : 7551
train acc:  0.8515625
train loss:  0.3252648711204529
train gradient:  0.1622959826750947
iteration : 7552
train acc:  0.8359375
train loss:  0.3831217586994171
train gradient:  0.19370887833272493
iteration : 7553
train acc:  0.8125
train loss:  0.3757038116455078
train gradient:  0.24543868559715293
iteration : 7554
train acc:  0.828125
train loss:  0.36133646965026855
train gradient:  0.23413564509313464
iteration : 7555
train acc:  0.90625
train loss:  0.30308854579925537
train gradient:  0.19747626146575364
iteration : 7556
train acc:  0.8046875
train loss:  0.45190176367759705
train gradient:  0.287550148260583
iteration : 7557
train acc:  0.8671875
train loss:  0.29189276695251465
train gradient:  0.16393601121513862
iteration : 7558
train acc:  0.8671875
train loss:  0.3107598125934601
train gradient:  0.14770002364444534
iteration : 7559
train acc:  0.859375
train loss:  0.3111972212791443
train gradient:  0.15226955958022748
iteration : 7560
train acc:  0.84375
train loss:  0.3159724771976471
train gradient:  0.1411204022570183
iteration : 7561
train acc:  0.8515625
train loss:  0.3653869926929474
train gradient:  0.188681908623895
iteration : 7562
train acc:  0.8046875
train loss:  0.38424456119537354
train gradient:  0.24612362656511777
iteration : 7563
train acc:  0.890625
train loss:  0.276813268661499
train gradient:  0.13565233281865224
iteration : 7564
train acc:  0.84375
train loss:  0.3570895791053772
train gradient:  0.17860168427390183
iteration : 7565
train acc:  0.8515625
train loss:  0.3272988200187683
train gradient:  0.2083674798470474
iteration : 7566
train acc:  0.8125
train loss:  0.4042530655860901
train gradient:  0.24079182031980317
iteration : 7567
train acc:  0.8203125
train loss:  0.3570637106895447
train gradient:  0.16644895017035183
iteration : 7568
train acc:  0.828125
train loss:  0.41685837507247925
train gradient:  0.2729894297571073
iteration : 7569
train acc:  0.8359375
train loss:  0.3471807837486267
train gradient:  0.22506046003260205
iteration : 7570
train acc:  0.828125
train loss:  0.39655694365501404
train gradient:  0.24955562471977813
iteration : 7571
train acc:  0.78125
train loss:  0.43684643507003784
train gradient:  0.28226828257456804
iteration : 7572
train acc:  0.8984375
train loss:  0.28458908200263977
train gradient:  0.13997921177110065
iteration : 7573
train acc:  0.8671875
train loss:  0.2741166055202484
train gradient:  0.12969371061331803
iteration : 7574
train acc:  0.8515625
train loss:  0.3488665819168091
train gradient:  0.19848418985368294
iteration : 7575
train acc:  0.890625
train loss:  0.29076695442199707
train gradient:  0.0898712392388594
iteration : 7576
train acc:  0.8671875
train loss:  0.3369738757610321
train gradient:  0.13261097252460805
iteration : 7577
train acc:  0.890625
train loss:  0.303056001663208
train gradient:  0.13851385239779307
iteration : 7578
train acc:  0.8828125
train loss:  0.29767948389053345
train gradient:  0.1786635928533942
iteration : 7579
train acc:  0.890625
train loss:  0.2933118939399719
train gradient:  0.09591678064339858
iteration : 7580
train acc:  0.8515625
train loss:  0.42324820160865784
train gradient:  0.22719068213621726
iteration : 7581
train acc:  0.8203125
train loss:  0.36996304988861084
train gradient:  0.17648432239543552
iteration : 7582
train acc:  0.84375
train loss:  0.3199959993362427
train gradient:  0.15783752630187836
iteration : 7583
train acc:  0.859375
train loss:  0.2990684509277344
train gradient:  0.1408512066651333
iteration : 7584
train acc:  0.8671875
train loss:  0.30004391074180603
train gradient:  0.15929802604041005
iteration : 7585
train acc:  0.8125
train loss:  0.36986854672431946
train gradient:  0.15856910722613474
iteration : 7586
train acc:  0.8359375
train loss:  0.4167010188102722
train gradient:  0.3366425859508634
iteration : 7587
train acc:  0.859375
train loss:  0.31669557094573975
train gradient:  0.2082483797576623
iteration : 7588
train acc:  0.890625
train loss:  0.2819516658782959
train gradient:  0.21575867864087178
iteration : 7589
train acc:  0.8125
train loss:  0.39849352836608887
train gradient:  0.30231019734000675
iteration : 7590
train acc:  0.8359375
train loss:  0.359391987323761
train gradient:  0.29103611647740185
iteration : 7591
train acc:  0.84375
train loss:  0.36444079875946045
train gradient:  0.2214543625906882
iteration : 7592
train acc:  0.8828125
train loss:  0.2785855829715729
train gradient:  0.12080984574132779
iteration : 7593
train acc:  0.875
train loss:  0.31400424242019653
train gradient:  0.2720534232154312
iteration : 7594
train acc:  0.8515625
train loss:  0.32752010226249695
train gradient:  0.18807854986825484
iteration : 7595
train acc:  0.8671875
train loss:  0.2993760406970978
train gradient:  0.15849841942658166
iteration : 7596
train acc:  0.828125
train loss:  0.363174170255661
train gradient:  0.20076303382335303
iteration : 7597
train acc:  0.890625
train loss:  0.2873741090297699
train gradient:  0.1404067411311124
iteration : 7598
train acc:  0.8671875
train loss:  0.2973242998123169
train gradient:  0.16447478008723582
iteration : 7599
train acc:  0.90625
train loss:  0.22686006128787994
train gradient:  0.11155329600491659
iteration : 7600
train acc:  0.9140625
train loss:  0.2993406653404236
train gradient:  0.18322532034671987
iteration : 7601
train acc:  0.7890625
train loss:  0.44545891880989075
train gradient:  0.3091324656989315
iteration : 7602
train acc:  0.828125
train loss:  0.33452069759368896
train gradient:  0.13644635602262473
iteration : 7603
train acc:  0.875
train loss:  0.3302970826625824
train gradient:  0.15616787808628307
iteration : 7604
train acc:  0.859375
train loss:  0.3406413793563843
train gradient:  0.20006219948134074
iteration : 7605
train acc:  0.8671875
train loss:  0.37839919328689575
train gradient:  0.233051687042928
iteration : 7606
train acc:  0.8828125
train loss:  0.3066423535346985
train gradient:  0.23469313502807415
iteration : 7607
train acc:  0.875
train loss:  0.32789015769958496
train gradient:  0.16518384363970512
iteration : 7608
train acc:  0.8359375
train loss:  0.3967554271221161
train gradient:  0.27440934914708554
iteration : 7609
train acc:  0.828125
train loss:  0.35733264684677124
train gradient:  0.1925864998604302
iteration : 7610
train acc:  0.84375
train loss:  0.3040749430656433
train gradient:  0.11853716162706174
iteration : 7611
train acc:  0.8515625
train loss:  0.33080166578292847
train gradient:  0.20184941616975322
iteration : 7612
train acc:  0.8515625
train loss:  0.33657634258270264
train gradient:  0.27521139573050185
iteration : 7613
train acc:  0.828125
train loss:  0.3347737193107605
train gradient:  0.20587397480305913
iteration : 7614
train acc:  0.8828125
train loss:  0.24601426720619202
train gradient:  0.10821829448786123
iteration : 7615
train acc:  0.8671875
train loss:  0.3903348445892334
train gradient:  0.27175291547925273
iteration : 7616
train acc:  0.8046875
train loss:  0.44348037242889404
train gradient:  0.3782036233301204
iteration : 7617
train acc:  0.796875
train loss:  0.3977959454059601
train gradient:  0.27452946615945967
iteration : 7618
train acc:  0.8515625
train loss:  0.2832769453525543
train gradient:  0.13456953960105733
iteration : 7619
train acc:  0.828125
train loss:  0.40243327617645264
train gradient:  0.20226030530085304
iteration : 7620
train acc:  0.859375
train loss:  0.341622531414032
train gradient:  0.1555393792563537
iteration : 7621
train acc:  0.8359375
train loss:  0.3236280083656311
train gradient:  0.3688577675870771
iteration : 7622
train acc:  0.859375
train loss:  0.31464385986328125
train gradient:  0.13490651837796871
iteration : 7623
train acc:  0.8671875
train loss:  0.29587888717651367
train gradient:  0.15110868783638065
iteration : 7624
train acc:  0.84375
train loss:  0.4023286998271942
train gradient:  0.225107736591725
iteration : 7625
train acc:  0.8828125
train loss:  0.3170391917228699
train gradient:  0.20510612271018341
iteration : 7626
train acc:  0.8125
train loss:  0.36024442315101624
train gradient:  0.16842853172364727
iteration : 7627
train acc:  0.859375
train loss:  0.33013230562210083
train gradient:  0.17824026230075057
iteration : 7628
train acc:  0.9453125
train loss:  0.20635852217674255
train gradient:  0.1189272296883137
iteration : 7629
train acc:  0.8203125
train loss:  0.4110991358757019
train gradient:  0.2875618718004327
iteration : 7630
train acc:  0.8828125
train loss:  0.30175521969795227
train gradient:  0.12347104651136277
iteration : 7631
train acc:  0.8359375
train loss:  0.320940226316452
train gradient:  0.24591828473862626
iteration : 7632
train acc:  0.8125
train loss:  0.4218559265136719
train gradient:  0.26256934378691543
iteration : 7633
train acc:  0.796875
train loss:  0.38019829988479614
train gradient:  0.22094680458606997
iteration : 7634
train acc:  0.890625
train loss:  0.27336007356643677
train gradient:  0.11367588981655442
iteration : 7635
train acc:  0.84375
train loss:  0.3766016960144043
train gradient:  0.2716572094769086
iteration : 7636
train acc:  0.8515625
train loss:  0.3672812581062317
train gradient:  0.21154860895740651
iteration : 7637
train acc:  0.890625
train loss:  0.33889955282211304
train gradient:  0.16277290197789748
iteration : 7638
train acc:  0.8671875
train loss:  0.33887016773223877
train gradient:  0.1776195273629715
iteration : 7639
train acc:  0.7890625
train loss:  0.4484306275844574
train gradient:  0.3154407129792684
iteration : 7640
train acc:  0.8671875
train loss:  0.3525984287261963
train gradient:  0.28924396832743193
iteration : 7641
train acc:  0.8828125
train loss:  0.3355909585952759
train gradient:  0.25105652730476913
iteration : 7642
train acc:  0.890625
train loss:  0.29573625326156616
train gradient:  0.19140689790820767
iteration : 7643
train acc:  0.828125
train loss:  0.39638859033584595
train gradient:  0.35002795535899056
iteration : 7644
train acc:  0.890625
train loss:  0.281887024641037
train gradient:  0.2118640499956876
iteration : 7645
train acc:  0.8515625
train loss:  0.3519965708255768
train gradient:  0.23229503562700127
iteration : 7646
train acc:  0.8515625
train loss:  0.3844143748283386
train gradient:  0.33278841033042666
iteration : 7647
train acc:  0.828125
train loss:  0.37808752059936523
train gradient:  0.23629024120719166
iteration : 7648
train acc:  0.8671875
train loss:  0.3523193597793579
train gradient:  0.18368295214187577
iteration : 7649
train acc:  0.8671875
train loss:  0.31073299050331116
train gradient:  0.13627401356164004
iteration : 7650
train acc:  0.875
train loss:  0.3075087070465088
train gradient:  0.1873371741615757
iteration : 7651
train acc:  0.84375
train loss:  0.3416426181793213
train gradient:  0.17515666910765249
iteration : 7652
train acc:  0.859375
train loss:  0.30602091550827026
train gradient:  0.22576372989932394
iteration : 7653
train acc:  0.8671875
train loss:  0.3233381509780884
train gradient:  0.12612443231577827
iteration : 7654
train acc:  0.8125
train loss:  0.4070547819137573
train gradient:  0.2254726777620871
iteration : 7655
train acc:  0.875
train loss:  0.32097911834716797
train gradient:  0.15615718295922543
iteration : 7656
train acc:  0.859375
train loss:  0.34416887164115906
train gradient:  0.17579176118008316
iteration : 7657
train acc:  0.8671875
train loss:  0.300924688577652
train gradient:  0.18926267180894063
iteration : 7658
train acc:  0.875
train loss:  0.3188593089580536
train gradient:  0.1470686163888414
iteration : 7659
train acc:  0.875
train loss:  0.30882778763771057
train gradient:  0.19252958538396484
iteration : 7660
train acc:  0.8671875
train loss:  0.3472902178764343
train gradient:  0.17435149820421933
iteration : 7661
train acc:  0.921875
train loss:  0.24365103244781494
train gradient:  0.11549177772649535
iteration : 7662
train acc:  0.828125
train loss:  0.39430516958236694
train gradient:  0.2737569145366137
iteration : 7663
train acc:  0.8515625
train loss:  0.31104469299316406
train gradient:  0.19462398043065876
iteration : 7664
train acc:  0.8125
train loss:  0.3961481750011444
train gradient:  0.2532435677970853
iteration : 7665
train acc:  0.8984375
train loss:  0.31158679723739624
train gradient:  0.20906994942641538
iteration : 7666
train acc:  0.859375
train loss:  0.308601438999176
train gradient:  0.1719766484043478
iteration : 7667
train acc:  0.8046875
train loss:  0.4507160782814026
train gradient:  0.265216617441882
iteration : 7668
train acc:  0.859375
train loss:  0.3169804513454437
train gradient:  0.13574159222714016
iteration : 7669
train acc:  0.875
train loss:  0.3007325530052185
train gradient:  0.19753790194003784
iteration : 7670
train acc:  0.8515625
train loss:  0.35305067896842957
train gradient:  0.20158773172504074
iteration : 7671
train acc:  0.8203125
train loss:  0.4007672071456909
train gradient:  0.3091862937261006
iteration : 7672
train acc:  0.8984375
train loss:  0.3558737635612488
train gradient:  0.15938921814106383
iteration : 7673
train acc:  0.9296875
train loss:  0.2319239377975464
train gradient:  0.10601073907500486
iteration : 7674
train acc:  0.84375
train loss:  0.31595098972320557
train gradient:  0.23687407907017843
iteration : 7675
train acc:  0.8125
train loss:  0.369532972574234
train gradient:  0.25057223681790297
iteration : 7676
train acc:  0.7734375
train loss:  0.4284043610095978
train gradient:  0.2526750352754999
iteration : 7677
train acc:  0.8125
train loss:  0.4136539399623871
train gradient:  0.29651247346739645
iteration : 7678
train acc:  0.890625
train loss:  0.2985895872116089
train gradient:  0.13710674130463357
iteration : 7679
train acc:  0.859375
train loss:  0.29313233494758606
train gradient:  0.18136279822438395
iteration : 7680
train acc:  0.8515625
train loss:  0.37164968252182007
train gradient:  0.33285792571815764
iteration : 7681
train acc:  0.8359375
train loss:  0.3399142622947693
train gradient:  0.18921837215411674
iteration : 7682
train acc:  0.859375
train loss:  0.31996309757232666
train gradient:  0.16868387345704736
iteration : 7683
train acc:  0.8515625
train loss:  0.34993380308151245
train gradient:  0.2505995770580767
iteration : 7684
train acc:  0.84375
train loss:  0.32679682970046997
train gradient:  0.18290327718039154
iteration : 7685
train acc:  0.859375
train loss:  0.3409825563430786
train gradient:  0.21521735962849559
iteration : 7686
train acc:  0.8828125
train loss:  0.3024512827396393
train gradient:  0.19166466331899226
iteration : 7687
train acc:  0.90625
train loss:  0.3231482207775116
train gradient:  0.16133577922780667
iteration : 7688
train acc:  0.8515625
train loss:  0.3070893883705139
train gradient:  0.21046864603477602
iteration : 7689
train acc:  0.875
train loss:  0.3058788776397705
train gradient:  0.1414532351481328
iteration : 7690
train acc:  0.78125
train loss:  0.4599997401237488
train gradient:  0.2556873506218551
iteration : 7691
train acc:  0.8046875
train loss:  0.39834195375442505
train gradient:  0.24720823341286954
iteration : 7692
train acc:  0.8671875
train loss:  0.37418174743652344
train gradient:  0.15848971219587357
iteration : 7693
train acc:  0.8828125
train loss:  0.3271259665489197
train gradient:  0.22041157372533093
iteration : 7694
train acc:  0.875
train loss:  0.30021944642066956
train gradient:  0.14475262213196916
iteration : 7695
train acc:  0.8515625
train loss:  0.3019232153892517
train gradient:  0.15718909265155404
iteration : 7696
train acc:  0.8359375
train loss:  0.39190003275871277
train gradient:  0.3042100086837887
iteration : 7697
train acc:  0.828125
train loss:  0.3779048025608063
train gradient:  0.2744867456874637
iteration : 7698
train acc:  0.8671875
train loss:  0.3038370609283447
train gradient:  0.17988605144995742
iteration : 7699
train acc:  0.8359375
train loss:  0.335044801235199
train gradient:  0.1585172729606858
iteration : 7700
train acc:  0.828125
train loss:  0.41102784872055054
train gradient:  0.2933989362984052
iteration : 7701
train acc:  0.8671875
train loss:  0.32687675952911377
train gradient:  0.20781544059587523
iteration : 7702
train acc:  0.875
train loss:  0.3038751482963562
train gradient:  0.19077359887801465
iteration : 7703
train acc:  0.8515625
train loss:  0.35074716806411743
train gradient:  0.18425960138206376
iteration : 7704
train acc:  0.859375
train loss:  0.3650023937225342
train gradient:  0.3150545491479529
iteration : 7705
train acc:  0.875
train loss:  0.34047114849090576
train gradient:  0.1934486799353175
iteration : 7706
train acc:  0.8828125
train loss:  0.34956321120262146
train gradient:  0.17856011425625212
iteration : 7707
train acc:  0.875
train loss:  0.30769991874694824
train gradient:  0.15753460602751518
iteration : 7708
train acc:  0.875
train loss:  0.36123356223106384
train gradient:  0.29594771074577314
iteration : 7709
train acc:  0.8515625
train loss:  0.3282267451286316
train gradient:  0.18763241545628156
iteration : 7710
train acc:  0.84375
train loss:  0.31484317779541016
train gradient:  0.14829112591699278
iteration : 7711
train acc:  0.890625
train loss:  0.34196990728378296
train gradient:  0.19213733063750896
iteration : 7712
train acc:  0.8515625
train loss:  0.3174176514148712
train gradient:  0.17601114280286095
iteration : 7713
train acc:  0.875
train loss:  0.33957624435424805
train gradient:  0.1440248883686761
iteration : 7714
train acc:  0.8828125
train loss:  0.28815755248069763
train gradient:  0.1707860784201828
iteration : 7715
train acc:  0.8828125
train loss:  0.2821367383003235
train gradient:  0.1544888180310098
iteration : 7716
train acc:  0.875
train loss:  0.27658557891845703
train gradient:  0.14065768154215655
iteration : 7717
train acc:  0.8125
train loss:  0.4297812283039093
train gradient:  0.3595324195135992
iteration : 7718
train acc:  0.890625
train loss:  0.34115344285964966
train gradient:  0.17009980585718215
iteration : 7719
train acc:  0.828125
train loss:  0.41067415475845337
train gradient:  0.2553402400050215
iteration : 7720
train acc:  0.8359375
train loss:  0.3203095495700836
train gradient:  0.1564396323203336
iteration : 7721
train acc:  0.890625
train loss:  0.296714723110199
train gradient:  0.14557731590627715
iteration : 7722
train acc:  0.859375
train loss:  0.40719395875930786
train gradient:  0.2405909305746525
iteration : 7723
train acc:  0.859375
train loss:  0.38191497325897217
train gradient:  0.1706122247723995
iteration : 7724
train acc:  0.875
train loss:  0.29446884989738464
train gradient:  0.14178062768963678
iteration : 7725
train acc:  0.8515625
train loss:  0.30707651376724243
train gradient:  0.20936861913024263
iteration : 7726
train acc:  0.875
train loss:  0.3274010419845581
train gradient:  0.16611652973000435
iteration : 7727
train acc:  0.8359375
train loss:  0.3618733882904053
train gradient:  0.2995132450960035
iteration : 7728
train acc:  0.8671875
train loss:  0.30483728647232056
train gradient:  0.20858997603450985
iteration : 7729
train acc:  0.84375
train loss:  0.35410261154174805
train gradient:  0.5997981027843311
iteration : 7730
train acc:  0.8671875
train loss:  0.2888779640197754
train gradient:  0.12386218954544186
iteration : 7731
train acc:  0.828125
train loss:  0.4260455369949341
train gradient:  0.2555034742328283
iteration : 7732
train acc:  0.8828125
train loss:  0.2838137745857239
train gradient:  0.12159899577740946
iteration : 7733
train acc:  0.8828125
train loss:  0.33771389722824097
train gradient:  0.1342065699980602
iteration : 7734
train acc:  0.8359375
train loss:  0.3745979070663452
train gradient:  0.2248752653028075
iteration : 7735
train acc:  0.8515625
train loss:  0.30509138107299805
train gradient:  0.1317798084636011
iteration : 7736
train acc:  0.828125
train loss:  0.452717661857605
train gradient:  0.3627291759363708
iteration : 7737
train acc:  0.8828125
train loss:  0.30872875452041626
train gradient:  0.2057495897383796
iteration : 7738
train acc:  0.8203125
train loss:  0.3581778407096863
train gradient:  0.1802480692306992
iteration : 7739
train acc:  0.828125
train loss:  0.3676987290382385
train gradient:  0.2423468127045148
iteration : 7740
train acc:  0.90625
train loss:  0.27316808700561523
train gradient:  0.10927399250607427
iteration : 7741
train acc:  0.84375
train loss:  0.40336287021636963
train gradient:  0.22349906542905962
iteration : 7742
train acc:  0.828125
train loss:  0.35771438479423523
train gradient:  0.19199148984483366
iteration : 7743
train acc:  0.84375
train loss:  0.3528810739517212
train gradient:  0.2819226594173777
iteration : 7744
train acc:  0.8828125
train loss:  0.2942352890968323
train gradient:  0.18619369634542415
iteration : 7745
train acc:  0.8515625
train loss:  0.3402634859085083
train gradient:  0.17226001163489307
iteration : 7746
train acc:  0.8828125
train loss:  0.2978065013885498
train gradient:  0.14328940942340032
iteration : 7747
train acc:  0.8515625
train loss:  0.3232012391090393
train gradient:  0.21709979860477824
iteration : 7748
train acc:  0.8125
train loss:  0.3961554169654846
train gradient:  0.29866477583647094
iteration : 7749
train acc:  0.859375
train loss:  0.3367929458618164
train gradient:  0.1495048540114572
iteration : 7750
train acc:  0.8984375
train loss:  0.30129504203796387
train gradient:  0.16675404798756538
iteration : 7751
train acc:  0.8671875
train loss:  0.3137538433074951
train gradient:  0.13516401411760712
iteration : 7752
train acc:  0.8203125
train loss:  0.3914809226989746
train gradient:  0.28131853839283283
iteration : 7753
train acc:  0.859375
train loss:  0.32260504364967346
train gradient:  0.13859251408171364
iteration : 7754
train acc:  0.84375
train loss:  0.3240777254104614
train gradient:  0.19738890067871118
iteration : 7755
train acc:  0.890625
train loss:  0.2793095111846924
train gradient:  0.14062773759314431
iteration : 7756
train acc:  0.8671875
train loss:  0.32506227493286133
train gradient:  0.17201581634179108
iteration : 7757
train acc:  0.8984375
train loss:  0.2731327414512634
train gradient:  0.12254501348314038
iteration : 7758
train acc:  0.8046875
train loss:  0.3661196827888489
train gradient:  0.2252340902557215
iteration : 7759
train acc:  0.8515625
train loss:  0.3047536611557007
train gradient:  0.1765507371397144
iteration : 7760
train acc:  0.859375
train loss:  0.3017351031303406
train gradient:  0.13524546739930898
iteration : 7761
train acc:  0.828125
train loss:  0.36620187759399414
train gradient:  0.19578662641571473
iteration : 7762
train acc:  0.859375
train loss:  0.3145689070224762
train gradient:  0.15667131700050135
iteration : 7763
train acc:  0.828125
train loss:  0.3682049810886383
train gradient:  0.23364283553510778
iteration : 7764
train acc:  0.875
train loss:  0.31917980313301086
train gradient:  0.20204332649034057
iteration : 7765
train acc:  0.875
train loss:  0.3487277030944824
train gradient:  0.12614744042477793
iteration : 7766
train acc:  0.90625
train loss:  0.22370412945747375
train gradient:  0.09685220925447967
iteration : 7767
train acc:  0.84375
train loss:  0.36986684799194336
train gradient:  0.20827142623836664
iteration : 7768
train acc:  0.8671875
train loss:  0.36516204476356506
train gradient:  0.2764571852917064
iteration : 7769
train acc:  0.796875
train loss:  0.3864783048629761
train gradient:  0.3322411341148307
iteration : 7770
train acc:  0.8046875
train loss:  0.36628735065460205
train gradient:  0.22044633320555324
iteration : 7771
train acc:  0.8671875
train loss:  0.25449055433273315
train gradient:  0.13224783439947121
iteration : 7772
train acc:  0.84375
train loss:  0.3354943096637726
train gradient:  0.25498267332354463
iteration : 7773
train acc:  0.84375
train loss:  0.3548451066017151
train gradient:  0.20769122890056269
iteration : 7774
train acc:  0.8671875
train loss:  0.32848408818244934
train gradient:  0.19122158817875892
iteration : 7775
train acc:  0.890625
train loss:  0.2500450611114502
train gradient:  0.1299143657145726
iteration : 7776
train acc:  0.8515625
train loss:  0.35938936471939087
train gradient:  0.30111722366896576
iteration : 7777
train acc:  0.859375
train loss:  0.36772945523262024
train gradient:  0.21480143924010442
iteration : 7778
train acc:  0.859375
train loss:  0.27705052495002747
train gradient:  0.15487297558444785
iteration : 7779
train acc:  0.8046875
train loss:  0.41477614641189575
train gradient:  0.3251333670487246
iteration : 7780
train acc:  0.7890625
train loss:  0.4366897940635681
train gradient:  0.269624468614185
iteration : 7781
train acc:  0.890625
train loss:  0.25117817521095276
train gradient:  0.10845595264017478
iteration : 7782
train acc:  0.796875
train loss:  0.3641473352909088
train gradient:  0.19511026147990115
iteration : 7783
train acc:  0.8828125
train loss:  0.2978174686431885
train gradient:  0.1262659164987414
iteration : 7784
train acc:  0.7734375
train loss:  0.4490395784378052
train gradient:  0.3035325678907295
iteration : 7785
train acc:  0.8359375
train loss:  0.3229716420173645
train gradient:  0.2010648236482742
iteration : 7786
train acc:  0.90625
train loss:  0.3252936005592346
train gradient:  0.2187186861287679
iteration : 7787
train acc:  0.84375
train loss:  0.32066041231155396
train gradient:  0.15071803592618505
iteration : 7788
train acc:  0.84375
train loss:  0.3359750807285309
train gradient:  0.21394416662026303
iteration : 7789
train acc:  0.8671875
train loss:  0.2665976881980896
train gradient:  0.12047907886720056
iteration : 7790
train acc:  0.828125
train loss:  0.3880764842033386
train gradient:  0.2237317805712387
iteration : 7791
train acc:  0.90625
train loss:  0.2653687596321106
train gradient:  0.13184480659255748
iteration : 7792
train acc:  0.8671875
train loss:  0.34799981117248535
train gradient:  0.18015844420940189
iteration : 7793
train acc:  0.7890625
train loss:  0.3853932023048401
train gradient:  0.22753826120534854
iteration : 7794
train acc:  0.8515625
train loss:  0.3511708974838257
train gradient:  0.1956966461789547
iteration : 7795
train acc:  0.875
train loss:  0.30679836869239807
train gradient:  0.14472499382330817
iteration : 7796
train acc:  0.8984375
train loss:  0.27572697401046753
train gradient:  0.10913231460945048
iteration : 7797
train acc:  0.828125
train loss:  0.29887843132019043
train gradient:  0.17679930762263307
iteration : 7798
train acc:  0.859375
train loss:  0.3312028646469116
train gradient:  0.23023684021802904
iteration : 7799
train acc:  0.8203125
train loss:  0.40009015798568726
train gradient:  0.29195426656482953
iteration : 7800
train acc:  0.84375
train loss:  0.32191935181617737
train gradient:  0.24117961759950882
iteration : 7801
train acc:  0.8515625
train loss:  0.3420112133026123
train gradient:  0.20708357245270004
iteration : 7802
train acc:  0.9140625
train loss:  0.23193912208080292
train gradient:  0.08322942669047487
iteration : 7803
train acc:  0.8828125
train loss:  0.32368263602256775
train gradient:  0.15007874420031242
iteration : 7804
train acc:  0.8359375
train loss:  0.37460988759994507
train gradient:  0.20568826673208762
iteration : 7805
train acc:  0.8359375
train loss:  0.32919543981552124
train gradient:  0.298483861587952
iteration : 7806
train acc:  0.875
train loss:  0.2862297296524048
train gradient:  0.12399362219548063
iteration : 7807
train acc:  0.8203125
train loss:  0.3534452021121979
train gradient:  0.20314736865521135
iteration : 7808
train acc:  0.921875
train loss:  0.29001176357269287
train gradient:  0.35071160093888876
iteration : 7809
train acc:  0.8671875
train loss:  0.30960553884506226
train gradient:  0.21118344206697812
iteration : 7810
train acc:  0.8515625
train loss:  0.2920059263706207
train gradient:  0.1484988346180883
iteration : 7811
train acc:  0.8203125
train loss:  0.37876877188682556
train gradient:  0.2403701091873967
iteration : 7812
train acc:  0.8984375
train loss:  0.2632454037666321
train gradient:  0.1355792378364491
iteration : 7813
train acc:  0.828125
train loss:  0.30799293518066406
train gradient:  0.19759428275240248
iteration : 7814
train acc:  0.84375
train loss:  0.3255860507488251
train gradient:  0.20982040620560705
iteration : 7815
train acc:  0.90625
train loss:  0.2648143172264099
train gradient:  0.11745350669504283
iteration : 7816
train acc:  0.8828125
train loss:  0.36468422412872314
train gradient:  0.22598073238785532
iteration : 7817
train acc:  0.890625
train loss:  0.2711377441883087
train gradient:  0.2351770315046392
iteration : 7818
train acc:  0.84375
train loss:  0.3408365845680237
train gradient:  0.19358695872147316
iteration : 7819
train acc:  0.8125
train loss:  0.3534991443157196
train gradient:  0.17934841660538264
iteration : 7820
train acc:  0.8515625
train loss:  0.3724435269832611
train gradient:  0.23441237314833802
iteration : 7821
train acc:  0.8671875
train loss:  0.3682297468185425
train gradient:  0.28874289463734637
iteration : 7822
train acc:  0.8203125
train loss:  0.3491332530975342
train gradient:  0.24698615544704894
iteration : 7823
train acc:  0.9140625
train loss:  0.298450767993927
train gradient:  0.15908967464030271
iteration : 7824
train acc:  0.84375
train loss:  0.29110103845596313
train gradient:  0.1216026941628328
iteration : 7825
train acc:  0.8671875
train loss:  0.36380305886268616
train gradient:  0.1740495991510737
iteration : 7826
train acc:  0.84375
train loss:  0.31867972016334534
train gradient:  0.2061801334706349
iteration : 7827
train acc:  0.8671875
train loss:  0.31312498450279236
train gradient:  0.1415624630357019
iteration : 7828
train acc:  0.7734375
train loss:  0.43893301486968994
train gradient:  0.3985541147392521
iteration : 7829
train acc:  0.8828125
train loss:  0.31674209237098694
train gradient:  0.16567753899607654
iteration : 7830
train acc:  0.84375
train loss:  0.3344099819660187
train gradient:  0.24107867123127735
iteration : 7831
train acc:  0.8125
train loss:  0.3809359073638916
train gradient:  0.19214743254311795
iteration : 7832
train acc:  0.8671875
train loss:  0.28511813282966614
train gradient:  0.16594742389719924
iteration : 7833
train acc:  0.890625
train loss:  0.3358471393585205
train gradient:  0.24562314605886798
iteration : 7834
train acc:  0.875
train loss:  0.29673659801483154
train gradient:  0.1893597690787176
iteration : 7835
train acc:  0.890625
train loss:  0.23969006538391113
train gradient:  0.13301640078724813
iteration : 7836
train acc:  0.828125
train loss:  0.36868128180503845
train gradient:  0.18153056850146787
iteration : 7837
train acc:  0.875
train loss:  0.28147828578948975
train gradient:  0.22679122729707307
iteration : 7838
train acc:  0.859375
train loss:  0.3243420422077179
train gradient:  0.26639504354288845
iteration : 7839
train acc:  0.8984375
train loss:  0.29708313941955566
train gradient:  0.21530050561046465
iteration : 7840
train acc:  0.828125
train loss:  0.3687434196472168
train gradient:  0.3592923583204481
iteration : 7841
train acc:  0.84375
train loss:  0.3392750024795532
train gradient:  0.23602929508594964
iteration : 7842
train acc:  0.8515625
train loss:  0.340305894613266
train gradient:  0.18556598917893263
iteration : 7843
train acc:  0.875
train loss:  0.2596215605735779
train gradient:  0.10379174019582035
iteration : 7844
train acc:  0.828125
train loss:  0.3481183648109436
train gradient:  0.178179401980447
iteration : 7845
train acc:  0.859375
train loss:  0.2920549809932709
train gradient:  0.13605843974046972
iteration : 7846
train acc:  0.8203125
train loss:  0.33426323533058167
train gradient:  0.1950152094975435
iteration : 7847
train acc:  0.859375
train loss:  0.32185906171798706
train gradient:  0.1809760664703799
iteration : 7848
train acc:  0.84375
train loss:  0.35546875
train gradient:  0.2190755559277808
iteration : 7849
train acc:  0.8671875
train loss:  0.29644373059272766
train gradient:  0.1886166026964763
iteration : 7850
train acc:  0.8671875
train loss:  0.3364306390285492
train gradient:  0.19993746832658887
iteration : 7851
train acc:  0.8359375
train loss:  0.37267330288887024
train gradient:  0.5794344326219671
iteration : 7852
train acc:  0.8671875
train loss:  0.37273070216178894
train gradient:  0.23552226333049514
iteration : 7853
train acc:  0.7890625
train loss:  0.4271070659160614
train gradient:  0.27524966268206763
iteration : 7854
train acc:  0.890625
train loss:  0.3012339770793915
train gradient:  0.18022141762405533
iteration : 7855
train acc:  0.84375
train loss:  0.3070705533027649
train gradient:  0.20533576673776044
iteration : 7856
train acc:  0.890625
train loss:  0.27488914132118225
train gradient:  0.1684364478623683
iteration : 7857
train acc:  0.875
train loss:  0.3030661344528198
train gradient:  0.15686885484081192
iteration : 7858
train acc:  0.8828125
train loss:  0.26957967877388
train gradient:  0.14472784508557268
iteration : 7859
train acc:  0.859375
train loss:  0.29572904109954834
train gradient:  0.16714159081907132
iteration : 7860
train acc:  0.8515625
train loss:  0.3148009181022644
train gradient:  0.24452509645637394
iteration : 7861
train acc:  0.8203125
train loss:  0.3371742367744446
train gradient:  0.15185670471452772
iteration : 7862
train acc:  0.8671875
train loss:  0.3633992075920105
train gradient:  0.33955514214460353
iteration : 7863
train acc:  0.8671875
train loss:  0.33472156524658203
train gradient:  0.23764512194054832
iteration : 7864
train acc:  0.8125
train loss:  0.38896721601486206
train gradient:  0.26117425056201515
iteration : 7865
train acc:  0.859375
train loss:  0.3071650266647339
train gradient:  0.17688543362489806
iteration : 7866
train acc:  0.8671875
train loss:  0.3335384130477905
train gradient:  0.1779587187045566
iteration : 7867
train acc:  0.859375
train loss:  0.3455415666103363
train gradient:  0.23280829979170842
iteration : 7868
train acc:  0.8515625
train loss:  0.3248146176338196
train gradient:  0.14434807223266682
iteration : 7869
train acc:  0.8984375
train loss:  0.25886213779449463
train gradient:  0.17490181453760045
iteration : 7870
train acc:  0.8125
train loss:  0.39275217056274414
train gradient:  0.31360016138205643
iteration : 7871
train acc:  0.8828125
train loss:  0.28048771619796753
train gradient:  0.11618640769923826
iteration : 7872
train acc:  0.8828125
train loss:  0.3085256814956665
train gradient:  0.17375297143907337
iteration : 7873
train acc:  0.828125
train loss:  0.40920406579971313
train gradient:  0.27482713461651276
iteration : 7874
train acc:  0.8671875
train loss:  0.3302697241306305
train gradient:  0.15519408572821575
iteration : 7875
train acc:  0.875
train loss:  0.35120248794555664
train gradient:  0.22301901130046725
iteration : 7876
train acc:  0.8671875
train loss:  0.3416169285774231
train gradient:  0.21506831307199353
iteration : 7877
train acc:  0.84375
train loss:  0.3244621157646179
train gradient:  0.15525560058880758
iteration : 7878
train acc:  0.8671875
train loss:  0.3243202567100525
train gradient:  0.2218917494345859
iteration : 7879
train acc:  0.859375
train loss:  0.31594955921173096
train gradient:  0.1502864442694109
iteration : 7880
train acc:  0.859375
train loss:  0.33361467719078064
train gradient:  0.2180082241601421
iteration : 7881
train acc:  0.8828125
train loss:  0.2778794765472412
train gradient:  0.13876560100598823
iteration : 7882
train acc:  0.8515625
train loss:  0.31037551164627075
train gradient:  0.16342780992748002
iteration : 7883
train acc:  0.828125
train loss:  0.349222332239151
train gradient:  0.2059969873134052
iteration : 7884
train acc:  0.84375
train loss:  0.3551643490791321
train gradient:  0.24893628236536225
iteration : 7885
train acc:  0.828125
train loss:  0.32610636949539185
train gradient:  0.1748073227685468
iteration : 7886
train acc:  0.8984375
train loss:  0.2666565775871277
train gradient:  0.17861846838746864
iteration : 7887
train acc:  0.8359375
train loss:  0.3734162151813507
train gradient:  0.2682804155149403
iteration : 7888
train acc:  0.859375
train loss:  0.32921701669692993
train gradient:  0.23127849948949103
iteration : 7889
train acc:  0.859375
train loss:  0.32996252179145813
train gradient:  0.17914302323021938
iteration : 7890
train acc:  0.859375
train loss:  0.32604268193244934
train gradient:  0.13070418900289915
iteration : 7891
train acc:  0.84375
train loss:  0.3261478543281555
train gradient:  0.20542963043917603
iteration : 7892
train acc:  0.828125
train loss:  0.3498407006263733
train gradient:  0.21534819878727252
iteration : 7893
train acc:  0.8671875
train loss:  0.2991728186607361
train gradient:  0.15498670237413137
iteration : 7894
train acc:  0.8515625
train loss:  0.3264225721359253
train gradient:  0.16310663847885098
iteration : 7895
train acc:  0.8671875
train loss:  0.30673182010650635
train gradient:  0.14219183298954563
iteration : 7896
train acc:  0.8359375
train loss:  0.36142855882644653
train gradient:  0.2138491265521668
iteration : 7897
train acc:  0.8515625
train loss:  0.34469425678253174
train gradient:  0.24696056039933795
iteration : 7898
train acc:  0.90625
train loss:  0.26656848192214966
train gradient:  0.15850468297524212
iteration : 7899
train acc:  0.8359375
train loss:  0.36853721737861633
train gradient:  0.20893954557477493
iteration : 7900
train acc:  0.8828125
train loss:  0.3293669819831848
train gradient:  0.20733637644092381
iteration : 7901
train acc:  0.8671875
train loss:  0.3154262602329254
train gradient:  0.17435429112836365
iteration : 7902
train acc:  0.8671875
train loss:  0.3242877721786499
train gradient:  0.15183762348698482
iteration : 7903
train acc:  0.8671875
train loss:  0.34810471534729004
train gradient:  0.1829189508765152
iteration : 7904
train acc:  0.8203125
train loss:  0.39115673303604126
train gradient:  0.2605354908141376
iteration : 7905
train acc:  0.8359375
train loss:  0.36928534507751465
train gradient:  0.20273557618641652
iteration : 7906
train acc:  0.84375
train loss:  0.37047457695007324
train gradient:  0.26351314334218723
iteration : 7907
train acc:  0.875
train loss:  0.3092419505119324
train gradient:  0.148457756774673
iteration : 7908
train acc:  0.84375
train loss:  0.39735132455825806
train gradient:  0.29052282993293127
iteration : 7909
train acc:  0.8828125
train loss:  0.30979079008102417
train gradient:  0.13954542217789884
iteration : 7910
train acc:  0.84375
train loss:  0.35655102133750916
train gradient:  0.20160228208120462
iteration : 7911
train acc:  0.8515625
train loss:  0.35923469066619873
train gradient:  0.2666384307023224
iteration : 7912
train acc:  0.8203125
train loss:  0.38540273904800415
train gradient:  0.22620323553250077
iteration : 7913
train acc:  0.8828125
train loss:  0.29750365018844604
train gradient:  0.16865082671694523
iteration : 7914
train acc:  0.828125
train loss:  0.34488677978515625
train gradient:  0.3082908577118488
iteration : 7915
train acc:  0.8828125
train loss:  0.3777083158493042
train gradient:  0.24918340900506025
iteration : 7916
train acc:  0.828125
train loss:  0.36851292848587036
train gradient:  0.2547748892265205
iteration : 7917
train acc:  0.8671875
train loss:  0.27488934993743896
train gradient:  0.14733974998199306
iteration : 7918
train acc:  0.890625
train loss:  0.2516648769378662
train gradient:  0.1963945787977055
iteration : 7919
train acc:  0.8125
train loss:  0.3725277781486511
train gradient:  0.24494583863661934
iteration : 7920
train acc:  0.8046875
train loss:  0.3667899966239929
train gradient:  0.19713609726448283
iteration : 7921
train acc:  0.8671875
train loss:  0.32089412212371826
train gradient:  0.16462362574535677
iteration : 7922
train acc:  0.921875
train loss:  0.24217045307159424
train gradient:  0.09528674193994612
iteration : 7923
train acc:  0.8671875
train loss:  0.3505098819732666
train gradient:  0.1777728742113784
iteration : 7924
train acc:  0.8828125
train loss:  0.30367398262023926
train gradient:  0.15305956785995245
iteration : 7925
train acc:  0.828125
train loss:  0.32601648569107056
train gradient:  0.16792358703476723
iteration : 7926
train acc:  0.8203125
train loss:  0.3373009264469147
train gradient:  0.1575391765086498
iteration : 7927
train acc:  0.8359375
train loss:  0.34443169832229614
train gradient:  0.2206310412108941
iteration : 7928
train acc:  0.90625
train loss:  0.26585859060287476
train gradient:  0.14113083151775502
iteration : 7929
train acc:  0.859375
train loss:  0.32681769132614136
train gradient:  0.2087521905810644
iteration : 7930
train acc:  0.8828125
train loss:  0.2529130280017853
train gradient:  0.20970827527528765
iteration : 7931
train acc:  0.8515625
train loss:  0.3686136305332184
train gradient:  0.17842149617278155
iteration : 7932
train acc:  0.8984375
train loss:  0.241074338555336
train gradient:  0.09922636682193842
iteration : 7933
train acc:  0.8671875
train loss:  0.3159407079219818
train gradient:  0.2788138795567805
iteration : 7934
train acc:  0.8359375
train loss:  0.4110260009765625
train gradient:  0.2686802510295228
iteration : 7935
train acc:  0.859375
train loss:  0.3471454083919525
train gradient:  0.21333024926191002
iteration : 7936
train acc:  0.84375
train loss:  0.35073721408843994
train gradient:  0.22160834387374873
iteration : 7937
train acc:  0.8515625
train loss:  0.33155277371406555
train gradient:  0.1659452643940274
iteration : 7938
train acc:  0.8125
train loss:  0.40103957056999207
train gradient:  0.2651312390244466
iteration : 7939
train acc:  0.875
train loss:  0.31420427560806274
train gradient:  0.1738681780217219
iteration : 7940
train acc:  0.890625
train loss:  0.27462083101272583
train gradient:  0.12234078415555535
iteration : 7941
train acc:  0.828125
train loss:  0.3186224699020386
train gradient:  0.184327536492356
iteration : 7942
train acc:  0.8828125
train loss:  0.28953930735588074
train gradient:  0.14201741956981934
iteration : 7943
train acc:  0.8515625
train loss:  0.3000655770301819
train gradient:  0.2373023196900979
iteration : 7944
train acc:  0.875
train loss:  0.3469542860984802
train gradient:  0.21406699973711407
iteration : 7945
train acc:  0.8203125
train loss:  0.3575081527233124
train gradient:  0.27368471701272407
iteration : 7946
train acc:  0.8515625
train loss:  0.31065231561660767
train gradient:  0.15021804623430754
iteration : 7947
train acc:  0.828125
train loss:  0.3572596311569214
train gradient:  0.1579730388239461
iteration : 7948
train acc:  0.7734375
train loss:  0.5109189748764038
train gradient:  0.4081071561124469
iteration : 7949
train acc:  0.796875
train loss:  0.4209292531013489
train gradient:  0.31043371781037515
iteration : 7950
train acc:  0.8125
train loss:  0.5116786360740662
train gradient:  0.30189098590531255
iteration : 7951
train acc:  0.9140625
train loss:  0.2667320966720581
train gradient:  0.13876460439695146
iteration : 7952
train acc:  0.8125
train loss:  0.4107787311077118
train gradient:  0.2833631329383036
iteration : 7953
train acc:  0.8515625
train loss:  0.2818903923034668
train gradient:  0.1441239521626389
iteration : 7954
train acc:  0.84375
train loss:  0.4447179138660431
train gradient:  0.30824703674689086
iteration : 7955
train acc:  0.8359375
train loss:  0.3584205210208893
train gradient:  0.17587656133684787
iteration : 7956
train acc:  0.8671875
train loss:  0.307059109210968
train gradient:  0.24516821193548302
iteration : 7957
train acc:  0.8671875
train loss:  0.2875036299228668
train gradient:  0.1410177760358231
iteration : 7958
train acc:  0.84375
train loss:  0.3414434492588043
train gradient:  0.14656707975074218
iteration : 7959
train acc:  0.859375
train loss:  0.3132646679878235
train gradient:  0.16586347958061012
iteration : 7960
train acc:  0.8828125
train loss:  0.32085517048835754
train gradient:  0.1645858376659587
iteration : 7961
train acc:  0.859375
train loss:  0.31753191351890564
train gradient:  0.13215364650563588
iteration : 7962
train acc:  0.8359375
train loss:  0.3741852641105652
train gradient:  0.20748819107265232
iteration : 7963
train acc:  0.859375
train loss:  0.3223384916782379
train gradient:  0.271097129469641
iteration : 7964
train acc:  0.90625
train loss:  0.23826684057712555
train gradient:  0.1558962457212666
iteration : 7965
train acc:  0.859375
train loss:  0.3214187026023865
train gradient:  0.16712308633512163
iteration : 7966
train acc:  0.9140625
train loss:  0.24577730894088745
train gradient:  0.10339608502863562
iteration : 7967
train acc:  0.875
train loss:  0.28753408789634705
train gradient:  0.18306909336516813
iteration : 7968
train acc:  0.828125
train loss:  0.3428269326686859
train gradient:  0.2631946965915418
iteration : 7969
train acc:  0.8125
train loss:  0.3938859701156616
train gradient:  0.22537600456557422
iteration : 7970
train acc:  0.84375
train loss:  0.3563976287841797
train gradient:  0.16951088750909565
iteration : 7971
train acc:  0.859375
train loss:  0.319608211517334
train gradient:  0.15856316078872268
iteration : 7972
train acc:  0.8125
train loss:  0.34263038635253906
train gradient:  0.21097620635307685
iteration : 7973
train acc:  0.8515625
train loss:  0.3660491406917572
train gradient:  0.17905048417113467
iteration : 7974
train acc:  0.8828125
train loss:  0.3046404719352722
train gradient:  0.12669218759878884
iteration : 7975
train acc:  0.8359375
train loss:  0.3677278459072113
train gradient:  0.200075895325757
iteration : 7976
train acc:  0.8125
train loss:  0.43147990107536316
train gradient:  0.3009204162431373
iteration : 7977
train acc:  0.8203125
train loss:  0.4046435058116913
train gradient:  0.23142979740393604
iteration : 7978
train acc:  0.84375
train loss:  0.39284998178482056
train gradient:  0.19897348797696518
iteration : 7979
train acc:  0.8125
train loss:  0.3935238718986511
train gradient:  0.21883203338496954
iteration : 7980
train acc:  0.859375
train loss:  0.2677093744277954
train gradient:  0.13295983691015056
iteration : 7981
train acc:  0.8125
train loss:  0.3778979778289795
train gradient:  0.24359413140807354
iteration : 7982
train acc:  0.859375
train loss:  0.3157113790512085
train gradient:  0.14547944858109962
iteration : 7983
train acc:  0.828125
train loss:  0.3357011377811432
train gradient:  0.2467094276119376
iteration : 7984
train acc:  0.8515625
train loss:  0.3664349317550659
train gradient:  0.15074616664594562
iteration : 7985
train acc:  0.8984375
train loss:  0.31151771545410156
train gradient:  0.16505202647493747
iteration : 7986
train acc:  0.84375
train loss:  0.2934767007827759
train gradient:  0.15891691644429373
iteration : 7987
train acc:  0.8203125
train loss:  0.3493077754974365
train gradient:  0.11679783274485317
iteration : 7988
train acc:  0.8515625
train loss:  0.31026405096054077
train gradient:  0.1789720593878518
iteration : 7989
train acc:  0.8203125
train loss:  0.35599154233932495
train gradient:  0.22096865073133193
iteration : 7990
train acc:  0.875
train loss:  0.3170228898525238
train gradient:  0.20436932817000286
iteration : 7991
train acc:  0.859375
train loss:  0.40068548917770386
train gradient:  0.25070747428118784
iteration : 7992
train acc:  0.8515625
train loss:  0.38681864738464355
train gradient:  0.17912788014759248
iteration : 7993
train acc:  0.859375
train loss:  0.367184579372406
train gradient:  0.17714901487487605
iteration : 7994
train acc:  0.8984375
train loss:  0.241393581032753
train gradient:  0.13387769322058585
iteration : 7995
train acc:  0.8828125
train loss:  0.319998174905777
train gradient:  0.24929679054384793
iteration : 7996
train acc:  0.90625
train loss:  0.2669549584388733
train gradient:  0.11532105993086227
iteration : 7997
train acc:  0.8984375
train loss:  0.2702268064022064
train gradient:  0.1528811678641531
iteration : 7998
train acc:  0.859375
train loss:  0.35582447052001953
train gradient:  0.14651217735318672
iteration : 7999
train acc:  0.7734375
train loss:  0.4786713421344757
train gradient:  0.44675834000491027
iteration : 8000
train acc:  0.8125
train loss:  0.3677859604358673
train gradient:  0.1992054220548368
iteration : 8001
train acc:  0.8203125
train loss:  0.399250328540802
train gradient:  0.2154473713060943
iteration : 8002
train acc:  0.9296875
train loss:  0.2580588757991791
train gradient:  0.11860864834115208
iteration : 8003
train acc:  0.84375
train loss:  0.29413753747940063
train gradient:  0.17503401054678963
iteration : 8004
train acc:  0.828125
train loss:  0.3575785160064697
train gradient:  0.23993884490674167
iteration : 8005
train acc:  0.828125
train loss:  0.348769873380661
train gradient:  0.22537701723070294
iteration : 8006
train acc:  0.8515625
train loss:  0.3494493067264557
train gradient:  0.20986158930154203
iteration : 8007
train acc:  0.859375
train loss:  0.3252078890800476
train gradient:  0.17611950853033007
iteration : 8008
train acc:  0.859375
train loss:  0.3631998300552368
train gradient:  0.16175767811507707
iteration : 8009
train acc:  0.859375
train loss:  0.35779982805252075
train gradient:  0.1670671899492378
iteration : 8010
train acc:  0.828125
train loss:  0.3221902847290039
train gradient:  0.1940408872866678
iteration : 8011
train acc:  0.8984375
train loss:  0.28120505809783936
train gradient:  0.29174506184194876
iteration : 8012
train acc:  0.8203125
train loss:  0.3721812963485718
train gradient:  0.2243227722057258
iteration : 8013
train acc:  0.8046875
train loss:  0.4273642897605896
train gradient:  0.31420359144338184
iteration : 8014
train acc:  0.8046875
train loss:  0.40645521879196167
train gradient:  0.24064481452830716
iteration : 8015
train acc:  0.875
train loss:  0.2884594798088074
train gradient:  0.14348217632199078
iteration : 8016
train acc:  0.8984375
train loss:  0.2753492593765259
train gradient:  0.16387813455410116
iteration : 8017
train acc:  0.8828125
train loss:  0.3933960795402527
train gradient:  0.23387082433112646
iteration : 8018
train acc:  0.7890625
train loss:  0.4398108422756195
train gradient:  0.2471211935863597
iteration : 8019
train acc:  0.90625
train loss:  0.27065831422805786
train gradient:  0.1850245573677038
iteration : 8020
train acc:  0.859375
train loss:  0.4239044189453125
train gradient:  0.30959068004333035
iteration : 8021
train acc:  0.8203125
train loss:  0.3197423219680786
train gradient:  0.15627107014625707
iteration : 8022
train acc:  0.859375
train loss:  0.3147265911102295
train gradient:  0.11242409019773146
iteration : 8023
train acc:  0.7890625
train loss:  0.4213840067386627
train gradient:  0.24959166388909543
iteration : 8024
train acc:  0.8515625
train loss:  0.305736243724823
train gradient:  0.17655757402456052
iteration : 8025
train acc:  0.90625
train loss:  0.26276516914367676
train gradient:  0.09367320363659966
iteration : 8026
train acc:  0.8359375
train loss:  0.33386778831481934
train gradient:  0.18943175372586823
iteration : 8027
train acc:  0.859375
train loss:  0.32548773288726807
train gradient:  0.14203509834805128
iteration : 8028
train acc:  0.8671875
train loss:  0.2962242066860199
train gradient:  0.14349184413003288
iteration : 8029
train acc:  0.90625
train loss:  0.293032705783844
train gradient:  0.20603219071928097
iteration : 8030
train acc:  0.8515625
train loss:  0.32769352197647095
train gradient:  0.26094554176925355
iteration : 8031
train acc:  0.8203125
train loss:  0.388899564743042
train gradient:  0.22049551301383055
iteration : 8032
train acc:  0.84375
train loss:  0.38808947801589966
train gradient:  0.19515847253595908
iteration : 8033
train acc:  0.90625
train loss:  0.276151180267334
train gradient:  0.15372239596413007
iteration : 8034
train acc:  0.8359375
train loss:  0.33690446615219116
train gradient:  0.18096068938321258
iteration : 8035
train acc:  0.8125
train loss:  0.4574936330318451
train gradient:  0.3462603374783477
iteration : 8036
train acc:  0.8359375
train loss:  0.3669094443321228
train gradient:  0.24180741296098862
iteration : 8037
train acc:  0.8515625
train loss:  0.35485315322875977
train gradient:  0.19737987028672782
iteration : 8038
train acc:  0.90625
train loss:  0.2501083016395569
train gradient:  0.10153735456704603
iteration : 8039
train acc:  0.8515625
train loss:  0.32391422986984253
train gradient:  0.17033102638827835
iteration : 8040
train acc:  0.90625
train loss:  0.2626223564147949
train gradient:  0.1618077015790296
iteration : 8041
train acc:  0.8515625
train loss:  0.3199489116668701
train gradient:  0.1260923523527417
iteration : 8042
train acc:  0.84375
train loss:  0.41604870557785034
train gradient:  0.35676104326406083
iteration : 8043
train acc:  0.828125
train loss:  0.39972543716430664
train gradient:  0.20886158800254417
iteration : 8044
train acc:  0.8359375
train loss:  0.3410026729106903
train gradient:  0.2569874722159017
iteration : 8045
train acc:  0.859375
train loss:  0.33519524335861206
train gradient:  0.1970940858712773
iteration : 8046
train acc:  0.859375
train loss:  0.3195844292640686
train gradient:  0.18352500344614892
iteration : 8047
train acc:  0.828125
train loss:  0.3734844923019409
train gradient:  0.24330792567641152
iteration : 8048
train acc:  0.859375
train loss:  0.3230781555175781
train gradient:  0.15897377883347807
iteration : 8049
train acc:  0.8515625
train loss:  0.2824113965034485
train gradient:  0.13287830556625807
iteration : 8050
train acc:  0.859375
train loss:  0.30757439136505127
train gradient:  0.1605146719900329
iteration : 8051
train acc:  0.8671875
train loss:  0.2829921543598175
train gradient:  0.12347376791859269
iteration : 8052
train acc:  0.8359375
train loss:  0.35760998725891113
train gradient:  0.1700750253531217
iteration : 8053
train acc:  0.84375
train loss:  0.34760135412216187
train gradient:  0.22109179291171555
iteration : 8054
train acc:  0.8515625
train loss:  0.3082220256328583
train gradient:  0.1381206303304661
iteration : 8055
train acc:  0.890625
train loss:  0.26530760526657104
train gradient:  0.14523425532989087
iteration : 8056
train acc:  0.7734375
train loss:  0.40931934118270874
train gradient:  0.19718226451996962
iteration : 8057
train acc:  0.8671875
train loss:  0.3071218430995941
train gradient:  0.2019162240678073
iteration : 8058
train acc:  0.7890625
train loss:  0.38881948590278625
train gradient:  0.31293498481094545
iteration : 8059
train acc:  0.8203125
train loss:  0.4003196656703949
train gradient:  0.2313485664313342
iteration : 8060
train acc:  0.828125
train loss:  0.39662349224090576
train gradient:  0.1946629159454646
iteration : 8061
train acc:  0.859375
train loss:  0.37722015380859375
train gradient:  0.21382884906788457
iteration : 8062
train acc:  0.8515625
train loss:  0.32802221179008484
train gradient:  0.16785652617319724
iteration : 8063
train acc:  0.8359375
train loss:  0.33311206102371216
train gradient:  0.14636349990384429
iteration : 8064
train acc:  0.859375
train loss:  0.3155093789100647
train gradient:  0.14642016450024667
iteration : 8065
train acc:  0.890625
train loss:  0.2512968182563782
train gradient:  0.11567725688845748
iteration : 8066
train acc:  0.875
train loss:  0.3755088448524475
train gradient:  0.20898918631994165
iteration : 8067
train acc:  0.8359375
train loss:  0.349218487739563
train gradient:  0.22772659954178237
iteration : 8068
train acc:  0.84375
train loss:  0.3328315019607544
train gradient:  0.17841104457694584
iteration : 8069
train acc:  0.796875
train loss:  0.47975122928619385
train gradient:  0.37272016390645013
iteration : 8070
train acc:  0.8515625
train loss:  0.2766214609146118
train gradient:  0.14161406154531558
iteration : 8071
train acc:  0.8359375
train loss:  0.3619546592235565
train gradient:  0.2540501784423528
iteration : 8072
train acc:  0.84375
train loss:  0.34596869349479675
train gradient:  0.19718442466015057
iteration : 8073
train acc:  0.8515625
train loss:  0.3519282937049866
train gradient:  0.1397403452201311
iteration : 8074
train acc:  0.8203125
train loss:  0.33935654163360596
train gradient:  0.1619400383644875
iteration : 8075
train acc:  0.8125
train loss:  0.4092096984386444
train gradient:  0.43762574265058624
iteration : 8076
train acc:  0.8671875
train loss:  0.3136833906173706
train gradient:  0.12954136311454006
iteration : 8077
train acc:  0.84375
train loss:  0.3404916524887085
train gradient:  0.19607180038676675
iteration : 8078
train acc:  0.84375
train loss:  0.3603071868419647
train gradient:  0.2075106701923089
iteration : 8079
train acc:  0.8359375
train loss:  0.32653185725212097
train gradient:  0.12844924227099302
iteration : 8080
train acc:  0.765625
train loss:  0.4242349863052368
train gradient:  0.2522860834872648
iteration : 8081
train acc:  0.9140625
train loss:  0.2432505488395691
train gradient:  0.11241154924747841
iteration : 8082
train acc:  0.8359375
train loss:  0.33756619691848755
train gradient:  0.1608091380196487
iteration : 8083
train acc:  0.890625
train loss:  0.2890026271343231
train gradient:  0.13161508116468557
iteration : 8084
train acc:  0.78125
train loss:  0.4415777325630188
train gradient:  0.3365243257103997
iteration : 8085
train acc:  0.8984375
train loss:  0.26076430082321167
train gradient:  0.10899707716261185
iteration : 8086
train acc:  0.828125
train loss:  0.37213560938835144
train gradient:  0.2710283770769258
iteration : 8087
train acc:  0.8515625
train loss:  0.2933053970336914
train gradient:  0.14828259660792784
iteration : 8088
train acc:  0.828125
train loss:  0.37585967779159546
train gradient:  0.24190502634323074
iteration : 8089
train acc:  0.828125
train loss:  0.3588064908981323
train gradient:  0.1844473481923459
iteration : 8090
train acc:  0.78125
train loss:  0.4119807779788971
train gradient:  0.26991163155488773
iteration : 8091
train acc:  0.8828125
train loss:  0.30844542384147644
train gradient:  0.20763894469465283
iteration : 8092
train acc:  0.859375
train loss:  0.37047189474105835
train gradient:  0.27951222228748984
iteration : 8093
train acc:  0.84375
train loss:  0.31801557540893555
train gradient:  0.1625899508328768
iteration : 8094
train acc:  0.859375
train loss:  0.3157975375652313
train gradient:  0.12341444898310584
iteration : 8095
train acc:  0.8359375
train loss:  0.3531085252761841
train gradient:  0.16846568740151405
iteration : 8096
train acc:  0.75
train loss:  0.4420205354690552
train gradient:  0.3003183233392429
iteration : 8097
train acc:  0.890625
train loss:  0.2472265213727951
train gradient:  0.11385148483657748
iteration : 8098
train acc:  0.8046875
train loss:  0.3280552625656128
train gradient:  0.34354368627687204
iteration : 8099
train acc:  0.84375
train loss:  0.34239232540130615
train gradient:  0.27352006395886963
iteration : 8100
train acc:  0.8046875
train loss:  0.34200966358184814
train gradient:  0.18225552951847393
iteration : 8101
train acc:  0.8515625
train loss:  0.2801838517189026
train gradient:  0.13412743347026523
iteration : 8102
train acc:  0.890625
train loss:  0.23841264843940735
train gradient:  0.1063732233340446
iteration : 8103
train acc:  0.875
train loss:  0.26404130458831787
train gradient:  0.14370326868058242
iteration : 8104
train acc:  0.859375
train loss:  0.3215771019458771
train gradient:  0.13891150308539518
iteration : 8105
train acc:  0.8984375
train loss:  0.2503179907798767
train gradient:  0.16089521601092058
iteration : 8106
train acc:  0.8046875
train loss:  0.42638272047042847
train gradient:  0.22149869892522936
iteration : 8107
train acc:  0.8515625
train loss:  0.33345213532447815
train gradient:  0.15377774153162188
iteration : 8108
train acc:  0.8671875
train loss:  0.28006142377853394
train gradient:  0.15058963706619835
iteration : 8109
train acc:  0.8671875
train loss:  0.3603668212890625
train gradient:  0.2467308764971608
iteration : 8110
train acc:  0.875
train loss:  0.3063960671424866
train gradient:  0.1718466335565554
iteration : 8111
train acc:  0.8671875
train loss:  0.3079494833946228
train gradient:  0.22374652799327938
iteration : 8112
train acc:  0.84375
train loss:  0.3419014513492584
train gradient:  0.179126768044863
iteration : 8113
train acc:  0.859375
train loss:  0.26944008469581604
train gradient:  0.19893038808610972
iteration : 8114
train acc:  0.875
train loss:  0.2852105498313904
train gradient:  0.13237234507403262
iteration : 8115
train acc:  0.8203125
train loss:  0.35116082429885864
train gradient:  0.28229994660750996
iteration : 8116
train acc:  0.859375
train loss:  0.358254075050354
train gradient:  0.22294232235426564
iteration : 8117
train acc:  0.8359375
train loss:  0.3682743012905121
train gradient:  0.2301678538790099
iteration : 8118
train acc:  0.8671875
train loss:  0.30329054594039917
train gradient:  0.14431166519309238
iteration : 8119
train acc:  0.859375
train loss:  0.3554482161998749
train gradient:  0.19291808571556918
iteration : 8120
train acc:  0.828125
train loss:  0.3308028280735016
train gradient:  0.1982899158916109
iteration : 8121
train acc:  0.8828125
train loss:  0.29318735003471375
train gradient:  0.197013605937643
iteration : 8122
train acc:  0.875
train loss:  0.3354153633117676
train gradient:  0.1772242838941366
iteration : 8123
train acc:  0.84375
train loss:  0.34954559803009033
train gradient:  0.1361616629908602
iteration : 8124
train acc:  0.875
train loss:  0.2845104932785034
train gradient:  0.17330041789616715
iteration : 8125
train acc:  0.84375
train loss:  0.39974159002304077
train gradient:  0.3202946609800003
iteration : 8126
train acc:  0.875
train loss:  0.3068224787712097
train gradient:  0.17878801499511757
iteration : 8127
train acc:  0.8359375
train loss:  0.3599238097667694
train gradient:  0.31304108761128757
iteration : 8128
train acc:  0.8046875
train loss:  0.38117140531539917
train gradient:  0.23553419741642323
iteration : 8129
train acc:  0.8125
train loss:  0.3656354248523712
train gradient:  0.19256373276329358
iteration : 8130
train acc:  0.828125
train loss:  0.3747439384460449
train gradient:  0.2963091033060187
iteration : 8131
train acc:  0.859375
train loss:  0.3373560309410095
train gradient:  0.25083608098191995
iteration : 8132
train acc:  0.8828125
train loss:  0.2883226275444031
train gradient:  0.12551851069721257
iteration : 8133
train acc:  0.8828125
train loss:  0.278643935918808
train gradient:  0.12974839326421486
iteration : 8134
train acc:  0.8515625
train loss:  0.292025089263916
train gradient:  0.1995790814236156
iteration : 8135
train acc:  0.859375
train loss:  0.3505106568336487
train gradient:  0.23552403812082323
iteration : 8136
train acc:  0.8203125
train loss:  0.3750970959663391
train gradient:  0.2192532549750696
iteration : 8137
train acc:  0.859375
train loss:  0.31366249918937683
train gradient:  0.14544045643193293
iteration : 8138
train acc:  0.875
train loss:  0.3057524561882019
train gradient:  0.11514772744116207
iteration : 8139
train acc:  0.8203125
train loss:  0.35883811116218567
train gradient:  0.2553085016399944
iteration : 8140
train acc:  0.84375
train loss:  0.3212096393108368
train gradient:  0.14755244608575996
iteration : 8141
train acc:  0.828125
train loss:  0.3408466577529907
train gradient:  0.193409429668557
iteration : 8142
train acc:  0.84375
train loss:  0.3174843192100525
train gradient:  0.14750655074961255
iteration : 8143
train acc:  0.8515625
train loss:  0.275876522064209
train gradient:  0.12545613614850373
iteration : 8144
train acc:  0.859375
train loss:  0.27852851152420044
train gradient:  0.17264360547794105
iteration : 8145
train acc:  0.875
train loss:  0.3183588981628418
train gradient:  0.1814291032867223
iteration : 8146
train acc:  0.8359375
train loss:  0.40759238600730896
train gradient:  0.21022295641710287
iteration : 8147
train acc:  0.8828125
train loss:  0.2751421332359314
train gradient:  0.11268944581662388
iteration : 8148
train acc:  0.8828125
train loss:  0.25184690952301025
train gradient:  0.13350023251296125
iteration : 8149
train acc:  0.8125
train loss:  0.34419339895248413
train gradient:  0.17006991000762145
iteration : 8150
train acc:  0.84375
train loss:  0.2985835075378418
train gradient:  0.16933657671495217
iteration : 8151
train acc:  0.8359375
train loss:  0.3544982373714447
train gradient:  0.23876244709681962
iteration : 8152
train acc:  0.8515625
train loss:  0.37147581577301025
train gradient:  0.23668523606645495
iteration : 8153
train acc:  0.8671875
train loss:  0.35133686661720276
train gradient:  0.16273785412259636
iteration : 8154
train acc:  0.84375
train loss:  0.43373894691467285
train gradient:  0.2649592204815148
iteration : 8155
train acc:  0.8359375
train loss:  0.3187692165374756
train gradient:  0.19065083399044472
iteration : 8156
train acc:  0.859375
train loss:  0.3721930980682373
train gradient:  0.2674598305980576
iteration : 8157
train acc:  0.84375
train loss:  0.3777526915073395
train gradient:  0.3460999725215481
iteration : 8158
train acc:  0.8671875
train loss:  0.2610928416252136
train gradient:  0.13326915923250493
iteration : 8159
train acc:  0.859375
train loss:  0.3826397657394409
train gradient:  0.23346722640918016
iteration : 8160
train acc:  0.8359375
train loss:  0.35418763756752014
train gradient:  0.2624285026533398
iteration : 8161
train acc:  0.7890625
train loss:  0.43749088048934937
train gradient:  0.414032007314579
iteration : 8162
train acc:  0.875
train loss:  0.34800341725349426
train gradient:  0.34664161914313435
iteration : 8163
train acc:  0.859375
train loss:  0.35490643978118896
train gradient:  0.21142385719129778
iteration : 8164
train acc:  0.8515625
train loss:  0.29340532422065735
train gradient:  0.11451854130706969
iteration : 8165
train acc:  0.8046875
train loss:  0.40802353620529175
train gradient:  0.3301433132086666
iteration : 8166
train acc:  0.8828125
train loss:  0.3258140981197357
train gradient:  0.2098457339461173
iteration : 8167
train acc:  0.875
train loss:  0.3156731426715851
train gradient:  0.21997502269925026
iteration : 8168
train acc:  0.875
train loss:  0.34302952885627747
train gradient:  0.19537067747553818
iteration : 8169
train acc:  0.7890625
train loss:  0.42188385128974915
train gradient:  0.4861434484841395
iteration : 8170
train acc:  0.8203125
train loss:  0.38819432258605957
train gradient:  0.2045403510486023
iteration : 8171
train acc:  0.890625
train loss:  0.3085319995880127
train gradient:  0.18153978287233263
iteration : 8172
train acc:  0.875
train loss:  0.26138532161712646
train gradient:  0.11289286582996577
iteration : 8173
train acc:  0.8203125
train loss:  0.48499512672424316
train gradient:  0.34200590781855705
iteration : 8174
train acc:  0.9296875
train loss:  0.2279828041791916
train gradient:  0.10573445232848382
iteration : 8175
train acc:  0.8359375
train loss:  0.40069660544395447
train gradient:  0.2853057056842614
iteration : 8176
train acc:  0.8828125
train loss:  0.32306307554244995
train gradient:  0.2100737700019384
iteration : 8177
train acc:  0.875
train loss:  0.3301042318344116
train gradient:  0.22386526700223447
iteration : 8178
train acc:  0.8515625
train loss:  0.3319699168205261
train gradient:  0.22798748788188455
iteration : 8179
train acc:  0.828125
train loss:  0.35654520988464355
train gradient:  0.16877887704560998
iteration : 8180
train acc:  0.8671875
train loss:  0.33428955078125
train gradient:  0.16703013758844365
iteration : 8181
train acc:  0.8203125
train loss:  0.3837617039680481
train gradient:  0.19057121872152105
iteration : 8182
train acc:  0.796875
train loss:  0.37291407585144043
train gradient:  0.20780055047475066
iteration : 8183
train acc:  0.8671875
train loss:  0.28303980827331543
train gradient:  0.1307600870389271
iteration : 8184
train acc:  0.828125
train loss:  0.36358386278152466
train gradient:  0.24632953941320707
iteration : 8185
train acc:  0.8203125
train loss:  0.3702878952026367
train gradient:  0.23923623599572164
iteration : 8186
train acc:  0.8515625
train loss:  0.29161280393600464
train gradient:  0.23086643866085182
iteration : 8187
train acc:  0.828125
train loss:  0.3212723731994629
train gradient:  0.17186778279720863
iteration : 8188
train acc:  0.8515625
train loss:  0.3079091012477875
train gradient:  0.1551400534400541
iteration : 8189
train acc:  0.84375
train loss:  0.3833937644958496
train gradient:  0.20049018267489938
iteration : 8190
train acc:  0.78125
train loss:  0.4960022568702698
train gradient:  0.33481552345534626
iteration : 8191
train acc:  0.828125
train loss:  0.347715824842453
train gradient:  0.14939474311308168
iteration : 8192
train acc:  0.890625
train loss:  0.27124232053756714
train gradient:  0.09527587027031603
iteration : 8193
train acc:  0.859375
train loss:  0.3386920988559723
train gradient:  0.22382534251866437
iteration : 8194
train acc:  0.796875
train loss:  0.41504043340682983
train gradient:  0.248769430111119
iteration : 8195
train acc:  0.890625
train loss:  0.289716511964798
train gradient:  0.1401573625750813
iteration : 8196
train acc:  0.828125
train loss:  0.344540536403656
train gradient:  0.19085767753861885
iteration : 8197
train acc:  0.8828125
train loss:  0.23043292760849
train gradient:  0.15049963047536902
iteration : 8198
train acc:  0.8671875
train loss:  0.31312477588653564
train gradient:  0.21511175757353115
iteration : 8199
train acc:  0.8828125
train loss:  0.2814900577068329
train gradient:  0.13825696674577204
iteration : 8200
train acc:  0.859375
train loss:  0.355064332485199
train gradient:  0.15620835033658428
iteration : 8201
train acc:  0.9375
train loss:  0.2339426875114441
train gradient:  0.12741634007792502
iteration : 8202
train acc:  0.8671875
train loss:  0.34128305315971375
train gradient:  0.1406054725341389
iteration : 8203
train acc:  0.8359375
train loss:  0.3964996337890625
train gradient:  0.23474805588658992
iteration : 8204
train acc:  0.875
train loss:  0.3009963631629944
train gradient:  0.14832296015570468
iteration : 8205
train acc:  0.8515625
train loss:  0.362557053565979
train gradient:  0.1519954693269295
iteration : 8206
train acc:  0.8125
train loss:  0.38426685333251953
train gradient:  0.22059866396845407
iteration : 8207
train acc:  0.828125
train loss:  0.3640705943107605
train gradient:  0.20814328236487356
iteration : 8208
train acc:  0.828125
train loss:  0.3545758128166199
train gradient:  0.20678316355109758
iteration : 8209
train acc:  0.8828125
train loss:  0.32504791021347046
train gradient:  0.1612815350630591
iteration : 8210
train acc:  0.875
train loss:  0.3254833221435547
train gradient:  0.16919982640685693
iteration : 8211
train acc:  0.8828125
train loss:  0.29138192534446716
train gradient:  0.11473845637053111
iteration : 8212
train acc:  0.8671875
train loss:  0.34373655915260315
train gradient:  0.2533053500958456
iteration : 8213
train acc:  0.859375
train loss:  0.31706100702285767
train gradient:  0.11936246225592284
iteration : 8214
train acc:  0.828125
train loss:  0.376950204372406
train gradient:  0.2243320135805765
iteration : 8215
train acc:  0.84375
train loss:  0.3370332419872284
train gradient:  0.1816290757765831
iteration : 8216
train acc:  0.8984375
train loss:  0.2785557806491852
train gradient:  0.10153493330978101
iteration : 8217
train acc:  0.8515625
train loss:  0.3152720034122467
train gradient:  0.13436143970822023
iteration : 8218
train acc:  0.875
train loss:  0.34578925371170044
train gradient:  0.23386206941052348
iteration : 8219
train acc:  0.8515625
train loss:  0.3323603570461273
train gradient:  0.15134855771537595
iteration : 8220
train acc:  0.828125
train loss:  0.3643871545791626
train gradient:  0.21281217961621748
iteration : 8221
train acc:  0.8203125
train loss:  0.4054080545902252
train gradient:  0.284994232844204
iteration : 8222
train acc:  0.796875
train loss:  0.4112014174461365
train gradient:  0.21125631156595953
iteration : 8223
train acc:  0.8671875
train loss:  0.3081973195075989
train gradient:  0.15450154750224282
iteration : 8224
train acc:  0.875
train loss:  0.3139359951019287
train gradient:  0.15538198697005393
iteration : 8225
train acc:  0.8046875
train loss:  0.36262381076812744
train gradient:  0.2021446045051775
iteration : 8226
train acc:  0.8125
train loss:  0.3783452808856964
train gradient:  0.2321317226483257
iteration : 8227
train acc:  0.8671875
train loss:  0.3799930810928345
train gradient:  0.22580720784172442
iteration : 8228
train acc:  0.8515625
train loss:  0.3536657691001892
train gradient:  0.1501609784998627
iteration : 8229
train acc:  0.84375
train loss:  0.34000152349472046
train gradient:  0.19101808506448303
iteration : 8230
train acc:  0.8984375
train loss:  0.288169264793396
train gradient:  0.11812723336303203
iteration : 8231
train acc:  0.890625
train loss:  0.29616841673851013
train gradient:  0.1642297708123558
iteration : 8232
train acc:  0.859375
train loss:  0.3425161838531494
train gradient:  0.20577158534870696
iteration : 8233
train acc:  0.875
train loss:  0.27372127771377563
train gradient:  0.13136138087752208
iteration : 8234
train acc:  0.8984375
train loss:  0.2518746256828308
train gradient:  0.14508034529694502
iteration : 8235
train acc:  0.8984375
train loss:  0.2427639365196228
train gradient:  0.09698853316096859
iteration : 8236
train acc:  0.8515625
train loss:  0.32996994256973267
train gradient:  0.1385857208151709
iteration : 8237
train acc:  0.8046875
train loss:  0.4152872562408447
train gradient:  0.1986566640597906
iteration : 8238
train acc:  0.8671875
train loss:  0.2971882224082947
train gradient:  0.12775562145194408
iteration : 8239
train acc:  0.8515625
train loss:  0.32735663652420044
train gradient:  0.1439177866637736
iteration : 8240
train acc:  0.8515625
train loss:  0.3405773937702179
train gradient:  0.23289445169124323
iteration : 8241
train acc:  0.8203125
train loss:  0.31941109895706177
train gradient:  0.21156757400645315
iteration : 8242
train acc:  0.8984375
train loss:  0.3497759699821472
train gradient:  0.13522272733029767
iteration : 8243
train acc:  0.828125
train loss:  0.40576404333114624
train gradient:  0.2297708346783332
iteration : 8244
train acc:  0.8828125
train loss:  0.32607364654541016
train gradient:  0.09856566132588293
iteration : 8245
train acc:  0.828125
train loss:  0.42728832364082336
train gradient:  0.40953090255046004
iteration : 8246
train acc:  0.875
train loss:  0.29611730575561523
train gradient:  0.15106776968703817
iteration : 8247
train acc:  0.8359375
train loss:  0.3347645699977875
train gradient:  0.14500455475704263
iteration : 8248
train acc:  0.8828125
train loss:  0.3412157893180847
train gradient:  0.1719717480418771
iteration : 8249
train acc:  0.8125
train loss:  0.35764652490615845
train gradient:  0.21485017334939077
iteration : 8250
train acc:  0.875
train loss:  0.281533420085907
train gradient:  0.11215515338493881
iteration : 8251
train acc:  0.9140625
train loss:  0.2238013744354248
train gradient:  0.20491351490518195
iteration : 8252
train acc:  0.8671875
train loss:  0.3406052589416504
train gradient:  0.27449651383132756
iteration : 8253
train acc:  0.8671875
train loss:  0.27304232120513916
train gradient:  0.13916747030160317
iteration : 8254
train acc:  0.8828125
train loss:  0.33345675468444824
train gradient:  0.19271696946146122
iteration : 8255
train acc:  0.796875
train loss:  0.4209117889404297
train gradient:  0.2780179796268062
iteration : 8256
train acc:  0.8125
train loss:  0.33946797251701355
train gradient:  0.16265657209316695
iteration : 8257
train acc:  0.8828125
train loss:  0.26903706789016724
train gradient:  0.10566500783764057
iteration : 8258
train acc:  0.875
train loss:  0.30093419551849365
train gradient:  0.14262778817110142
iteration : 8259
train acc:  0.8203125
train loss:  0.42702412605285645
train gradient:  0.27727982120742833
iteration : 8260
train acc:  0.8984375
train loss:  0.25151151418685913
train gradient:  0.11673190482756508
iteration : 8261
train acc:  0.890625
train loss:  0.25244757533073425
train gradient:  0.11596780992609994
iteration : 8262
train acc:  0.8515625
train loss:  0.3801324963569641
train gradient:  0.2541231708932184
iteration : 8263
train acc:  0.890625
train loss:  0.3272746801376343
train gradient:  0.16349836335761375
iteration : 8264
train acc:  0.84375
train loss:  0.3960530161857605
train gradient:  0.21946041776724962
iteration : 8265
train acc:  0.8671875
train loss:  0.3127719759941101
train gradient:  0.14696997433448042
iteration : 8266
train acc:  0.8671875
train loss:  0.29854509234428406
train gradient:  0.22667583972064625
iteration : 8267
train acc:  0.8671875
train loss:  0.27642062306404114
train gradient:  0.1418111723733558
iteration : 8268
train acc:  0.8046875
train loss:  0.40313681960105896
train gradient:  0.307877168838538
iteration : 8269
train acc:  0.9375
train loss:  0.26939916610717773
train gradient:  0.12549260065835638
iteration : 8270
train acc:  0.8828125
train loss:  0.29032352566719055
train gradient:  0.13848409497192532
iteration : 8271
train acc:  0.8046875
train loss:  0.40900954604148865
train gradient:  0.286293593330011
iteration : 8272
train acc:  0.828125
train loss:  0.3346085548400879
train gradient:  0.27543655717495824
iteration : 8273
train acc:  0.8359375
train loss:  0.37964898347854614
train gradient:  0.24124243322332756
iteration : 8274
train acc:  0.8359375
train loss:  0.36608076095581055
train gradient:  0.21919060305295168
iteration : 8275
train acc:  0.875
train loss:  0.3505818843841553
train gradient:  0.1824590329832361
iteration : 8276
train acc:  0.828125
train loss:  0.33601921796798706
train gradient:  0.22844273577340488
iteration : 8277
train acc:  0.8203125
train loss:  0.4318770170211792
train gradient:  0.2836831397380258
iteration : 8278
train acc:  0.8671875
train loss:  0.3506659269332886
train gradient:  0.15627880191365096
iteration : 8279
train acc:  0.8828125
train loss:  0.3272967040538788
train gradient:  0.13867747735729444
iteration : 8280
train acc:  0.84375
train loss:  0.37799304723739624
train gradient:  0.22035154403358842
iteration : 8281
train acc:  0.8984375
train loss:  0.28634780645370483
train gradient:  0.17386300681987038
iteration : 8282
train acc:  0.8515625
train loss:  0.36182886362075806
train gradient:  0.23844597075834464
iteration : 8283
train acc:  0.828125
train loss:  0.3452902138233185
train gradient:  0.26792161202584724
iteration : 8284
train acc:  0.796875
train loss:  0.4433164894580841
train gradient:  0.3074204727781937
iteration : 8285
train acc:  0.7890625
train loss:  0.44058093428611755
train gradient:  0.34700569514175134
iteration : 8286
train acc:  0.890625
train loss:  0.2889411747455597
train gradient:  0.33729407590561444
iteration : 8287
train acc:  0.875
train loss:  0.3328186273574829
train gradient:  0.18188226580069983
iteration : 8288
train acc:  0.890625
train loss:  0.3202146887779236
train gradient:  0.15487687065642575
iteration : 8289
train acc:  0.859375
train loss:  0.37359559535980225
train gradient:  0.22904310094140662
iteration : 8290
train acc:  0.859375
train loss:  0.37024736404418945
train gradient:  0.19937222412900096
iteration : 8291
train acc:  0.8515625
train loss:  0.3201103210449219
train gradient:  0.24042822370110778
iteration : 8292
train acc:  0.7890625
train loss:  0.435122013092041
train gradient:  0.2725107871578806
iteration : 8293
train acc:  0.828125
train loss:  0.4476245045661926
train gradient:  0.28944012093545524
iteration : 8294
train acc:  0.859375
train loss:  0.2909489870071411
train gradient:  0.13743437412083678
iteration : 8295
train acc:  0.875
train loss:  0.308315247297287
train gradient:  0.11143736260353987
iteration : 8296
train acc:  0.8671875
train loss:  0.28135445713996887
train gradient:  0.11496575505060316
iteration : 8297
train acc:  0.8515625
train loss:  0.3426300883293152
train gradient:  0.13997011976651497
iteration : 8298
train acc:  0.921875
train loss:  0.3137255609035492
train gradient:  0.1593199829201129
iteration : 8299
train acc:  0.890625
train loss:  0.27387964725494385
train gradient:  0.1605402327568169
iteration : 8300
train acc:  0.84375
train loss:  0.3490889072418213
train gradient:  0.15406428980971426
iteration : 8301
train acc:  0.8671875
train loss:  0.3943905234336853
train gradient:  0.22224126064644434
iteration : 8302
train acc:  0.859375
train loss:  0.2919044494628906
train gradient:  0.17557158561898287
iteration : 8303
train acc:  0.8125
train loss:  0.37874215841293335
train gradient:  0.27995418792294874
iteration : 8304
train acc:  0.8828125
train loss:  0.2822859287261963
train gradient:  0.12702875444681483
iteration : 8305
train acc:  0.8046875
train loss:  0.36876752972602844
train gradient:  0.19283300371943501
iteration : 8306
train acc:  0.859375
train loss:  0.345981627702713
train gradient:  0.14833861179659996
iteration : 8307
train acc:  0.859375
train loss:  0.29795700311660767
train gradient:  0.18569240538099446
iteration : 8308
train acc:  0.9140625
train loss:  0.2625500559806824
train gradient:  0.1655757569002982
iteration : 8309
train acc:  0.796875
train loss:  0.4606313705444336
train gradient:  0.3663361487455851
iteration : 8310
train acc:  0.8671875
train loss:  0.29181206226348877
train gradient:  0.12343205562699358
iteration : 8311
train acc:  0.828125
train loss:  0.4006426930427551
train gradient:  0.1961453901519609
iteration : 8312
train acc:  0.8515625
train loss:  0.32765528559684753
train gradient:  0.15968738266982613
iteration : 8313
train acc:  0.8125
train loss:  0.3310462534427643
train gradient:  0.15699047781298453
iteration : 8314
train acc:  0.890625
train loss:  0.3286714553833008
train gradient:  0.1647849386279564
iteration : 8315
train acc:  0.890625
train loss:  0.31418919563293457
train gradient:  0.16080798991412024
iteration : 8316
train acc:  0.859375
train loss:  0.33961397409439087
train gradient:  0.19577551097004392
iteration : 8317
train acc:  0.84375
train loss:  0.35371842980384827
train gradient:  0.1918101214929662
iteration : 8318
train acc:  0.8125
train loss:  0.3947392702102661
train gradient:  0.17854756537475355
iteration : 8319
train acc:  0.84375
train loss:  0.34314215183258057
train gradient:  0.1746325466989087
iteration : 8320
train acc:  0.828125
train loss:  0.38120150566101074
train gradient:  0.16573566308783327
iteration : 8321
train acc:  0.8046875
train loss:  0.4243929088115692
train gradient:  0.2796272847703761
iteration : 8322
train acc:  0.8515625
train loss:  0.3520466089248657
train gradient:  0.17064993695926228
iteration : 8323
train acc:  0.8515625
train loss:  0.3200642466545105
train gradient:  0.17820087319170624
iteration : 8324
train acc:  0.8828125
train loss:  0.2840350866317749
train gradient:  0.13416705191004225
iteration : 8325
train acc:  0.8046875
train loss:  0.37769776582717896
train gradient:  0.22172685388059848
iteration : 8326
train acc:  0.828125
train loss:  0.35959139466285706
train gradient:  0.1986782344152053
iteration : 8327
train acc:  0.890625
train loss:  0.27706605195999146
train gradient:  0.10544070427761369
iteration : 8328
train acc:  0.8671875
train loss:  0.3576519191265106
train gradient:  0.16408694955162717
iteration : 8329
train acc:  0.84375
train loss:  0.3282236158847809
train gradient:  0.13388619091062273
iteration : 8330
train acc:  0.8671875
train loss:  0.302383691072464
train gradient:  0.14305303371490002
iteration : 8331
train acc:  0.7890625
train loss:  0.3989381790161133
train gradient:  0.37247859922344834
iteration : 8332
train acc:  0.8671875
train loss:  0.32837074995040894
train gradient:  0.15933775335588962
iteration : 8333
train acc:  0.8359375
train loss:  0.3457229733467102
train gradient:  0.15251963530410192
iteration : 8334
train acc:  0.875
train loss:  0.3227313160896301
train gradient:  0.11583752275022795
iteration : 8335
train acc:  0.8359375
train loss:  0.3353637158870697
train gradient:  0.17426373765241226
iteration : 8336
train acc:  0.796875
train loss:  0.3926478922367096
train gradient:  0.184375070655523
iteration : 8337
train acc:  0.875
train loss:  0.2696532905101776
train gradient:  0.13026911093472998
iteration : 8338
train acc:  0.8828125
train loss:  0.30248355865478516
train gradient:  0.13013330111967603
iteration : 8339
train acc:  0.828125
train loss:  0.38828521966934204
train gradient:  0.17831512618909745
iteration : 8340
train acc:  0.796875
train loss:  0.3788830637931824
train gradient:  0.22615469899120827
iteration : 8341
train acc:  0.7890625
train loss:  0.45659276843070984
train gradient:  0.29473391118568615
iteration : 8342
train acc:  0.8828125
train loss:  0.31549271941185
train gradient:  0.2288207739218848
iteration : 8343
train acc:  0.828125
train loss:  0.3591267466545105
train gradient:  0.21029622747222182
iteration : 8344
train acc:  0.890625
train loss:  0.3011198937892914
train gradient:  0.17604497661238167
iteration : 8345
train acc:  0.84375
train loss:  0.340911865234375
train gradient:  0.1771079163443308
iteration : 8346
train acc:  0.8984375
train loss:  0.25978145003318787
train gradient:  0.1084875038036143
iteration : 8347
train acc:  0.828125
train loss:  0.3674287796020508
train gradient:  0.15334777781048353
iteration : 8348
train acc:  0.8515625
train loss:  0.31825509667396545
train gradient:  0.16414429178498097
iteration : 8349
train acc:  0.8671875
train loss:  0.3046972453594208
train gradient:  0.1358669913755947
iteration : 8350
train acc:  0.8515625
train loss:  0.3234124183654785
train gradient:  0.1823530024866604
iteration : 8351
train acc:  0.796875
train loss:  0.3980403542518616
train gradient:  0.14076726947496862
iteration : 8352
train acc:  0.796875
train loss:  0.3680124282836914
train gradient:  0.15934444911112977
iteration : 8353
train acc:  0.90625
train loss:  0.2822363078594208
train gradient:  0.13745504755358706
iteration : 8354
train acc:  0.8671875
train loss:  0.2972705364227295
train gradient:  0.10926981640106766
iteration : 8355
train acc:  0.796875
train loss:  0.4148716926574707
train gradient:  0.3995495218128629
iteration : 8356
train acc:  0.8515625
train loss:  0.352405846118927
train gradient:  0.1950002146633081
iteration : 8357
train acc:  0.8359375
train loss:  0.4019826352596283
train gradient:  0.22906024411696194
iteration : 8358
train acc:  0.859375
train loss:  0.3208129405975342
train gradient:  0.14334383718595406
iteration : 8359
train acc:  0.859375
train loss:  0.3144862651824951
train gradient:  0.19268041325594398
iteration : 8360
train acc:  0.8515625
train loss:  0.31770944595336914
train gradient:  0.11172950080079508
iteration : 8361
train acc:  0.8515625
train loss:  0.2874056398868561
train gradient:  0.10464426667031668
iteration : 8362
train acc:  0.890625
train loss:  0.30819880962371826
train gradient:  0.1517803777358081
iteration : 8363
train acc:  0.8125
train loss:  0.37322020530700684
train gradient:  0.16879734997626517
iteration : 8364
train acc:  0.8671875
train loss:  0.3282177448272705
train gradient:  0.19088105398487856
iteration : 8365
train acc:  0.8359375
train loss:  0.3656124472618103
train gradient:  0.20330146804713928
iteration : 8366
train acc:  0.796875
train loss:  0.3603982925415039
train gradient:  0.17041791243224447
iteration : 8367
train acc:  0.859375
train loss:  0.32886701822280884
train gradient:  0.17800806183109574
iteration : 8368
train acc:  0.859375
train loss:  0.3491414785385132
train gradient:  0.16836948356695747
iteration : 8369
train acc:  0.8359375
train loss:  0.3539891839027405
train gradient:  0.21826670076355265
iteration : 8370
train acc:  0.8671875
train loss:  0.3401370644569397
train gradient:  0.1322463612490426
iteration : 8371
train acc:  0.8515625
train loss:  0.3481510281562805
train gradient:  0.15858253854268495
iteration : 8372
train acc:  0.8359375
train loss:  0.3755190670490265
train gradient:  0.19433677239770114
iteration : 8373
train acc:  0.796875
train loss:  0.4361221194267273
train gradient:  0.3101099945227373
iteration : 8374
train acc:  0.859375
train loss:  0.3130866587162018
train gradient:  0.16414569863381145
iteration : 8375
train acc:  0.859375
train loss:  0.29137909412384033
train gradient:  0.10649308239797979
iteration : 8376
train acc:  0.84375
train loss:  0.323229044675827
train gradient:  0.14911868951312152
iteration : 8377
train acc:  0.8984375
train loss:  0.2751382887363434
train gradient:  0.13879140832099757
iteration : 8378
train acc:  0.8515625
train loss:  0.36653369665145874
train gradient:  0.24677666272104082
iteration : 8379
train acc:  0.84375
train loss:  0.33989250659942627
train gradient:  0.20414864874655375
iteration : 8380
train acc:  0.8828125
train loss:  0.3375301957130432
train gradient:  0.2723779070884313
iteration : 8381
train acc:  0.9375
train loss:  0.23054899275302887
train gradient:  0.09362170228193363
iteration : 8382
train acc:  0.859375
train loss:  0.3448525667190552
train gradient:  0.16642037981095145
iteration : 8383
train acc:  0.875
train loss:  0.2985340356826782
train gradient:  0.12905329756426787
iteration : 8384
train acc:  0.8828125
train loss:  0.2882959842681885
train gradient:  0.09886486545982744
iteration : 8385
train acc:  0.8671875
train loss:  0.3267430067062378
train gradient:  0.15242235054175007
iteration : 8386
train acc:  0.8515625
train loss:  0.36728447675704956
train gradient:  0.16376344986834396
iteration : 8387
train acc:  0.8359375
train loss:  0.35799622535705566
train gradient:  0.19055455264185206
iteration : 8388
train acc:  0.8828125
train loss:  0.3011750876903534
train gradient:  0.15272535340764434
iteration : 8389
train acc:  0.8984375
train loss:  0.34599727392196655
train gradient:  0.16454607807110003
iteration : 8390
train acc:  0.8359375
train loss:  0.4321932792663574
train gradient:  0.2705006706558608
iteration : 8391
train acc:  0.8671875
train loss:  0.31243184208869934
train gradient:  0.15215036724027653
iteration : 8392
train acc:  0.859375
train loss:  0.2901744842529297
train gradient:  0.18092100521482207
iteration : 8393
train acc:  0.8125
train loss:  0.36098921298980713
train gradient:  0.36778021934824495
iteration : 8394
train acc:  0.8359375
train loss:  0.35934680700302124
train gradient:  0.209861023491944
iteration : 8395
train acc:  0.8125
train loss:  0.4239664673805237
train gradient:  0.1922335504395919
iteration : 8396
train acc:  0.8203125
train loss:  0.3540913164615631
train gradient:  0.20379003546123556
iteration : 8397
train acc:  0.890625
train loss:  0.27570733428001404
train gradient:  0.12776451151896662
iteration : 8398
train acc:  0.8671875
train loss:  0.31816062331199646
train gradient:  0.2042372379538817
iteration : 8399
train acc:  0.875
train loss:  0.3263764977455139
train gradient:  0.1981678634013498
iteration : 8400
train acc:  0.828125
train loss:  0.3450753092765808
train gradient:  0.1420539803989099
iteration : 8401
train acc:  0.9140625
train loss:  0.2986606955528259
train gradient:  0.14914187651944488
iteration : 8402
train acc:  0.8515625
train loss:  0.30200040340423584
train gradient:  0.14717643750863277
iteration : 8403
train acc:  0.859375
train loss:  0.3156965374946594
train gradient:  0.19290119156078156
iteration : 8404
train acc:  0.8984375
train loss:  0.29430946707725525
train gradient:  0.31266801861199855
iteration : 8405
train acc:  0.8203125
train loss:  0.4354479908943176
train gradient:  0.3212860177824597
iteration : 8406
train acc:  0.8359375
train loss:  0.4071968197822571
train gradient:  0.2450509628520224
iteration : 8407
train acc:  0.8828125
train loss:  0.27079451084136963
train gradient:  0.14482159848204118
iteration : 8408
train acc:  0.90625
train loss:  0.2752127945423126
train gradient:  0.17633148981019545
iteration : 8409
train acc:  0.875
train loss:  0.31377071142196655
train gradient:  0.13754085979246933
iteration : 8410
train acc:  0.8671875
train loss:  0.3187047243118286
train gradient:  0.22216714117545622
iteration : 8411
train acc:  0.890625
train loss:  0.30458205938339233
train gradient:  0.11484262875847498
iteration : 8412
train acc:  0.875
train loss:  0.3244549036026001
train gradient:  0.17485121234552145
iteration : 8413
train acc:  0.875
train loss:  0.3087371289730072
train gradient:  0.13165211894183912
iteration : 8414
train acc:  0.8046875
train loss:  0.40613871812820435
train gradient:  0.1995558637576924
iteration : 8415
train acc:  0.8515625
train loss:  0.306339830160141
train gradient:  0.1820745686639897
iteration : 8416
train acc:  0.8828125
train loss:  0.3802768886089325
train gradient:  0.20970488609883697
iteration : 8417
train acc:  0.84375
train loss:  0.37653979659080505
train gradient:  0.19396775293193907
iteration : 8418
train acc:  0.828125
train loss:  0.3294945955276489
train gradient:  0.19771324616026617
iteration : 8419
train acc:  0.8515625
train loss:  0.3246018886566162
train gradient:  0.12024632435056382
iteration : 8420
train acc:  0.8125
train loss:  0.3735886514186859
train gradient:  0.29142369494617876
iteration : 8421
train acc:  0.8359375
train loss:  0.33866626024246216
train gradient:  0.21112690437290313
iteration : 8422
train acc:  0.9140625
train loss:  0.2651367485523224
train gradient:  0.10521041023528102
iteration : 8423
train acc:  0.890625
train loss:  0.2779611647129059
train gradient:  0.12571148764982554
iteration : 8424
train acc:  0.828125
train loss:  0.35996586084365845
train gradient:  0.2386077729378818
iteration : 8425
train acc:  0.890625
train loss:  0.27126267552375793
train gradient:  0.15869414376491026
iteration : 8426
train acc:  0.8671875
train loss:  0.2977507710456848
train gradient:  0.17508639856293218
iteration : 8427
train acc:  0.8515625
train loss:  0.35801804065704346
train gradient:  0.19803804434675754
iteration : 8428
train acc:  0.8203125
train loss:  0.3141603469848633
train gradient:  0.17867761662836712
iteration : 8429
train acc:  0.7734375
train loss:  0.3932754695415497
train gradient:  0.22729355428944756
iteration : 8430
train acc:  0.875
train loss:  0.25819045305252075
train gradient:  0.16388498457534634
iteration : 8431
train acc:  0.8828125
train loss:  0.2627343237400055
train gradient:  0.1298977450015365
iteration : 8432
train acc:  0.8671875
train loss:  0.3059501647949219
train gradient:  0.15143765427543737
iteration : 8433
train acc:  0.859375
train loss:  0.30550476908683777
train gradient:  0.25374770632592303
iteration : 8434
train acc:  0.8359375
train loss:  0.3688018321990967
train gradient:  0.18004698876171177
iteration : 8435
train acc:  0.8671875
train loss:  0.3037298321723938
train gradient:  0.17767066699705147
iteration : 8436
train acc:  0.859375
train loss:  0.35280877351760864
train gradient:  0.2037114374071965
iteration : 8437
train acc:  0.8515625
train loss:  0.33444178104400635
train gradient:  0.2413240439485469
iteration : 8438
train acc:  0.84375
train loss:  0.38964253664016724
train gradient:  0.18289832270514145
iteration : 8439
train acc:  0.8515625
train loss:  0.3096490502357483
train gradient:  0.15168199061353682
iteration : 8440
train acc:  0.8671875
train loss:  0.2532210946083069
train gradient:  0.1666581127869054
iteration : 8441
train acc:  0.8125
train loss:  0.4690857529640198
train gradient:  0.26750526791289264
iteration : 8442
train acc:  0.890625
train loss:  0.27026858925819397
train gradient:  0.0965206745484044
iteration : 8443
train acc:  0.8125
train loss:  0.34884363412857056
train gradient:  0.17163433502960834
iteration : 8444
train acc:  0.8828125
train loss:  0.31867295503616333
train gradient:  0.16901320087137955
iteration : 8445
train acc:  0.8203125
train loss:  0.31709614396095276
train gradient:  0.10667120543041166
iteration : 8446
train acc:  0.875
train loss:  0.3092691898345947
train gradient:  0.11964971593595235
iteration : 8447
train acc:  0.8203125
train loss:  0.3792373538017273
train gradient:  0.20052244840798833
iteration : 8448
train acc:  0.8828125
train loss:  0.2627362310886383
train gradient:  0.12959736754328416
iteration : 8449
train acc:  0.828125
train loss:  0.37130606174468994
train gradient:  0.23236043140220974
iteration : 8450
train acc:  0.8828125
train loss:  0.32675907015800476
train gradient:  0.15670539798439653
iteration : 8451
train acc:  0.84375
train loss:  0.3680438995361328
train gradient:  0.19593425987539365
iteration : 8452
train acc:  0.8984375
train loss:  0.28431078791618347
train gradient:  0.09832986409904451
iteration : 8453
train acc:  0.8671875
train loss:  0.3361439108848572
train gradient:  0.23876988841654223
iteration : 8454
train acc:  0.8984375
train loss:  0.28789079189300537
train gradient:  0.1114047521156527
iteration : 8455
train acc:  0.875
train loss:  0.2865257263183594
train gradient:  0.19449588121460126
iteration : 8456
train acc:  0.8046875
train loss:  0.4253106117248535
train gradient:  0.33435783982430706
iteration : 8457
train acc:  0.8515625
train loss:  0.3000493049621582
train gradient:  0.1656680246659626
iteration : 8458
train acc:  0.8515625
train loss:  0.306127667427063
train gradient:  0.1432742423691218
iteration : 8459
train acc:  0.765625
train loss:  0.5557883977890015
train gradient:  0.4846696338818273
iteration : 8460
train acc:  0.890625
train loss:  0.2782011330127716
train gradient:  0.12185188634385047
iteration : 8461
train acc:  0.8671875
train loss:  0.3020980656147003
train gradient:  0.1606895658546817
iteration : 8462
train acc:  0.875
train loss:  0.2972773313522339
train gradient:  0.11962501432605038
iteration : 8463
train acc:  0.8828125
train loss:  0.2502930164337158
train gradient:  0.1447568627288528
iteration : 8464
train acc:  0.8359375
train loss:  0.3903238773345947
train gradient:  0.24673807503047085
iteration : 8465
train acc:  0.859375
train loss:  0.33848172426223755
train gradient:  0.2401890490060332
iteration : 8466
train acc:  0.8359375
train loss:  0.3969901502132416
train gradient:  0.23083769351491332
iteration : 8467
train acc:  0.8359375
train loss:  0.3648751974105835
train gradient:  0.1861513556519509
iteration : 8468
train acc:  0.8671875
train loss:  0.3286207914352417
train gradient:  0.12072994037790385
iteration : 8469
train acc:  0.84375
train loss:  0.34873074293136597
train gradient:  0.19084549343193669
iteration : 8470
train acc:  0.859375
train loss:  0.35834383964538574
train gradient:  0.18195678956600425
iteration : 8471
train acc:  0.90625
train loss:  0.2831133306026459
train gradient:  0.1407186898558602
iteration : 8472
train acc:  0.8046875
train loss:  0.35513147711753845
train gradient:  0.2839167857143136
iteration : 8473
train acc:  0.8671875
train loss:  0.3460099697113037
train gradient:  0.13374100437646735
iteration : 8474
train acc:  0.859375
train loss:  0.3316764533519745
train gradient:  0.14960316503196186
iteration : 8475
train acc:  0.8203125
train loss:  0.38915330171585083
train gradient:  0.21303652482471358
iteration : 8476
train acc:  0.8359375
train loss:  0.3344123065471649
train gradient:  0.1568589867951573
iteration : 8477
train acc:  0.8515625
train loss:  0.3604496717453003
train gradient:  0.197293838138574
iteration : 8478
train acc:  0.8125
train loss:  0.3614117503166199
train gradient:  0.18745288855092596
iteration : 8479
train acc:  0.875
train loss:  0.27497103810310364
train gradient:  0.10219571272842795
iteration : 8480
train acc:  0.828125
train loss:  0.3244746923446655
train gradient:  0.1731615759106283
iteration : 8481
train acc:  0.890625
train loss:  0.2979816496372223
train gradient:  0.14346603387352985
iteration : 8482
train acc:  0.859375
train loss:  0.3605480194091797
train gradient:  0.1838930817778623
iteration : 8483
train acc:  0.828125
train loss:  0.3969433307647705
train gradient:  0.23271828394272753
iteration : 8484
train acc:  0.8203125
train loss:  0.38978055119514465
train gradient:  0.18481556119010484
iteration : 8485
train acc:  0.890625
train loss:  0.2866097688674927
train gradient:  0.17239266505935535
iteration : 8486
train acc:  0.8828125
train loss:  0.34240156412124634
train gradient:  0.16144937453909303
iteration : 8487
train acc:  0.828125
train loss:  0.40639710426330566
train gradient:  0.26747492794100136
iteration : 8488
train acc:  0.796875
train loss:  0.38432446122169495
train gradient:  0.21139755817479833
iteration : 8489
train acc:  0.828125
train loss:  0.3662364184856415
train gradient:  0.17910899062426022
iteration : 8490
train acc:  0.8359375
train loss:  0.35963642597198486
train gradient:  0.2190767734456918
iteration : 8491
train acc:  0.90625
train loss:  0.2470114529132843
train gradient:  0.11934518387788283
iteration : 8492
train acc:  0.8828125
train loss:  0.3001296818256378
train gradient:  0.1388081950481451
iteration : 8493
train acc:  0.8671875
train loss:  0.38129523396492004
train gradient:  0.19215602042146984
iteration : 8494
train acc:  0.84375
train loss:  0.31305164098739624
train gradient:  0.1455782679225487
iteration : 8495
train acc:  0.875
train loss:  0.32123613357543945
train gradient:  0.1632273645198866
iteration : 8496
train acc:  0.859375
train loss:  0.30067378282546997
train gradient:  0.1295049206259748
iteration : 8497
train acc:  0.8515625
train loss:  0.3092409372329712
train gradient:  0.1169281563812405
iteration : 8498
train acc:  0.8046875
train loss:  0.3864715099334717
train gradient:  0.17512663324759115
iteration : 8499
train acc:  0.84375
train loss:  0.3389144837856293
train gradient:  0.16615287364188253
iteration : 8500
train acc:  0.875
train loss:  0.2840272784233093
train gradient:  0.12146153184989181
iteration : 8501
train acc:  0.8515625
train loss:  0.3929930627346039
train gradient:  0.16909543575926336
iteration : 8502
train acc:  0.875
train loss:  0.281742125749588
train gradient:  0.1237042126053945
iteration : 8503
train acc:  0.84375
train loss:  0.32032346725463867
train gradient:  0.16520767445792875
iteration : 8504
train acc:  0.859375
train loss:  0.3300098478794098
train gradient:  0.21205019316959428
iteration : 8505
train acc:  0.875
train loss:  0.35398948192596436
train gradient:  0.18443395365707677
iteration : 8506
train acc:  0.8828125
train loss:  0.31434834003448486
train gradient:  0.12301234331127656
iteration : 8507
train acc:  0.8515625
train loss:  0.38981711864471436
train gradient:  0.23904832791156444
iteration : 8508
train acc:  0.8125
train loss:  0.43126988410949707
train gradient:  0.21987902438423249
iteration : 8509
train acc:  0.8828125
train loss:  0.25171661376953125
train gradient:  0.11462658431943913
iteration : 8510
train acc:  0.8359375
train loss:  0.3493334949016571
train gradient:  0.26761371703684916
iteration : 8511
train acc:  0.8203125
train loss:  0.33752918243408203
train gradient:  0.18950077166356988
iteration : 8512
train acc:  0.84375
train loss:  0.3657079339027405
train gradient:  0.1614610005823857
iteration : 8513
train acc:  0.875
train loss:  0.3140675723552704
train gradient:  0.13647080833911462
iteration : 8514
train acc:  0.828125
train loss:  0.3799923062324524
train gradient:  0.175824930083303
iteration : 8515
train acc:  0.890625
train loss:  0.3042187988758087
train gradient:  0.10198609631083527
iteration : 8516
train acc:  0.8359375
train loss:  0.35568660497665405
train gradient:  0.33552694638087666
iteration : 8517
train acc:  0.8359375
train loss:  0.3770110607147217
train gradient:  0.15943773954109064
iteration : 8518
train acc:  0.796875
train loss:  0.46748289465904236
train gradient:  0.3125746434041544
iteration : 8519
train acc:  0.890625
train loss:  0.2785913348197937
train gradient:  0.140667993825768
iteration : 8520
train acc:  0.859375
train loss:  0.27729102969169617
train gradient:  0.14318267703273618
iteration : 8521
train acc:  0.8359375
train loss:  0.36302536725997925
train gradient:  0.20943476285024623
iteration : 8522
train acc:  0.8671875
train loss:  0.3139660358428955
train gradient:  0.21016037697569268
iteration : 8523
train acc:  0.8671875
train loss:  0.3262210786342621
train gradient:  0.1519394814838499
iteration : 8524
train acc:  0.875
train loss:  0.30036863684654236
train gradient:  0.12345685386682675
iteration : 8525
train acc:  0.859375
train loss:  0.33006492257118225
train gradient:  0.20976838782838977
iteration : 8526
train acc:  0.8671875
train loss:  0.32941552996635437
train gradient:  0.2615550087749143
iteration : 8527
train acc:  0.8828125
train loss:  0.2845507264137268
train gradient:  0.165975951614113
iteration : 8528
train acc:  0.859375
train loss:  0.31583452224731445
train gradient:  0.15701057798433027
iteration : 8529
train acc:  0.8671875
train loss:  0.2742469608783722
train gradient:  0.12578076343110756
iteration : 8530
train acc:  0.859375
train loss:  0.38115131855010986
train gradient:  0.19245923367510084
iteration : 8531
train acc:  0.890625
train loss:  0.24642980098724365
train gradient:  0.1606093160397683
iteration : 8532
train acc:  0.8046875
train loss:  0.4167059659957886
train gradient:  0.17757346338249705
iteration : 8533
train acc:  0.875
train loss:  0.2944709062576294
train gradient:  0.1438150392820048
iteration : 8534
train acc:  0.796875
train loss:  0.4284113645553589
train gradient:  0.2729862695698593
iteration : 8535
train acc:  0.875
train loss:  0.29828476905822754
train gradient:  0.10319331558753846
iteration : 8536
train acc:  0.84375
train loss:  0.33426299691200256
train gradient:  0.163592602347015
iteration : 8537
train acc:  0.8828125
train loss:  0.3285180926322937
train gradient:  0.1417008968446298
iteration : 8538
train acc:  0.8671875
train loss:  0.32277053594589233
train gradient:  0.12518854205804808
iteration : 8539
train acc:  0.8515625
train loss:  0.35832688212394714
train gradient:  0.1475774624828176
iteration : 8540
train acc:  0.8515625
train loss:  0.35003581643104553
train gradient:  0.15978076685008374
iteration : 8541
train acc:  0.859375
train loss:  0.3432133197784424
train gradient:  0.12722441349000885
iteration : 8542
train acc:  0.8203125
train loss:  0.4039914011955261
train gradient:  0.24081124141290094
iteration : 8543
train acc:  0.828125
train loss:  0.3367738723754883
train gradient:  0.19959972841849055
iteration : 8544
train acc:  0.8359375
train loss:  0.3107950985431671
train gradient:  0.20457671497718544
iteration : 8545
train acc:  0.8671875
train loss:  0.30829548835754395
train gradient:  0.12239194454207414
iteration : 8546
train acc:  0.8125
train loss:  0.37246859073638916
train gradient:  0.2924806694875058
iteration : 8547
train acc:  0.8828125
train loss:  0.29438504576683044
train gradient:  0.1769302409542748
iteration : 8548
train acc:  0.8828125
train loss:  0.2901064455509186
train gradient:  0.1843118786521033
iteration : 8549
train acc:  0.8515625
train loss:  0.35205209255218506
train gradient:  0.16830214138003752
iteration : 8550
train acc:  0.8359375
train loss:  0.3488863706588745
train gradient:  0.17862581071318484
iteration : 8551
train acc:  0.8515625
train loss:  0.4171881079673767
train gradient:  0.2936274584337597
iteration : 8552
train acc:  0.84375
train loss:  0.32371973991394043
train gradient:  0.1993116652756889
iteration : 8553
train acc:  0.8828125
train loss:  0.28223639726638794
train gradient:  0.14007140535739276
iteration : 8554
train acc:  0.84375
train loss:  0.3595966398715973
train gradient:  0.1683217892748325
iteration : 8555
train acc:  0.8203125
train loss:  0.403148889541626
train gradient:  0.21760939998478346
iteration : 8556
train acc:  0.8984375
train loss:  0.264645516872406
train gradient:  0.1646995085000581
iteration : 8557
train acc:  0.8515625
train loss:  0.31932002305984497
train gradient:  0.21806779162377285
iteration : 8558
train acc:  0.8984375
train loss:  0.3291541337966919
train gradient:  0.1196898685717322
iteration : 8559
train acc:  0.8515625
train loss:  0.3250824213027954
train gradient:  0.1668264978362754
iteration : 8560
train acc:  0.84375
train loss:  0.3538856506347656
train gradient:  0.14413787401237851
iteration : 8561
train acc:  0.8203125
train loss:  0.3828398585319519
train gradient:  0.20404830625256876
iteration : 8562
train acc:  0.8203125
train loss:  0.38715964555740356
train gradient:  0.16947052001471122
iteration : 8563
train acc:  0.875
train loss:  0.36380788683891296
train gradient:  0.1871687715998605
iteration : 8564
train acc:  0.8203125
train loss:  0.3310938775539398
train gradient:  0.21028545179400382
iteration : 8565
train acc:  0.890625
train loss:  0.2636081874370575
train gradient:  0.12441583346823491
iteration : 8566
train acc:  0.8203125
train loss:  0.37244802713394165
train gradient:  0.22651166281761947
iteration : 8567
train acc:  0.8671875
train loss:  0.27581286430358887
train gradient:  0.129664544280908
iteration : 8568
train acc:  0.859375
train loss:  0.2891671061515808
train gradient:  0.16577120942677215
iteration : 8569
train acc:  0.8515625
train loss:  0.3098772168159485
train gradient:  0.20460353521261493
iteration : 8570
train acc:  0.84375
train loss:  0.37353429198265076
train gradient:  0.2562315297456739
iteration : 8571
train acc:  0.84375
train loss:  0.38121429085731506
train gradient:  0.1460174543365552
iteration : 8572
train acc:  0.890625
train loss:  0.33170685172080994
train gradient:  0.19395351290900753
iteration : 8573
train acc:  0.78125
train loss:  0.4138841927051544
train gradient:  0.23371873168199148
iteration : 8574
train acc:  0.828125
train loss:  0.3377549946308136
train gradient:  0.16610736582311314
iteration : 8575
train acc:  0.84375
train loss:  0.32727301120758057
train gradient:  0.14137360252965542
iteration : 8576
train acc:  0.875
train loss:  0.2946986258029938
train gradient:  0.15733076975038246
iteration : 8577
train acc:  0.8515625
train loss:  0.3466205894947052
train gradient:  0.16006924895112234
iteration : 8578
train acc:  0.8671875
train loss:  0.28516459465026855
train gradient:  0.12610694761414074
iteration : 8579
train acc:  0.875
train loss:  0.3013749122619629
train gradient:  0.1505464579595049
iteration : 8580
train acc:  0.8125
train loss:  0.3656514286994934
train gradient:  0.18955503499395765
iteration : 8581
train acc:  0.796875
train loss:  0.3946114182472229
train gradient:  0.2933979375731238
iteration : 8582
train acc:  0.8203125
train loss:  0.30812233686447144
train gradient:  0.16765888160760278
iteration : 8583
train acc:  0.859375
train loss:  0.32174140214920044
train gradient:  0.11484108050619164
iteration : 8584
train acc:  0.8359375
train loss:  0.3465118706226349
train gradient:  0.220542978141172
iteration : 8585
train acc:  0.8515625
train loss:  0.3341396152973175
train gradient:  0.2038223418055467
iteration : 8586
train acc:  0.90625
train loss:  0.3152414560317993
train gradient:  0.24575556084736147
iteration : 8587
train acc:  0.8515625
train loss:  0.3434203565120697
train gradient:  0.1393193578174543
iteration : 8588
train acc:  0.8984375
train loss:  0.25163763761520386
train gradient:  0.1300209573225759
iteration : 8589
train acc:  0.8515625
train loss:  0.39189720153808594
train gradient:  0.22633868842796
iteration : 8590
train acc:  0.8671875
train loss:  0.3338000178337097
train gradient:  0.32370501957562087
iteration : 8591
train acc:  0.875
train loss:  0.30007874965667725
train gradient:  0.14953219881149027
iteration : 8592
train acc:  0.828125
train loss:  0.38865959644317627
train gradient:  0.23970833092531585
iteration : 8593
train acc:  0.8359375
train loss:  0.3399055600166321
train gradient:  0.21011532455497434
iteration : 8594
train acc:  0.890625
train loss:  0.27616453170776367
train gradient:  0.1110275399899798
iteration : 8595
train acc:  0.8671875
train loss:  0.27447864413261414
train gradient:  0.1563173265422269
iteration : 8596
train acc:  0.84375
train loss:  0.3239039480686188
train gradient:  0.22126848170288949
iteration : 8597
train acc:  0.859375
train loss:  0.31205153465270996
train gradient:  0.2754401608735665
iteration : 8598
train acc:  0.8359375
train loss:  0.3686455190181732
train gradient:  0.22364834980364007
iteration : 8599
train acc:  0.8359375
train loss:  0.3515791893005371
train gradient:  0.17823261026028953
iteration : 8600
train acc:  0.8359375
train loss:  0.34071066975593567
train gradient:  0.1549697981273011
iteration : 8601
train acc:  0.875
train loss:  0.30149078369140625
train gradient:  0.19290198885798238
iteration : 8602
train acc:  0.8125
train loss:  0.3747028112411499
train gradient:  0.20625222076020838
iteration : 8603
train acc:  0.8203125
train loss:  0.40005818009376526
train gradient:  0.28522932150470576
iteration : 8604
train acc:  0.8359375
train loss:  0.3454252779483795
train gradient:  0.18493176879309486
iteration : 8605
train acc:  0.8515625
train loss:  0.32024624943733215
train gradient:  0.20313688105730052
iteration : 8606
train acc:  0.859375
train loss:  0.2810366153717041
train gradient:  0.15566290166162314
iteration : 8607
train acc:  0.8203125
train loss:  0.43945127725601196
train gradient:  0.4718692660525338
iteration : 8608
train acc:  0.8046875
train loss:  0.3796766400337219
train gradient:  0.18855049241324529
iteration : 8609
train acc:  0.8671875
train loss:  0.33484500646591187
train gradient:  0.19128640016997916
iteration : 8610
train acc:  0.8046875
train loss:  0.4176989793777466
train gradient:  0.25689017891131244
iteration : 8611
train acc:  0.8671875
train loss:  0.29511165618896484
train gradient:  0.16701266922118577
iteration : 8612
train acc:  0.8515625
train loss:  0.34910064935684204
train gradient:  0.23565570462502028
iteration : 8613
train acc:  0.8515625
train loss:  0.3886268138885498
train gradient:  0.33959976520604196
iteration : 8614
train acc:  0.875
train loss:  0.28519153594970703
train gradient:  0.15964351035657354
iteration : 8615
train acc:  0.890625
train loss:  0.2749703824520111
train gradient:  0.2326032560675773
iteration : 8616
train acc:  0.90625
train loss:  0.2779058516025543
train gradient:  0.149069025717664
iteration : 8617
train acc:  0.859375
train loss:  0.3288249969482422
train gradient:  0.2586350808305471
iteration : 8618
train acc:  0.875
train loss:  0.323528915643692
train gradient:  0.1568741458002091
iteration : 8619
train acc:  0.8046875
train loss:  0.35188964009284973
train gradient:  0.21528519133766144
iteration : 8620
train acc:  0.8984375
train loss:  0.28547120094299316
train gradient:  0.12612907615627067
iteration : 8621
train acc:  0.78125
train loss:  0.4450508952140808
train gradient:  0.26615654118762544
iteration : 8622
train acc:  0.7890625
train loss:  0.3933058977127075
train gradient:  0.24520154471869557
iteration : 8623
train acc:  0.9296875
train loss:  0.2920937240123749
train gradient:  0.1260934632745209
iteration : 8624
train acc:  0.84375
train loss:  0.3692268431186676
train gradient:  0.1971110332016766
iteration : 8625
train acc:  0.8515625
train loss:  0.35811126232147217
train gradient:  0.26570778538143663
iteration : 8626
train acc:  0.828125
train loss:  0.3999207019805908
train gradient:  0.22512833393996168
iteration : 8627
train acc:  0.875
train loss:  0.28687191009521484
train gradient:  0.1402134583550802
iteration : 8628
train acc:  0.890625
train loss:  0.2958981990814209
train gradient:  0.11114073677814473
iteration : 8629
train acc:  0.890625
train loss:  0.27996185421943665
train gradient:  0.12465016355920647
iteration : 8630
train acc:  0.890625
train loss:  0.29905009269714355
train gradient:  0.1331729910905704
iteration : 8631
train acc:  0.828125
train loss:  0.37031686305999756
train gradient:  0.18595543287851823
iteration : 8632
train acc:  0.8359375
train loss:  0.39019566774368286
train gradient:  0.19079838679502703
iteration : 8633
train acc:  0.9140625
train loss:  0.25048768520355225
train gradient:  0.12957295383757744
iteration : 8634
train acc:  0.890625
train loss:  0.29298120737075806
train gradient:  0.21577095899538282
iteration : 8635
train acc:  0.8515625
train loss:  0.3693578243255615
train gradient:  0.19307974648864318
iteration : 8636
train acc:  0.859375
train loss:  0.35751333832740784
train gradient:  0.21321272985000259
iteration : 8637
train acc:  0.8515625
train loss:  0.3372901976108551
train gradient:  0.2300955777937373
iteration : 8638
train acc:  0.8515625
train loss:  0.31281810998916626
train gradient:  0.15915011360034714
iteration : 8639
train acc:  0.8984375
train loss:  0.23440438508987427
train gradient:  0.14018173741541434
iteration : 8640
train acc:  0.921875
train loss:  0.2782399654388428
train gradient:  0.07588749619907849
iteration : 8641
train acc:  0.875
train loss:  0.2546347975730896
train gradient:  0.10946849258439978
iteration : 8642
train acc:  0.8828125
train loss:  0.32378461956977844
train gradient:  0.13687565562458437
iteration : 8643
train acc:  0.828125
train loss:  0.39183521270751953
train gradient:  0.3286797371647238
iteration : 8644
train acc:  0.7734375
train loss:  0.4728991687297821
train gradient:  0.3113911786893412
iteration : 8645
train acc:  0.8359375
train loss:  0.28184962272644043
train gradient:  0.22922934725214836
iteration : 8646
train acc:  0.9140625
train loss:  0.2707209289073944
train gradient:  0.21264232113493586
iteration : 8647
train acc:  0.8671875
train loss:  0.29188618063926697
train gradient:  0.16361329426090362
iteration : 8648
train acc:  0.859375
train loss:  0.3117211163043976
train gradient:  0.18108131397274124
iteration : 8649
train acc:  0.8828125
train loss:  0.2968526780605316
train gradient:  0.164275807652505
iteration : 8650
train acc:  0.8828125
train loss:  0.3326859772205353
train gradient:  0.16435567266063578
iteration : 8651
train acc:  0.7734375
train loss:  0.43892085552215576
train gradient:  0.3090475201192903
iteration : 8652
train acc:  0.828125
train loss:  0.38141947984695435
train gradient:  0.21057642089710688
iteration : 8653
train acc:  0.8828125
train loss:  0.24081924557685852
train gradient:  0.09249410110047622
iteration : 8654
train acc:  0.875
train loss:  0.3017765283584595
train gradient:  0.19431868388356535
iteration : 8655
train acc:  0.875
train loss:  0.3104853630065918
train gradient:  0.15811760414313947
iteration : 8656
train acc:  0.8359375
train loss:  0.3179738521575928
train gradient:  0.21527021348299882
iteration : 8657
train acc:  0.890625
train loss:  0.28344523906707764
train gradient:  0.1177628011677173
iteration : 8658
train acc:  0.875
train loss:  0.27497366070747375
train gradient:  0.13491272381875663
iteration : 8659
train acc:  0.8515625
train loss:  0.2986195683479309
train gradient:  0.12816039558522807
iteration : 8660
train acc:  0.8125
train loss:  0.3317781984806061
train gradient:  0.17678668467787317
iteration : 8661
train acc:  0.7890625
train loss:  0.37517207860946655
train gradient:  0.2821272418574573
iteration : 8662
train acc:  0.8359375
train loss:  0.34757381677627563
train gradient:  0.17309391158587062
iteration : 8663
train acc:  0.8515625
train loss:  0.32683223485946655
train gradient:  0.16842728646845728
iteration : 8664
train acc:  0.8671875
train loss:  0.3429528474807739
train gradient:  0.26221178773892007
iteration : 8665
train acc:  0.8828125
train loss:  0.2709709107875824
train gradient:  0.15406774690279898
iteration : 8666
train acc:  0.8828125
train loss:  0.2913942337036133
train gradient:  0.11170123254952381
iteration : 8667
train acc:  0.8359375
train loss:  0.44641125202178955
train gradient:  0.38535093976279206
iteration : 8668
train acc:  0.84375
train loss:  0.320785790681839
train gradient:  0.20457549586551185
iteration : 8669
train acc:  0.8671875
train loss:  0.3142908215522766
train gradient:  0.18667040705020252
iteration : 8670
train acc:  0.8671875
train loss:  0.3077101409435272
train gradient:  0.20762078403364687
iteration : 8671
train acc:  0.796875
train loss:  0.3756309151649475
train gradient:  0.2553761311989071
iteration : 8672
train acc:  0.859375
train loss:  0.2679789364337921
train gradient:  0.1205550899557935
iteration : 8673
train acc:  0.8359375
train loss:  0.35101938247680664
train gradient:  0.19148491042302684
iteration : 8674
train acc:  0.8828125
train loss:  0.3495420813560486
train gradient:  0.15392329786499576
iteration : 8675
train acc:  0.84375
train loss:  0.33731913566589355
train gradient:  0.18254740640884046
iteration : 8676
train acc:  0.8359375
train loss:  0.4052391052246094
train gradient:  0.30716552697310323
iteration : 8677
train acc:  0.8046875
train loss:  0.40866994857788086
train gradient:  0.27635913951321045
iteration : 8678
train acc:  0.90625
train loss:  0.27984774112701416
train gradient:  0.1681953597034151
iteration : 8679
train acc:  0.828125
train loss:  0.4116608202457428
train gradient:  0.211887473336053
iteration : 8680
train acc:  0.890625
train loss:  0.290513813495636
train gradient:  0.18377298584638566
iteration : 8681
train acc:  0.8671875
train loss:  0.32079458236694336
train gradient:  0.20150151922826343
iteration : 8682
train acc:  0.859375
train loss:  0.2745051383972168
train gradient:  0.13317298827521068
iteration : 8683
train acc:  0.8359375
train loss:  0.33540213108062744
train gradient:  0.2580387635181558
iteration : 8684
train acc:  0.828125
train loss:  0.38749197125434875
train gradient:  0.2866084077655099
iteration : 8685
train acc:  0.8828125
train loss:  0.31515446305274963
train gradient:  0.20408897985316843
iteration : 8686
train acc:  0.8671875
train loss:  0.3055434226989746
train gradient:  0.17389827075336303
iteration : 8687
train acc:  0.84375
train loss:  0.3385101854801178
train gradient:  0.16095945005092221
iteration : 8688
train acc:  0.8046875
train loss:  0.48350998759269714
train gradient:  0.4033172819201564
iteration : 8689
train acc:  0.84375
train loss:  0.41120821237564087
train gradient:  0.32443364698752347
iteration : 8690
train acc:  0.8515625
train loss:  0.2960289716720581
train gradient:  0.1890002706170154
iteration : 8691
train acc:  0.84375
train loss:  0.3468130826950073
train gradient:  0.2193702354030735
iteration : 8692
train acc:  0.8046875
train loss:  0.3976150453090668
train gradient:  0.32457160717757993
iteration : 8693
train acc:  0.8515625
train loss:  0.3371500074863434
train gradient:  0.1976894773272977
iteration : 8694
train acc:  0.90625
train loss:  0.27156567573547363
train gradient:  0.1338830369418669
iteration : 8695
train acc:  0.8046875
train loss:  0.40007901191711426
train gradient:  0.24074964789880476
iteration : 8696
train acc:  0.8828125
train loss:  0.32168930768966675
train gradient:  0.15105338605371688
iteration : 8697
train acc:  0.84375
train loss:  0.3298022150993347
train gradient:  0.1752719317765907
iteration : 8698
train acc:  0.859375
train loss:  0.3154265880584717
train gradient:  0.16746768821600555
iteration : 8699
train acc:  0.7890625
train loss:  0.4087583124637604
train gradient:  0.24767699586249972
iteration : 8700
train acc:  0.8671875
train loss:  0.27928924560546875
train gradient:  0.11643321900009626
iteration : 8701
train acc:  0.8515625
train loss:  0.34466248750686646
train gradient:  0.15842234430575047
iteration : 8702
train acc:  0.859375
train loss:  0.37308207154273987
train gradient:  0.27544165224206024
iteration : 8703
train acc:  0.8828125
train loss:  0.2965449392795563
train gradient:  0.30398309847742305
iteration : 8704
train acc:  0.84375
train loss:  0.36917489767074585
train gradient:  0.22827543588483445
iteration : 8705
train acc:  0.8828125
train loss:  0.29647234082221985
train gradient:  0.1358473294008197
iteration : 8706
train acc:  0.8671875
train loss:  0.3298034369945526
train gradient:  0.14860155245393533
iteration : 8707
train acc:  0.859375
train loss:  0.32535097002983093
train gradient:  0.163941064966251
iteration : 8708
train acc:  0.9296875
train loss:  0.24376262724399567
train gradient:  0.13557523700824475
iteration : 8709
train acc:  0.8359375
train loss:  0.33711153268814087
train gradient:  0.17345787500487495
iteration : 8710
train acc:  0.8359375
train loss:  0.3354368805885315
train gradient:  0.19666951039445732
iteration : 8711
train acc:  0.8671875
train loss:  0.2907352149486542
train gradient:  0.1496168298197616
iteration : 8712
train acc:  0.7890625
train loss:  0.37118250131607056
train gradient:  0.2326456976709054
iteration : 8713
train acc:  0.7890625
train loss:  0.4899178445339203
train gradient:  0.4087874395051535
iteration : 8714
train acc:  0.9375
train loss:  0.25966280698776245
train gradient:  0.1304803409610326
iteration : 8715
train acc:  0.8515625
train loss:  0.2985944151878357
train gradient:  0.16914244226218203
iteration : 8716
train acc:  0.859375
train loss:  0.29572218656539917
train gradient:  0.13808861428116875
iteration : 8717
train acc:  0.8671875
train loss:  0.2927265465259552
train gradient:  0.10681591436437103
iteration : 8718
train acc:  0.8046875
train loss:  0.41299235820770264
train gradient:  0.21775857387915298
iteration : 8719
train acc:  0.8203125
train loss:  0.3654264211654663
train gradient:  0.19769178387260014
iteration : 8720
train acc:  0.8671875
train loss:  0.2859824299812317
train gradient:  0.19585978826369013
iteration : 8721
train acc:  0.84375
train loss:  0.3179089426994324
train gradient:  0.2077088398073912
iteration : 8722
train acc:  0.828125
train loss:  0.363528311252594
train gradient:  0.20655151870285737
iteration : 8723
train acc:  0.921875
train loss:  0.23448431491851807
train gradient:  0.09319161722258912
iteration : 8724
train acc:  0.8203125
train loss:  0.3109557628631592
train gradient:  0.1779060988926932
iteration : 8725
train acc:  0.8515625
train loss:  0.4233255684375763
train gradient:  0.2399818972189168
iteration : 8726
train acc:  0.875
train loss:  0.2978505492210388
train gradient:  0.2064425186250168
iteration : 8727
train acc:  0.828125
train loss:  0.363800048828125
train gradient:  0.22578678134060576
iteration : 8728
train acc:  0.8828125
train loss:  0.30853360891342163
train gradient:  0.21877441150405297
iteration : 8729
train acc:  0.828125
train loss:  0.37316468358039856
train gradient:  0.2467992277008491
iteration : 8730
train acc:  0.8359375
train loss:  0.3330762982368469
train gradient:  0.19256205132424417
iteration : 8731
train acc:  0.859375
train loss:  0.31250521540641785
train gradient:  0.24998789863814114
iteration : 8732
train acc:  0.8203125
train loss:  0.40477532148361206
train gradient:  0.20794104086993132
iteration : 8733
train acc:  0.875
train loss:  0.3114723563194275
train gradient:  0.16253774886173056
iteration : 8734
train acc:  0.84375
train loss:  0.4059901833534241
train gradient:  0.3153062615941004
iteration : 8735
train acc:  0.8203125
train loss:  0.35555583238601685
train gradient:  0.1875037741333725
iteration : 8736
train acc:  0.796875
train loss:  0.37889420986175537
train gradient:  0.16284709374311046
iteration : 8737
train acc:  0.84375
train loss:  0.34412476420402527
train gradient:  0.248736460768916
iteration : 8738
train acc:  0.796875
train loss:  0.4221486449241638
train gradient:  0.19450547280140495
iteration : 8739
train acc:  0.8203125
train loss:  0.3112722933292389
train gradient:  0.16596548752159906
iteration : 8740
train acc:  0.8359375
train loss:  0.3878632187843323
train gradient:  0.22172657751901098
iteration : 8741
train acc:  0.8359375
train loss:  0.34036821126937866
train gradient:  0.1814810894301595
iteration : 8742
train acc:  0.828125
train loss:  0.4087594151496887
train gradient:  0.21321497287599905
iteration : 8743
train acc:  0.8828125
train loss:  0.30914369225502014
train gradient:  0.16434037690882544
iteration : 8744
train acc:  0.8359375
train loss:  0.33451223373413086
train gradient:  0.13261197275532235
iteration : 8745
train acc:  0.8359375
train loss:  0.3410269618034363
train gradient:  0.19006802425219899
iteration : 8746
train acc:  0.9453125
train loss:  0.2346763014793396
train gradient:  0.1057558326056261
iteration : 8747
train acc:  0.8515625
train loss:  0.3165375590324402
train gradient:  0.16111861868972194
iteration : 8748
train acc:  0.84375
train loss:  0.3968737721443176
train gradient:  0.23473191093923712
iteration : 8749
train acc:  0.8046875
train loss:  0.38495126366615295
train gradient:  0.21992986276930815
iteration : 8750
train acc:  0.8203125
train loss:  0.3330349922180176
train gradient:  0.15319636325870048
iteration : 8751
train acc:  0.90625
train loss:  0.2549538016319275
train gradient:  0.10593088669652086
iteration : 8752
train acc:  0.84375
train loss:  0.3012211322784424
train gradient:  0.2062550815090864
iteration : 8753
train acc:  0.828125
train loss:  0.3616600036621094
train gradient:  0.233725092981226
iteration : 8754
train acc:  0.859375
train loss:  0.32389259338378906
train gradient:  0.17070874701311017
iteration : 8755
train acc:  0.8828125
train loss:  0.2619704306125641
train gradient:  0.10564178772597246
iteration : 8756
train acc:  0.875
train loss:  0.2788984179496765
train gradient:  0.0734190711723327
iteration : 8757
train acc:  0.859375
train loss:  0.31525474786758423
train gradient:  0.1679848606431656
iteration : 8758
train acc:  0.8828125
train loss:  0.24090798199176788
train gradient:  0.11205853118590907
iteration : 8759
train acc:  0.8671875
train loss:  0.2608771324157715
train gradient:  0.09521813031744866
iteration : 8760
train acc:  0.8359375
train loss:  0.3960566520690918
train gradient:  0.27232735554778054
iteration : 8761
train acc:  0.8046875
train loss:  0.4112030267715454
train gradient:  0.24278112384064351
iteration : 8762
train acc:  0.8203125
train loss:  0.3800830841064453
train gradient:  0.25585307381805744
iteration : 8763
train acc:  0.90625
train loss:  0.299320787191391
train gradient:  0.11817927035510489
iteration : 8764
train acc:  0.8515625
train loss:  0.32119154930114746
train gradient:  0.14345986191356164
iteration : 8765
train acc:  0.78125
train loss:  0.3729928731918335
train gradient:  0.15651466401799635
iteration : 8766
train acc:  0.859375
train loss:  0.2891971468925476
train gradient:  0.16695085137659516
iteration : 8767
train acc:  0.859375
train loss:  0.3400774598121643
train gradient:  0.21350179780142886
iteration : 8768
train acc:  0.8125
train loss:  0.32064932584762573
train gradient:  0.25191702682633854
iteration : 8769
train acc:  0.8828125
train loss:  0.30259108543395996
train gradient:  0.15579310202035257
iteration : 8770
train acc:  0.84375
train loss:  0.2934425473213196
train gradient:  0.14792499759871536
iteration : 8771
train acc:  0.8671875
train loss:  0.3306673765182495
train gradient:  0.11796827928335557
iteration : 8772
train acc:  0.8984375
train loss:  0.2571941018104553
train gradient:  0.10524741952848009
iteration : 8773
train acc:  0.828125
train loss:  0.3575955629348755
train gradient:  0.14969274677341315
iteration : 8774
train acc:  0.84375
train loss:  0.3461383879184723
train gradient:  0.14419129696846414
iteration : 8775
train acc:  0.875
train loss:  0.28395599126815796
train gradient:  0.12366342773313127
iteration : 8776
train acc:  0.9375
train loss:  0.19597505033016205
train gradient:  0.07115148146837026
iteration : 8777
train acc:  0.8203125
train loss:  0.3692054748535156
train gradient:  0.19031045002906
iteration : 8778
train acc:  0.8359375
train loss:  0.364242285490036
train gradient:  0.15226733388189723
iteration : 8779
train acc:  0.8125
train loss:  0.38066279888153076
train gradient:  0.22647282492571105
iteration : 8780
train acc:  0.8359375
train loss:  0.33805370330810547
train gradient:  0.2166313702056104
iteration : 8781
train acc:  0.859375
train loss:  0.3154003322124481
train gradient:  0.11242915783965493
iteration : 8782
train acc:  0.84375
train loss:  0.37709927558898926
train gradient:  0.24614071838348783
iteration : 8783
train acc:  0.890625
train loss:  0.2676351070404053
train gradient:  0.11041829705269375
iteration : 8784
train acc:  0.8125
train loss:  0.44436532258987427
train gradient:  0.22014826587436953
iteration : 8785
train acc:  0.875
train loss:  0.2579542398452759
train gradient:  0.16398372421875718
iteration : 8786
train acc:  0.890625
train loss:  0.3326883912086487
train gradient:  0.2125246989754554
iteration : 8787
train acc:  0.8671875
train loss:  0.2994430661201477
train gradient:  0.13246924683724942
iteration : 8788
train acc:  0.8125
train loss:  0.43440699577331543
train gradient:  0.23424943242233526
iteration : 8789
train acc:  0.828125
train loss:  0.3446057140827179
train gradient:  0.16391771285959067
iteration : 8790
train acc:  0.84375
train loss:  0.3604418635368347
train gradient:  0.28693606807380095
iteration : 8791
train acc:  0.828125
train loss:  0.31039971113204956
train gradient:  0.133169181693519
iteration : 8792
train acc:  0.890625
train loss:  0.2914167046546936
train gradient:  0.1964838694078721
iteration : 8793
train acc:  0.859375
train loss:  0.26477256417274475
train gradient:  0.12207338008485401
iteration : 8794
train acc:  0.890625
train loss:  0.3065871596336365
train gradient:  0.14671913005852366
iteration : 8795
train acc:  0.828125
train loss:  0.41939201951026917
train gradient:  0.20645283720902044
iteration : 8796
train acc:  0.8515625
train loss:  0.35220494866371155
train gradient:  0.16490100761386184
iteration : 8797
train acc:  0.859375
train loss:  0.2847495973110199
train gradient:  0.15125282056595585
iteration : 8798
train acc:  0.90625
train loss:  0.23517051339149475
train gradient:  0.1534012096274302
iteration : 8799
train acc:  0.8671875
train loss:  0.33119040727615356
train gradient:  0.19775944843549859
iteration : 8800
train acc:  0.875
train loss:  0.3119352459907532
train gradient:  0.26590750925920204
iteration : 8801
train acc:  0.828125
train loss:  0.4169909656047821
train gradient:  0.2711232853321463
iteration : 8802
train acc:  0.8515625
train loss:  0.3251059055328369
train gradient:  0.2513180641637602
iteration : 8803
train acc:  0.8203125
train loss:  0.34169989824295044
train gradient:  0.1946764207583265
iteration : 8804
train acc:  0.8671875
train loss:  0.33934998512268066
train gradient:  0.12222926522499471
iteration : 8805
train acc:  0.84375
train loss:  0.34791478514671326
train gradient:  0.223192098090929
iteration : 8806
train acc:  0.8828125
train loss:  0.2648817300796509
train gradient:  0.09237190224428243
iteration : 8807
train acc:  0.8828125
train loss:  0.30240023136138916
train gradient:  0.13271300353014776
iteration : 8808
train acc:  0.875
train loss:  0.3536922335624695
train gradient:  0.15952908501227675
iteration : 8809
train acc:  0.7734375
train loss:  0.4831279218196869
train gradient:  0.3609832804411014
iteration : 8810
train acc:  0.8828125
train loss:  0.3508217930793762
train gradient:  0.209087470916535
iteration : 8811
train acc:  0.859375
train loss:  0.3417837619781494
train gradient:  0.18872972232661497
iteration : 8812
train acc:  0.8515625
train loss:  0.3744816780090332
train gradient:  0.18947859963545693
iteration : 8813
train acc:  0.875
train loss:  0.3403904438018799
train gradient:  0.20761386139589327
iteration : 8814
train acc:  0.8671875
train loss:  0.30810225009918213
train gradient:  0.15295109484992264
iteration : 8815
train acc:  0.84375
train loss:  0.3633151352405548
train gradient:  0.2117340698226267
iteration : 8816
train acc:  0.8671875
train loss:  0.31803175806999207
train gradient:  0.1904691218926065
iteration : 8817
train acc:  0.875
train loss:  0.34374839067459106
train gradient:  0.2094041191539679
iteration : 8818
train acc:  0.8984375
train loss:  0.23721304535865784
train gradient:  0.1043331154703651
iteration : 8819
train acc:  0.90625
train loss:  0.2844104766845703
train gradient:  0.17587146682755112
iteration : 8820
train acc:  0.875
train loss:  0.3179389536380768
train gradient:  0.14282850289927027
iteration : 8821
train acc:  0.8828125
train loss:  0.2353784590959549
train gradient:  0.11153725915823282
iteration : 8822
train acc:  0.84375
train loss:  0.4024657607078552
train gradient:  0.28096221572909874
iteration : 8823
train acc:  0.8671875
train loss:  0.3293115496635437
train gradient:  0.18667260398283025
iteration : 8824
train acc:  0.8671875
train loss:  0.33921030163764954
train gradient:  0.15709929189544286
iteration : 8825
train acc:  0.8671875
train loss:  0.3288573622703552
train gradient:  0.17071457367864437
iteration : 8826
train acc:  0.875
train loss:  0.3370668888092041
train gradient:  0.1146073290092438
iteration : 8827
train acc:  0.84375
train loss:  0.36432501673698425
train gradient:  0.17422354657043068
iteration : 8828
train acc:  0.8359375
train loss:  0.3438400626182556
train gradient:  0.17900006952255476
iteration : 8829
train acc:  0.8828125
train loss:  0.2932698130607605
train gradient:  0.21544723960245452
iteration : 8830
train acc:  0.890625
train loss:  0.25074779987335205
train gradient:  0.12320405044696112
iteration : 8831
train acc:  0.8828125
train loss:  0.2678387761116028
train gradient:  0.12113642136120979
iteration : 8832
train acc:  0.890625
train loss:  0.30278074741363525
train gradient:  0.18373704021666332
iteration : 8833
train acc:  0.875
train loss:  0.2624097168445587
train gradient:  0.14977030933212074
iteration : 8834
train acc:  0.859375
train loss:  0.3190714120864868
train gradient:  0.1632772270393033
iteration : 8835
train acc:  0.8203125
train loss:  0.3749978244304657
train gradient:  0.21464877832941848
iteration : 8836
train acc:  0.8671875
train loss:  0.3183698058128357
train gradient:  0.19690791234176447
iteration : 8837
train acc:  0.84375
train loss:  0.3021817207336426
train gradient:  0.19580392309894668
iteration : 8838
train acc:  0.8046875
train loss:  0.4127961993217468
train gradient:  0.23066076647483025
iteration : 8839
train acc:  0.8984375
train loss:  0.272046834230423
train gradient:  0.13539593513232487
iteration : 8840
train acc:  0.90625
train loss:  0.26069778203964233
train gradient:  0.12307047549918325
iteration : 8841
train acc:  0.859375
train loss:  0.332512229681015
train gradient:  0.10869137767318028
iteration : 8842
train acc:  0.8359375
train loss:  0.34098732471466064
train gradient:  0.12836584419241776
iteration : 8843
train acc:  0.8984375
train loss:  0.26698118448257446
train gradient:  0.1132304667123475
iteration : 8844
train acc:  0.8671875
train loss:  0.36896562576293945
train gradient:  0.2708375074838747
iteration : 8845
train acc:  0.828125
train loss:  0.35824501514434814
train gradient:  0.2866532436489825
iteration : 8846
train acc:  0.8515625
train loss:  0.3936057686805725
train gradient:  0.20641376223875127
iteration : 8847
train acc:  0.8984375
train loss:  0.2823393642902374
train gradient:  0.14726794534170773
iteration : 8848
train acc:  0.828125
train loss:  0.3971186876296997
train gradient:  0.3800227987253847
iteration : 8849
train acc:  0.859375
train loss:  0.2995411157608032
train gradient:  0.15901503998743166
iteration : 8850
train acc:  0.8515625
train loss:  0.3239622414112091
train gradient:  0.16342039517017276
iteration : 8851
train acc:  0.796875
train loss:  0.43119463324546814
train gradient:  0.2337541912848652
iteration : 8852
train acc:  0.890625
train loss:  0.35714709758758545
train gradient:  0.23574664386516447
iteration : 8853
train acc:  0.9140625
train loss:  0.2698308825492859
train gradient:  0.10103833673305614
iteration : 8854
train acc:  0.796875
train loss:  0.39592301845550537
train gradient:  0.18073143215298657
iteration : 8855
train acc:  0.859375
train loss:  0.3027173578739166
train gradient:  0.11034346463168075
iteration : 8856
train acc:  0.859375
train loss:  0.3039693832397461
train gradient:  0.10461013955643476
iteration : 8857
train acc:  0.8828125
train loss:  0.29727232456207275
train gradient:  0.12516127196642673
iteration : 8858
train acc:  0.859375
train loss:  0.33539220690727234
train gradient:  0.18466099394299457
iteration : 8859
train acc:  0.875
train loss:  0.27461710572242737
train gradient:  0.21620909795080162
iteration : 8860
train acc:  0.8125
train loss:  0.3597756624221802
train gradient:  0.13612434171117221
iteration : 8861
train acc:  0.828125
train loss:  0.3807235062122345
train gradient:  0.169222290575599
iteration : 8862
train acc:  0.890625
train loss:  0.2515242099761963
train gradient:  0.1267458888356007
iteration : 8863
train acc:  0.8515625
train loss:  0.34236329793930054
train gradient:  0.16750118470913145
iteration : 8864
train acc:  0.8359375
train loss:  0.3386843800544739
train gradient:  0.1690593526730032
iteration : 8865
train acc:  0.84375
train loss:  0.3762906789779663
train gradient:  0.20998835792804732
iteration : 8866
train acc:  0.8515625
train loss:  0.2944644093513489
train gradient:  0.12161459385840064
iteration : 8867
train acc:  0.8515625
train loss:  0.3591592311859131
train gradient:  0.20472297195923675
iteration : 8868
train acc:  0.90625
train loss:  0.24425318837165833
train gradient:  0.15455052833179359
iteration : 8869
train acc:  0.859375
train loss:  0.34867244958877563
train gradient:  0.213535201782926
iteration : 8870
train acc:  0.8125
train loss:  0.34115445613861084
train gradient:  0.15322705709993295
iteration : 8871
train acc:  0.8671875
train loss:  0.31092286109924316
train gradient:  0.15619124753270827
iteration : 8872
train acc:  0.859375
train loss:  0.30153512954711914
train gradient:  0.14962197966411614
iteration : 8873
train acc:  0.8359375
train loss:  0.38558438420295715
train gradient:  0.19802333301591463
iteration : 8874
train acc:  0.8515625
train loss:  0.3328530788421631
train gradient:  0.1718515053723856
iteration : 8875
train acc:  0.828125
train loss:  0.41029292345046997
train gradient:  0.2883527022177825
iteration : 8876
train acc:  0.9296875
train loss:  0.27596163749694824
train gradient:  0.12437870780594268
iteration : 8877
train acc:  0.828125
train loss:  0.3435855507850647
train gradient:  0.1947163975307982
iteration : 8878
train acc:  0.84375
train loss:  0.4334757924079895
train gradient:  0.28220198830313764
iteration : 8879
train acc:  0.8125
train loss:  0.37037259340286255
train gradient:  0.1785823500050644
iteration : 8880
train acc:  0.8515625
train loss:  0.3623385727405548
train gradient:  0.17575395161827095
iteration : 8881
train acc:  0.875
train loss:  0.299411416053772
train gradient:  0.14737129673978466
iteration : 8882
train acc:  0.8203125
train loss:  0.35264527797698975
train gradient:  0.21560876107115629
iteration : 8883
train acc:  0.84375
train loss:  0.3378285765647888
train gradient:  0.15897681925448987
iteration : 8884
train acc:  0.8671875
train loss:  0.3762952387332916
train gradient:  0.24832538749938854
iteration : 8885
train acc:  0.8515625
train loss:  0.32808631658554077
train gradient:  0.17984282421506692
iteration : 8886
train acc:  0.8671875
train loss:  0.36960506439208984
train gradient:  0.25562917009915964
iteration : 8887
train acc:  0.84375
train loss:  0.3545188307762146
train gradient:  0.2099895840778528
iteration : 8888
train acc:  0.8515625
train loss:  0.3500497341156006
train gradient:  0.1372210844433731
iteration : 8889
train acc:  0.875
train loss:  0.2561241090297699
train gradient:  0.10460717162244937
iteration : 8890
train acc:  0.84375
train loss:  0.351521760225296
train gradient:  0.17433136581456143
iteration : 8891
train acc:  0.828125
train loss:  0.3448912799358368
train gradient:  0.21418855778766688
iteration : 8892
train acc:  0.8515625
train loss:  0.29365167021751404
train gradient:  0.154989936611971
iteration : 8893
train acc:  0.8515625
train loss:  0.2913750410079956
train gradient:  0.13440642944299977
iteration : 8894
train acc:  0.8828125
train loss:  0.28681010007858276
train gradient:  0.09725193447704146
iteration : 8895
train acc:  0.8515625
train loss:  0.32770463824272156
train gradient:  0.11529206486005586
iteration : 8896
train acc:  0.8671875
train loss:  0.28864777088165283
train gradient:  0.19954189858109994
iteration : 8897
train acc:  0.859375
train loss:  0.2825014591217041
train gradient:  0.11453830388745363
iteration : 8898
train acc:  0.7578125
train loss:  0.49616575241088867
train gradient:  0.33127280348169147
iteration : 8899
train acc:  0.84375
train loss:  0.324875146150589
train gradient:  0.138582989565581
iteration : 8900
train acc:  0.859375
train loss:  0.3391415476799011
train gradient:  0.22135147083292872
iteration : 8901
train acc:  0.8828125
train loss:  0.3149009943008423
train gradient:  0.14886156317060065
iteration : 8902
train acc:  0.8828125
train loss:  0.3042740821838379
train gradient:  0.1521415940652992
iteration : 8903
train acc:  0.859375
train loss:  0.2962679862976074
train gradient:  0.21918775662388468
iteration : 8904
train acc:  0.859375
train loss:  0.28091108798980713
train gradient:  0.14618058921201255
iteration : 8905
train acc:  0.8828125
train loss:  0.2869550585746765
train gradient:  0.12580704717958682
iteration : 8906
train acc:  0.8828125
train loss:  0.25769859552383423
train gradient:  0.11593846364806926
iteration : 8907
train acc:  0.859375
train loss:  0.2815297842025757
train gradient:  0.13657213110409688
iteration : 8908
train acc:  0.8984375
train loss:  0.2669059634208679
train gradient:  0.13296601949491424
iteration : 8909
train acc:  0.84375
train loss:  0.3683142066001892
train gradient:  0.2162422661986383
iteration : 8910
train acc:  0.8671875
train loss:  0.3329138457775116
train gradient:  0.16390035245902956
iteration : 8911
train acc:  0.90625
train loss:  0.26597094535827637
train gradient:  0.09572786905991006
iteration : 8912
train acc:  0.8359375
train loss:  0.34199008345603943
train gradient:  0.1888190169624351
iteration : 8913
train acc:  0.828125
train loss:  0.36993446946144104
train gradient:  0.18073295984109472
iteration : 8914
train acc:  0.875
train loss:  0.28079620003700256
train gradient:  0.18087700404728682
iteration : 8915
train acc:  0.859375
train loss:  0.364219069480896
train gradient:  0.34376172235321173
iteration : 8916
train acc:  0.890625
train loss:  0.3663732409477234
train gradient:  0.22151896049519115
iteration : 8917
train acc:  0.78125
train loss:  0.4400598704814911
train gradient:  0.3192665728772439
iteration : 8918
train acc:  0.859375
train loss:  0.34874603152275085
train gradient:  0.21691370043140512
iteration : 8919
train acc:  0.8984375
train loss:  0.2789398431777954
train gradient:  0.17584752503767387
iteration : 8920
train acc:  0.84375
train loss:  0.38016119599342346
train gradient:  0.1478302958470694
iteration : 8921
train acc:  0.8828125
train loss:  0.27324920892715454
train gradient:  0.09694272150721306
iteration : 8922
train acc:  0.8828125
train loss:  0.29914140701293945
train gradient:  0.23837941653117328
iteration : 8923
train acc:  0.859375
train loss:  0.2878004312515259
train gradient:  0.11809120572232555
iteration : 8924
train acc:  0.8203125
train loss:  0.41819238662719727
train gradient:  0.2334013129119763
iteration : 8925
train acc:  0.84375
train loss:  0.3476628065109253
train gradient:  0.16316229812552902
iteration : 8926
train acc:  0.859375
train loss:  0.317055344581604
train gradient:  0.13909306496690543
iteration : 8927
train acc:  0.8515625
train loss:  0.3418627977371216
train gradient:  0.18694314669951576
iteration : 8928
train acc:  0.8828125
train loss:  0.30423682928085327
train gradient:  0.15037287201576496
iteration : 8929
train acc:  0.8671875
train loss:  0.3413846790790558
train gradient:  0.22259989546350756
iteration : 8930
train acc:  0.8515625
train loss:  0.325791597366333
train gradient:  0.1955698353046484
iteration : 8931
train acc:  0.828125
train loss:  0.3473322093486786
train gradient:  0.1857109079671549
iteration : 8932
train acc:  0.8671875
train loss:  0.28507477045059204
train gradient:  0.15015630694792623
iteration : 8933
train acc:  0.921875
train loss:  0.228981152176857
train gradient:  0.10639732977816503
iteration : 8934
train acc:  0.8203125
train loss:  0.42339786887168884
train gradient:  0.28739984912777544
iteration : 8935
train acc:  0.8359375
train loss:  0.4103061556816101
train gradient:  0.2061491386854356
iteration : 8936
train acc:  0.859375
train loss:  0.32886525988578796
train gradient:  0.20165674318952898
iteration : 8937
train acc:  0.8359375
train loss:  0.3104000985622406
train gradient:  0.21088058829503573
iteration : 8938
train acc:  0.828125
train loss:  0.3516919016838074
train gradient:  0.2214507874764805
iteration : 8939
train acc:  0.8359375
train loss:  0.3381631374359131
train gradient:  0.19000250338133517
iteration : 8940
train acc:  0.8515625
train loss:  0.3549431562423706
train gradient:  0.24197614989760796
iteration : 8941
train acc:  0.859375
train loss:  0.3201032280921936
train gradient:  0.12833303762705106
iteration : 8942
train acc:  0.8671875
train loss:  0.3209555745124817
train gradient:  0.1298293947193348
iteration : 8943
train acc:  0.828125
train loss:  0.356838196516037
train gradient:  0.1541766835795333
iteration : 8944
train acc:  0.890625
train loss:  0.3121914267539978
train gradient:  0.17664474428042543
iteration : 8945
train acc:  0.84375
train loss:  0.35836517810821533
train gradient:  0.21726487834571107
iteration : 8946
train acc:  0.8359375
train loss:  0.30897995829582214
train gradient:  0.13402628991929422
iteration : 8947
train acc:  0.90625
train loss:  0.23394142091274261
train gradient:  0.11676111028926695
iteration : 8948
train acc:  0.875
train loss:  0.30003461241722107
train gradient:  0.16934788480449325
iteration : 8949
train acc:  0.8515625
train loss:  0.30580848455429077
train gradient:  0.18730693792391545
iteration : 8950
train acc:  0.859375
train loss:  0.283325731754303
train gradient:  0.0929997569859047
iteration : 8951
train acc:  0.828125
train loss:  0.41339558362960815
train gradient:  0.22091099677060408
iteration : 8952
train acc:  0.8671875
train loss:  0.34627288579940796
train gradient:  0.12268306943991408
iteration : 8953
train acc:  0.8671875
train loss:  0.2984704077243805
train gradient:  0.21105689451991944
iteration : 8954
train acc:  0.8828125
train loss:  0.2905246317386627
train gradient:  0.14363823301560336
iteration : 8955
train acc:  0.890625
train loss:  0.24095827341079712
train gradient:  0.11664999878508973
iteration : 8956
train acc:  0.890625
train loss:  0.2935664653778076
train gradient:  0.1301073698087822
iteration : 8957
train acc:  0.90625
train loss:  0.25923269987106323
train gradient:  0.08174905202679114
iteration : 8958
train acc:  0.875
train loss:  0.29934030771255493
train gradient:  0.15133214949084736
iteration : 8959
train acc:  0.90625
train loss:  0.28419584035873413
train gradient:  0.15456822908780432
iteration : 8960
train acc:  0.78125
train loss:  0.42029237747192383
train gradient:  0.238645961758826
iteration : 8961
train acc:  0.7890625
train loss:  0.46044355630874634
train gradient:  0.38119093621646505
iteration : 8962
train acc:  0.796875
train loss:  0.5334030389785767
train gradient:  0.2979470341773339
iteration : 8963
train acc:  0.9140625
train loss:  0.2814643979072571
train gradient:  0.18991371959488137
iteration : 8964
train acc:  0.8359375
train loss:  0.39531517028808594
train gradient:  0.1863836358142536
iteration : 8965
train acc:  0.859375
train loss:  0.32239049673080444
train gradient:  0.13437062765068564
iteration : 8966
train acc:  0.828125
train loss:  0.3499680161476135
train gradient:  0.17377182069140418
iteration : 8967
train acc:  0.828125
train loss:  0.33842068910598755
train gradient:  0.158329801299868
iteration : 8968
train acc:  0.8125
train loss:  0.4260976314544678
train gradient:  0.22280496280972212
iteration : 8969
train acc:  0.890625
train loss:  0.3266716003417969
train gradient:  0.1594842038800303
iteration : 8970
train acc:  0.9296875
train loss:  0.2455897033214569
train gradient:  0.1599479322193647
iteration : 8971
train acc:  0.828125
train loss:  0.38138991594314575
train gradient:  0.2501310465516181
iteration : 8972
train acc:  0.875
train loss:  0.30600228905677795
train gradient:  0.22884815081857096
iteration : 8973
train acc:  0.8671875
train loss:  0.27861565351486206
train gradient:  0.13963852105469446
iteration : 8974
train acc:  0.8671875
train loss:  0.3313758373260498
train gradient:  0.20064718193540504
iteration : 8975
train acc:  0.859375
train loss:  0.33083194494247437
train gradient:  0.1940194399636563
iteration : 8976
train acc:  0.8359375
train loss:  0.36112192273139954
train gradient:  0.18786417529885635
iteration : 8977
train acc:  0.8203125
train loss:  0.3644099235534668
train gradient:  0.1838127104308391
iteration : 8978
train acc:  0.875
train loss:  0.27381187677383423
train gradient:  0.11668402150116083
iteration : 8979
train acc:  0.859375
train loss:  0.35228395462036133
train gradient:  0.22951595321079446
iteration : 8980
train acc:  0.7890625
train loss:  0.386951208114624
train gradient:  0.3036667851448026
iteration : 8981
train acc:  0.8671875
train loss:  0.299498587846756
train gradient:  0.1788423801068223
iteration : 8982
train acc:  0.875
train loss:  0.2960587739944458
train gradient:  0.19369710165663168
iteration : 8983
train acc:  0.8515625
train loss:  0.34313178062438965
train gradient:  0.13639109898247515
iteration : 8984
train acc:  0.8828125
train loss:  0.2791244387626648
train gradient:  0.2156716332141218
iteration : 8985
train acc:  0.8515625
train loss:  0.3895518183708191
train gradient:  0.20421924490091797
iteration : 8986
train acc:  0.8515625
train loss:  0.3291361927986145
train gradient:  0.19848801204118255
iteration : 8987
train acc:  0.8125
train loss:  0.4177866578102112
train gradient:  0.21925252195915873
iteration : 8988
train acc:  0.90625
train loss:  0.233483225107193
train gradient:  0.09465882018781478
iteration : 8989
train acc:  0.8359375
train loss:  0.4255070090293884
train gradient:  0.2608240692276121
iteration : 8990
train acc:  0.875
train loss:  0.3078107237815857
train gradient:  0.16262350321663477
iteration : 8991
train acc:  0.8359375
train loss:  0.3625085949897766
train gradient:  0.17798143187990498
iteration : 8992
train acc:  0.859375
train loss:  0.3683207035064697
train gradient:  0.13531827324300238
iteration : 8993
train acc:  0.8515625
train loss:  0.2968806028366089
train gradient:  0.17032683873172208
iteration : 8994
train acc:  0.8125
train loss:  0.38987451791763306
train gradient:  0.18766285193361615
iteration : 8995
train acc:  0.8125
train loss:  0.4154071509838104
train gradient:  0.20019772275984177
iteration : 8996
train acc:  0.859375
train loss:  0.2982373833656311
train gradient:  0.10784268865564751
iteration : 8997
train acc:  0.8125
train loss:  0.36721184849739075
train gradient:  0.20541573277201708
iteration : 8998
train acc:  0.84375
train loss:  0.312949001789093
train gradient:  0.15473387237017983
iteration : 8999
train acc:  0.890625
train loss:  0.26962628960609436
train gradient:  0.1538004535940846
iteration : 9000
train acc:  0.8671875
train loss:  0.31522348523139954
train gradient:  0.17177216516281812
iteration : 9001
train acc:  0.9140625
train loss:  0.3036687672138214
train gradient:  0.16086328089917198
iteration : 9002
train acc:  0.8359375
train loss:  0.3472163677215576
train gradient:  0.20146669663974714
iteration : 9003
train acc:  0.8671875
train loss:  0.36947011947631836
train gradient:  0.19377832445242044
iteration : 9004
train acc:  0.828125
train loss:  0.3251475989818573
train gradient:  0.13122512469419598
iteration : 9005
train acc:  0.8515625
train loss:  0.2988666594028473
train gradient:  0.14423785830352476
iteration : 9006
train acc:  0.8671875
train loss:  0.3029387891292572
train gradient:  0.14570579071617878
iteration : 9007
train acc:  0.8046875
train loss:  0.410432904958725
train gradient:  0.2978990507701637
iteration : 9008
train acc:  0.8515625
train loss:  0.3375880718231201
train gradient:  0.2445082668501719
iteration : 9009
train acc:  0.84375
train loss:  0.3442668318748474
train gradient:  0.17114396111274477
iteration : 9010
train acc:  0.8984375
train loss:  0.30663996934890747
train gradient:  0.12303137019852345
iteration : 9011
train acc:  0.8828125
train loss:  0.3179946541786194
train gradient:  0.17283919264082745
iteration : 9012
train acc:  0.859375
train loss:  0.33262860774993896
train gradient:  0.14656007081295735
iteration : 9013
train acc:  0.859375
train loss:  0.2979468107223511
train gradient:  0.1701998340374283
iteration : 9014
train acc:  0.8828125
train loss:  0.30932754278182983
train gradient:  0.1736051239410002
iteration : 9015
train acc:  0.8984375
train loss:  0.23362812399864197
train gradient:  0.16096827777193579
iteration : 9016
train acc:  0.8046875
train loss:  0.38632553815841675
train gradient:  0.16075608763922633
iteration : 9017
train acc:  0.859375
train loss:  0.34536272287368774
train gradient:  0.1287399852740433
iteration : 9018
train acc:  0.8671875
train loss:  0.311249703168869
train gradient:  0.15425218512569766
iteration : 9019
train acc:  0.84375
train loss:  0.3412511646747589
train gradient:  0.14360042848078064
iteration : 9020
train acc:  0.8828125
train loss:  0.29309511184692383
train gradient:  0.17589117602406612
iteration : 9021
train acc:  0.8828125
train loss:  0.28426066040992737
train gradient:  0.14581056068198112
iteration : 9022
train acc:  0.859375
train loss:  0.3009636402130127
train gradient:  0.14583798913788648
iteration : 9023
train acc:  0.8828125
train loss:  0.2684193551540375
train gradient:  0.119733498789405
iteration : 9024
train acc:  0.84375
train loss:  0.34667134284973145
train gradient:  0.18681584653664834
iteration : 9025
train acc:  0.828125
train loss:  0.36609527468681335
train gradient:  0.1889340632218991
iteration : 9026
train acc:  0.875
train loss:  0.294319212436676
train gradient:  0.12762092152140642
iteration : 9027
train acc:  0.8359375
train loss:  0.3094732165336609
train gradient:  0.19469760674583889
iteration : 9028
train acc:  0.828125
train loss:  0.3803887367248535
train gradient:  0.251555769526985
iteration : 9029
train acc:  0.875
train loss:  0.3508227467536926
train gradient:  0.19042148619761487
iteration : 9030
train acc:  0.8828125
train loss:  0.30505287647247314
train gradient:  0.11356570559093894
iteration : 9031
train acc:  0.8671875
train loss:  0.3442261219024658
train gradient:  0.1471662925083123
iteration : 9032
train acc:  0.8671875
train loss:  0.3654097318649292
train gradient:  0.19194273714092344
iteration : 9033
train acc:  0.875
train loss:  0.28524214029312134
train gradient:  0.13773076803715256
iteration : 9034
train acc:  0.8125
train loss:  0.44274967908859253
train gradient:  0.26139450339910414
iteration : 9035
train acc:  0.8125
train loss:  0.37050819396972656
train gradient:  0.24308353415550937
iteration : 9036
train acc:  0.8515625
train loss:  0.33833062648773193
train gradient:  0.17147114099355754
iteration : 9037
train acc:  0.8828125
train loss:  0.2883647680282593
train gradient:  0.10213064379242201
iteration : 9038
train acc:  0.84375
train loss:  0.3596895933151245
train gradient:  0.16812604921432867
iteration : 9039
train acc:  0.8515625
train loss:  0.2938864827156067
train gradient:  0.15326899873760624
iteration : 9040
train acc:  0.796875
train loss:  0.40895551443099976
train gradient:  0.2748447427882371
iteration : 9041
train acc:  0.8125
train loss:  0.47236526012420654
train gradient:  0.24046861967482652
iteration : 9042
train acc:  0.8671875
train loss:  0.33340030908584595
train gradient:  0.14986717877600617
iteration : 9043
train acc:  0.875
train loss:  0.27851325273513794
train gradient:  0.12162085873702887
iteration : 9044
train acc:  0.859375
train loss:  0.3126121461391449
train gradient:  0.14408936113602577
iteration : 9045
train acc:  0.8359375
train loss:  0.26195237040519714
train gradient:  0.11660840490042966
iteration : 9046
train acc:  0.8359375
train loss:  0.3287079632282257
train gradient:  0.17007101159590515
iteration : 9047
train acc:  0.84375
train loss:  0.3321879208087921
train gradient:  0.1772416440950135
iteration : 9048
train acc:  0.84375
train loss:  0.30772313475608826
train gradient:  0.17870242777751166
iteration : 9049
train acc:  0.8828125
train loss:  0.3116697669029236
train gradient:  0.12421166037170513
iteration : 9050
train acc:  0.875
train loss:  0.2771613597869873
train gradient:  0.24549691096212956
iteration : 9051
train acc:  0.828125
train loss:  0.33351364731788635
train gradient:  0.13517532896745266
iteration : 9052
train acc:  0.8515625
train loss:  0.3524247705936432
train gradient:  0.22288247530920072
iteration : 9053
train acc:  0.828125
train loss:  0.34876471757888794
train gradient:  0.19266783135316817
iteration : 9054
train acc:  0.875
train loss:  0.278852641582489
train gradient:  0.09142850486206068
iteration : 9055
train acc:  0.8828125
train loss:  0.2893199324607849
train gradient:  0.14804254698803473
iteration : 9056
train acc:  0.859375
train loss:  0.2790161371231079
train gradient:  0.12142202563961219
iteration : 9057
train acc:  0.8359375
train loss:  0.35121190547943115
train gradient:  0.1662924353238656
iteration : 9058
train acc:  0.8125
train loss:  0.41372668743133545
train gradient:  0.20692484170443307
iteration : 9059
train acc:  0.875
train loss:  0.3131558299064636
train gradient:  0.13904223679750954
iteration : 9060
train acc:  0.859375
train loss:  0.321320503950119
train gradient:  0.14538427672793788
iteration : 9061
train acc:  0.859375
train loss:  0.2743525505065918
train gradient:  0.17677718914451562
iteration : 9062
train acc:  0.8671875
train loss:  0.29602596163749695
train gradient:  0.1397273016496282
iteration : 9063
train acc:  0.859375
train loss:  0.31001657247543335
train gradient:  0.14949788852822987
iteration : 9064
train acc:  0.890625
train loss:  0.2667068839073181
train gradient:  0.11330460942591704
iteration : 9065
train acc:  0.84375
train loss:  0.4038761854171753
train gradient:  0.20641779617335723
iteration : 9066
train acc:  0.828125
train loss:  0.3375539183616638
train gradient:  0.15144389798193544
iteration : 9067
train acc:  0.8828125
train loss:  0.2444533407688141
train gradient:  0.09424867577565288
iteration : 9068
train acc:  0.875
train loss:  0.29638251662254333
train gradient:  0.15287782940028735
iteration : 9069
train acc:  0.90625
train loss:  0.23919788002967834
train gradient:  0.1623352770678641
iteration : 9070
train acc:  0.8046875
train loss:  0.3752215802669525
train gradient:  0.17084756400086856
iteration : 9071
train acc:  0.9140625
train loss:  0.2572394013404846
train gradient:  0.12804994348380952
iteration : 9072
train acc:  0.84375
train loss:  0.3761831521987915
train gradient:  0.24167146359535113
iteration : 9073
train acc:  0.875
train loss:  0.3273097276687622
train gradient:  0.13546295010601112
iteration : 9074
train acc:  0.8203125
train loss:  0.42160606384277344
train gradient:  0.27103191962252104
iteration : 9075
train acc:  0.8515625
train loss:  0.3454596996307373
train gradient:  0.1858842656841604
iteration : 9076
train acc:  0.8828125
train loss:  0.27458298206329346
train gradient:  0.1894278028233949
iteration : 9077
train acc:  0.8203125
train loss:  0.3916325867176056
train gradient:  0.19571263652900425
iteration : 9078
train acc:  0.8984375
train loss:  0.26529520750045776
train gradient:  0.14314313970177148
iteration : 9079
train acc:  0.90625
train loss:  0.2609008848667145
train gradient:  0.1578140700189547
iteration : 9080
train acc:  0.8671875
train loss:  0.36125892400741577
train gradient:  0.195914809257222
iteration : 9081
train acc:  0.8125
train loss:  0.4055728316307068
train gradient:  0.2404262829572591
iteration : 9082
train acc:  0.859375
train loss:  0.29769039154052734
train gradient:  0.14719689054595492
iteration : 9083
train acc:  0.8515625
train loss:  0.3341447710990906
train gradient:  0.16568603697876214
iteration : 9084
train acc:  0.875
train loss:  0.3448321223258972
train gradient:  0.1319562142375259
iteration : 9085
train acc:  0.8203125
train loss:  0.39712977409362793
train gradient:  0.20640412760972482
iteration : 9086
train acc:  0.828125
train loss:  0.3360930383205414
train gradient:  0.21959021340001889
iteration : 9087
train acc:  0.875
train loss:  0.2989993691444397
train gradient:  0.12121202101810341
iteration : 9088
train acc:  0.875
train loss:  0.28585439920425415
train gradient:  0.2189234736362998
iteration : 9089
train acc:  0.84375
train loss:  0.2906301021575928
train gradient:  0.1701531162608606
iteration : 9090
train acc:  0.84375
train loss:  0.35901471972465515
train gradient:  0.21218546513401448
iteration : 9091
train acc:  0.921875
train loss:  0.23497110605239868
train gradient:  0.10365951660467085
iteration : 9092
train acc:  0.875
train loss:  0.3288835883140564
train gradient:  0.24940828638333362
iteration : 9093
train acc:  0.8359375
train loss:  0.31298840045928955
train gradient:  0.1952817834188471
iteration : 9094
train acc:  0.828125
train loss:  0.490017294883728
train gradient:  0.41789051206231687
iteration : 9095
train acc:  0.8828125
train loss:  0.3183600604534149
train gradient:  0.12123740172169248
iteration : 9096
train acc:  0.8984375
train loss:  0.2495279312133789
train gradient:  0.08312738454764994
iteration : 9097
train acc:  0.9140625
train loss:  0.2413153499364853
train gradient:  0.09200201580370185
iteration : 9098
train acc:  0.890625
train loss:  0.2636355757713318
train gradient:  0.19982221382298926
iteration : 9099
train acc:  0.859375
train loss:  0.34431928396224976
train gradient:  0.17476968697638345
iteration : 9100
train acc:  0.796875
train loss:  0.44425809383392334
train gradient:  0.2503681379170219
iteration : 9101
train acc:  0.8046875
train loss:  0.42094504833221436
train gradient:  0.2395578919110268
iteration : 9102
train acc:  0.8515625
train loss:  0.3329578936100006
train gradient:  0.18844787331714596
iteration : 9103
train acc:  0.8359375
train loss:  0.38229644298553467
train gradient:  0.20163775870010991
iteration : 9104
train acc:  0.8984375
train loss:  0.2662895619869232
train gradient:  0.1076393374730482
iteration : 9105
train acc:  0.875
train loss:  0.3537108898162842
train gradient:  0.16628079386884592
iteration : 9106
train acc:  0.8515625
train loss:  0.3436647057533264
train gradient:  0.16210114049179775
iteration : 9107
train acc:  0.859375
train loss:  0.30644965171813965
train gradient:  0.15041110107959216
iteration : 9108
train acc:  0.8828125
train loss:  0.2830865681171417
train gradient:  0.10643136963587489
iteration : 9109
train acc:  0.859375
train loss:  0.3268517255783081
train gradient:  0.1645389745310189
iteration : 9110
train acc:  0.8671875
train loss:  0.2833666205406189
train gradient:  0.15693882397301917
iteration : 9111
train acc:  0.859375
train loss:  0.3013859987258911
train gradient:  0.14543416844083462
iteration : 9112
train acc:  0.8046875
train loss:  0.46863076090812683
train gradient:  0.27637466656226084
iteration : 9113
train acc:  0.875
train loss:  0.2840198576450348
train gradient:  0.12608765736953736
iteration : 9114
train acc:  0.7890625
train loss:  0.4113425016403198
train gradient:  0.21534109477268285
iteration : 9115
train acc:  0.8828125
train loss:  0.2608387768268585
train gradient:  0.13823997725458748
iteration : 9116
train acc:  0.859375
train loss:  0.3648216724395752
train gradient:  0.21085726773250968
iteration : 9117
train acc:  0.8828125
train loss:  0.2645368278026581
train gradient:  0.14569344289962416
iteration : 9118
train acc:  0.8515625
train loss:  0.3677326440811157
train gradient:  0.19668805261796452
iteration : 9119
train acc:  0.8984375
train loss:  0.2613452672958374
train gradient:  0.09264208451716929
iteration : 9120
train acc:  0.84375
train loss:  0.2976647615432739
train gradient:  0.1299885814776976
iteration : 9121
train acc:  0.890625
train loss:  0.32472163438796997
train gradient:  0.18484701034273748
iteration : 9122
train acc:  0.859375
train loss:  0.3174929618835449
train gradient:  0.2025430221689624
iteration : 9123
train acc:  0.90625
train loss:  0.25211474299430847
train gradient:  0.09580439653599478
iteration : 9124
train acc:  0.875
train loss:  0.285689115524292
train gradient:  0.23037663753849325
iteration : 9125
train acc:  0.8515625
train loss:  0.274325966835022
train gradient:  0.12034213623103934
iteration : 9126
train acc:  0.8046875
train loss:  0.4636686444282532
train gradient:  0.22619464414647694
iteration : 9127
train acc:  0.8203125
train loss:  0.39900124073028564
train gradient:  0.23545095645510916
iteration : 9128
train acc:  0.8359375
train loss:  0.3634486794471741
train gradient:  0.18678236132305062
iteration : 9129
train acc:  0.8515625
train loss:  0.3068890869617462
train gradient:  0.26237949386324266
iteration : 9130
train acc:  0.859375
train loss:  0.33841124176979065
train gradient:  0.12887882534750286
iteration : 9131
train acc:  0.8515625
train loss:  0.3240295648574829
train gradient:  0.1320931014964492
iteration : 9132
train acc:  0.859375
train loss:  0.3623584508895874
train gradient:  0.1492668710060925
iteration : 9133
train acc:  0.8515625
train loss:  0.40403324365615845
train gradient:  0.24452648073299452
iteration : 9134
train acc:  0.8671875
train loss:  0.300500750541687
train gradient:  0.14991911880419614
iteration : 9135
train acc:  0.8046875
train loss:  0.3910379111766815
train gradient:  0.17660341372857816
iteration : 9136
train acc:  0.875
train loss:  0.31261706352233887
train gradient:  0.17430303428975183
iteration : 9137
train acc:  0.8359375
train loss:  0.3917185664176941
train gradient:  0.21439919366975957
iteration : 9138
train acc:  0.8828125
train loss:  0.27614885568618774
train gradient:  0.12209418659204585
iteration : 9139
train acc:  0.828125
train loss:  0.29858243465423584
train gradient:  0.11034446627785456
iteration : 9140
train acc:  0.8671875
train loss:  0.30662840604782104
train gradient:  0.13075300357189718
iteration : 9141
train acc:  0.90625
train loss:  0.36181342601776123
train gradient:  0.23753272819615617
iteration : 9142
train acc:  0.875
train loss:  0.39216291904449463
train gradient:  0.19654306840849067
iteration : 9143
train acc:  0.859375
train loss:  0.3402821123600006
train gradient:  0.13565718904309448
iteration : 9144
train acc:  0.8984375
train loss:  0.3077680468559265
train gradient:  0.11931577383747634
iteration : 9145
train acc:  0.859375
train loss:  0.28578394651412964
train gradient:  0.1873701471447768
iteration : 9146
train acc:  0.859375
train loss:  0.3541860580444336
train gradient:  0.1637647659311212
iteration : 9147
train acc:  0.84375
train loss:  0.38548362255096436
train gradient:  0.19822536021056103
iteration : 9148
train acc:  0.84375
train loss:  0.31773003935813904
train gradient:  0.20231397637538695
iteration : 9149
train acc:  0.796875
train loss:  0.3649829030036926
train gradient:  0.18203142315719056
iteration : 9150
train acc:  0.8515625
train loss:  0.29230988025665283
train gradient:  0.1505992816463571
iteration : 9151
train acc:  0.8828125
train loss:  0.34633392095565796
train gradient:  0.19225542920843508
iteration : 9152
train acc:  0.890625
train loss:  0.2715705335140228
train gradient:  0.09700726929794336
iteration : 9153
train acc:  0.859375
train loss:  0.29937559366226196
train gradient:  0.1770076346333484
iteration : 9154
train acc:  0.8203125
train loss:  0.3366474509239197
train gradient:  0.2858011684013657
iteration : 9155
train acc:  0.796875
train loss:  0.37602779269218445
train gradient:  0.16802949558452995
iteration : 9156
train acc:  0.8046875
train loss:  0.435238778591156
train gradient:  0.2920556340904731
iteration : 9157
train acc:  0.8515625
train loss:  0.36414635181427
train gradient:  0.19464008637339852
iteration : 9158
train acc:  0.859375
train loss:  0.3645080327987671
train gradient:  0.15906693519146753
iteration : 9159
train acc:  0.84375
train loss:  0.3231699466705322
train gradient:  0.1915398519489276
iteration : 9160
train acc:  0.8515625
train loss:  0.3303554356098175
train gradient:  0.1805824933092021
iteration : 9161
train acc:  0.8671875
train loss:  0.3408088982105255
train gradient:  0.1645012730649139
iteration : 9162
train acc:  0.84375
train loss:  0.34324783086776733
train gradient:  0.1883470809717847
iteration : 9163
train acc:  0.890625
train loss:  0.2777562439441681
train gradient:  0.14519151924297727
iteration : 9164
train acc:  0.90625
train loss:  0.23720549046993256
train gradient:  0.10299041800068526
iteration : 9165
train acc:  0.875
train loss:  0.3053429126739502
train gradient:  0.10236880261671064
iteration : 9166
train acc:  0.8984375
train loss:  0.33554959297180176
train gradient:  0.1267925968751302
iteration : 9167
train acc:  0.78125
train loss:  0.3795096278190613
train gradient:  0.17528404066484102
iteration : 9168
train acc:  0.828125
train loss:  0.31843262910842896
train gradient:  0.1832187108011602
iteration : 9169
train acc:  0.875
train loss:  0.32437193393707275
train gradient:  0.13318791956936657
iteration : 9170
train acc:  0.8515625
train loss:  0.38483262062072754
train gradient:  0.26594783977856595
iteration : 9171
train acc:  0.796875
train loss:  0.4285077750682831
train gradient:  0.2533641290814499
iteration : 9172
train acc:  0.8359375
train loss:  0.38001111149787903
train gradient:  0.20005871462105185
iteration : 9173
train acc:  0.875
train loss:  0.3674013614654541
train gradient:  0.273712706034506
iteration : 9174
train acc:  0.8828125
train loss:  0.2800765335559845
train gradient:  0.11414734537563018
iteration : 9175
train acc:  0.8359375
train loss:  0.4152883291244507
train gradient:  0.2816934530157199
iteration : 9176
train acc:  0.8125
train loss:  0.37060195207595825
train gradient:  0.22022382669287682
iteration : 9177
train acc:  0.90625
train loss:  0.290849506855011
train gradient:  0.13741601140506876
iteration : 9178
train acc:  0.8671875
train loss:  0.32038235664367676
train gradient:  0.1590444714362822
iteration : 9179
train acc:  0.859375
train loss:  0.2996733784675598
train gradient:  0.16945935298196585
iteration : 9180
train acc:  0.8359375
train loss:  0.34720999002456665
train gradient:  0.17908689422118557
iteration : 9181
train acc:  0.8671875
train loss:  0.3460332751274109
train gradient:  0.19323902248212008
iteration : 9182
train acc:  0.8671875
train loss:  0.29659441113471985
train gradient:  0.1754373571909583
iteration : 9183
train acc:  0.875
train loss:  0.33257004618644714
train gradient:  0.16210809862987266
iteration : 9184
train acc:  0.8125
train loss:  0.41790783405303955
train gradient:  0.334866686177891
iteration : 9185
train acc:  0.8828125
train loss:  0.2998073697090149
train gradient:  0.1559235338291362
iteration : 9186
train acc:  0.9140625
train loss:  0.2346213459968567
train gradient:  0.12547381713638095
iteration : 9187
train acc:  0.859375
train loss:  0.32554665207862854
train gradient:  0.19801169126095033
iteration : 9188
train acc:  0.859375
train loss:  0.2973492741584778
train gradient:  0.13493430474999496
iteration : 9189
train acc:  0.859375
train loss:  0.34098291397094727
train gradient:  0.1917391352824888
iteration : 9190
train acc:  0.890625
train loss:  0.2892013490200043
train gradient:  0.1095480023180053
iteration : 9191
train acc:  0.8203125
train loss:  0.38074544072151184
train gradient:  0.20922151977622888
iteration : 9192
train acc:  0.8828125
train loss:  0.27932316064834595
train gradient:  0.13886262285455026
iteration : 9193
train acc:  0.8359375
train loss:  0.3939465880393982
train gradient:  0.274605769587009
iteration : 9194
train acc:  0.8828125
train loss:  0.2716805636882782
train gradient:  0.14781528325651888
iteration : 9195
train acc:  0.8671875
train loss:  0.3151719570159912
train gradient:  0.21821098098099495
iteration : 9196
train acc:  0.8984375
train loss:  0.28913092613220215
train gradient:  0.12399137492163534
iteration : 9197
train acc:  0.8671875
train loss:  0.3154079020023346
train gradient:  0.1518074887700467
iteration : 9198
train acc:  0.8515625
train loss:  0.4049234986305237
train gradient:  0.19029306660624415
iteration : 9199
train acc:  0.921875
train loss:  0.25744736194610596
train gradient:  0.10908843320773919
iteration : 9200
train acc:  0.875
train loss:  0.22978433966636658
train gradient:  0.09902911194781903
iteration : 9201
train acc:  0.8203125
train loss:  0.3960579037666321
train gradient:  0.24864667272610463
iteration : 9202
train acc:  0.8984375
train loss:  0.26561230421066284
train gradient:  0.13669904416336592
iteration : 9203
train acc:  0.8984375
train loss:  0.24249961972236633
train gradient:  0.10254877181806142
iteration : 9204
train acc:  0.8671875
train loss:  0.30285704135894775
train gradient:  0.18010925599806404
iteration : 9205
train acc:  0.875
train loss:  0.2846776843070984
train gradient:  0.10034712547443521
iteration : 9206
train acc:  0.890625
train loss:  0.30081427097320557
train gradient:  0.14768756789213883
iteration : 9207
train acc:  0.8125
train loss:  0.3064323663711548
train gradient:  0.15708607460934937
iteration : 9208
train acc:  0.8515625
train loss:  0.34619617462158203
train gradient:  0.2291855357926907
iteration : 9209
train acc:  0.8671875
train loss:  0.3493615388870239
train gradient:  0.2053349339503408
iteration : 9210
train acc:  0.8125
train loss:  0.3324583172798157
train gradient:  0.20018527791187013
iteration : 9211
train acc:  0.875
train loss:  0.25505977869033813
train gradient:  0.11056996562304643
iteration : 9212
train acc:  0.859375
train loss:  0.35204118490219116
train gradient:  0.26215618874634605
iteration : 9213
train acc:  0.8359375
train loss:  0.3706376552581787
train gradient:  0.16871111797156607
iteration : 9214
train acc:  0.9140625
train loss:  0.2587273120880127
train gradient:  0.13263224054977957
iteration : 9215
train acc:  0.8515625
train loss:  0.2994302809238434
train gradient:  0.1910952592938444
iteration : 9216
train acc:  0.859375
train loss:  0.3128766417503357
train gradient:  0.17355399103829622
iteration : 9217
train acc:  0.890625
train loss:  0.3048856854438782
train gradient:  0.16162461939525408
iteration : 9218
train acc:  0.84375
train loss:  0.3283897042274475
train gradient:  0.15924910556419777
iteration : 9219
train acc:  0.8125
train loss:  0.4031882584095001
train gradient:  0.2296057028365888
iteration : 9220
train acc:  0.859375
train loss:  0.34510016441345215
train gradient:  0.16356696862473422
iteration : 9221
train acc:  0.8125
train loss:  0.33834266662597656
train gradient:  0.1600015639827706
iteration : 9222
train acc:  0.859375
train loss:  0.32819652557373047
train gradient:  0.20033785791559294
iteration : 9223
train acc:  0.8984375
train loss:  0.27287790179252625
train gradient:  0.16291241728155526
iteration : 9224
train acc:  0.890625
train loss:  0.31550514698028564
train gradient:  0.16130772596597215
iteration : 9225
train acc:  0.828125
train loss:  0.39803969860076904
train gradient:  0.27943920266677313
iteration : 9226
train acc:  0.859375
train loss:  0.36758339405059814
train gradient:  0.2534987915381575
iteration : 9227
train acc:  0.890625
train loss:  0.26307541131973267
train gradient:  0.17252794333298282
iteration : 9228
train acc:  0.8203125
train loss:  0.32161062955856323
train gradient:  0.18819305172760775
iteration : 9229
train acc:  0.8125
train loss:  0.3458792269229889
train gradient:  0.18851175633670858
iteration : 9230
train acc:  0.8515625
train loss:  0.3572544455528259
train gradient:  0.1552311417602001
iteration : 9231
train acc:  0.8125
train loss:  0.4077739119529724
train gradient:  0.2320065741151106
iteration : 9232
train acc:  0.84375
train loss:  0.31070858240127563
train gradient:  0.1903288882072653
iteration : 9233
train acc:  0.8359375
train loss:  0.33941128849983215
train gradient:  0.22567452870222288
iteration : 9234
train acc:  0.859375
train loss:  0.374603271484375
train gradient:  0.18697063157727561
iteration : 9235
train acc:  0.8515625
train loss:  0.325608491897583
train gradient:  0.18064238465930366
iteration : 9236
train acc:  0.8125
train loss:  0.370912104845047
train gradient:  0.27049300730083153
iteration : 9237
train acc:  0.859375
train loss:  0.31360340118408203
train gradient:  0.1859275650476515
iteration : 9238
train acc:  0.8046875
train loss:  0.4369218945503235
train gradient:  0.24060858047827444
iteration : 9239
train acc:  0.875
train loss:  0.3418537974357605
train gradient:  0.19747223751552379
iteration : 9240
train acc:  0.8828125
train loss:  0.26581019163131714
train gradient:  0.23812643153666346
iteration : 9241
train acc:  0.8828125
train loss:  0.34534984827041626
train gradient:  0.23019416633407
iteration : 9242
train acc:  0.84375
train loss:  0.3546222746372223
train gradient:  0.2008674687277605
iteration : 9243
train acc:  0.859375
train loss:  0.3526509404182434
train gradient:  0.16293711544427122
iteration : 9244
train acc:  0.8828125
train loss:  0.3269665241241455
train gradient:  0.23915718556719323
iteration : 9245
train acc:  0.8515625
train loss:  0.34311217069625854
train gradient:  0.2071804524565367
iteration : 9246
train acc:  0.828125
train loss:  0.37397685647010803
train gradient:  0.2749829367777748
iteration : 9247
train acc:  0.859375
train loss:  0.3324522376060486
train gradient:  0.2137469356101244
iteration : 9248
train acc:  0.8125
train loss:  0.4063129127025604
train gradient:  0.20409291932636153
iteration : 9249
train acc:  0.90625
train loss:  0.2329523265361786
train gradient:  0.09785083799842625
iteration : 9250
train acc:  0.859375
train loss:  0.28091222047805786
train gradient:  0.12661106640100883
iteration : 9251
train acc:  0.8359375
train loss:  0.3929685652256012
train gradient:  0.2851869150154038
iteration : 9252
train acc:  0.8984375
train loss:  0.2538607120513916
train gradient:  0.11398067970851541
iteration : 9253
train acc:  0.9140625
train loss:  0.27821141481399536
train gradient:  0.11324595348141596
iteration : 9254
train acc:  0.8046875
train loss:  0.3752294182777405
train gradient:  0.16778713395987827
iteration : 9255
train acc:  0.8984375
train loss:  0.32314765453338623
train gradient:  0.14219519338260622
iteration : 9256
train acc:  0.8359375
train loss:  0.33244234323501587
train gradient:  0.16279655912460775
iteration : 9257
train acc:  0.796875
train loss:  0.400377482175827
train gradient:  0.2686817814683877
iteration : 9258
train acc:  0.8359375
train loss:  0.3793054223060608
train gradient:  0.2747746166866554
iteration : 9259
train acc:  0.8203125
train loss:  0.35390132665634155
train gradient:  0.13383243866657435
iteration : 9260
train acc:  0.84375
train loss:  0.31336474418640137
train gradient:  0.13612408402141135
iteration : 9261
train acc:  0.8203125
train loss:  0.3749834895133972
train gradient:  0.1917949692671635
iteration : 9262
train acc:  0.84375
train loss:  0.3556480407714844
train gradient:  0.2181656374055719
iteration : 9263
train acc:  0.859375
train loss:  0.33246129751205444
train gradient:  0.16067080876562861
iteration : 9264
train acc:  0.8046875
train loss:  0.4032512605190277
train gradient:  0.4025603995245769
iteration : 9265
train acc:  0.8046875
train loss:  0.43651819229125977
train gradient:  0.24304306226620187
iteration : 9266
train acc:  0.796875
train loss:  0.3806540369987488
train gradient:  0.1680070242684001
iteration : 9267
train acc:  0.8828125
train loss:  0.2884078621864319
train gradient:  0.1219628750660851
iteration : 9268
train acc:  0.8125
train loss:  0.42219048738479614
train gradient:  0.22594584734230022
iteration : 9269
train acc:  0.8515625
train loss:  0.3190321922302246
train gradient:  0.14105858358657275
iteration : 9270
train acc:  0.8359375
train loss:  0.3661256432533264
train gradient:  0.24209087686255376
iteration : 9271
train acc:  0.8359375
train loss:  0.34778180718421936
train gradient:  0.2158292022720796
iteration : 9272
train acc:  0.8125
train loss:  0.3625688850879669
train gradient:  0.17064005029262852
iteration : 9273
train acc:  0.8046875
train loss:  0.3509344458580017
train gradient:  0.18112948899777825
iteration : 9274
train acc:  0.859375
train loss:  0.3136579990386963
train gradient:  0.20145031769973695
iteration : 9275
train acc:  0.8359375
train loss:  0.37813982367515564
train gradient:  0.18192637395738367
iteration : 9276
train acc:  0.8671875
train loss:  0.33595508337020874
train gradient:  0.13109482329625208
iteration : 9277
train acc:  0.8515625
train loss:  0.3029951751232147
train gradient:  0.164458147284481
iteration : 9278
train acc:  0.8515625
train loss:  0.3414604663848877
train gradient:  0.13889342345835612
iteration : 9279
train acc:  0.90625
train loss:  0.26281219720840454
train gradient:  0.12540125146851863
iteration : 9280
train acc:  0.890625
train loss:  0.26478812098503113
train gradient:  0.1330034540506913
iteration : 9281
train acc:  0.859375
train loss:  0.38767698407173157
train gradient:  0.21183253230068771
iteration : 9282
train acc:  0.8359375
train loss:  0.3990630507469177
train gradient:  0.16699767262323173
iteration : 9283
train acc:  0.8828125
train loss:  0.31229472160339355
train gradient:  0.17988460493011924
iteration : 9284
train acc:  0.8515625
train loss:  0.3379424214363098
train gradient:  0.23476763897028352
iteration : 9285
train acc:  0.890625
train loss:  0.2946743071079254
train gradient:  0.14667641291915848
iteration : 9286
train acc:  0.7265625
train loss:  0.5194299221038818
train gradient:  0.5128536683728631
iteration : 9287
train acc:  0.8671875
train loss:  0.3582953214645386
train gradient:  0.17095832141872264
iteration : 9288
train acc:  0.8359375
train loss:  0.3658149242401123
train gradient:  0.1514266068405682
iteration : 9289
train acc:  0.890625
train loss:  0.2611654996871948
train gradient:  0.1358389539131454
iteration : 9290
train acc:  0.8515625
train loss:  0.3545668423175812
train gradient:  0.1208177617213015
iteration : 9291
train acc:  0.8984375
train loss:  0.28883612155914307
train gradient:  0.1443787728475681
iteration : 9292
train acc:  0.8359375
train loss:  0.360884428024292
train gradient:  0.19043112171544535
iteration : 9293
train acc:  0.8046875
train loss:  0.40382832288742065
train gradient:  0.17630128723499436
iteration : 9294
train acc:  0.8984375
train loss:  0.2353219836950302
train gradient:  0.13107331245153736
iteration : 9295
train acc:  0.8359375
train loss:  0.32442301511764526
train gradient:  0.1716647118159203
iteration : 9296
train acc:  0.8828125
train loss:  0.26896369457244873
train gradient:  0.16201329046958218
iteration : 9297
train acc:  0.890625
train loss:  0.3109428286552429
train gradient:  0.11962002455839525
iteration : 9298
train acc:  0.8671875
train loss:  0.31564784049987793
train gradient:  0.18904863927083046
iteration : 9299
train acc:  0.8046875
train loss:  0.39082399010658264
train gradient:  0.2872812208799968
iteration : 9300
train acc:  0.859375
train loss:  0.31932950019836426
train gradient:  0.15834386144341478
iteration : 9301
train acc:  0.8828125
train loss:  0.31116819381713867
train gradient:  0.1379072630421468
iteration : 9302
train acc:  0.8984375
train loss:  0.3613739609718323
train gradient:  0.16528513306083786
iteration : 9303
train acc:  0.8515625
train loss:  0.3150835335254669
train gradient:  0.13938147150609392
iteration : 9304
train acc:  0.8203125
train loss:  0.40857160091400146
train gradient:  0.20787941012696895
iteration : 9305
train acc:  0.8125
train loss:  0.33378341794013977
train gradient:  0.1402309555315763
iteration : 9306
train acc:  0.875
train loss:  0.2809756398200989
train gradient:  0.11100226084011297
iteration : 9307
train acc:  0.8046875
train loss:  0.3732243776321411
train gradient:  0.22550586384369536
iteration : 9308
train acc:  0.8671875
train loss:  0.27588343620300293
train gradient:  0.13979451830492315
iteration : 9309
train acc:  0.8203125
train loss:  0.3703097105026245
train gradient:  0.1765568958072217
iteration : 9310
train acc:  0.8984375
train loss:  0.29322072863578796
train gradient:  0.14463017805008765
iteration : 9311
train acc:  0.8984375
train loss:  0.26788678765296936
train gradient:  0.136720539433969
iteration : 9312
train acc:  0.8671875
train loss:  0.32343846559524536
train gradient:  0.1307296519774894
iteration : 9313
train acc:  0.84375
train loss:  0.28679168224334717
train gradient:  0.10694068214185812
iteration : 9314
train acc:  0.875
train loss:  0.28901737928390503
train gradient:  0.1436817032643718
iteration : 9315
train acc:  0.8359375
train loss:  0.40675362944602966
train gradient:  0.21301872819041312
iteration : 9316
train acc:  0.859375
train loss:  0.32564711570739746
train gradient:  0.14372958321898102
iteration : 9317
train acc:  0.8515625
train loss:  0.35229653120040894
train gradient:  0.20288155930145665
iteration : 9318
train acc:  0.8203125
train loss:  0.42631563544273376
train gradient:  0.25610523239432753
iteration : 9319
train acc:  0.8515625
train loss:  0.30152469873428345
train gradient:  0.12648981539258036
iteration : 9320
train acc:  0.8671875
train loss:  0.3093048334121704
train gradient:  0.17129943392198982
iteration : 9321
train acc:  0.890625
train loss:  0.28571707010269165
train gradient:  0.21838854369665542
iteration : 9322
train acc:  0.8984375
train loss:  0.24206490814685822
train gradient:  0.1034783339122441
iteration : 9323
train acc:  0.84375
train loss:  0.335409939289093
train gradient:  0.23390617425295707
iteration : 9324
train acc:  0.859375
train loss:  0.33913323283195496
train gradient:  0.17020933401199143
iteration : 9325
train acc:  0.8671875
train loss:  0.3477204442024231
train gradient:  0.19991524124569698
iteration : 9326
train acc:  0.9140625
train loss:  0.2478802502155304
train gradient:  0.14200965944231836
iteration : 9327
train acc:  0.7890625
train loss:  0.4296748638153076
train gradient:  0.28863102250105105
iteration : 9328
train acc:  0.8359375
train loss:  0.4042697548866272
train gradient:  0.21886205649109386
iteration : 9329
train acc:  0.8203125
train loss:  0.36073726415634155
train gradient:  0.23649469489425026
iteration : 9330
train acc:  0.90625
train loss:  0.263296902179718
train gradient:  0.11273202904854332
iteration : 9331
train acc:  0.8828125
train loss:  0.25442618131637573
train gradient:  0.11234432798268625
iteration : 9332
train acc:  0.8671875
train loss:  0.27853959798812866
train gradient:  0.11806348695743968
iteration : 9333
train acc:  0.90625
train loss:  0.2659013271331787
train gradient:  0.11864510936083582
iteration : 9334
train acc:  0.765625
train loss:  0.38872385025024414
train gradient:  0.164397833950565
iteration : 9335
train acc:  0.8203125
train loss:  0.40350770950317383
train gradient:  0.20152610789287995
iteration : 9336
train acc:  0.8828125
train loss:  0.2787097096443176
train gradient:  0.12875283080459402
iteration : 9337
train acc:  0.9140625
train loss:  0.2652537226676941
train gradient:  0.20321967479566422
iteration : 9338
train acc:  0.8125
train loss:  0.38061943650245667
train gradient:  0.2589097091869033
iteration : 9339
train acc:  0.8125
train loss:  0.3746425211429596
train gradient:  0.14385156644764346
iteration : 9340
train acc:  0.84375
train loss:  0.34755802154541016
train gradient:  0.1568517524182152
iteration : 9341
train acc:  0.875
train loss:  0.28535979986190796
train gradient:  0.11574165208291895
iteration : 9342
train acc:  0.8125
train loss:  0.3442642092704773
train gradient:  0.17826166266122728
iteration : 9343
train acc:  0.9140625
train loss:  0.2674029469490051
train gradient:  0.10690865722789562
iteration : 9344
train acc:  0.8515625
train loss:  0.2816660404205322
train gradient:  0.11058837949812418
iteration : 9345
train acc:  0.8515625
train loss:  0.38469386100769043
train gradient:  0.2613997968573606
iteration : 9346
train acc:  0.8828125
train loss:  0.2701684236526489
train gradient:  0.12685236234540154
iteration : 9347
train acc:  0.8515625
train loss:  0.2918965816497803
train gradient:  0.12596103398833103
iteration : 9348
train acc:  0.8046875
train loss:  0.43176355957984924
train gradient:  0.18900078730167644
iteration : 9349
train acc:  0.8203125
train loss:  0.3898082375526428
train gradient:  0.22540974816436177
iteration : 9350
train acc:  0.7734375
train loss:  0.39738917350769043
train gradient:  0.22721262497937578
iteration : 9351
train acc:  0.890625
train loss:  0.25769275426864624
train gradient:  0.12040906512632629
iteration : 9352
train acc:  0.8828125
train loss:  0.30965250730514526
train gradient:  0.17150798018333102
iteration : 9353
train acc:  0.828125
train loss:  0.31874558329582214
train gradient:  0.12283439945958283
iteration : 9354
train acc:  0.8203125
train loss:  0.3583151698112488
train gradient:  0.17230257322959694
iteration : 9355
train acc:  0.8203125
train loss:  0.36323463916778564
train gradient:  0.30158436079849593
iteration : 9356
train acc:  0.859375
train loss:  0.30312713980674744
train gradient:  0.18278101769575722
iteration : 9357
train acc:  0.875
train loss:  0.2951980233192444
train gradient:  0.15706885722419933
iteration : 9358
train acc:  0.9140625
train loss:  0.23683284223079681
train gradient:  0.13526206203584043
iteration : 9359
train acc:  0.890625
train loss:  0.30392974615097046
train gradient:  0.12867508731363175
iteration : 9360
train acc:  0.9140625
train loss:  0.2689882814884186
train gradient:  0.11832612958481527
iteration : 9361
train acc:  0.8984375
train loss:  0.30239561200141907
train gradient:  0.17679063609405588
iteration : 9362
train acc:  0.8828125
train loss:  0.2517724931240082
train gradient:  0.13777902456128804
iteration : 9363
train acc:  0.8984375
train loss:  0.21615366637706757
train gradient:  0.08420709339146629
iteration : 9364
train acc:  0.8671875
train loss:  0.3005424737930298
train gradient:  0.14032871641894765
iteration : 9365
train acc:  0.859375
train loss:  0.32670286297798157
train gradient:  0.1646693395606038
iteration : 9366
train acc:  0.8515625
train loss:  0.30148571729660034
train gradient:  0.18397874410813347
iteration : 9367
train acc:  0.8203125
train loss:  0.3516212999820709
train gradient:  0.1561436034840622
iteration : 9368
train acc:  0.84375
train loss:  0.37710681557655334
train gradient:  0.21836288018638977
iteration : 9369
train acc:  0.8828125
train loss:  0.304361492395401
train gradient:  0.15513155226077135
iteration : 9370
train acc:  0.8125
train loss:  0.4163435697555542
train gradient:  0.241769114948013
iteration : 9371
train acc:  0.8515625
train loss:  0.3439958691596985
train gradient:  0.1977148723090711
iteration : 9372
train acc:  0.8828125
train loss:  0.2702108919620514
train gradient:  0.12237565421888262
iteration : 9373
train acc:  0.9140625
train loss:  0.22552642226219177
train gradient:  0.12950661577210693
iteration : 9374
train acc:  0.875
train loss:  0.2801164984703064
train gradient:  0.23423973276698778
iteration : 9375
train acc:  0.8515625
train loss:  0.3274138569831848
train gradient:  0.18759767271198657
iteration : 9376
train acc:  0.8359375
train loss:  0.3805699050426483
train gradient:  0.22772973442645336
iteration : 9377
train acc:  0.859375
train loss:  0.35355693101882935
train gradient:  0.15322422586919737
iteration : 9378
train acc:  0.8203125
train loss:  0.3633902370929718
train gradient:  0.2601685400466075
iteration : 9379
train acc:  0.8359375
train loss:  0.3239980936050415
train gradient:  0.18745359803925726
iteration : 9380
train acc:  0.8359375
train loss:  0.3808603286743164
train gradient:  0.34256593412676617
iteration : 9381
train acc:  0.875
train loss:  0.27422231435775757
train gradient:  0.20578120889881538
iteration : 9382
train acc:  0.828125
train loss:  0.33652281761169434
train gradient:  0.25584940594444217
iteration : 9383
train acc:  0.8828125
train loss:  0.3051506280899048
train gradient:  0.1247358734544266
iteration : 9384
train acc:  0.8359375
train loss:  0.4285151958465576
train gradient:  0.23582648696523284
iteration : 9385
train acc:  0.8125
train loss:  0.36748990416526794
train gradient:  0.17934256673668642
iteration : 9386
train acc:  0.796875
train loss:  0.4818246364593506
train gradient:  0.35046414177056473
iteration : 9387
train acc:  0.8984375
train loss:  0.26323401927948
train gradient:  0.13177723434464067
iteration : 9388
train acc:  0.8203125
train loss:  0.43141430616378784
train gradient:  0.2298324366927686
iteration : 9389
train acc:  0.84375
train loss:  0.34651458263397217
train gradient:  0.24698340903214225
iteration : 9390
train acc:  0.8671875
train loss:  0.3327479362487793
train gradient:  0.1879135866243544
iteration : 9391
train acc:  0.84375
train loss:  0.3796212375164032
train gradient:  0.22789989360203813
iteration : 9392
train acc:  0.8515625
train loss:  0.3166256546974182
train gradient:  0.14845554296720626
iteration : 9393
train acc:  0.8125
train loss:  0.43895435333251953
train gradient:  0.28970896182741374
iteration : 9394
train acc:  0.875
train loss:  0.3269028663635254
train gradient:  0.17831256349950814
iteration : 9395
train acc:  0.8828125
train loss:  0.31155675649642944
train gradient:  0.13756047099436594
iteration : 9396
train acc:  0.828125
train loss:  0.3089098036289215
train gradient:  0.15282596613720867
iteration : 9397
train acc:  0.78125
train loss:  0.48960816860198975
train gradient:  0.24787797347648882
iteration : 9398
train acc:  0.828125
train loss:  0.35797229409217834
train gradient:  0.1762199079399226
iteration : 9399
train acc:  0.84375
train loss:  0.3595117926597595
train gradient:  0.2076837326204211
iteration : 9400
train acc:  0.890625
train loss:  0.2641574740409851
train gradient:  0.07134928153870879
iteration : 9401
train acc:  0.875
train loss:  0.2731072008609772
train gradient:  0.16506427489075837
iteration : 9402
train acc:  0.8515625
train loss:  0.3401344418525696
train gradient:  0.1896770492931245
iteration : 9403
train acc:  0.8515625
train loss:  0.32757747173309326
train gradient:  0.1936683050870926
iteration : 9404
train acc:  0.8046875
train loss:  0.4219323694705963
train gradient:  0.22313868955822314
iteration : 9405
train acc:  0.875
train loss:  0.28134632110595703
train gradient:  0.14850898963833287
iteration : 9406
train acc:  0.890625
train loss:  0.244835764169693
train gradient:  0.1518689330623661
iteration : 9407
train acc:  0.84375
train loss:  0.3015214800834656
train gradient:  0.13274784760175729
iteration : 9408
train acc:  0.7734375
train loss:  0.46773219108581543
train gradient:  0.39157926142947647
iteration : 9409
train acc:  0.84375
train loss:  0.3704482614994049
train gradient:  0.21483387709486074
iteration : 9410
train acc:  0.84375
train loss:  0.3765246868133545
train gradient:  0.18283883859249936
iteration : 9411
train acc:  0.859375
train loss:  0.3653917908668518
train gradient:  0.22822299686407188
iteration : 9412
train acc:  0.8203125
train loss:  0.3762056231498718
train gradient:  0.2124624823987089
iteration : 9413
train acc:  0.859375
train loss:  0.3301295042037964
train gradient:  0.20409187284559752
iteration : 9414
train acc:  0.90625
train loss:  0.31664758920669556
train gradient:  0.1170152384282581
iteration : 9415
train acc:  0.84375
train loss:  0.3620971441268921
train gradient:  0.15072687551221303
iteration : 9416
train acc:  0.828125
train loss:  0.36541974544525146
train gradient:  0.20268103047223524
iteration : 9417
train acc:  0.8046875
train loss:  0.3943966329097748
train gradient:  0.23800223811906904
iteration : 9418
train acc:  0.875
train loss:  0.3084498941898346
train gradient:  0.16505980696948747
iteration : 9419
train acc:  0.859375
train loss:  0.3094838559627533
train gradient:  0.14661064466582446
iteration : 9420
train acc:  0.890625
train loss:  0.2764502763748169
train gradient:  0.12121793581355851
iteration : 9421
train acc:  0.8671875
train loss:  0.3322269320487976
train gradient:  0.19829277318014688
iteration : 9422
train acc:  0.9140625
train loss:  0.25749725103378296
train gradient:  0.10834240670818154
iteration : 9423
train acc:  0.8671875
train loss:  0.2732591927051544
train gradient:  0.1263448128044593
iteration : 9424
train acc:  0.859375
train loss:  0.29253125190734863
train gradient:  0.14287717701750063
iteration : 9425
train acc:  0.875
train loss:  0.3468172550201416
train gradient:  0.1531731520740893
iteration : 9426
train acc:  0.8671875
train loss:  0.28647029399871826
train gradient:  0.1406531176316366
iteration : 9427
train acc:  0.8203125
train loss:  0.4088727831840515
train gradient:  0.20159968051058447
iteration : 9428
train acc:  0.8984375
train loss:  0.3009052276611328
train gradient:  0.11181377791637635
iteration : 9429
train acc:  0.8515625
train loss:  0.3158537745475769
train gradient:  0.16462060669329512
iteration : 9430
train acc:  0.8828125
train loss:  0.3480873703956604
train gradient:  0.16944255281884055
iteration : 9431
train acc:  0.765625
train loss:  0.46249526739120483
train gradient:  0.24772285579839504
iteration : 9432
train acc:  0.8359375
train loss:  0.3864036202430725
train gradient:  0.2330489212725611
iteration : 9433
train acc:  0.828125
train loss:  0.32302647829055786
train gradient:  0.13992530942319326
iteration : 9434
train acc:  0.8515625
train loss:  0.31546682119369507
train gradient:  0.15654285846869964
iteration : 9435
train acc:  0.859375
train loss:  0.37821340560913086
train gradient:  0.2501669907806835
iteration : 9436
train acc:  0.828125
train loss:  0.3697647154331207
train gradient:  0.17203309362678362
iteration : 9437
train acc:  0.8515625
train loss:  0.37327471375465393
train gradient:  0.24325838991348916
iteration : 9438
train acc:  0.84375
train loss:  0.36420392990112305
train gradient:  0.17854577354039086
iteration : 9439
train acc:  0.8984375
train loss:  0.2492661476135254
train gradient:  0.13901913735168187
iteration : 9440
train acc:  0.859375
train loss:  0.34556135535240173
train gradient:  0.15698952559784374
iteration : 9441
train acc:  0.8046875
train loss:  0.4168476462364197
train gradient:  0.2340825470518354
iteration : 9442
train acc:  0.8828125
train loss:  0.29481858015060425
train gradient:  0.11495548086942772
iteration : 9443
train acc:  0.8046875
train loss:  0.40126508474349976
train gradient:  0.21128256633013953
iteration : 9444
train acc:  0.84375
train loss:  0.31714966893196106
train gradient:  0.1259314442401557
iteration : 9445
train acc:  0.875
train loss:  0.3056641221046448
train gradient:  0.10848743128938562
iteration : 9446
train acc:  0.84375
train loss:  0.3406602442264557
train gradient:  0.13814447365617208
iteration : 9447
train acc:  0.8125
train loss:  0.31167325377464294
train gradient:  0.20273183676037282
iteration : 9448
train acc:  0.8203125
train loss:  0.37484657764434814
train gradient:  0.1953479157844453
iteration : 9449
train acc:  0.8828125
train loss:  0.28349626064300537
train gradient:  0.10460263399974709
iteration : 9450
train acc:  0.875
train loss:  0.2866394519805908
train gradient:  0.2765328538350902
iteration : 9451
train acc:  0.84375
train loss:  0.3114055395126343
train gradient:  0.1484031850909242
iteration : 9452
train acc:  0.8984375
train loss:  0.3328794836997986
train gradient:  0.12021081098960214
iteration : 9453
train acc:  0.875
train loss:  0.3369073271751404
train gradient:  0.1519807683916568
iteration : 9454
train acc:  0.8671875
train loss:  0.32405275106430054
train gradient:  0.15110049443144574
iteration : 9455
train acc:  0.8515625
train loss:  0.36082714796066284
train gradient:  0.18422860528429366
iteration : 9456
train acc:  0.8203125
train loss:  0.40581193566322327
train gradient:  0.21021533494620545
iteration : 9457
train acc:  0.8984375
train loss:  0.26629477739334106
train gradient:  0.14470902770102884
iteration : 9458
train acc:  0.859375
train loss:  0.36708706617355347
train gradient:  0.24800223579441166
iteration : 9459
train acc:  0.8203125
train loss:  0.41168564558029175
train gradient:  0.22520919871897882
iteration : 9460
train acc:  0.8046875
train loss:  0.396981805562973
train gradient:  0.24065664549664073
iteration : 9461
train acc:  0.84375
train loss:  0.338634729385376
train gradient:  0.18963644012568576
iteration : 9462
train acc:  0.84375
train loss:  0.29456180334091187
train gradient:  0.12868434941594095
iteration : 9463
train acc:  0.8203125
train loss:  0.3627914786338806
train gradient:  0.20645840593561066
iteration : 9464
train acc:  0.828125
train loss:  0.35256272554397583
train gradient:  0.19254823118040937
iteration : 9465
train acc:  0.8515625
train loss:  0.3785504996776581
train gradient:  0.20324326554407102
iteration : 9466
train acc:  0.875
train loss:  0.2651135325431824
train gradient:  0.13571374517459106
iteration : 9467
train acc:  0.765625
train loss:  0.4211115837097168
train gradient:  0.24272178154258855
iteration : 9468
train acc:  0.8203125
train loss:  0.4042569398880005
train gradient:  0.3160952688196457
iteration : 9469
train acc:  0.8515625
train loss:  0.3157017230987549
train gradient:  0.1465932642849065
iteration : 9470
train acc:  0.8828125
train loss:  0.27238407731056213
train gradient:  0.11833558732382217
iteration : 9471
train acc:  0.8671875
train loss:  0.35817503929138184
train gradient:  0.2133059272724076
iteration : 9472
train acc:  0.875
train loss:  0.3007197380065918
train gradient:  0.15101008944134933
iteration : 9473
train acc:  0.84375
train loss:  0.36830735206604004
train gradient:  0.16554110228667063
iteration : 9474
train acc:  0.8359375
train loss:  0.37433984875679016
train gradient:  0.15351490500058282
iteration : 9475
train acc:  0.8359375
train loss:  0.34956347942352295
train gradient:  0.1665501062617453
iteration : 9476
train acc:  0.8515625
train loss:  0.32932668924331665
train gradient:  0.1191749446403894
iteration : 9477
train acc:  0.8671875
train loss:  0.32411837577819824
train gradient:  0.17942951839149257
iteration : 9478
train acc:  0.8671875
train loss:  0.3073376417160034
train gradient:  0.12913442453605622
iteration : 9479
train acc:  0.875
train loss:  0.28825080394744873
train gradient:  0.09848467507993731
iteration : 9480
train acc:  0.890625
train loss:  0.2840825319290161
train gradient:  0.10102688086864815
iteration : 9481
train acc:  0.8515625
train loss:  0.33394724130630493
train gradient:  0.11782738370870342
iteration : 9482
train acc:  0.8125
train loss:  0.33717429637908936
train gradient:  0.16572266874340397
iteration : 9483
train acc:  0.8828125
train loss:  0.2974269390106201
train gradient:  0.19549074639144343
iteration : 9484
train acc:  0.84375
train loss:  0.290656715631485
train gradient:  0.125305352338427
iteration : 9485
train acc:  0.8984375
train loss:  0.27914708852767944
train gradient:  0.1494990300099169
iteration : 9486
train acc:  0.875
train loss:  0.3179203271865845
train gradient:  0.13640492462377715
iteration : 9487
train acc:  0.8515625
train loss:  0.31201621890068054
train gradient:  0.13639847643398634
iteration : 9488
train acc:  0.8359375
train loss:  0.358991801738739
train gradient:  0.2720407081991746
iteration : 9489
train acc:  0.8828125
train loss:  0.3053893446922302
train gradient:  0.15000009727027347
iteration : 9490
train acc:  0.875
train loss:  0.3183724880218506
train gradient:  0.11968970106793006
iteration : 9491
train acc:  0.8671875
train loss:  0.272178053855896
train gradient:  0.12009007136014688
iteration : 9492
train acc:  0.8359375
train loss:  0.4069262146949768
train gradient:  0.18450399746477025
iteration : 9493
train acc:  0.875
train loss:  0.24780061841011047
train gradient:  0.11280973556468525
iteration : 9494
train acc:  0.796875
train loss:  0.3888045847415924
train gradient:  0.28050924110090397
iteration : 9495
train acc:  0.8203125
train loss:  0.38698944449424744
train gradient:  0.18475665895943455
iteration : 9496
train acc:  0.8359375
train loss:  0.35720914602279663
train gradient:  0.1718518657346264
iteration : 9497
train acc:  0.828125
train loss:  0.3582633137702942
train gradient:  0.276568909723976
iteration : 9498
train acc:  0.859375
train loss:  0.28893935680389404
train gradient:  0.10921631892728163
iteration : 9499
train acc:  0.859375
train loss:  0.31840068101882935
train gradient:  0.16768123416234332
iteration : 9500
train acc:  0.875
train loss:  0.29619213938713074
train gradient:  0.1881852412394214
iteration : 9501
train acc:  0.8515625
train loss:  0.27852562069892883
train gradient:  0.15506160143352707
iteration : 9502
train acc:  0.859375
train loss:  0.3082391321659088
train gradient:  0.16310106661983131
iteration : 9503
train acc:  0.890625
train loss:  0.30106425285339355
train gradient:  0.12006098351425074
iteration : 9504
train acc:  0.8671875
train loss:  0.2981799840927124
train gradient:  0.10399307789881615
iteration : 9505
train acc:  0.8984375
train loss:  0.26083904504776
train gradient:  0.1400669822341077
iteration : 9506
train acc:  0.8671875
train loss:  0.30181944370269775
train gradient:  0.14667495798183194
iteration : 9507
train acc:  0.8515625
train loss:  0.3157844543457031
train gradient:  0.21131282284632713
iteration : 9508
train acc:  0.859375
train loss:  0.33658453822135925
train gradient:  0.13444817017886274
iteration : 9509
train acc:  0.8359375
train loss:  0.3839307427406311
train gradient:  0.22304664721219314
iteration : 9510
train acc:  0.875
train loss:  0.2762646973133087
train gradient:  0.14165381937009963
iteration : 9511
train acc:  0.8828125
train loss:  0.3484365940093994
train gradient:  0.15738118008238283
iteration : 9512
train acc:  0.8984375
train loss:  0.25955796241760254
train gradient:  0.1592520241672907
iteration : 9513
train acc:  0.875
train loss:  0.29388150572776794
train gradient:  0.1601408014806101
iteration : 9514
train acc:  0.8828125
train loss:  0.3064401149749756
train gradient:  0.14288883926764842
iteration : 9515
train acc:  0.9296875
train loss:  0.20915386080741882
train gradient:  0.08773599186491848
iteration : 9516
train acc:  0.84375
train loss:  0.31654560565948486
train gradient:  0.1614453790564799
iteration : 9517
train acc:  0.84375
train loss:  0.3093287944793701
train gradient:  0.1930026776988331
iteration : 9518
train acc:  0.859375
train loss:  0.31866586208343506
train gradient:  0.11416343044577107
iteration : 9519
train acc:  0.859375
train loss:  0.35041141510009766
train gradient:  0.2989487564292413
iteration : 9520
train acc:  0.890625
train loss:  0.2683846354484558
train gradient:  0.11735691524548214
iteration : 9521
train acc:  0.890625
train loss:  0.3699975311756134
train gradient:  0.24651717428048875
iteration : 9522
train acc:  0.796875
train loss:  0.4333653450012207
train gradient:  0.2282926069533411
iteration : 9523
train acc:  0.8203125
train loss:  0.35024309158325195
train gradient:  0.18054104853189207
iteration : 9524
train acc:  0.8046875
train loss:  0.38239482045173645
train gradient:  0.17912453903466868
iteration : 9525
train acc:  0.8671875
train loss:  0.33624178171157837
train gradient:  0.13839467670825772
iteration : 9526
train acc:  0.84375
train loss:  0.3750550448894501
train gradient:  0.20187862863065292
iteration : 9527
train acc:  0.8125
train loss:  0.4355431795120239
train gradient:  0.26270354202644863
iteration : 9528
train acc:  0.8046875
train loss:  0.43074944615364075
train gradient:  0.3202734855411711
iteration : 9529
train acc:  0.84375
train loss:  0.34036991000175476
train gradient:  0.20457534172552627
iteration : 9530
train acc:  0.875
train loss:  0.2695138454437256
train gradient:  0.14669831999784905
iteration : 9531
train acc:  0.8671875
train loss:  0.321075975894928
train gradient:  0.14460279093518566
iteration : 9532
train acc:  0.8671875
train loss:  0.27611371874809265
train gradient:  0.09456304133202205
iteration : 9533
train acc:  0.859375
train loss:  0.3005751967430115
train gradient:  0.12615975876605368
iteration : 9534
train acc:  0.8125
train loss:  0.4282362461090088
train gradient:  0.32301830113271696
iteration : 9535
train acc:  0.8671875
train loss:  0.3071581721305847
train gradient:  0.15738451260174913
iteration : 9536
train acc:  0.828125
train loss:  0.385891854763031
train gradient:  0.23578381531348613
iteration : 9537
train acc:  0.90625
train loss:  0.2609136700630188
train gradient:  0.09772991090035132
iteration : 9538
train acc:  0.859375
train loss:  0.2968742847442627
train gradient:  0.15878481868525496
iteration : 9539
train acc:  0.8359375
train loss:  0.3198806643486023
train gradient:  0.13750914692026114
iteration : 9540
train acc:  0.859375
train loss:  0.29378336668014526
train gradient:  0.13817179436051527
iteration : 9541
train acc:  0.875
train loss:  0.30835020542144775
train gradient:  0.16925867701198097
iteration : 9542
train acc:  0.90625
train loss:  0.2884756326675415
train gradient:  0.20714401484297723
iteration : 9543
train acc:  0.875
train loss:  0.33015239238739014
train gradient:  0.22631767585130264
iteration : 9544
train acc:  0.8671875
train loss:  0.27625423669815063
train gradient:  0.12859970721090286
iteration : 9545
train acc:  0.8671875
train loss:  0.26491454243659973
train gradient:  0.1340818778386299
iteration : 9546
train acc:  0.7890625
train loss:  0.4328034818172455
train gradient:  0.20390566066131022
iteration : 9547
train acc:  0.8203125
train loss:  0.4562469720840454
train gradient:  0.2648022789249956
iteration : 9548
train acc:  0.8203125
train loss:  0.34830236434936523
train gradient:  0.25109122021087965
iteration : 9549
train acc:  0.8515625
train loss:  0.31587785482406616
train gradient:  0.16381562345495493
iteration : 9550
train acc:  0.8828125
train loss:  0.29548048973083496
train gradient:  0.16913982529671776
iteration : 9551
train acc:  0.8125
train loss:  0.4535381495952606
train gradient:  0.26809247799105135
iteration : 9552
train acc:  0.859375
train loss:  0.29109036922454834
train gradient:  0.132597113911195
iteration : 9553
train acc:  0.8515625
train loss:  0.29768139123916626
train gradient:  0.1963134875488133
iteration : 9554
train acc:  0.84375
train loss:  0.35177451372146606
train gradient:  0.15385062200727428
iteration : 9555
train acc:  0.8515625
train loss:  0.34146833419799805
train gradient:  0.3288635609851941
iteration : 9556
train acc:  0.8046875
train loss:  0.40043556690216064
train gradient:  0.2586685556802573
iteration : 9557
train acc:  0.9140625
train loss:  0.2437305748462677
train gradient:  0.13581994287199223
iteration : 9558
train acc:  0.8671875
train loss:  0.2907167673110962
train gradient:  0.26265570237820157
iteration : 9559
train acc:  0.8671875
train loss:  0.31961601972579956
train gradient:  0.13663701830277683
iteration : 9560
train acc:  0.828125
train loss:  0.37801191210746765
train gradient:  0.16088945716508224
iteration : 9561
train acc:  0.8203125
train loss:  0.316650390625
train gradient:  0.1720204394307509
iteration : 9562
train acc:  0.8515625
train loss:  0.3464658260345459
train gradient:  0.18907829777043184
iteration : 9563
train acc:  0.828125
train loss:  0.3590041995048523
train gradient:  0.18724353032715957
iteration : 9564
train acc:  0.84375
train loss:  0.33023184537887573
train gradient:  0.1757544840904567
iteration : 9565
train acc:  0.8671875
train loss:  0.2922874391078949
train gradient:  0.1830652066256169
iteration : 9566
train acc:  0.8515625
train loss:  0.3117614984512329
train gradient:  0.17701022882719283
iteration : 9567
train acc:  0.8984375
train loss:  0.32835713028907776
train gradient:  0.1976840733005366
iteration : 9568
train acc:  0.875
train loss:  0.30057215690612793
train gradient:  0.1382598672589918
iteration : 9569
train acc:  0.84375
train loss:  0.3630535304546356
train gradient:  0.1913050907171756
iteration : 9570
train acc:  0.90625
train loss:  0.22880500555038452
train gradient:  0.09996559036632749
iteration : 9571
train acc:  0.828125
train loss:  0.3568587899208069
train gradient:  0.1993889305005591
iteration : 9572
train acc:  0.859375
train loss:  0.3205340504646301
train gradient:  0.16229491745155755
iteration : 9573
train acc:  0.84375
train loss:  0.2994557321071625
train gradient:  0.2914347345268974
iteration : 9574
train acc:  0.875
train loss:  0.3161810338497162
train gradient:  0.16869701889914296
iteration : 9575
train acc:  0.8515625
train loss:  0.30209624767303467
train gradient:  0.13735037552568827
iteration : 9576
train acc:  0.890625
train loss:  0.2955944538116455
train gradient:  0.18251216392438846
iteration : 9577
train acc:  0.828125
train loss:  0.3445378839969635
train gradient:  0.12935034214494462
iteration : 9578
train acc:  0.8359375
train loss:  0.36928001046180725
train gradient:  0.20942989390536654
iteration : 9579
train acc:  0.75
train loss:  0.5283908247947693
train gradient:  0.3020012565348467
iteration : 9580
train acc:  0.8515625
train loss:  0.3520048260688782
train gradient:  0.19540161054904356
iteration : 9581
train acc:  0.859375
train loss:  0.3114188313484192
train gradient:  0.1540760197613759
iteration : 9582
train acc:  0.8828125
train loss:  0.24938631057739258
train gradient:  0.13450117425531657
iteration : 9583
train acc:  0.84375
train loss:  0.3137857913970947
train gradient:  0.12475071196444333
iteration : 9584
train acc:  0.8359375
train loss:  0.3702414035797119
train gradient:  0.23207727224819452
iteration : 9585
train acc:  0.8203125
train loss:  0.36419951915740967
train gradient:  0.1813994463245342
iteration : 9586
train acc:  0.8828125
train loss:  0.26945504546165466
train gradient:  0.12352909956031548
iteration : 9587
train acc:  0.8515625
train loss:  0.364463746547699
train gradient:  0.7172941874152807
iteration : 9588
train acc:  0.8203125
train loss:  0.45806625485420227
train gradient:  0.29130956629353033
iteration : 9589
train acc:  0.9296875
train loss:  0.220441535115242
train gradient:  0.11684423868393619
iteration : 9590
train acc:  0.8984375
train loss:  0.28015998005867004
train gradient:  0.17121141634552742
iteration : 9591
train acc:  0.84375
train loss:  0.33051639795303345
train gradient:  0.2665922852142115
iteration : 9592
train acc:  0.7578125
train loss:  0.49864572286605835
train gradient:  0.4337351929085628
iteration : 9593
train acc:  0.875
train loss:  0.36943304538726807
train gradient:  0.3367770053427452
iteration : 9594
train acc:  0.8203125
train loss:  0.3466712534427643
train gradient:  0.20163323274776146
iteration : 9595
train acc:  0.796875
train loss:  0.3995128273963928
train gradient:  0.3085979617405532
iteration : 9596
train acc:  0.8984375
train loss:  0.25860750675201416
train gradient:  0.12469646585144323
iteration : 9597
train acc:  0.8984375
train loss:  0.3201277256011963
train gradient:  0.12395566527037546
iteration : 9598
train acc:  0.875
train loss:  0.26310995221138
train gradient:  0.15324356792717175
iteration : 9599
train acc:  0.8359375
train loss:  0.3432717025279999
train gradient:  0.1341499253026336
iteration : 9600
train acc:  0.8671875
train loss:  0.3367787301540375
train gradient:  0.1546239948893535
iteration : 9601
train acc:  0.8671875
train loss:  0.288631796836853
train gradient:  0.12393586943071863
iteration : 9602
train acc:  0.859375
train loss:  0.3150216341018677
train gradient:  0.14349125016583927
iteration : 9603
train acc:  0.8359375
train loss:  0.31031256914138794
train gradient:  0.1718809449619045
iteration : 9604
train acc:  0.8203125
train loss:  0.3620525896549225
train gradient:  0.3030310890122071
iteration : 9605
train acc:  0.8828125
train loss:  0.27536869049072266
train gradient:  0.14376783076864919
iteration : 9606
train acc:  0.859375
train loss:  0.2859298586845398
train gradient:  0.16282498684408833
iteration : 9607
train acc:  0.8359375
train loss:  0.34735235571861267
train gradient:  0.845991415422156
iteration : 9608
train acc:  0.84375
train loss:  0.3210676312446594
train gradient:  0.17059334051492603
iteration : 9609
train acc:  0.8359375
train loss:  0.41670703887939453
train gradient:  0.2918741734892149
iteration : 9610
train acc:  0.859375
train loss:  0.3122314214706421
train gradient:  0.2498485125976002
iteration : 9611
train acc:  0.875
train loss:  0.33099210262298584
train gradient:  0.14776827229855172
iteration : 9612
train acc:  0.78125
train loss:  0.4105513095855713
train gradient:  0.31876007939002804
iteration : 9613
train acc:  0.84375
train loss:  0.37419000267982483
train gradient:  0.2207352054286095
iteration : 9614
train acc:  0.859375
train loss:  0.3102923631668091
train gradient:  0.17004820201642168
iteration : 9615
train acc:  0.875
train loss:  0.3323410153388977
train gradient:  0.13635163125137706
iteration : 9616
train acc:  0.9140625
train loss:  0.2416251301765442
train gradient:  0.11844568398301676
iteration : 9617
train acc:  0.84375
train loss:  0.33823537826538086
train gradient:  0.1977461598996516
iteration : 9618
train acc:  0.84375
train loss:  0.3215959072113037
train gradient:  0.16116901432803304
iteration : 9619
train acc:  0.84375
train loss:  0.34724488854408264
train gradient:  0.1344206866169586
iteration : 9620
train acc:  0.90625
train loss:  0.2800382673740387
train gradient:  0.12834944907599777
iteration : 9621
train acc:  0.8046875
train loss:  0.38850557804107666
train gradient:  0.2437950675056239
iteration : 9622
train acc:  0.875
train loss:  0.29429757595062256
train gradient:  0.14391271624619656
iteration : 9623
train acc:  0.859375
train loss:  0.3977718949317932
train gradient:  0.2679948927517608
iteration : 9624
train acc:  0.8515625
train loss:  0.3821040987968445
train gradient:  0.19537121951213368
iteration : 9625
train acc:  0.8515625
train loss:  0.3284170627593994
train gradient:  0.12876530628172989
iteration : 9626
train acc:  0.8125
train loss:  0.4189324378967285
train gradient:  0.249967320297392
iteration : 9627
train acc:  0.8984375
train loss:  0.2708234488964081
train gradient:  0.11939774816495234
iteration : 9628
train acc:  0.859375
train loss:  0.33897149562835693
train gradient:  0.15565289932378362
iteration : 9629
train acc:  0.8515625
train loss:  0.31609219312667847
train gradient:  0.11698167809950279
iteration : 9630
train acc:  0.875
train loss:  0.32990962266921997
train gradient:  0.15358221245826567
iteration : 9631
train acc:  0.84375
train loss:  0.3823753297328949
train gradient:  0.23292679652308915
iteration : 9632
train acc:  0.90625
train loss:  0.2757871150970459
train gradient:  0.13222211822719146
iteration : 9633
train acc:  0.8359375
train loss:  0.3039517402648926
train gradient:  0.1262798471746494
iteration : 9634
train acc:  0.8828125
train loss:  0.2503581643104553
train gradient:  0.10360132601589245
iteration : 9635
train acc:  0.859375
train loss:  0.2787778675556183
train gradient:  0.1276098581044287
iteration : 9636
train acc:  0.8359375
train loss:  0.3829893171787262
train gradient:  0.20517915753579713
iteration : 9637
train acc:  0.859375
train loss:  0.30402839183807373
train gradient:  0.1910436071617627
iteration : 9638
train acc:  0.8203125
train loss:  0.39194077253341675
train gradient:  0.1753445209113967
iteration : 9639
train acc:  0.875
train loss:  0.2730218172073364
train gradient:  0.1757640640498504
iteration : 9640
train acc:  0.8984375
train loss:  0.23255383968353271
train gradient:  0.10669466024459666
iteration : 9641
train acc:  0.84375
train loss:  0.35278332233428955
train gradient:  0.17943362408331148
iteration : 9642
train acc:  0.9140625
train loss:  0.28999534249305725
train gradient:  0.20005798552511014
iteration : 9643
train acc:  0.859375
train loss:  0.322613388299942
train gradient:  0.12874628001661897
iteration : 9644
train acc:  0.8515625
train loss:  0.36461785435676575
train gradient:  0.14499015224286876
iteration : 9645
train acc:  0.84375
train loss:  0.3921276926994324
train gradient:  0.22317271305981115
iteration : 9646
train acc:  0.8984375
train loss:  0.26585495471954346
train gradient:  0.11796189391210483
iteration : 9647
train acc:  0.875
train loss:  0.3107529580593109
train gradient:  0.17635505461980439
iteration : 9648
train acc:  0.8046875
train loss:  0.4495953321456909
train gradient:  0.32612137816143655
iteration : 9649
train acc:  0.8828125
train loss:  0.278322696685791
train gradient:  0.16253286488270077
iteration : 9650
train acc:  0.8515625
train loss:  0.3289164900779724
train gradient:  0.1573514765742143
iteration : 9651
train acc:  0.8984375
train loss:  0.30654144287109375
train gradient:  0.13093395145923012
iteration : 9652
train acc:  0.796875
train loss:  0.4356498420238495
train gradient:  0.3334552944976367
iteration : 9653
train acc:  0.859375
train loss:  0.3158622086048126
train gradient:  0.16419630012201775
iteration : 9654
train acc:  0.8984375
train loss:  0.2924175560474396
train gradient:  0.1545237093246294
iteration : 9655
train acc:  0.84375
train loss:  0.3504241108894348
train gradient:  0.20099806175369536
iteration : 9656
train acc:  0.828125
train loss:  0.38417860865592957
train gradient:  0.20887931649983893
iteration : 9657
train acc:  0.8046875
train loss:  0.4625215530395508
train gradient:  0.22581034831243144
iteration : 9658
train acc:  0.8203125
train loss:  0.40613695979118347
train gradient:  0.2025975607372436
iteration : 9659
train acc:  0.8671875
train loss:  0.3399852514266968
train gradient:  0.1465344647589148
iteration : 9660
train acc:  0.8359375
train loss:  0.41832298040390015
train gradient:  0.24584717635442765
iteration : 9661
train acc:  0.875
train loss:  0.2548592984676361
train gradient:  0.1327355736238526
iteration : 9662
train acc:  0.875
train loss:  0.30848535895347595
train gradient:  0.24690949347992755
iteration : 9663
train acc:  0.84375
train loss:  0.3630635142326355
train gradient:  0.23951772544565794
iteration : 9664
train acc:  0.8203125
train loss:  0.36597079038619995
train gradient:  0.22328277649652972
iteration : 9665
train acc:  0.8671875
train loss:  0.3161545693874359
train gradient:  0.13859119245033802
iteration : 9666
train acc:  0.84375
train loss:  0.3500168025493622
train gradient:  0.28291511773265277
iteration : 9667
train acc:  0.8828125
train loss:  0.2854982912540436
train gradient:  0.1038638340932225
iteration : 9668
train acc:  0.859375
train loss:  0.3049817681312561
train gradient:  0.19911259661386702
iteration : 9669
train acc:  0.875
train loss:  0.29646721482276917
train gradient:  0.1431882396056417
iteration : 9670
train acc:  0.875
train loss:  0.3754833936691284
train gradient:  0.3025208543181769
iteration : 9671
train acc:  0.890625
train loss:  0.29723846912384033
train gradient:  0.15762300811751895
iteration : 9672
train acc:  0.859375
train loss:  0.3557576537132263
train gradient:  0.27355088441549413
iteration : 9673
train acc:  0.875
train loss:  0.31274616718292236
train gradient:  0.1578013271468025
iteration : 9674
train acc:  0.890625
train loss:  0.35852423310279846
train gradient:  0.1840190111164643
iteration : 9675
train acc:  0.8671875
train loss:  0.29977133870124817
train gradient:  0.13753716655411827
iteration : 9676
train acc:  0.8671875
train loss:  0.34939029812812805
train gradient:  0.16947157656746112
iteration : 9677
train acc:  0.8125
train loss:  0.31823113560676575
train gradient:  0.3454800435097201
iteration : 9678
train acc:  0.8828125
train loss:  0.28870540857315063
train gradient:  0.15957922079325793
iteration : 9679
train acc:  0.84375
train loss:  0.300818532705307
train gradient:  0.10844971661500237
iteration : 9680
train acc:  0.890625
train loss:  0.2868940234184265
train gradient:  0.13588110906284276
iteration : 9681
train acc:  0.8515625
train loss:  0.34963375329971313
train gradient:  0.20339500589954385
iteration : 9682
train acc:  0.8828125
train loss:  0.2862508296966553
train gradient:  0.1573937577623989
iteration : 9683
train acc:  0.8671875
train loss:  0.3212631940841675
train gradient:  0.17026920295036693
iteration : 9684
train acc:  0.859375
train loss:  0.33081454038619995
train gradient:  0.20248712753756254
iteration : 9685
train acc:  0.875
train loss:  0.28252995014190674
train gradient:  0.13690976055866722
iteration : 9686
train acc:  0.859375
train loss:  0.27258872985839844
train gradient:  0.16950898642634493
iteration : 9687
train acc:  0.8671875
train loss:  0.3173152804374695
train gradient:  0.1658300757146522
iteration : 9688
train acc:  0.8203125
train loss:  0.3349086046218872
train gradient:  0.15551152712811267
iteration : 9689
train acc:  0.8203125
train loss:  0.32082828879356384
train gradient:  0.18161940080606226
iteration : 9690
train acc:  0.8515625
train loss:  0.31912147998809814
train gradient:  0.1647227553740102
iteration : 9691
train acc:  0.8125
train loss:  0.34939056634902954
train gradient:  0.166420349872196
iteration : 9692
train acc:  0.921875
train loss:  0.30743208527565
train gradient:  0.2134960226914971
iteration : 9693
train acc:  0.875
train loss:  0.303588330745697
train gradient:  0.12367094853216114
iteration : 9694
train acc:  0.9140625
train loss:  0.27357298135757446
train gradient:  0.16287775746846683
iteration : 9695
train acc:  0.8203125
train loss:  0.43157997727394104
train gradient:  0.4158615214228042
iteration : 9696
train acc:  0.8125
train loss:  0.4154566526412964
train gradient:  0.2868705767628518
iteration : 9697
train acc:  0.8828125
train loss:  0.2679198980331421
train gradient:  0.09822868712328149
iteration : 9698
train acc:  0.828125
train loss:  0.38114047050476074
train gradient:  0.29468237740058717
iteration : 9699
train acc:  0.8359375
train loss:  0.31242749094963074
train gradient:  0.15270533234199085
iteration : 9700
train acc:  0.8828125
train loss:  0.28349512815475464
train gradient:  0.11210568566571531
iteration : 9701
train acc:  0.84375
train loss:  0.3390739858150482
train gradient:  0.17583192475296638
iteration : 9702
train acc:  0.84375
train loss:  0.3516230583190918
train gradient:  0.1793453252857706
iteration : 9703
train acc:  0.8671875
train loss:  0.31125974655151367
train gradient:  0.1283540234397838
iteration : 9704
train acc:  0.8046875
train loss:  0.3897687792778015
train gradient:  0.1915775482674495
iteration : 9705
train acc:  0.875
train loss:  0.2613549828529358
train gradient:  0.10673695146271066
iteration : 9706
train acc:  0.8671875
train loss:  0.30796271562576294
train gradient:  0.15503318223653123
iteration : 9707
train acc:  0.859375
train loss:  0.319436252117157
train gradient:  0.15574754107800765
iteration : 9708
train acc:  0.921875
train loss:  0.22030800580978394
train gradient:  0.0848160933122347
iteration : 9709
train acc:  0.8359375
train loss:  0.2672181725502014
train gradient:  0.13245317371701637
iteration : 9710
train acc:  0.84375
train loss:  0.32437941431999207
train gradient:  0.10500609743332687
iteration : 9711
train acc:  0.8515625
train loss:  0.33022958040237427
train gradient:  0.1635447604103347
iteration : 9712
train acc:  0.8125
train loss:  0.3942473232746124
train gradient:  0.2210468717188453
iteration : 9713
train acc:  0.8828125
train loss:  0.3040781021118164
train gradient:  0.13458155428035762
iteration : 9714
train acc:  0.828125
train loss:  0.33808067440986633
train gradient:  0.14505769216953318
iteration : 9715
train acc:  0.8515625
train loss:  0.34155431389808655
train gradient:  0.15958735075246674
iteration : 9716
train acc:  0.8125
train loss:  0.40230345726013184
train gradient:  0.29102717394975874
iteration : 9717
train acc:  0.8125
train loss:  0.4450935125350952
train gradient:  0.26163109368564275
iteration : 9718
train acc:  0.828125
train loss:  0.3888213634490967
train gradient:  0.24469005509755948
iteration : 9719
train acc:  0.8515625
train loss:  0.31979650259017944
train gradient:  0.15032721314253006
iteration : 9720
train acc:  0.8515625
train loss:  0.31963324546813965
train gradient:  0.16883371082020812
iteration : 9721
train acc:  0.8515625
train loss:  0.31962376832962036
train gradient:  0.1239169091213846
iteration : 9722
train acc:  0.8515625
train loss:  0.31262463331222534
train gradient:  0.1209036474865054
iteration : 9723
train acc:  0.859375
train loss:  0.3061429560184479
train gradient:  0.1281071675578252
iteration : 9724
train acc:  0.875
train loss:  0.2857622802257538
train gradient:  0.10027823349030228
iteration : 9725
train acc:  0.9140625
train loss:  0.2997651696205139
train gradient:  0.1516026432831375
iteration : 9726
train acc:  0.8671875
train loss:  0.34673789143562317
train gradient:  0.15898860249575686
iteration : 9727
train acc:  0.9140625
train loss:  0.2689242660999298
train gradient:  0.11198124207206218
iteration : 9728
train acc:  0.8203125
train loss:  0.36551669239997864
train gradient:  0.18080320802179917
iteration : 9729
train acc:  0.875
train loss:  0.26661497354507446
train gradient:  0.08384397499505973
iteration : 9730
train acc:  0.8984375
train loss:  0.24168843030929565
train gradient:  0.0871211214502153
iteration : 9731
train acc:  0.8515625
train loss:  0.3094117045402527
train gradient:  0.17976155182669973
iteration : 9732
train acc:  0.84375
train loss:  0.3682957887649536
train gradient:  0.2983415359359335
iteration : 9733
train acc:  0.8515625
train loss:  0.3504890203475952
train gradient:  0.1603020454309838
iteration : 9734
train acc:  0.890625
train loss:  0.2898518443107605
train gradient:  0.1455416634623785
iteration : 9735
train acc:  0.8515625
train loss:  0.3958459496498108
train gradient:  0.18405672386017458
iteration : 9736
train acc:  0.8515625
train loss:  0.3867679536342621
train gradient:  0.27699520618010376
iteration : 9737
train acc:  0.8203125
train loss:  0.43602457642555237
train gradient:  0.2470344987690055
iteration : 9738
train acc:  0.84375
train loss:  0.36187076568603516
train gradient:  0.24554886479697013
iteration : 9739
train acc:  0.8984375
train loss:  0.25388848781585693
train gradient:  0.07102018429331548
iteration : 9740
train acc:  0.8671875
train loss:  0.3014655113220215
train gradient:  0.1263456498994628
iteration : 9741
train acc:  0.8828125
train loss:  0.3041447401046753
train gradient:  0.1619563822449392
iteration : 9742
train acc:  0.9375
train loss:  0.20551684498786926
train gradient:  0.0987073845725465
iteration : 9743
train acc:  0.8359375
train loss:  0.3064178228378296
train gradient:  0.11328750229207284
iteration : 9744
train acc:  0.8359375
train loss:  0.30402815341949463
train gradient:  0.09829892073191904
iteration : 9745
train acc:  0.8125
train loss:  0.3897535502910614
train gradient:  0.1865126175067368
iteration : 9746
train acc:  0.84375
train loss:  0.33045777678489685
train gradient:  0.22962549882290068
iteration : 9747
train acc:  0.8515625
train loss:  0.37005576491355896
train gradient:  0.1736040694494935
iteration : 9748
train acc:  0.8515625
train loss:  0.3665497303009033
train gradient:  0.18994985958842436
iteration : 9749
train acc:  0.8828125
train loss:  0.31159934401512146
train gradient:  0.1449423492520076
iteration : 9750
train acc:  0.875
train loss:  0.29058724641799927
train gradient:  0.10255370485821064
iteration : 9751
train acc:  0.8203125
train loss:  0.3449118733406067
train gradient:  0.1656943049937486
iteration : 9752
train acc:  0.8125
train loss:  0.4264829158782959
train gradient:  0.304096686125858
iteration : 9753
train acc:  0.796875
train loss:  0.37744462490081787
train gradient:  0.18706021220057123
iteration : 9754
train acc:  0.84375
train loss:  0.3879333734512329
train gradient:  0.1946679954289548
iteration : 9755
train acc:  0.8203125
train loss:  0.3784371614456177
train gradient:  0.2149335481073894
iteration : 9756
train acc:  0.8671875
train loss:  0.277948796749115
train gradient:  0.10734225963185985
iteration : 9757
train acc:  0.90625
train loss:  0.2550714612007141
train gradient:  0.12032568323125474
iteration : 9758
train acc:  0.875
train loss:  0.28881335258483887
train gradient:  0.1743263033505778
iteration : 9759
train acc:  0.84375
train loss:  0.32211536169052124
train gradient:  0.16343147993253834
iteration : 9760
train acc:  0.8125
train loss:  0.3934193253517151
train gradient:  0.21252966897016842
iteration : 9761
train acc:  0.84375
train loss:  0.30890804529190063
train gradient:  0.11218899875174064
iteration : 9762
train acc:  0.8359375
train loss:  0.34858644008636475
train gradient:  0.2200418511790754
iteration : 9763
train acc:  0.8515625
train loss:  0.3628542423248291
train gradient:  0.15601547203492994
iteration : 9764
train acc:  0.8828125
train loss:  0.3020450472831726
train gradient:  0.12141487689494351
iteration : 9765
train acc:  0.90625
train loss:  0.28784096240997314
train gradient:  0.15693911651824755
iteration : 9766
train acc:  0.9375
train loss:  0.24450764060020447
train gradient:  0.18723416480395877
iteration : 9767
train acc:  0.8046875
train loss:  0.4099932312965393
train gradient:  0.30060062061872816
iteration : 9768
train acc:  0.8046875
train loss:  0.43914008140563965
train gradient:  0.22627246634960835
iteration : 9769
train acc:  0.8515625
train loss:  0.3420480787754059
train gradient:  0.22688540143824804
iteration : 9770
train acc:  0.875
train loss:  0.2708693742752075
train gradient:  0.14681665992201343
iteration : 9771
train acc:  0.84375
train loss:  0.34296703338623047
train gradient:  0.22042824897903135
iteration : 9772
train acc:  0.859375
train loss:  0.3072358965873718
train gradient:  0.142722609794239
iteration : 9773
train acc:  0.875
train loss:  0.29855093359947205
train gradient:  0.12332925431026819
iteration : 9774
train acc:  0.8671875
train loss:  0.31078070402145386
train gradient:  0.16211261075833502
iteration : 9775
train acc:  0.796875
train loss:  0.3991428017616272
train gradient:  0.29045776931765244
iteration : 9776
train acc:  0.859375
train loss:  0.3125850558280945
train gradient:  0.15803121676428222
iteration : 9777
train acc:  0.875
train loss:  0.2775147557258606
train gradient:  0.11598288105164482
iteration : 9778
train acc:  0.84375
train loss:  0.37712109088897705
train gradient:  0.2291851924606218
iteration : 9779
train acc:  0.859375
train loss:  0.38257449865341187
train gradient:  0.4507575978508296
iteration : 9780
train acc:  0.890625
train loss:  0.2792743444442749
train gradient:  0.10904934549916799
iteration : 9781
train acc:  0.828125
train loss:  0.3444601893424988
train gradient:  0.19137208783917475
iteration : 9782
train acc:  0.7890625
train loss:  0.5163318514823914
train gradient:  0.2963983955955613
iteration : 9783
train acc:  0.890625
train loss:  0.35717806220054626
train gradient:  0.25169295042138007
iteration : 9784
train acc:  0.8828125
train loss:  0.30109453201293945
train gradient:  0.11766116211196073
iteration : 9785
train acc:  0.8359375
train loss:  0.36506718397140503
train gradient:  0.21661323917920006
iteration : 9786
train acc:  0.890625
train loss:  0.32790902256965637
train gradient:  0.15450261417757316
iteration : 9787
train acc:  0.8671875
train loss:  0.36027657985687256
train gradient:  0.18681940851870932
iteration : 9788
train acc:  0.8984375
train loss:  0.2618202567100525
train gradient:  0.14691229643604625
iteration : 9789
train acc:  0.9140625
train loss:  0.3123544156551361
train gradient:  0.20237792587278308
iteration : 9790
train acc:  0.875
train loss:  0.33757442235946655
train gradient:  0.24318169093689046
iteration : 9791
train acc:  0.8203125
train loss:  0.3768558204174042
train gradient:  0.3983707281957767
iteration : 9792
train acc:  0.8828125
train loss:  0.2559517025947571
train gradient:  0.10365919547639646
iteration : 9793
train acc:  0.8359375
train loss:  0.385478138923645
train gradient:  0.20249812592669547
iteration : 9794
train acc:  0.890625
train loss:  0.28140169382095337
train gradient:  0.15558107764286222
iteration : 9795
train acc:  0.90625
train loss:  0.2447284609079361
train gradient:  0.09145297085382631
iteration : 9796
train acc:  0.890625
train loss:  0.3411417007446289
train gradient:  0.20520268103140465
iteration : 9797
train acc:  0.796875
train loss:  0.3258568346500397
train gradient:  0.11547660945251409
iteration : 9798
train acc:  0.859375
train loss:  0.3175647556781769
train gradient:  0.1585847023813402
iteration : 9799
train acc:  0.8359375
train loss:  0.3364245295524597
train gradient:  0.13707718450025486
iteration : 9800
train acc:  0.8828125
train loss:  0.2850232422351837
train gradient:  0.12821806238901737
iteration : 9801
train acc:  0.8125
train loss:  0.36990848183631897
train gradient:  0.2647044794838212
iteration : 9802
train acc:  0.8984375
train loss:  0.2678168714046478
train gradient:  0.1721538520849406
iteration : 9803
train acc:  0.875
train loss:  0.2922191619873047
train gradient:  1.3099116719772004
iteration : 9804
train acc:  0.8671875
train loss:  0.3029516637325287
train gradient:  0.145718851568492
iteration : 9805
train acc:  0.8671875
train loss:  0.3200643062591553
train gradient:  0.12169177008301912
iteration : 9806
train acc:  0.8046875
train loss:  0.3738243579864502
train gradient:  0.15224559216359726
iteration : 9807
train acc:  0.8203125
train loss:  0.3462616205215454
train gradient:  0.1718582780854372
iteration : 9808
train acc:  0.8984375
train loss:  0.2668347954750061
train gradient:  0.09799624231288913
iteration : 9809
train acc:  0.796875
train loss:  0.415053129196167
train gradient:  0.23520595213425355
iteration : 9810
train acc:  0.8125
train loss:  0.3620182275772095
train gradient:  0.17946208408504283
iteration : 9811
train acc:  0.921875
train loss:  0.2788308262825012
train gradient:  0.12412958589368624
iteration : 9812
train acc:  0.8203125
train loss:  0.42908740043640137
train gradient:  0.2577134323580342
iteration : 9813
train acc:  0.8984375
train loss:  0.2644979953765869
train gradient:  0.10711564608837076
iteration : 9814
train acc:  0.8984375
train loss:  0.2977256774902344
train gradient:  0.10445275169139852
iteration : 9815
train acc:  0.8046875
train loss:  0.3940565586090088
train gradient:  0.17571074456975186
iteration : 9816
train acc:  0.8671875
train loss:  0.330051988363266
train gradient:  0.1445704730195747
iteration : 9817
train acc:  0.8125
train loss:  0.3767521381378174
train gradient:  0.25573382474893547
iteration : 9818
train acc:  0.8359375
train loss:  0.37656205892562866
train gradient:  0.21308755481420694
iteration : 9819
train acc:  0.8515625
train loss:  0.3124408721923828
train gradient:  0.18336029127768355
iteration : 9820
train acc:  0.84375
train loss:  0.34655529260635376
train gradient:  0.15530417730179666
iteration : 9821
train acc:  0.8515625
train loss:  0.3515954613685608
train gradient:  0.16409199018790266
iteration : 9822
train acc:  0.90625
train loss:  0.21349184215068817
train gradient:  0.08699345850945366
iteration : 9823
train acc:  0.8125
train loss:  0.3393537700176239
train gradient:  0.217842796719528
iteration : 9824
train acc:  0.875
train loss:  0.33373570442199707
train gradient:  0.13544464162042297
iteration : 9825
train acc:  0.7890625
train loss:  0.3818548917770386
train gradient:  0.25801670499248286
iteration : 9826
train acc:  0.8125
train loss:  0.3954124450683594
train gradient:  0.2311134031538004
iteration : 9827
train acc:  0.8203125
train loss:  0.33886978030204773
train gradient:  0.1579634072569076
iteration : 9828
train acc:  0.8515625
train loss:  0.28507792949676514
train gradient:  0.1786700030918188
iteration : 9829
train acc:  0.875
train loss:  0.29641398787498474
train gradient:  0.13870530359542285
iteration : 9830
train acc:  0.9296875
train loss:  0.23264120519161224
train gradient:  0.10505996340327098
iteration : 9831
train acc:  0.875
train loss:  0.2938079237937927
train gradient:  0.17492678142003854
iteration : 9832
train acc:  0.859375
train loss:  0.28588297963142395
train gradient:  0.12999196722491935
iteration : 9833
train acc:  0.890625
train loss:  0.26570814847946167
train gradient:  0.15756365193444608
iteration : 9834
train acc:  0.8515625
train loss:  0.34790945053100586
train gradient:  0.16030435055476547
iteration : 9835
train acc:  0.90625
train loss:  0.2922991216182709
train gradient:  0.14656543421856233
iteration : 9836
train acc:  0.8515625
train loss:  0.3594862222671509
train gradient:  0.13283583886072586
iteration : 9837
train acc:  0.8671875
train loss:  0.31051182746887207
train gradient:  0.13316086325771936
iteration : 9838
train acc:  0.84375
train loss:  0.3456907272338867
train gradient:  0.23213825097019183
iteration : 9839
train acc:  0.875
train loss:  0.31408852338790894
train gradient:  0.13530397379347858
iteration : 9840
train acc:  0.8671875
train loss:  0.29361116886138916
train gradient:  0.13304189078575235
iteration : 9841
train acc:  0.8671875
train loss:  0.320017009973526
train gradient:  0.15629592081615812
iteration : 9842
train acc:  0.890625
train loss:  0.3445992171764374
train gradient:  0.21161038868631887
iteration : 9843
train acc:  0.8984375
train loss:  0.2963190972805023
train gradient:  0.09845887892899224
iteration : 9844
train acc:  0.8828125
train loss:  0.35245680809020996
train gradient:  0.19227825528301637
iteration : 9845
train acc:  0.796875
train loss:  0.39056259393692017
train gradient:  0.21810753071935773
iteration : 9846
train acc:  0.859375
train loss:  0.33482062816619873
train gradient:  0.11461050332267435
iteration : 9847
train acc:  0.8125
train loss:  0.382632851600647
train gradient:  0.30284576072174413
iteration : 9848
train acc:  0.8515625
train loss:  0.40826416015625
train gradient:  0.3904790667227672
iteration : 9849
train acc:  0.8828125
train loss:  0.2831234335899353
train gradient:  0.16075524957143592
iteration : 9850
train acc:  0.7890625
train loss:  0.4473870098590851
train gradient:  0.2523050149996874
iteration : 9851
train acc:  0.84375
train loss:  0.31981921195983887
train gradient:  0.1697182478060602
iteration : 9852
train acc:  0.8828125
train loss:  0.3174557685852051
train gradient:  0.1668460278966325
iteration : 9853
train acc:  0.8828125
train loss:  0.3555860221385956
train gradient:  0.21415705744690017
iteration : 9854
train acc:  0.8671875
train loss:  0.3211500644683838
train gradient:  0.13285881680222217
iteration : 9855
train acc:  0.875
train loss:  0.26754581928253174
train gradient:  0.1276464157186909
iteration : 9856
train acc:  0.84375
train loss:  0.28992632031440735
train gradient:  0.1904293853715035
iteration : 9857
train acc:  0.78125
train loss:  0.42637932300567627
train gradient:  0.293114272412819
iteration : 9858
train acc:  0.796875
train loss:  0.4250164031982422
train gradient:  0.2874655448922958
iteration : 9859
train acc:  0.8515625
train loss:  0.36703261733055115
train gradient:  0.272583644838536
iteration : 9860
train acc:  0.890625
train loss:  0.21725662052631378
train gradient:  0.08413364710577283
iteration : 9861
train acc:  0.8671875
train loss:  0.43872636556625366
train gradient:  0.27821397113833735
iteration : 9862
train acc:  0.828125
train loss:  0.3745149075984955
train gradient:  0.11945223127146308
iteration : 9863
train acc:  0.859375
train loss:  0.29558539390563965
train gradient:  0.1428197894703295
iteration : 9864
train acc:  0.8515625
train loss:  0.3623752295970917
train gradient:  0.14185488535145804
iteration : 9865
train acc:  0.8125
train loss:  0.3690476417541504
train gradient:  0.15875214551628647
iteration : 9866
train acc:  0.828125
train loss:  0.4303101599216461
train gradient:  0.24204692775159548
iteration : 9867
train acc:  0.8203125
train loss:  0.3530697226524353
train gradient:  0.1608193877773263
iteration : 9868
train acc:  0.8515625
train loss:  0.2952114939689636
train gradient:  0.13144826565837883
iteration : 9869
train acc:  0.8515625
train loss:  0.3328877389431
train gradient:  0.16791279095062667
iteration : 9870
train acc:  0.8515625
train loss:  0.34810084104537964
train gradient:  0.14332516027299952
iteration : 9871
train acc:  0.875
train loss:  0.31911715865135193
train gradient:  0.16450051473547694
iteration : 9872
train acc:  0.921875
train loss:  0.18909765779972076
train gradient:  0.08600864213597674
iteration : 9873
train acc:  0.8671875
train loss:  0.3333430290222168
train gradient:  0.18232080956156982
iteration : 9874
train acc:  0.859375
train loss:  0.3119407892227173
train gradient:  0.12410070692474502
iteration : 9875
train acc:  0.8359375
train loss:  0.3533172607421875
train gradient:  0.1919681628356114
iteration : 9876
train acc:  0.8359375
train loss:  0.33491379022598267
train gradient:  0.1583006495859297
iteration : 9877
train acc:  0.84375
train loss:  0.42542678117752075
train gradient:  0.28840125362642166
iteration : 9878
train acc:  0.890625
train loss:  0.2814052104949951
train gradient:  0.12920642003597743
iteration : 9879
train acc:  0.859375
train loss:  0.30894899368286133
train gradient:  0.15232890326677215
iteration : 9880
train acc:  0.84375
train loss:  0.346069872379303
train gradient:  0.16884332387375145
iteration : 9881
train acc:  0.875
train loss:  0.3227490186691284
train gradient:  0.12457780654226072
iteration : 9882
train acc:  0.828125
train loss:  0.39991629123687744
train gradient:  0.21464577957366623
iteration : 9883
train acc:  0.8125
train loss:  0.4091298282146454
train gradient:  0.23677325953836342
iteration : 9884
train acc:  0.84375
train loss:  0.2973133325576782
train gradient:  0.13340580434866783
iteration : 9885
train acc:  0.84375
train loss:  0.414961040019989
train gradient:  0.2851234886073308
iteration : 9886
train acc:  0.8203125
train loss:  0.35365375876426697
train gradient:  0.15413330281242255
iteration : 9887
train acc:  0.890625
train loss:  0.31424975395202637
train gradient:  0.1259169374590923
iteration : 9888
train acc:  0.8828125
train loss:  0.2520263195037842
train gradient:  0.10614298048399376
iteration : 9889
train acc:  0.8671875
train loss:  0.339944988489151
train gradient:  0.2117369361277847
iteration : 9890
train acc:  0.7890625
train loss:  0.42338818311691284
train gradient:  0.23901732941220213
iteration : 9891
train acc:  0.84375
train loss:  0.341372549533844
train gradient:  0.14075969068811772
iteration : 9892
train acc:  0.8046875
train loss:  0.3949590027332306
train gradient:  0.17591282165123068
iteration : 9893
train acc:  0.890625
train loss:  0.2719672918319702
train gradient:  0.15001307568507954
iteration : 9894
train acc:  0.875
train loss:  0.31990307569503784
train gradient:  0.34944754119440297
iteration : 9895
train acc:  0.84375
train loss:  0.34043043851852417
train gradient:  0.186148134079615
iteration : 9896
train acc:  0.84375
train loss:  0.4077734649181366
train gradient:  0.24721000433544757
iteration : 9897
train acc:  0.7734375
train loss:  0.44864654541015625
train gradient:  0.5087818303294631
iteration : 9898
train acc:  0.8515625
train loss:  0.3052491843700409
train gradient:  0.14025332524859307
iteration : 9899
train acc:  0.84375
train loss:  0.35029739141464233
train gradient:  0.21065156781883004
iteration : 9900
train acc:  0.859375
train loss:  0.3028813600540161
train gradient:  0.12933930092258686
iteration : 9901
train acc:  0.84375
train loss:  0.35402101278305054
train gradient:  0.18142557073418875
iteration : 9902
train acc:  0.8984375
train loss:  0.27813488245010376
train gradient:  0.16817994571604455
iteration : 9903
train acc:  0.859375
train loss:  0.33924219012260437
train gradient:  0.10977002037063964
iteration : 9904
train acc:  0.90625
train loss:  0.28480085730552673
train gradient:  0.17598823936723756
iteration : 9905
train acc:  0.875
train loss:  0.2793118953704834
train gradient:  0.12707698786136168
iteration : 9906
train acc:  0.875
train loss:  0.33214396238327026
train gradient:  0.18366113092234027
iteration : 9907
train acc:  0.8203125
train loss:  0.45156407356262207
train gradient:  0.2603213048838095
iteration : 9908
train acc:  0.8515625
train loss:  0.3154171109199524
train gradient:  0.13161289629658343
iteration : 9909
train acc:  0.828125
train loss:  0.3948569595813751
train gradient:  0.27010741923835563
iteration : 9910
train acc:  0.8671875
train loss:  0.2990923523902893
train gradient:  0.1361042809566917
iteration : 9911
train acc:  0.8515625
train loss:  0.3814592957496643
train gradient:  0.18341417459936368
iteration : 9912
train acc:  0.84375
train loss:  0.2995980381965637
train gradient:  0.14012206023788887
iteration : 9913
train acc:  0.875
train loss:  0.2935130000114441
train gradient:  0.1498725434837529
iteration : 9914
train acc:  0.90625
train loss:  0.28356269001960754
train gradient:  0.13882770545945572
iteration : 9915
train acc:  0.796875
train loss:  0.4190021753311157
train gradient:  0.22132647591905608
iteration : 9916
train acc:  0.890625
train loss:  0.2962875962257385
train gradient:  0.11775629597098908
iteration : 9917
train acc:  0.8125
train loss:  0.3941289782524109
train gradient:  0.2081301743567296
iteration : 9918
train acc:  0.78125
train loss:  0.4473998546600342
train gradient:  0.2763863097508788
iteration : 9919
train acc:  0.875
train loss:  0.2887384295463562
train gradient:  0.16743016393819016
iteration : 9920
train acc:  0.875
train loss:  0.27718132734298706
train gradient:  0.11852855518266854
iteration : 9921
train acc:  0.8828125
train loss:  0.29894334077835083
train gradient:  0.16851686116534964
iteration : 9922
train acc:  0.859375
train loss:  0.3097093999385834
train gradient:  0.1328376354534532
iteration : 9923
train acc:  0.8671875
train loss:  0.2722148299217224
train gradient:  0.14519798059364117
iteration : 9924
train acc:  0.8203125
train loss:  0.38292196393013
train gradient:  0.17639824975370272
iteration : 9925
train acc:  0.890625
train loss:  0.2860950231552124
train gradient:  0.11954566747783028
iteration : 9926
train acc:  0.875
train loss:  0.29063040018081665
train gradient:  0.15764721059503742
iteration : 9927
train acc:  0.8125
train loss:  0.34794166684150696
train gradient:  0.15954279245115272
iteration : 9928
train acc:  0.84375
train loss:  0.39363086223602295
train gradient:  0.2664278686943185
iteration : 9929
train acc:  0.875
train loss:  0.3207301199436188
train gradient:  0.16251288727942226
iteration : 9930
train acc:  0.8359375
train loss:  0.33036717772483826
train gradient:  0.15471643299940382
iteration : 9931
train acc:  0.8515625
train loss:  0.3485979437828064
train gradient:  0.16162852423478286
iteration : 9932
train acc:  0.875
train loss:  0.31677860021591187
train gradient:  0.13197791248913157
iteration : 9933
train acc:  0.890625
train loss:  0.2907510995864868
train gradient:  0.11870647595448829
iteration : 9934
train acc:  0.859375
train loss:  0.3852987587451935
train gradient:  0.3476417280489273
iteration : 9935
train acc:  0.7890625
train loss:  0.37552762031555176
train gradient:  0.20180395231806128
iteration : 9936
train acc:  0.8125
train loss:  0.38914164900779724
train gradient:  0.21822087667554288
iteration : 9937
train acc:  0.90625
train loss:  0.23704566061496735
train gradient:  0.14621063146024477
iteration : 9938
train acc:  0.8359375
train loss:  0.3267902135848999
train gradient:  0.1177328145763555
iteration : 9939
train acc:  0.859375
train loss:  0.34641969203948975
train gradient:  0.18313732739323707
iteration : 9940
train acc:  0.78125
train loss:  0.40960967540740967
train gradient:  0.22204866224683495
iteration : 9941
train acc:  0.828125
train loss:  0.3372129797935486
train gradient:  0.18907476664412287
iteration : 9942
train acc:  0.84375
train loss:  0.34215229749679565
train gradient:  0.1274659845945352
iteration : 9943
train acc:  0.828125
train loss:  0.388178288936615
train gradient:  0.18266068189929907
iteration : 9944
train acc:  0.9296875
train loss:  0.26469510793685913
train gradient:  0.12262607992024403
iteration : 9945
train acc:  0.859375
train loss:  0.3664768636226654
train gradient:  0.23704271920912212
iteration : 9946
train acc:  0.8359375
train loss:  0.35901135206222534
train gradient:  0.20336434519499696
iteration : 9947
train acc:  0.859375
train loss:  0.30374807119369507
train gradient:  0.14867238101090163
iteration : 9948
train acc:  0.9296875
train loss:  0.18694381415843964
train gradient:  0.07357446426718565
iteration : 9949
train acc:  0.8359375
train loss:  0.3261452913284302
train gradient:  0.14511020957772308
iteration : 9950
train acc:  0.84375
train loss:  0.3290770649909973
train gradient:  0.18980831782622976
iteration : 9951
train acc:  0.828125
train loss:  0.3255295157432556
train gradient:  0.20146007525780477
iteration : 9952
train acc:  0.9296875
train loss:  0.2209535539150238
train gradient:  0.1704459056102911
iteration : 9953
train acc:  0.859375
train loss:  0.3310866355895996
train gradient:  0.18142373421448102
iteration : 9954
train acc:  0.8125
train loss:  0.41868066787719727
train gradient:  0.2229709704305478
iteration : 9955
train acc:  0.8671875
train loss:  0.2806846499443054
train gradient:  0.13093707619033065
iteration : 9956
train acc:  0.8359375
train loss:  0.40680772066116333
train gradient:  0.20350357716123774
iteration : 9957
train acc:  0.859375
train loss:  0.31838101148605347
train gradient:  0.20420448188406404
iteration : 9958
train acc:  0.84375
train loss:  0.29714301228523254
train gradient:  0.11223446560701117
iteration : 9959
train acc:  0.8671875
train loss:  0.33570849895477295
train gradient:  0.12564378823009942
iteration : 9960
train acc:  0.8828125
train loss:  0.32436227798461914
train gradient:  0.15838418880149918
iteration : 9961
train acc:  0.875
train loss:  0.3267924189567566
train gradient:  0.21283757034727474
iteration : 9962
train acc:  0.890625
train loss:  0.24407100677490234
train gradient:  0.11858909425098615
iteration : 9963
train acc:  0.875
train loss:  0.30823463201522827
train gradient:  0.14648277001136029
iteration : 9964
train acc:  0.8359375
train loss:  0.43170714378356934
train gradient:  0.3056768911359824
iteration : 9965
train acc:  0.890625
train loss:  0.2785347104072571
train gradient:  0.08263696302763904
iteration : 9966
train acc:  0.8828125
train loss:  0.25701409578323364
train gradient:  0.1033166336909841
iteration : 9967
train acc:  0.84375
train loss:  0.35082724690437317
train gradient:  0.2082618306720165
iteration : 9968
train acc:  0.828125
train loss:  0.3645089566707611
train gradient:  0.16825586335320145
iteration : 9969
train acc:  0.8125
train loss:  0.35106560587882996
train gradient:  0.14359865947247685
iteration : 9970
train acc:  0.8671875
train loss:  0.2859117388725281
train gradient:  0.11296216142669077
iteration : 9971
train acc:  0.8984375
train loss:  0.22738942503929138
train gradient:  0.09310055102193739
iteration : 9972
train acc:  0.8515625
train loss:  0.3518865406513214
train gradient:  0.1692138621788556
iteration : 9973
train acc:  0.890625
train loss:  0.2889121472835541
train gradient:  0.12324912097114162
iteration : 9974
train acc:  0.8828125
train loss:  0.36066871881484985
train gradient:  0.23244153979899634
iteration : 9975
train acc:  0.828125
train loss:  0.3921373188495636
train gradient:  0.21797322992483564
iteration : 9976
train acc:  0.8828125
train loss:  0.2896749973297119
train gradient:  0.11685976422445933
iteration : 9977
train acc:  0.8359375
train loss:  0.36969006061553955
train gradient:  0.177145774978441
iteration : 9978
train acc:  0.859375
train loss:  0.3075997829437256
train gradient:  0.16440888968661443
iteration : 9979
train acc:  0.8359375
train loss:  0.3264254033565521
train gradient:  0.16807047533995834
iteration : 9980
train acc:  0.8515625
train loss:  0.3631424009799957
train gradient:  0.22560015029610223
iteration : 9981
train acc:  0.8671875
train loss:  0.3078625500202179
train gradient:  0.13972863650302325
iteration : 9982
train acc:  0.859375
train loss:  0.3053719997406006
train gradient:  0.1315851465120404
iteration : 9983
train acc:  0.8671875
train loss:  0.288154274225235
train gradient:  0.11276792824835485
iteration : 9984
train acc:  0.8359375
train loss:  0.33478254079818726
train gradient:  0.14733322775238258
iteration : 9985
train acc:  0.8671875
train loss:  0.3160467743873596
train gradient:  0.12709872598267075
iteration : 9986
train acc:  0.8828125
train loss:  0.24556082487106323
train gradient:  0.13614084258918402
iteration : 9987
train acc:  0.8671875
train loss:  0.28693997859954834
train gradient:  0.1466048192206208
iteration : 9988
train acc:  0.8359375
train loss:  0.29789793491363525
train gradient:  0.2129382656387841
iteration : 9989
train acc:  0.7578125
train loss:  0.45729637145996094
train gradient:  0.2538110547718371
iteration : 9990
train acc:  0.84375
train loss:  0.3468160331249237
train gradient:  0.18481686486799143
iteration : 9991
train acc:  0.8515625
train loss:  0.31576699018478394
train gradient:  0.18234860472741202
iteration : 9992
train acc:  0.875
train loss:  0.3421146869659424
train gradient:  0.15868319411968285
iteration : 9993
train acc:  0.859375
train loss:  0.35582247376441956
train gradient:  0.20186896215607042
iteration : 9994
train acc:  0.8125
train loss:  0.38448798656463623
train gradient:  0.23361932336111368
iteration : 9995
train acc:  0.8671875
train loss:  0.3152977228164673
train gradient:  0.15062076525239243
iteration : 9996
train acc:  0.875
train loss:  0.26093459129333496
train gradient:  0.14338509476361566
iteration : 9997
train acc:  0.890625
train loss:  0.2616860866546631
train gradient:  0.15752124519312055
iteration : 9998
train acc:  0.859375
train loss:  0.3234190344810486
train gradient:  0.18109966499128705
iteration : 9999
train acc:  0.8515625
train loss:  0.4325501322746277
train gradient:  0.2964952186550535
iteration : 10000
train acc:  0.8671875
train loss:  0.31957849860191345
train gradient:  0.1363528013579539
iteration : 10001
train acc:  0.890625
train loss:  0.30314934253692627
train gradient:  0.12732223496650824
iteration : 10002
train acc:  0.8671875
train loss:  0.35172179341316223
train gradient:  0.21349716102935168
iteration : 10003
train acc:  0.84375
train loss:  0.35149887204170227
train gradient:  0.21204062545200775
iteration : 10004
train acc:  0.8046875
train loss:  0.3550143241882324
train gradient:  0.20912038782089948
iteration : 10005
train acc:  0.8359375
train loss:  0.358362078666687
train gradient:  0.13244842635879445
iteration : 10006
train acc:  0.8359375
train loss:  0.33971577882766724
train gradient:  0.1662215026248683
iteration : 10007
train acc:  0.8359375
train loss:  0.30547088384628296
train gradient:  0.13639925736038624
iteration : 10008
train acc:  0.8828125
train loss:  0.3011459708213806
train gradient:  0.1440997594878621
iteration : 10009
train acc:  0.90625
train loss:  0.3224502503871918
train gradient:  0.24010579921233494
iteration : 10010
train acc:  0.8828125
train loss:  0.2599726617336273
train gradient:  0.08711087591345418
iteration : 10011
train acc:  0.875
train loss:  0.32186558842658997
train gradient:  0.18217650982939745
iteration : 10012
train acc:  0.875
train loss:  0.29932981729507446
train gradient:  0.13233076874939037
iteration : 10013
train acc:  0.8671875
train loss:  0.30746757984161377
train gradient:  0.1606357304727034
iteration : 10014
train acc:  0.8515625
train loss:  0.35641032457351685
train gradient:  0.1931045536282774
iteration : 10015
train acc:  0.8515625
train loss:  0.3118734657764435
train gradient:  0.18346029574113998
iteration : 10016
train acc:  0.921875
train loss:  0.2648947238922119
train gradient:  0.13840174582065234
iteration : 10017
train acc:  0.8828125
train loss:  0.32877689599990845
train gradient:  0.14225463591366377
iteration : 10018
train acc:  0.8046875
train loss:  0.3752083480358124
train gradient:  0.18225617299820113
iteration : 10019
train acc:  0.8359375
train loss:  0.37053221464157104
train gradient:  0.19007080629352113
iteration : 10020
train acc:  0.890625
train loss:  0.34001800417900085
train gradient:  0.18242420386593328
iteration : 10021
train acc:  0.890625
train loss:  0.2735065817832947
train gradient:  0.14037965492863075
iteration : 10022
train acc:  0.8125
train loss:  0.3741251826286316
train gradient:  0.18975710247011535
iteration : 10023
train acc:  0.859375
train loss:  0.345961332321167
train gradient:  0.17560039805693922
iteration : 10024
train acc:  0.859375
train loss:  0.3179887533187866
train gradient:  0.11470275892083928
iteration : 10025
train acc:  0.84375
train loss:  0.3179911971092224
train gradient:  0.14709459633900923
iteration : 10026
train acc:  0.875
train loss:  0.3222144842147827
train gradient:  0.19027396560116613
iteration : 10027
train acc:  0.8828125
train loss:  0.305858314037323
train gradient:  0.1500226589754879
iteration : 10028
train acc:  0.859375
train loss:  0.2872101664543152
train gradient:  0.14161770560920073
iteration : 10029
train acc:  0.8203125
train loss:  0.3528037369251251
train gradient:  0.24539658216304447
iteration : 10030
train acc:  0.9296875
train loss:  0.2575642466545105
train gradient:  0.10944568598416984
iteration : 10031
train acc:  0.8359375
train loss:  0.28014639019966125
train gradient:  0.13395044339577478
iteration : 10032
train acc:  0.890625
train loss:  0.24184134602546692
train gradient:  0.12361128182766813
iteration : 10033
train acc:  0.9140625
train loss:  0.2811890244483948
train gradient:  0.12304272382296515
iteration : 10034
train acc:  0.859375
train loss:  0.290503591299057
train gradient:  0.1319784171023314
iteration : 10035
train acc:  0.859375
train loss:  0.3215649127960205
train gradient:  0.1322770286634704
iteration : 10036
train acc:  0.875
train loss:  0.27364033460617065
train gradient:  0.13488371863813292
iteration : 10037
train acc:  0.8984375
train loss:  0.30945631861686707
train gradient:  0.12090344630191652
iteration : 10038
train acc:  0.828125
train loss:  0.43501585721969604
train gradient:  0.21119563658050441
iteration : 10039
train acc:  0.8671875
train loss:  0.33757704496383667
train gradient:  0.13669179309468782
iteration : 10040
train acc:  0.8828125
train loss:  0.2981926202774048
train gradient:  0.10722209528284851
iteration : 10041
train acc:  0.890625
train loss:  0.2561975121498108
train gradient:  0.11120438515005664
iteration : 10042
train acc:  0.8828125
train loss:  0.3448219895362854
train gradient:  0.14638348306026688
iteration : 10043
train acc:  0.859375
train loss:  0.2918587923049927
train gradient:  0.23879003132491372
iteration : 10044
train acc:  0.8203125
train loss:  0.45464372634887695
train gradient:  0.37948355149420065
iteration : 10045
train acc:  0.828125
train loss:  0.30798330903053284
train gradient:  0.15054691721347266
iteration : 10046
train acc:  0.8359375
train loss:  0.351551353931427
train gradient:  0.141520694132992
iteration : 10047
train acc:  0.859375
train loss:  0.3152812123298645
train gradient:  0.16313824235760432
iteration : 10048
train acc:  0.8515625
train loss:  0.32693347334861755
train gradient:  0.15431084714328602
iteration : 10049
train acc:  0.84375
train loss:  0.3539455831050873
train gradient:  0.20614933302879507
iteration : 10050
train acc:  0.8671875
train loss:  0.30634427070617676
train gradient:  0.13056086073005901
iteration : 10051
train acc:  0.84375
train loss:  0.3917139768600464
train gradient:  0.3566622549588354
iteration : 10052
train acc:  0.84375
train loss:  0.3713931143283844
train gradient:  0.16504517863655876
iteration : 10053
train acc:  0.8359375
train loss:  0.3413431644439697
train gradient:  0.20725768757894653
iteration : 10054
train acc:  0.8203125
train loss:  0.39213863015174866
train gradient:  0.2107366077965039
iteration : 10055
train acc:  0.8671875
train loss:  0.29509490728378296
train gradient:  0.145237258566895
iteration : 10056
train acc:  0.890625
train loss:  0.25628262758255005
train gradient:  0.15173814167850763
iteration : 10057
train acc:  0.8359375
train loss:  0.32348114252090454
train gradient:  0.14852405976464733
iteration : 10058
train acc:  0.8984375
train loss:  0.2889796197414398
train gradient:  0.13951760348245076
iteration : 10059
train acc:  0.8125
train loss:  0.342297226190567
train gradient:  0.17550467069602757
iteration : 10060
train acc:  0.875
train loss:  0.30237963795661926
train gradient:  0.1679866630968597
iteration : 10061
train acc:  0.859375
train loss:  0.28849077224731445
train gradient:  0.15447960047151132
iteration : 10062
train acc:  0.8359375
train loss:  0.3545510470867157
train gradient:  0.1738697110607928
iteration : 10063
train acc:  0.859375
train loss:  0.34982895851135254
train gradient:  0.16736400176718286
iteration : 10064
train acc:  0.921875
train loss:  0.2783828377723694
train gradient:  0.11845848142773437
iteration : 10065
train acc:  0.8828125
train loss:  0.2814021706581116
train gradient:  0.1610412602782494
iteration : 10066
train acc:  0.7890625
train loss:  0.42016860842704773
train gradient:  0.2820449167001542
iteration : 10067
train acc:  0.8359375
train loss:  0.30892014503479004
train gradient:  0.16533366255192714
iteration : 10068
train acc:  0.875
train loss:  0.340535968542099
train gradient:  0.1325317218416711
iteration : 10069
train acc:  0.796875
train loss:  0.4174065589904785
train gradient:  0.38172034082253986
iteration : 10070
train acc:  0.90625
train loss:  0.23179861903190613
train gradient:  0.10248803303336738
iteration : 10071
train acc:  0.875
train loss:  0.31651321053504944
train gradient:  0.27313262291198986
iteration : 10072
train acc:  0.890625
train loss:  0.34541255235671997
train gradient:  0.17462922636388772
iteration : 10073
train acc:  0.8359375
train loss:  0.3317658305168152
train gradient:  0.1573625406000667
iteration : 10074
train acc:  0.8828125
train loss:  0.2934918999671936
train gradient:  0.15514084502982023
iteration : 10075
train acc:  0.828125
train loss:  0.3606935739517212
train gradient:  0.15758254719288478
iteration : 10076
train acc:  0.859375
train loss:  0.3080618977546692
train gradient:  0.16029730938254674
iteration : 10077
train acc:  0.84375
train loss:  0.31939059495925903
train gradient:  0.15358159461236726
iteration : 10078
train acc:  0.890625
train loss:  0.26480334997177124
train gradient:  0.11573501862311525
iteration : 10079
train acc:  0.8125
train loss:  0.3383389115333557
train gradient:  0.16829972788588957
iteration : 10080
train acc:  0.828125
train loss:  0.3118339776992798
train gradient:  0.15990755739982884
iteration : 10081
train acc:  0.796875
train loss:  0.4526907801628113
train gradient:  0.29034000306884894
iteration : 10082
train acc:  0.8515625
train loss:  0.449322909116745
train gradient:  0.32437141547718007
iteration : 10083
train acc:  0.890625
train loss:  0.31307607889175415
train gradient:  0.18008271473205373
iteration : 10084
train acc:  0.875
train loss:  0.3275386095046997
train gradient:  0.23584685164771715
iteration : 10085
train acc:  0.875
train loss:  0.29139643907546997
train gradient:  0.14994305629666466
iteration : 10086
train acc:  0.890625
train loss:  0.24113056063652039
train gradient:  0.10860656735950813
iteration : 10087
train acc:  0.8515625
train loss:  0.31373125314712524
train gradient:  0.1299157113866886
iteration : 10088
train acc:  0.8828125
train loss:  0.2968992590904236
train gradient:  0.13531577820598203
iteration : 10089
train acc:  0.8671875
train loss:  0.30492308735847473
train gradient:  0.17094475409268714
iteration : 10090
train acc:  0.8359375
train loss:  0.3493843972682953
train gradient:  0.20683499795170152
iteration : 10091
train acc:  0.8828125
train loss:  0.3035488724708557
train gradient:  0.16941185016220067
iteration : 10092
train acc:  0.90625
train loss:  0.26576581597328186
train gradient:  0.17276360855826295
iteration : 10093
train acc:  0.8828125
train loss:  0.26335835456848145
train gradient:  0.10971611472903499
iteration : 10094
train acc:  0.8671875
train loss:  0.32415175437927246
train gradient:  0.12668895908920236
iteration : 10095
train acc:  0.890625
train loss:  0.3017560839653015
train gradient:  0.1282537481140747
iteration : 10096
train acc:  0.8828125
train loss:  0.31163808703422546
train gradient:  0.14175843645069564
iteration : 10097
train acc:  0.8515625
train loss:  0.37421178817749023
train gradient:  0.2173048192388476
iteration : 10098
train acc:  0.8671875
train loss:  0.305192232131958
train gradient:  0.15580989213384366
iteration : 10099
train acc:  0.8203125
train loss:  0.35517776012420654
train gradient:  0.18269864758195947
iteration : 10100
train acc:  0.84375
train loss:  0.31571534276008606
train gradient:  0.20335809019852322
iteration : 10101
train acc:  0.8125
train loss:  0.46004611253738403
train gradient:  0.23341426777310811
iteration : 10102
train acc:  0.828125
train loss:  0.3985394239425659
train gradient:  0.22480731651057245
iteration : 10103
train acc:  0.8515625
train loss:  0.3541994094848633
train gradient:  0.19837995012090545
iteration : 10104
train acc:  0.8359375
train loss:  0.3688485026359558
train gradient:  0.20920926267023582
iteration : 10105
train acc:  0.796875
train loss:  0.39129993319511414
train gradient:  0.20029386099158805
iteration : 10106
train acc:  0.828125
train loss:  0.40369051694869995
train gradient:  0.24793764587835707
iteration : 10107
train acc:  0.7890625
train loss:  0.3662903308868408
train gradient:  0.17573372191775097
iteration : 10108
train acc:  0.8828125
train loss:  0.2976570427417755
train gradient:  0.16078908233352857
iteration : 10109
train acc:  0.8203125
train loss:  0.3782007694244385
train gradient:  0.2005039511493794
iteration : 10110
train acc:  0.875
train loss:  0.2708291709423065
train gradient:  0.09100704520683578
iteration : 10111
train acc:  0.8515625
train loss:  0.330499529838562
train gradient:  0.17626754210590206
iteration : 10112
train acc:  0.8828125
train loss:  0.3277570307254791
train gradient:  0.1658347532443592
iteration : 10113
train acc:  0.828125
train loss:  0.3793831169605255
train gradient:  0.26756874650451046
iteration : 10114
train acc:  0.8203125
train loss:  0.34962987899780273
train gradient:  0.17676692870520627
iteration : 10115
train acc:  0.8203125
train loss:  0.4081589877605438
train gradient:  0.25821958330961176
iteration : 10116
train acc:  0.890625
train loss:  0.26717835664749146
train gradient:  0.1387497087113519
iteration : 10117
train acc:  0.90625
train loss:  0.2476302981376648
train gradient:  0.10123022017999084
iteration : 10118
train acc:  0.875
train loss:  0.3482791781425476
train gradient:  0.17535071063178148
iteration : 10119
train acc:  0.8046875
train loss:  0.38175880908966064
train gradient:  0.19418353962029977
iteration : 10120
train acc:  0.84375
train loss:  0.34560340642929077
train gradient:  0.16043514523137836
iteration : 10121
train acc:  0.828125
train loss:  0.3809221386909485
train gradient:  0.1529290704197659
iteration : 10122
train acc:  0.8515625
train loss:  0.39564406871795654
train gradient:  0.2859559976250919
iteration : 10123
train acc:  0.828125
train loss:  0.35549497604370117
train gradient:  0.167948740063033
iteration : 10124
train acc:  0.8828125
train loss:  0.24810491502285004
train gradient:  0.09690585534665684
iteration : 10125
train acc:  0.859375
train loss:  0.324609637260437
train gradient:  0.13079617176559494
iteration : 10126
train acc:  0.8984375
train loss:  0.2556653320789337
train gradient:  0.11751019897507316
iteration : 10127
train acc:  0.8671875
train loss:  0.31829267740249634
train gradient:  0.19255561433435525
iteration : 10128
train acc:  0.84375
train loss:  0.4125349521636963
train gradient:  0.27009027709395633
iteration : 10129
train acc:  0.875
train loss:  0.3298282027244568
train gradient:  0.20036953151252448
iteration : 10130
train acc:  0.8984375
train loss:  0.29300498962402344
train gradient:  0.16735118213339692
iteration : 10131
train acc:  0.859375
train loss:  0.2868959307670593
train gradient:  0.1362259250770851
iteration : 10132
train acc:  0.8203125
train loss:  0.37534165382385254
train gradient:  0.18590346117978834
iteration : 10133
train acc:  0.84375
train loss:  0.33088749647140503
train gradient:  0.13006658336022703
iteration : 10134
train acc:  0.8359375
train loss:  0.3243396282196045
train gradient:  0.21460570894017822
iteration : 10135
train acc:  0.8828125
train loss:  0.289699524641037
train gradient:  0.14551036341618517
iteration : 10136
train acc:  0.90625
train loss:  0.2560785710811615
train gradient:  0.08622228324566826
iteration : 10137
train acc:  0.859375
train loss:  0.31076449155807495
train gradient:  0.17548275031822544
iteration : 10138
train acc:  0.8671875
train loss:  0.3101799488067627
train gradient:  0.17506150516926428
iteration : 10139
train acc:  0.8984375
train loss:  0.2809924781322479
train gradient:  0.1463984214548759
iteration : 10140
train acc:  0.84375
train loss:  0.3464148938655853
train gradient:  0.1999487287044088
iteration : 10141
train acc:  0.890625
train loss:  0.30535030364990234
train gradient:  0.17782248097783904
iteration : 10142
train acc:  0.875
train loss:  0.27440497279167175
train gradient:  0.15609376353184673
iteration : 10143
train acc:  0.859375
train loss:  0.3014798164367676
train gradient:  0.13962775731677746
iteration : 10144
train acc:  0.84375
train loss:  0.3462781310081482
train gradient:  0.20674612388978691
iteration : 10145
train acc:  0.8671875
train loss:  0.28700605034828186
train gradient:  0.1548097227828788
iteration : 10146
train acc:  0.8671875
train loss:  0.2857760488986969
train gradient:  0.14055860532839093
iteration : 10147
train acc:  0.9296875
train loss:  0.24069705605506897
train gradient:  0.09791274293754115
iteration : 10148
train acc:  0.84375
train loss:  0.3774907886981964
train gradient:  0.2243687395158413
iteration : 10149
train acc:  0.84375
train loss:  0.3711373507976532
train gradient:  0.18019910458696148
iteration : 10150
train acc:  0.9140625
train loss:  0.23502129316329956
train gradient:  0.08244895503329917
iteration : 10151
train acc:  0.828125
train loss:  0.33531904220581055
train gradient:  0.15928398898615587
iteration : 10152
train acc:  0.8828125
train loss:  0.3127271234989166
train gradient:  0.12321130342904342
iteration : 10153
train acc:  0.8828125
train loss:  0.26710373163223267
train gradient:  0.11142506267469135
iteration : 10154
train acc:  0.90625
train loss:  0.24484843015670776
train gradient:  0.11015401901879303
iteration : 10155
train acc:  0.8515625
train loss:  0.32916712760925293
train gradient:  0.18573698674138905
iteration : 10156
train acc:  0.875
train loss:  0.2950802445411682
train gradient:  0.1514860199352855
iteration : 10157
train acc:  0.8203125
train loss:  0.31761908531188965
train gradient:  0.1500425631079848
iteration : 10158
train acc:  0.8671875
train loss:  0.35742413997650146
train gradient:  0.19953697993670894
iteration : 10159
train acc:  0.84375
train loss:  0.3316344618797302
train gradient:  0.16176776548905555
iteration : 10160
train acc:  0.796875
train loss:  0.45121851563453674
train gradient:  0.28133124598835035
iteration : 10161
train acc:  0.8515625
train loss:  0.3426205813884735
train gradient:  0.16397721169065496
iteration : 10162
train acc:  0.859375
train loss:  0.2732919454574585
train gradient:  0.11905181122374195
iteration : 10163
train acc:  0.8671875
train loss:  0.3326474130153656
train gradient:  0.1733630275883621
iteration : 10164
train acc:  0.8671875
train loss:  0.3083433508872986
train gradient:  0.13946004222088162
iteration : 10165
train acc:  0.90625
train loss:  0.2568504810333252
train gradient:  0.11297930426261792
iteration : 10166
train acc:  0.828125
train loss:  0.3509089946746826
train gradient:  0.26527882581384926
iteration : 10167
train acc:  0.875
train loss:  0.33409154415130615
train gradient:  0.1072934473685602
iteration : 10168
train acc:  0.859375
train loss:  0.27779620885849
train gradient:  0.09686631836400457
iteration : 10169
train acc:  0.8125
train loss:  0.34588703513145447
train gradient:  0.16554750263109017
iteration : 10170
train acc:  0.8046875
train loss:  0.38602665066719055
train gradient:  0.2032028155247227
iteration : 10171
train acc:  0.8671875
train loss:  0.27829688787460327
train gradient:  0.1139329253729796
iteration : 10172
train acc:  0.84375
train loss:  0.3309171497821808
train gradient:  0.17857419025325605
iteration : 10173
train acc:  0.8828125
train loss:  0.3456744849681854
train gradient:  0.1845021417160218
iteration : 10174
train acc:  0.859375
train loss:  0.3559328317642212
train gradient:  0.20167411964973736
iteration : 10175
train acc:  0.8671875
train loss:  0.34032106399536133
train gradient:  0.12153596516731112
iteration : 10176
train acc:  0.8203125
train loss:  0.3175821006298065
train gradient:  0.1434828717265701
iteration : 10177
train acc:  0.8359375
train loss:  0.41703104972839355
train gradient:  0.2945129206165929
iteration : 10178
train acc:  0.859375
train loss:  0.3276733160018921
train gradient:  0.19355053281968088
iteration : 10179
train acc:  0.7890625
train loss:  0.39812663197517395
train gradient:  0.21927335280014648
iteration : 10180
train acc:  0.875
train loss:  0.26870566606521606
train gradient:  0.149781649583004
iteration : 10181
train acc:  0.828125
train loss:  0.34196949005126953
train gradient:  0.16777269313492793
iteration : 10182
train acc:  0.859375
train loss:  0.33975908160209656
train gradient:  0.20804701209608786
iteration : 10183
train acc:  0.875
train loss:  0.2814050316810608
train gradient:  0.18085391088044211
iteration : 10184
train acc:  0.8671875
train loss:  0.3219476342201233
train gradient:  0.1611402621143338
iteration : 10185
train acc:  0.8828125
train loss:  0.2983725666999817
train gradient:  0.1503797759698804
iteration : 10186
train acc:  0.8671875
train loss:  0.30130937695503235
train gradient:  0.17874106404383217
iteration : 10187
train acc:  0.859375
train loss:  0.27946382761001587
train gradient:  0.13447115554444605
iteration : 10188
train acc:  0.921875
train loss:  0.20751994848251343
train gradient:  0.0933545103064457
iteration : 10189
train acc:  0.8671875
train loss:  0.28294581174850464
train gradient:  0.13545709993014163
iteration : 10190
train acc:  0.828125
train loss:  0.34653159976005554
train gradient:  0.216732042273739
iteration : 10191
train acc:  0.8828125
train loss:  0.3193001449108124
train gradient:  0.15082760987730973
iteration : 10192
train acc:  0.8984375
train loss:  0.28051188588142395
train gradient:  0.1527812209219303
iteration : 10193
train acc:  0.8984375
train loss:  0.27527186274528503
train gradient:  0.15672461643255736
iteration : 10194
train acc:  0.859375
train loss:  0.3020169734954834
train gradient:  0.1346897678385085
iteration : 10195
train acc:  0.875
train loss:  0.3210834860801697
train gradient:  0.14851709478232833
iteration : 10196
train acc:  0.859375
train loss:  0.2990439534187317
train gradient:  0.12081119772951143
iteration : 10197
train acc:  0.921875
train loss:  0.2217186540365219
train gradient:  0.1074958466090671
iteration : 10198
train acc:  0.8515625
train loss:  0.33350324630737305
train gradient:  0.13014033282106974
iteration : 10199
train acc:  0.90625
train loss:  0.2305261194705963
train gradient:  0.15872406709032044
iteration : 10200
train acc:  0.921875
train loss:  0.292226105928421
train gradient:  0.10870034358445815
iteration : 10201
train acc:  0.84375
train loss:  0.35373327136039734
train gradient:  0.18327592583348395
iteration : 10202
train acc:  0.890625
train loss:  0.31795576214790344
train gradient:  0.11036749814754843
iteration : 10203
train acc:  0.8203125
train loss:  0.38482776284217834
train gradient:  0.17846305550775082
iteration : 10204
train acc:  0.8671875
train loss:  0.2848793566226959
train gradient:  0.10829645494404981
iteration : 10205
train acc:  0.8515625
train loss:  0.3143927752971649
train gradient:  0.1896820943406028
iteration : 10206
train acc:  0.859375
train loss:  0.3272266983985901
train gradient:  0.19146145474203594
iteration : 10207
train acc:  0.890625
train loss:  0.2673060894012451
train gradient:  0.13974777100656388
iteration : 10208
train acc:  0.8671875
train loss:  0.2577498257160187
train gradient:  0.15832114707644007
iteration : 10209
train acc:  0.8125
train loss:  0.41833001375198364
train gradient:  0.28477803842155563
iteration : 10210
train acc:  0.875
train loss:  0.2495606541633606
train gradient:  0.09005877103139727
iteration : 10211
train acc:  0.875
train loss:  0.30549356341362
train gradient:  0.1327565522600006
iteration : 10212
train acc:  0.8515625
train loss:  0.37316781282424927
train gradient:  0.19707011835530455
iteration : 10213
train acc:  0.8515625
train loss:  0.33065265417099
train gradient:  0.163706089587389
iteration : 10214
train acc:  0.8125
train loss:  0.38153162598609924
train gradient:  0.22310006917753017
iteration : 10215
train acc:  0.890625
train loss:  0.29593053460121155
train gradient:  0.13199318828600196
iteration : 10216
train acc:  0.8984375
train loss:  0.290893018245697
train gradient:  0.178081367661395
iteration : 10217
train acc:  0.8203125
train loss:  0.40932390093803406
train gradient:  0.19798209829587765
iteration : 10218
train acc:  0.9140625
train loss:  0.23837022483348846
train gradient:  0.14581278164993605
iteration : 10219
train acc:  0.8828125
train loss:  0.291766494512558
train gradient:  0.1398456613664618
iteration : 10220
train acc:  0.84375
train loss:  0.3646997809410095
train gradient:  0.3111640029278155
iteration : 10221
train acc:  0.875
train loss:  0.30689021944999695
train gradient:  0.12837550285631943
iteration : 10222
train acc:  0.921875
train loss:  0.23002584278583527
train gradient:  0.08729452696934836
iteration : 10223
train acc:  0.8359375
train loss:  0.3849923312664032
train gradient:  0.2523646833840291
iteration : 10224
train acc:  0.8359375
train loss:  0.38838422298431396
train gradient:  0.2318173037690423
iteration : 10225
train acc:  0.8671875
train loss:  0.2795225977897644
train gradient:  0.19454880737875008
iteration : 10226
train acc:  0.84375
train loss:  0.3688613176345825
train gradient:  0.24417556697380344
iteration : 10227
train acc:  0.8828125
train loss:  0.32324910163879395
train gradient:  0.1797744530899883
iteration : 10228
train acc:  0.8125
train loss:  0.38023942708969116
train gradient:  0.19282813327896214
iteration : 10229
train acc:  0.8046875
train loss:  0.36796486377716064
train gradient:  0.16119196957333295
iteration : 10230
train acc:  0.8515625
train loss:  0.3088749647140503
train gradient:  0.15498970299585105
iteration : 10231
train acc:  0.828125
train loss:  0.3380739092826843
train gradient:  0.18801670804654858
iteration : 10232
train acc:  0.8828125
train loss:  0.2602497339248657
train gradient:  0.09530352427962828
iteration : 10233
train acc:  0.8515625
train loss:  0.3287765383720398
train gradient:  0.21628587652538916
iteration : 10234
train acc:  0.8671875
train loss:  0.2762557864189148
train gradient:  0.1660943998153722
iteration : 10235
train acc:  0.828125
train loss:  0.4430003762245178
train gradient:  0.22787698372048204
iteration : 10236
train acc:  0.8671875
train loss:  0.3444424867630005
train gradient:  0.14662409263681225
iteration : 10237
train acc:  0.890625
train loss:  0.2382119745016098
train gradient:  0.1212203230408014
iteration : 10238
train acc:  0.859375
train loss:  0.3182064890861511
train gradient:  0.14483503026941735
iteration : 10239
train acc:  0.8671875
train loss:  0.27385208010673523
train gradient:  0.11803740543948685
iteration : 10240
train acc:  0.890625
train loss:  0.2880261540412903
train gradient:  0.19399180171487296
iteration : 10241
train acc:  0.8203125
train loss:  0.3727800250053406
train gradient:  0.17566451890807921
iteration : 10242
train acc:  0.84375
train loss:  0.37042897939682007
train gradient:  0.21461766310350266
iteration : 10243
train acc:  0.84375
train loss:  0.35771310329437256
train gradient:  0.15937994294586497
iteration : 10244
train acc:  0.8515625
train loss:  0.357408344745636
train gradient:  0.246721106196309
iteration : 10245
train acc:  0.8515625
train loss:  0.3626907467842102
train gradient:  0.2918715255543581
iteration : 10246
train acc:  0.859375
train loss:  0.3731228709220886
train gradient:  0.14865106022734043
iteration : 10247
train acc:  0.9140625
train loss:  0.30273669958114624
train gradient:  0.10465847232238501
iteration : 10248
train acc:  0.8828125
train loss:  0.29062753915786743
train gradient:  0.1387766470029545
iteration : 10249
train acc:  0.890625
train loss:  0.2801520526409149
train gradient:  0.140161428529313
iteration : 10250
train acc:  0.859375
train loss:  0.30589982867240906
train gradient:  0.1462565916690431
iteration : 10251
train acc:  0.90625
train loss:  0.27518603205680847
train gradient:  0.11929231683363582
iteration : 10252
train acc:  0.890625
train loss:  0.31445571780204773
train gradient:  0.17914040486333388
iteration : 10253
train acc:  0.8203125
train loss:  0.3853270411491394
train gradient:  0.2536856280157068
iteration : 10254
train acc:  0.8828125
train loss:  0.2910711467266083
train gradient:  0.10362139407467963
iteration : 10255
train acc:  0.8671875
train loss:  0.27691179513931274
train gradient:  0.16065322866795984
iteration : 10256
train acc:  0.8515625
train loss:  0.29303789138793945
train gradient:  0.15867566462825403
iteration : 10257
train acc:  0.8359375
train loss:  0.3300352692604065
train gradient:  0.20807452305575158
iteration : 10258
train acc:  0.8359375
train loss:  0.34031108021736145
train gradient:  0.15788775320419035
iteration : 10259
train acc:  0.8203125
train loss:  0.3921627402305603
train gradient:  0.2147582204594038
iteration : 10260
train acc:  0.8359375
train loss:  0.4136001467704773
train gradient:  0.2898491711326865
iteration : 10261
train acc:  0.9140625
train loss:  0.24005639553070068
train gradient:  0.10538831352323283
iteration : 10262
train acc:  0.859375
train loss:  0.3230842053890228
train gradient:  0.18123834180729737
iteration : 10263
train acc:  0.8828125
train loss:  0.2746378779411316
train gradient:  0.10176665489624957
iteration : 10264
train acc:  0.90625
train loss:  0.24083051085472107
train gradient:  0.14083218550035376
iteration : 10265
train acc:  0.828125
train loss:  0.3739432394504547
train gradient:  0.2253113121449382
iteration : 10266
train acc:  0.9375
train loss:  0.20286686718463898
train gradient:  0.08190239755925495
iteration : 10267
train acc:  0.859375
train loss:  0.34964627027511597
train gradient:  0.18396641266728597
iteration : 10268
train acc:  0.828125
train loss:  0.3901708722114563
train gradient:  0.18516423812934565
iteration : 10269
train acc:  0.8359375
train loss:  0.34809964895248413
train gradient:  0.17950823423288934
iteration : 10270
train acc:  0.859375
train loss:  0.2852975130081177
train gradient:  0.12412299586868042
iteration : 10271
train acc:  0.8359375
train loss:  0.3286571204662323
train gradient:  0.12462428166840485
iteration : 10272
train acc:  0.8515625
train loss:  0.34168848395347595
train gradient:  0.16884056614692539
iteration : 10273
train acc:  0.875
train loss:  0.24530136585235596
train gradient:  0.09392983289436746
iteration : 10274
train acc:  0.84375
train loss:  0.3601934611797333
train gradient:  0.15205506365124508
iteration : 10275
train acc:  0.875
train loss:  0.3432048559188843
train gradient:  0.1975507547634577
iteration : 10276
train acc:  0.8828125
train loss:  0.32567986845970154
train gradient:  0.12077156877181787
iteration : 10277
train acc:  0.8984375
train loss:  0.31335875391960144
train gradient:  0.16259626541399888
iteration : 10278
train acc:  0.8359375
train loss:  0.3509489893913269
train gradient:  0.1513384661089198
iteration : 10279
train acc:  0.8515625
train loss:  0.3921082019805908
train gradient:  0.19403824308647039
iteration : 10280
train acc:  0.8203125
train loss:  0.33417055010795593
train gradient:  0.16361624773129232
iteration : 10281
train acc:  0.8359375
train loss:  0.3540722131729126
train gradient:  0.19181595041050825
iteration : 10282
train acc:  0.875
train loss:  0.33366650342941284
train gradient:  0.09907731836260548
iteration : 10283
train acc:  0.78125
train loss:  0.37031275033950806
train gradient:  0.18118013808053265
iteration : 10284
train acc:  0.84375
train loss:  0.3586638569831848
train gradient:  0.23940339353494533
iteration : 10285
train acc:  0.8984375
train loss:  0.3019390106201172
train gradient:  0.11038150775036643
iteration : 10286
train acc:  0.8515625
train loss:  0.3280044496059418
train gradient:  0.14285985833762826
iteration : 10287
train acc:  0.8671875
train loss:  0.3464556336402893
train gradient:  0.19595679619199477
iteration : 10288
train acc:  0.8359375
train loss:  0.3657890260219574
train gradient:  0.19759086561116784
iteration : 10289
train acc:  0.859375
train loss:  0.35337167978286743
train gradient:  0.18547441656423422
iteration : 10290
train acc:  0.890625
train loss:  0.2900761663913727
train gradient:  0.12244866052009691
iteration : 10291
train acc:  0.8203125
train loss:  0.3729684054851532
train gradient:  0.1570922899966843
iteration : 10292
train acc:  0.8671875
train loss:  0.35309386253356934
train gradient:  0.1970521391755849
iteration : 10293
train acc:  0.84375
train loss:  0.33999645709991455
train gradient:  0.19270487357820007
iteration : 10294
train acc:  0.8984375
train loss:  0.2697632312774658
train gradient:  0.08373263637542344
iteration : 10295
train acc:  0.8671875
train loss:  0.3667982816696167
train gradient:  0.19848149908759805
iteration : 10296
train acc:  0.859375
train loss:  0.3063395321369171
train gradient:  0.16968826187074926
iteration : 10297
train acc:  0.8515625
train loss:  0.2988811433315277
train gradient:  0.13536013539904054
iteration : 10298
train acc:  0.8984375
train loss:  0.2528080344200134
train gradient:  0.10006242776748585
iteration : 10299
train acc:  0.8203125
train loss:  0.35393792390823364
train gradient:  0.15177038440742033
iteration : 10300
train acc:  0.8359375
train loss:  0.31664007902145386
train gradient:  0.16477496044514017
iteration : 10301
train acc:  0.84375
train loss:  0.3017053008079529
train gradient:  0.11378798758861178
iteration : 10302
train acc:  0.8359375
train loss:  0.3691697120666504
train gradient:  0.2659826788098677
iteration : 10303
train acc:  0.890625
train loss:  0.2636328935623169
train gradient:  0.12129992298228046
iteration : 10304
train acc:  0.78125
train loss:  0.42362767457962036
train gradient:  0.2064393004827353
iteration : 10305
train acc:  0.890625
train loss:  0.2746340036392212
train gradient:  0.15475309789018743
iteration : 10306
train acc:  0.8828125
train loss:  0.31896236538887024
train gradient:  0.37317958941253865
iteration : 10307
train acc:  0.875
train loss:  0.3469926416873932
train gradient:  0.1680246983383952
iteration : 10308
train acc:  0.8359375
train loss:  0.325539231300354
train gradient:  0.1713616959938542
iteration : 10309
train acc:  0.890625
train loss:  0.31598275899887085
train gradient:  0.1576104129320201
iteration : 10310
train acc:  0.8984375
train loss:  0.24359965324401855
train gradient:  0.09840769965535631
iteration : 10311
train acc:  0.8515625
train loss:  0.35111361742019653
train gradient:  0.2316886026215536
iteration : 10312
train acc:  0.875
train loss:  0.30807003378868103
train gradient:  0.10227643453324675
iteration : 10313
train acc:  0.8046875
train loss:  0.3193877935409546
train gradient:  0.176263187604478
iteration : 10314
train acc:  0.875
train loss:  0.2570032477378845
train gradient:  0.12859054829840635
iteration : 10315
train acc:  0.8984375
train loss:  0.21801325678825378
train gradient:  0.09949446358634369
iteration : 10316
train acc:  0.9140625
train loss:  0.2940809726715088
train gradient:  0.11916158025142135
iteration : 10317
train acc:  0.890625
train loss:  0.2839013934135437
train gradient:  0.14986467163668968
iteration : 10318
train acc:  0.90625
train loss:  0.2810803949832916
train gradient:  0.13517116254252515
iteration : 10319
train acc:  0.8515625
train loss:  0.34354764223098755
train gradient:  0.1894842547292254
iteration : 10320
train acc:  0.8828125
train loss:  0.2464011013507843
train gradient:  0.1448513370300562
iteration : 10321
train acc:  0.8515625
train loss:  0.3367805778980255
train gradient:  0.16386581293797048
iteration : 10322
train acc:  0.890625
train loss:  0.27620023488998413
train gradient:  0.18464227760689117
iteration : 10323
train acc:  0.8515625
train loss:  0.3224392235279083
train gradient:  0.1348177153220776
iteration : 10324
train acc:  0.828125
train loss:  0.468194842338562
train gradient:  0.2989031766862318
iteration : 10325
train acc:  0.890625
train loss:  0.27889859676361084
train gradient:  0.1695539013563144
iteration : 10326
train acc:  0.8203125
train loss:  0.35995158553123474
train gradient:  0.26936537160282337
iteration : 10327
train acc:  0.859375
train loss:  0.33538711071014404
train gradient:  0.14383261935847205
iteration : 10328
train acc:  0.8359375
train loss:  0.33503639698028564
train gradient:  0.158255717717805
iteration : 10329
train acc:  0.9296875
train loss:  0.25038325786590576
train gradient:  0.0915163472741434
iteration : 10330
train acc:  0.78125
train loss:  0.36329180002212524
train gradient:  0.16435943383326576
iteration : 10331
train acc:  0.8515625
train loss:  0.3293401896953583
train gradient:  0.15673130179394482
iteration : 10332
train acc:  0.8203125
train loss:  0.39858317375183105
train gradient:  0.23543762485979675
iteration : 10333
train acc:  0.859375
train loss:  0.35140132904052734
train gradient:  0.1654545377379788
iteration : 10334
train acc:  0.8828125
train loss:  0.312031626701355
train gradient:  0.146793071305851
iteration : 10335
train acc:  0.90625
train loss:  0.2269960641860962
train gradient:  0.10331556911679361
iteration : 10336
train acc:  0.8984375
train loss:  0.27178722620010376
train gradient:  0.1224977039813584
iteration : 10337
train acc:  0.84375
train loss:  0.3102194666862488
train gradient:  0.10397715745334012
iteration : 10338
train acc:  0.8203125
train loss:  0.40192970633506775
train gradient:  0.20740603305510386
iteration : 10339
train acc:  0.859375
train loss:  0.3184272050857544
train gradient:  0.14088659248326585
iteration : 10340
train acc:  0.8828125
train loss:  0.3162750005722046
train gradient:  0.18408721774766681
iteration : 10341
train acc:  0.8984375
train loss:  0.2644030451774597
train gradient:  0.1515309912085938
iteration : 10342
train acc:  0.859375
train loss:  0.3819296360015869
train gradient:  0.1510179390428994
iteration : 10343
train acc:  0.8515625
train loss:  0.3056267201900482
train gradient:  0.15063359578533775
iteration : 10344
train acc:  0.8359375
train loss:  0.3510371744632721
train gradient:  0.18352248012389266
iteration : 10345
train acc:  0.90625
train loss:  0.2873727083206177
train gradient:  0.10629535564335443
iteration : 10346
train acc:  0.890625
train loss:  0.26502180099487305
train gradient:  0.09525854582908035
iteration : 10347
train acc:  0.875
train loss:  0.2923365533351898
train gradient:  0.09841548100894794
iteration : 10348
train acc:  0.8828125
train loss:  0.32057318091392517
train gradient:  0.1357947287303113
iteration : 10349
train acc:  0.8828125
train loss:  0.2921734154224396
train gradient:  0.10708500282935669
iteration : 10350
train acc:  0.8359375
train loss:  0.3674814701080322
train gradient:  0.20748154338984354
iteration : 10351
train acc:  0.859375
train loss:  0.3401851952075958
train gradient:  0.14457327790122268
iteration : 10352
train acc:  0.8046875
train loss:  0.319893479347229
train gradient:  0.13025676548382045
iteration : 10353
train acc:  0.875
train loss:  0.278506875038147
train gradient:  0.17504119954013997
iteration : 10354
train acc:  0.8671875
train loss:  0.29381799697875977
train gradient:  0.1447296227363254
iteration : 10355
train acc:  0.8125
train loss:  0.4082677960395813
train gradient:  0.18479111935014622
iteration : 10356
train acc:  0.8828125
train loss:  0.25041860342025757
train gradient:  0.09911237999752871
iteration : 10357
train acc:  0.8203125
train loss:  0.3892105221748352
train gradient:  0.19953543273076885
iteration : 10358
train acc:  0.8203125
train loss:  0.4063374996185303
train gradient:  0.3345603974090095
iteration : 10359
train acc:  0.8046875
train loss:  0.32968422770500183
train gradient:  0.16161826438717722
iteration : 10360
train acc:  0.8828125
train loss:  0.3066665530204773
train gradient:  0.18226579040610508
iteration : 10361
train acc:  0.8125
train loss:  0.37053900957107544
train gradient:  0.28699432498029004
iteration : 10362
train acc:  0.8203125
train loss:  0.3730586767196655
train gradient:  0.2136262706519452
iteration : 10363
train acc:  0.890625
train loss:  0.3039212226867676
train gradient:  0.11283738808900144
iteration : 10364
train acc:  0.8671875
train loss:  0.3049623966217041
train gradient:  0.14862615779619603
iteration : 10365
train acc:  0.8828125
train loss:  0.2949548065662384
train gradient:  0.14421531067526208
iteration : 10366
train acc:  0.8359375
train loss:  0.3515200614929199
train gradient:  0.18904912073732924
iteration : 10367
train acc:  0.84375
train loss:  0.33482280373573303
train gradient:  0.18494840104502563
iteration : 10368
train acc:  0.890625
train loss:  0.2959703207015991
train gradient:  0.10129790606616125
iteration : 10369
train acc:  0.8359375
train loss:  0.3933372497558594
train gradient:  0.27161374213974543
iteration : 10370
train acc:  0.9296875
train loss:  0.23148389160633087
train gradient:  0.09024653508048344
iteration : 10371
train acc:  0.84375
train loss:  0.3336202800273895
train gradient:  0.1787615250561122
iteration : 10372
train acc:  0.7734375
train loss:  0.39497143030166626
train gradient:  0.2109789469082838
iteration : 10373
train acc:  0.8046875
train loss:  0.4180881679058075
train gradient:  0.3004591342801876
iteration : 10374
train acc:  0.8828125
train loss:  0.25880610942840576
train gradient:  0.10400519393203675
iteration : 10375
train acc:  0.859375
train loss:  0.2671475410461426
train gradient:  0.09645789233519658
iteration : 10376
train acc:  0.90625
train loss:  0.250021368265152
train gradient:  0.11253510336554223
iteration : 10377
train acc:  0.8515625
train loss:  0.3301894962787628
train gradient:  0.15029613245271833
iteration : 10378
train acc:  0.90625
train loss:  0.2760927081108093
train gradient:  0.08601524719678148
iteration : 10379
train acc:  0.8203125
train loss:  0.36901843547821045
train gradient:  0.24179018980839512
iteration : 10380
train acc:  0.8984375
train loss:  0.2676072418689728
train gradient:  0.09133566107167541
iteration : 10381
train acc:  0.859375
train loss:  0.3018457889556885
train gradient:  0.1552566880092251
iteration : 10382
train acc:  0.8515625
train loss:  0.29721885919570923
train gradient:  0.160649977487128
iteration : 10383
train acc:  0.8984375
train loss:  0.24420014023780823
train gradient:  0.1143588658520297
iteration : 10384
train acc:  0.8359375
train loss:  0.3553922474384308
train gradient:  0.17383114052247772
iteration : 10385
train acc:  0.875
train loss:  0.28798940777778625
train gradient:  0.1878452799650469
iteration : 10386
train acc:  0.9140625
train loss:  0.2843793034553528
train gradient:  0.0782752512795695
iteration : 10387
train acc:  0.8515625
train loss:  0.31402847170829773
train gradient:  0.1831877158096741
iteration : 10388
train acc:  0.8671875
train loss:  0.2906489968299866
train gradient:  0.1526022640403603
iteration : 10389
train acc:  0.859375
train loss:  0.3247050642967224
train gradient:  0.18510445738296247
iteration : 10390
train acc:  0.8984375
train loss:  0.29337793588638306
train gradient:  0.15054006090140926
iteration : 10391
train acc:  0.84375
train loss:  0.3029915988445282
train gradient:  0.14442167337382553
iteration : 10392
train acc:  0.859375
train loss:  0.30396026372909546
train gradient:  0.15088880154599263
iteration : 10393
train acc:  0.8125
train loss:  0.44068336486816406
train gradient:  0.23397273944010466
iteration : 10394
train acc:  0.84375
train loss:  0.30875062942504883
train gradient:  0.1259470441179034
iteration : 10395
train acc:  0.9453125
train loss:  0.20612625777721405
train gradient:  0.10592252574994826
iteration : 10396
train acc:  0.84375
train loss:  0.3458690643310547
train gradient:  0.1661663752128218
iteration : 10397
train acc:  0.8828125
train loss:  0.28651729226112366
train gradient:  0.10827825957539489
iteration : 10398
train acc:  0.8828125
train loss:  0.32239288091659546
train gradient:  0.17266633767824202
iteration : 10399
train acc:  0.8203125
train loss:  0.3806763291358948
train gradient:  0.21698337295239617
iteration : 10400
train acc:  0.875
train loss:  0.28270453214645386
train gradient:  0.12049619077808738
iteration : 10401
train acc:  0.8828125
train loss:  0.3392469584941864
train gradient:  0.15128091482873393
iteration : 10402
train acc:  0.828125
train loss:  0.32290029525756836
train gradient:  0.26014947837305946
iteration : 10403
train acc:  0.84375
train loss:  0.31627246737480164
train gradient:  0.1657298397753202
iteration : 10404
train acc:  0.8203125
train loss:  0.3997740149497986
train gradient:  0.21422059073874894
iteration : 10405
train acc:  0.890625
train loss:  0.2868368625640869
train gradient:  0.12194480797953489
iteration : 10406
train acc:  0.8671875
train loss:  0.3121601939201355
train gradient:  0.18568487591349425
iteration : 10407
train acc:  0.84375
train loss:  0.31899207830429077
train gradient:  0.17613716526477852
iteration : 10408
train acc:  0.8828125
train loss:  0.3059755861759186
train gradient:  0.12904712446223304
iteration : 10409
train acc:  0.796875
train loss:  0.46214789152145386
train gradient:  0.3908018962259464
iteration : 10410
train acc:  0.8203125
train loss:  0.3570946455001831
train gradient:  0.18083735244721344
iteration : 10411
train acc:  0.859375
train loss:  0.36899781227111816
train gradient:  0.2183966118363351
iteration : 10412
train acc:  0.828125
train loss:  0.32349467277526855
train gradient:  0.157797320093146
iteration : 10413
train acc:  0.8359375
train loss:  0.3672078847885132
train gradient:  0.18568599137958952
iteration : 10414
train acc:  0.828125
train loss:  0.3734084367752075
train gradient:  0.17142086237422838
iteration : 10415
train acc:  0.8984375
train loss:  0.23327574133872986
train gradient:  0.07759762536828052
iteration : 10416
train acc:  0.7890625
train loss:  0.40862807631492615
train gradient:  0.3089654581179132
iteration : 10417
train acc:  0.8515625
train loss:  0.2995133399963379
train gradient:  0.13492692733829215
iteration : 10418
train acc:  0.875
train loss:  0.3819704055786133
train gradient:  0.16469909549622724
iteration : 10419
train acc:  0.8125
train loss:  0.3632705807685852
train gradient:  0.1988768902399224
iteration : 10420
train acc:  0.8359375
train loss:  0.3324120044708252
train gradient:  0.185948580067063
iteration : 10421
train acc:  0.875
train loss:  0.32267463207244873
train gradient:  0.16316630937890278
iteration : 10422
train acc:  0.859375
train loss:  0.2932223081588745
train gradient:  0.11666199532111468
iteration : 10423
train acc:  0.8515625
train loss:  0.3049079477787018
train gradient:  0.1690316037852473
iteration : 10424
train acc:  0.8671875
train loss:  0.3424490988254547
train gradient:  0.16671017600226812
iteration : 10425
train acc:  0.8515625
train loss:  0.3355233371257782
train gradient:  0.1533135611987264
iteration : 10426
train acc:  0.90625
train loss:  0.27562451362609863
train gradient:  0.1131599567932061
iteration : 10427
train acc:  0.84375
train loss:  0.3262844979763031
train gradient:  0.12580590980846015
iteration : 10428
train acc:  0.8671875
train loss:  0.296030193567276
train gradient:  0.1305282812270146
iteration : 10429
train acc:  0.890625
train loss:  0.35841086506843567
train gradient:  0.30161444377573926
iteration : 10430
train acc:  0.796875
train loss:  0.43501991033554077
train gradient:  0.28626181827485053
iteration : 10431
train acc:  0.8515625
train loss:  0.37431269884109497
train gradient:  0.22123991513997626
iteration : 10432
train acc:  0.8125
train loss:  0.32282939553260803
train gradient:  0.19222559149507323
iteration : 10433
train acc:  0.84375
train loss:  0.3802035450935364
train gradient:  0.2036003090174294
iteration : 10434
train acc:  0.8515625
train loss:  0.325758159160614
train gradient:  0.1524980800064585
iteration : 10435
train acc:  0.8671875
train loss:  0.35625603795051575
train gradient:  0.1858735687498329
iteration : 10436
train acc:  0.859375
train loss:  0.30083537101745605
train gradient:  0.1318714270931926
iteration : 10437
train acc:  0.8203125
train loss:  0.3834777772426605
train gradient:  0.16545961489641095
iteration : 10438
train acc:  0.84375
train loss:  0.3194958567619324
train gradient:  0.15704623987665517
iteration : 10439
train acc:  0.8203125
train loss:  0.3304111361503601
train gradient:  0.15150895366061584
iteration : 10440
train acc:  0.84375
train loss:  0.34044885635375977
train gradient:  0.1396265253297985
iteration : 10441
train acc:  0.828125
train loss:  0.34354129433631897
train gradient:  0.145862562724263
iteration : 10442
train acc:  0.9296875
train loss:  0.23930880427360535
train gradient:  0.13460395574921807
iteration : 10443
train acc:  0.8984375
train loss:  0.28211867809295654
train gradient:  0.12105563093174189
iteration : 10444
train acc:  0.828125
train loss:  0.34531915187835693
train gradient:  0.1693875762854428
iteration : 10445
train acc:  0.875
train loss:  0.3181077837944031
train gradient:  0.13871953949200017
iteration : 10446
train acc:  0.875
train loss:  0.28481870889663696
train gradient:  0.123747599086891
iteration : 10447
train acc:  0.796875
train loss:  0.4834843873977661
train gradient:  0.27802895187035637
iteration : 10448
train acc:  0.8671875
train loss:  0.3164239227771759
train gradient:  0.13088733399944613
iteration : 10449
train acc:  0.8984375
train loss:  0.28691884875297546
train gradient:  0.15432266148452994
iteration : 10450
train acc:  0.8125
train loss:  0.39649683237075806
train gradient:  0.22007996260120077
iteration : 10451
train acc:  0.8359375
train loss:  0.36188071966171265
train gradient:  0.1768845758279221
iteration : 10452
train acc:  0.8828125
train loss:  0.2854861915111542
train gradient:  0.1283997680551065
iteration : 10453
train acc:  0.8984375
train loss:  0.24864833056926727
train gradient:  0.13833293486793302
iteration : 10454
train acc:  0.8359375
train loss:  0.3109397292137146
train gradient:  0.11986283555483188
iteration : 10455
train acc:  0.8515625
train loss:  0.2890551686286926
train gradient:  0.15054531529097054
iteration : 10456
train acc:  0.8515625
train loss:  0.3935127258300781
train gradient:  0.2453565496353906
iteration : 10457
train acc:  0.875
train loss:  0.3595402240753174
train gradient:  0.17109272203059722
iteration : 10458
train acc:  0.8515625
train loss:  0.3832530677318573
train gradient:  0.14372389472237945
iteration : 10459
train acc:  0.8515625
train loss:  0.30549895763397217
train gradient:  0.11876206343818552
iteration : 10460
train acc:  0.8359375
train loss:  0.36828961968421936
train gradient:  0.26372713394638936
iteration : 10461
train acc:  0.8515625
train loss:  0.29891300201416016
train gradient:  0.11586735712192826
iteration : 10462
train acc:  0.859375
train loss:  0.35731950402259827
train gradient:  0.1617804893904232
iteration : 10463
train acc:  0.859375
train loss:  0.2891419231891632
train gradient:  0.13843741259782527
iteration : 10464
train acc:  0.859375
train loss:  0.3406956195831299
train gradient:  0.20926206824506738
iteration : 10465
train acc:  0.875
train loss:  0.29658323526382446
train gradient:  0.09970171000783339
iteration : 10466
train acc:  0.859375
train loss:  0.3174847960472107
train gradient:  0.17715111062625333
iteration : 10467
train acc:  0.890625
train loss:  0.3049614131450653
train gradient:  0.16435279804592473
iteration : 10468
train acc:  0.8125
train loss:  0.4146198034286499
train gradient:  0.1967898011101193
iteration : 10469
train acc:  0.9375
train loss:  0.20792022347450256
train gradient:  0.08495109124079352
iteration : 10470
train acc:  0.8515625
train loss:  0.3243778944015503
train gradient:  0.15069690829350868
iteration : 10471
train acc:  0.84375
train loss:  0.304529070854187
train gradient:  0.2393160372530043
iteration : 10472
train acc:  0.8203125
train loss:  0.37799373269081116
train gradient:  0.1584597637689311
iteration : 10473
train acc:  0.8671875
train loss:  0.31258082389831543
train gradient:  0.0972957612251942
iteration : 10474
train acc:  0.8359375
train loss:  0.3287079930305481
train gradient:  0.1425182143811382
iteration : 10475
train acc:  0.8828125
train loss:  0.2781639099121094
train gradient:  0.16010895138453105
iteration : 10476
train acc:  0.8125
train loss:  0.3656977415084839
train gradient:  0.1626834403741198
iteration : 10477
train acc:  0.859375
train loss:  0.32565057277679443
train gradient:  0.15258833987383963
iteration : 10478
train acc:  0.8046875
train loss:  0.38669297099113464
train gradient:  0.17034727953658282
iteration : 10479
train acc:  0.875
train loss:  0.2682681083679199
train gradient:  0.11726680026320375
iteration : 10480
train acc:  0.890625
train loss:  0.30881965160369873
train gradient:  0.09312841377240166
iteration : 10481
train acc:  0.8515625
train loss:  0.3110879063606262
train gradient:  0.1580542877001438
iteration : 10482
train acc:  0.8515625
train loss:  0.33793967962265015
train gradient:  0.15983664973213985
iteration : 10483
train acc:  0.8515625
train loss:  0.34351229667663574
train gradient:  0.15985800437760572
iteration : 10484
train acc:  0.8671875
train loss:  0.31856414675712585
train gradient:  0.09709216318691474
iteration : 10485
train acc:  0.84375
train loss:  0.31426090002059937
train gradient:  0.11529773734054598
iteration : 10486
train acc:  0.828125
train loss:  0.34978875517845154
train gradient:  0.15659545350933946
iteration : 10487
train acc:  0.8671875
train loss:  0.32528918981552124
train gradient:  0.1647609837538458
iteration : 10488
train acc:  0.8828125
train loss:  0.2898813486099243
train gradient:  0.13390952121346245
iteration : 10489
train acc:  0.796875
train loss:  0.35691922903060913
train gradient:  0.1487727535326697
iteration : 10490
train acc:  0.8125
train loss:  0.3959023952484131
train gradient:  0.18088512575306503
iteration : 10491
train acc:  0.8515625
train loss:  0.32552459836006165
train gradient:  0.1177602935495064
iteration : 10492
train acc:  0.8203125
train loss:  0.3887419104576111
train gradient:  0.19302950197025326
iteration : 10493
train acc:  0.796875
train loss:  0.4597351551055908
train gradient:  0.3945731252107313
iteration : 10494
train acc:  0.8671875
train loss:  0.2747892737388611
train gradient:  0.09809037419110485
iteration : 10495
train acc:  0.890625
train loss:  0.2678338885307312
train gradient:  0.09930386241399379
iteration : 10496
train acc:  0.875
train loss:  0.2542768120765686
train gradient:  0.11064027012480211
iteration : 10497
train acc:  0.90625
train loss:  0.24593038856983185
train gradient:  0.08279162738263485
iteration : 10498
train acc:  0.890625
train loss:  0.27782657742500305
train gradient:  0.1087968905307006
iteration : 10499
train acc:  0.90625
train loss:  0.26843899488449097
train gradient:  0.09228489131354647
iteration : 10500
train acc:  0.890625
train loss:  0.25241348147392273
train gradient:  0.10910826444654034
iteration : 10501
train acc:  0.8515625
train loss:  0.3150692582130432
train gradient:  0.13273061269571795
iteration : 10502
train acc:  0.8046875
train loss:  0.34374791383743286
train gradient:  0.20714883522546262
iteration : 10503
train acc:  0.8515625
train loss:  0.3054957091808319
train gradient:  0.1201706952295373
iteration : 10504
train acc:  0.8828125
train loss:  0.2805008292198181
train gradient:  0.11185428309741677
iteration : 10505
train acc:  0.8671875
train loss:  0.2716871201992035
train gradient:  0.07799029471435999
iteration : 10506
train acc:  0.8671875
train loss:  0.2748800218105316
train gradient:  0.1234438119599381
iteration : 10507
train acc:  0.8515625
train loss:  0.29794907569885254
train gradient:  0.14960525198990554
iteration : 10508
train acc:  0.8984375
train loss:  0.27379536628723145
train gradient:  0.10553704598304932
iteration : 10509
train acc:  0.8671875
train loss:  0.27264028787612915
train gradient:  0.12708591214698325
iteration : 10510
train acc:  0.828125
train loss:  0.38600850105285645
train gradient:  0.22426923786886188
iteration : 10511
train acc:  0.796875
train loss:  0.4014897346496582
train gradient:  0.1648587128781595
iteration : 10512
train acc:  0.8828125
train loss:  0.29112130403518677
train gradient:  0.19718559949772813
iteration : 10513
train acc:  0.8828125
train loss:  0.28360456228256226
train gradient:  0.1551206466693072
iteration : 10514
train acc:  0.921875
train loss:  0.28515464067459106
train gradient:  0.12958507929589064
iteration : 10515
train acc:  0.875
train loss:  0.3052985668182373
train gradient:  0.19206336583158382
iteration : 10516
train acc:  0.8828125
train loss:  0.2664301097393036
train gradient:  0.10957510076158768
iteration : 10517
train acc:  0.8359375
train loss:  0.32438910007476807
train gradient:  0.1562666068390208
iteration : 10518
train acc:  0.921875
train loss:  0.2469167709350586
train gradient:  0.2584625222102676
iteration : 10519
train acc:  0.8671875
train loss:  0.38188570737838745
train gradient:  0.22551453016182732
iteration : 10520
train acc:  0.84375
train loss:  0.31650620698928833
train gradient:  0.19374076467179352
iteration : 10521
train acc:  0.859375
train loss:  0.2898579239845276
train gradient:  0.13968308317454742
iteration : 10522
train acc:  0.8203125
train loss:  0.39787983894348145
train gradient:  0.2688771845071683
iteration : 10523
train acc:  0.8828125
train loss:  0.2547842264175415
train gradient:  0.16386152254342923
iteration : 10524
train acc:  0.875
train loss:  0.30148783326148987
train gradient:  0.17188797759166657
iteration : 10525
train acc:  0.8515625
train loss:  0.3307945430278778
train gradient:  0.19089953680504657
iteration : 10526
train acc:  0.828125
train loss:  0.37794843316078186
train gradient:  0.16375918750964596
iteration : 10527
train acc:  0.8671875
train loss:  0.3399182856082916
train gradient:  0.11911190163248732
iteration : 10528
train acc:  0.875
train loss:  0.3165692090988159
train gradient:  0.13669807937744172
iteration : 10529
train acc:  0.8203125
train loss:  0.407596230506897
train gradient:  0.2151680709305658
iteration : 10530
train acc:  0.859375
train loss:  0.3815386891365051
train gradient:  0.18555578400092698
iteration : 10531
train acc:  0.859375
train loss:  0.280950129032135
train gradient:  0.15953573599323123
iteration : 10532
train acc:  0.8515625
train loss:  0.38990914821624756
train gradient:  0.17013364010615806
iteration : 10533
train acc:  0.84375
train loss:  0.36768993735313416
train gradient:  0.2061017358762064
iteration : 10534
train acc:  0.796875
train loss:  0.38281306624412537
train gradient:  0.20073488354315874
iteration : 10535
train acc:  0.8359375
train loss:  0.3067469894886017
train gradient:  0.12260037203200617
iteration : 10536
train acc:  0.8203125
train loss:  0.3687358498573303
train gradient:  0.20869792922246505
iteration : 10537
train acc:  0.8203125
train loss:  0.36681312322616577
train gradient:  0.15889588762685808
iteration : 10538
train acc:  0.828125
train loss:  0.37696605920791626
train gradient:  0.21848507806432027
iteration : 10539
train acc:  0.796875
train loss:  0.4174448251724243
train gradient:  0.21922843739838815
iteration : 10540
train acc:  0.8515625
train loss:  0.3012987971305847
train gradient:  0.18863925994266212
iteration : 10541
train acc:  0.890625
train loss:  0.30449193716049194
train gradient:  0.20528535579119628
iteration : 10542
train acc:  0.890625
train loss:  0.2648398280143738
train gradient:  0.145105843531845
iteration : 10543
train acc:  0.8359375
train loss:  0.3381889760494232
train gradient:  0.2098620648517741
iteration : 10544
train acc:  0.84375
train loss:  0.3635704219341278
train gradient:  0.16291535479375308
iteration : 10545
train acc:  0.8671875
train loss:  0.3626649081707001
train gradient:  0.18068197007583595
iteration : 10546
train acc:  0.8984375
train loss:  0.23779454827308655
train gradient:  0.08395137919811356
iteration : 10547
train acc:  0.859375
train loss:  0.3350735306739807
train gradient:  0.18894050023450376
iteration : 10548
train acc:  0.8515625
train loss:  0.302696168422699
train gradient:  0.1366605356330928
iteration : 10549
train acc:  0.796875
train loss:  0.43260085582733154
train gradient:  0.3346015371637735
iteration : 10550
train acc:  0.890625
train loss:  0.33585378527641296
train gradient:  0.13818787034744878
iteration : 10551
train acc:  0.8671875
train loss:  0.2989318370819092
train gradient:  0.12516831841713874
iteration : 10552
train acc:  0.859375
train loss:  0.4072069823741913
train gradient:  0.26907900414382235
iteration : 10553
train acc:  0.859375
train loss:  0.3289479613304138
train gradient:  0.2372086217828038
iteration : 10554
train acc:  0.828125
train loss:  0.4050092399120331
train gradient:  0.33285763550607783
iteration : 10555
train acc:  0.890625
train loss:  0.2739794850349426
train gradient:  0.12248418847704215
iteration : 10556
train acc:  0.7734375
train loss:  0.43880099058151245
train gradient:  0.20927220918128497
iteration : 10557
train acc:  0.9140625
train loss:  0.24488943815231323
train gradient:  0.11792139235284127
iteration : 10558
train acc:  0.8515625
train loss:  0.41490674018859863
train gradient:  0.34229312221955877
iteration : 10559
train acc:  0.875
train loss:  0.27113208174705505
train gradient:  0.12670498276977038
iteration : 10560
train acc:  0.859375
train loss:  0.26443788409233093
train gradient:  0.09914324211182106
iteration : 10561
train acc:  0.8125
train loss:  0.4323563873767853
train gradient:  0.2270237707614253
iteration : 10562
train acc:  0.859375
train loss:  0.3410463333129883
train gradient:  0.191662177349018
iteration : 10563
train acc:  0.875
train loss:  0.29876554012298584
train gradient:  0.13113369875296393
iteration : 10564
train acc:  0.859375
train loss:  0.349643737077713
train gradient:  0.16517361446500733
iteration : 10565
train acc:  0.890625
train loss:  0.2656906247138977
train gradient:  0.15846161021469574
iteration : 10566
train acc:  0.8515625
train loss:  0.31380850076675415
train gradient:  0.13157196467250196
iteration : 10567
train acc:  0.8046875
train loss:  0.38900837302207947
train gradient:  0.2167294693007678
iteration : 10568
train acc:  0.8671875
train loss:  0.2951495945453644
train gradient:  0.09514016476104359
iteration : 10569
train acc:  0.8671875
train loss:  0.32647454738616943
train gradient:  0.18527202268297543
iteration : 10570
train acc:  0.7890625
train loss:  0.39512407779693604
train gradient:  0.20088556781994246
iteration : 10571
train acc:  0.8359375
train loss:  0.358185738325119
train gradient:  0.12455963064908804
iteration : 10572
train acc:  0.890625
train loss:  0.33089765906333923
train gradient:  0.16114518886008927
iteration : 10573
train acc:  0.8125
train loss:  0.3841562569141388
train gradient:  0.20737762070805799
iteration : 10574
train acc:  0.9140625
train loss:  0.229267418384552
train gradient:  0.08275111848507305
iteration : 10575
train acc:  0.828125
train loss:  0.37099987268447876
train gradient:  0.125796507985933
iteration : 10576
train acc:  0.859375
train loss:  0.31518447399139404
train gradient:  0.2131501295383479
iteration : 10577
train acc:  0.8828125
train loss:  0.33331868052482605
train gradient:  0.24798854601402212
iteration : 10578
train acc:  0.828125
train loss:  0.3371012210845947
train gradient:  0.13782687679421252
iteration : 10579
train acc:  0.8828125
train loss:  0.3269868493080139
train gradient:  0.1662432864635835
iteration : 10580
train acc:  0.828125
train loss:  0.3265772759914398
train gradient:  0.1602017592430401
iteration : 10581
train acc:  0.859375
train loss:  0.3086916208267212
train gradient:  0.15408977011819716
iteration : 10582
train acc:  0.8828125
train loss:  0.2913132905960083
train gradient:  0.13102141312187762
iteration : 10583
train acc:  0.875
train loss:  0.31327182054519653
train gradient:  0.1570064405803329
iteration : 10584
train acc:  0.875
train loss:  0.29124176502227783
train gradient:  0.11687351149250151
iteration : 10585
train acc:  0.8515625
train loss:  0.29920703172683716
train gradient:  0.10944690247243546
iteration : 10586
train acc:  0.8359375
train loss:  0.35116153955459595
train gradient:  0.11772590041287766
iteration : 10587
train acc:  0.859375
train loss:  0.3324821889400482
train gradient:  0.1832026055782343
iteration : 10588
train acc:  0.84375
train loss:  0.3224007189273834
train gradient:  0.13412033284343877
iteration : 10589
train acc:  0.8671875
train loss:  0.2868892550468445
train gradient:  0.132667356193223
iteration : 10590
train acc:  0.8515625
train loss:  0.38586005568504333
train gradient:  0.1497794209580114
iteration : 10591
train acc:  0.8671875
train loss:  0.26780515909194946
train gradient:  0.12468799223477053
iteration : 10592
train acc:  0.8984375
train loss:  0.2585670053958893
train gradient:  0.10487911981026643
iteration : 10593
train acc:  0.8828125
train loss:  0.33694350719451904
train gradient:  0.1725721113100016
iteration : 10594
train acc:  0.8515625
train loss:  0.3772731125354767
train gradient:  0.2207959803371255
iteration : 10595
train acc:  0.8359375
train loss:  0.3576878011226654
train gradient:  0.15155783325456426
iteration : 10596
train acc:  0.8671875
train loss:  0.29717370867729187
train gradient:  0.14711499176422788
iteration : 10597
train acc:  0.8828125
train loss:  0.30728617310523987
train gradient:  0.10894170137310358
iteration : 10598
train acc:  0.8515625
train loss:  0.3520434498786926
train gradient:  0.18251139470968578
iteration : 10599
train acc:  0.8203125
train loss:  0.39602458477020264
train gradient:  0.1969448086422585
iteration : 10600
train acc:  0.8203125
train loss:  0.4087238013744354
train gradient:  0.25415789732253313
iteration : 10601
train acc:  0.859375
train loss:  0.320754736661911
train gradient:  0.15299332219008044
iteration : 10602
train acc:  0.8046875
train loss:  0.324427992105484
train gradient:  0.201860957425813
iteration : 10603
train acc:  0.8046875
train loss:  0.41825470328330994
train gradient:  0.23543696728084998
iteration : 10604
train acc:  0.84375
train loss:  0.33309313654899597
train gradient:  0.15029214520963263
iteration : 10605
train acc:  0.828125
train loss:  0.36432576179504395
train gradient:  0.23171474459800603
iteration : 10606
train acc:  0.8984375
train loss:  0.30852189660072327
train gradient:  0.11854955384652266
iteration : 10607
train acc:  0.9140625
train loss:  0.22872844338417053
train gradient:  0.0971215080147548
iteration : 10608
train acc:  0.9140625
train loss:  0.26955491304397583
train gradient:  0.0885170799173188
iteration : 10609
train acc:  0.8359375
train loss:  0.31768810749053955
train gradient:  0.13265464082305362
iteration : 10610
train acc:  0.8359375
train loss:  0.33434855937957764
train gradient:  0.14681291199881003
iteration : 10611
train acc:  0.8359375
train loss:  0.3476516902446747
train gradient:  0.1395964442079765
iteration : 10612
train acc:  0.8515625
train loss:  0.31441962718963623
train gradient:  0.14602895481712644
iteration : 10613
train acc:  0.8359375
train loss:  0.36672455072402954
train gradient:  0.1837444258806488
iteration : 10614
train acc:  0.8515625
train loss:  0.3066166043281555
train gradient:  0.11668578318966423
iteration : 10615
train acc:  0.84375
train loss:  0.3383799195289612
train gradient:  0.18370595700176562
iteration : 10616
train acc:  0.8671875
train loss:  0.30237409472465515
train gradient:  0.14915526719305428
iteration : 10617
train acc:  0.859375
train loss:  0.28761446475982666
train gradient:  0.1748421772929422
iteration : 10618
train acc:  0.8515625
train loss:  0.327393501996994
train gradient:  0.15596298931023828
iteration : 10619
train acc:  0.8515625
train loss:  0.3400050401687622
train gradient:  0.18805470257388957
iteration : 10620
train acc:  0.8515625
train loss:  0.3604111671447754
train gradient:  0.1755642951247089
iteration : 10621
train acc:  0.84375
train loss:  0.33093559741973877
train gradient:  0.1346440310703071
iteration : 10622
train acc:  0.8046875
train loss:  0.3857126832008362
train gradient:  0.16453626918409955
iteration : 10623
train acc:  0.890625
train loss:  0.3165655732154846
train gradient:  0.1253150535675123
iteration : 10624
train acc:  0.875
train loss:  0.30445563793182373
train gradient:  0.13328002689952198
iteration : 10625
train acc:  0.8984375
train loss:  0.25277838110923767
train gradient:  0.09427669191128642
iteration : 10626
train acc:  0.875
train loss:  0.256391704082489
train gradient:  0.13966761677860717
iteration : 10627
train acc:  0.8515625
train loss:  0.3756551742553711
train gradient:  0.18215733444797466
iteration : 10628
train acc:  0.9375
train loss:  0.19848762452602386
train gradient:  0.06595372993713337
iteration : 10629
train acc:  0.890625
train loss:  0.304935485124588
train gradient:  0.13094222641471892
iteration : 10630
train acc:  0.8359375
train loss:  0.33415454626083374
train gradient:  0.15852755967772944
iteration : 10631
train acc:  0.8984375
train loss:  0.2646659016609192
train gradient:  0.1310867477037971
iteration : 10632
train acc:  0.796875
train loss:  0.3670842945575714
train gradient:  0.1692513965960765
iteration : 10633
train acc:  0.8671875
train loss:  0.2803468108177185
train gradient:  0.13126690054072698
iteration : 10634
train acc:  0.90625
train loss:  0.23214420676231384
train gradient:  0.10246475865488687
iteration : 10635
train acc:  0.8359375
train loss:  0.3912261128425598
train gradient:  0.15692069323188448
iteration : 10636
train acc:  0.84375
train loss:  0.3565261960029602
train gradient:  0.28064136685759744
iteration : 10637
train acc:  0.8359375
train loss:  0.3829193115234375
train gradient:  0.18673180609497583
iteration : 10638
train acc:  0.8984375
train loss:  0.23186244070529938
train gradient:  0.12119059607953196
iteration : 10639
train acc:  0.796875
train loss:  0.419750839471817
train gradient:  0.22751569559457074
iteration : 10640
train acc:  0.9296875
train loss:  0.24277208745479584
train gradient:  0.07770005009222605
iteration : 10641
train acc:  0.84375
train loss:  0.38040900230407715
train gradient:  0.17387720282804436
iteration : 10642
train acc:  0.8828125
train loss:  0.26408737897872925
train gradient:  0.12230072731746279
iteration : 10643
train acc:  0.8828125
train loss:  0.2330807000398636
train gradient:  0.1404058201124771
iteration : 10644
train acc:  0.8828125
train loss:  0.3017028570175171
train gradient:  0.1445524041290203
iteration : 10645
train acc:  0.8359375
train loss:  0.3939662575721741
train gradient:  0.1767506711565202
iteration : 10646
train acc:  0.8984375
train loss:  0.2872471809387207
train gradient:  0.09347836597730448
iteration : 10647
train acc:  0.8515625
train loss:  0.2853568494319916
train gradient:  0.1168804398694194
iteration : 10648
train acc:  0.796875
train loss:  0.390644371509552
train gradient:  0.23626270251647372
iteration : 10649
train acc:  0.9140625
train loss:  0.25191283226013184
train gradient:  0.0855867616410748
iteration : 10650
train acc:  0.875
train loss:  0.33177173137664795
train gradient:  0.20564699830822275
iteration : 10651
train acc:  0.7890625
train loss:  0.4254535138607025
train gradient:  0.19076992182478378
iteration : 10652
train acc:  0.8828125
train loss:  0.2935415208339691
train gradient:  0.1910349648190321
iteration : 10653
train acc:  0.8828125
train loss:  0.28336137533187866
train gradient:  0.10672668643081303
iteration : 10654
train acc:  0.8359375
train loss:  0.3468726873397827
train gradient:  0.15913938856594165
iteration : 10655
train acc:  0.7890625
train loss:  0.3586823642253876
train gradient:  0.21869387500737747
iteration : 10656
train acc:  0.8203125
train loss:  0.403585284948349
train gradient:  0.15481144103121652
iteration : 10657
train acc:  0.8828125
train loss:  0.2597026526927948
train gradient:  0.14238126489955355
iteration : 10658
train acc:  0.9140625
train loss:  0.2858263850212097
train gradient:  0.13740213156314693
iteration : 10659
train acc:  0.8828125
train loss:  0.3534667491912842
train gradient:  0.1682529307252274
iteration : 10660
train acc:  0.84375
train loss:  0.3231297433376312
train gradient:  0.14667957398504147
iteration : 10661
train acc:  0.8984375
train loss:  0.31251147389411926
train gradient:  0.11891731942801613
iteration : 10662
train acc:  0.9140625
train loss:  0.23890122771263123
train gradient:  0.13417650360575356
iteration : 10663
train acc:  0.875
train loss:  0.3192588686943054
train gradient:  0.18846723753653732
iteration : 10664
train acc:  0.875
train loss:  0.29017525911331177
train gradient:  0.11720720846802495
iteration : 10665
train acc:  0.8828125
train loss:  0.3192979395389557
train gradient:  0.1335599467271678
iteration : 10666
train acc:  0.8203125
train loss:  0.38310104608535767
train gradient:  0.19631252510247377
iteration : 10667
train acc:  0.8671875
train loss:  0.27967727184295654
train gradient:  0.12881152932489387
iteration : 10668
train acc:  0.796875
train loss:  0.3984670042991638
train gradient:  0.20325845638440102
iteration : 10669
train acc:  0.8984375
train loss:  0.2667161822319031
train gradient:  0.15482173479935696
iteration : 10670
train acc:  0.8046875
train loss:  0.3949812054634094
train gradient:  0.21261704751865426
iteration : 10671
train acc:  0.8515625
train loss:  0.35219255089759827
train gradient:  0.20605331080418404
iteration : 10672
train acc:  0.828125
train loss:  0.3476001024246216
train gradient:  0.21208868965244881
iteration : 10673
train acc:  0.8671875
train loss:  0.30598920583724976
train gradient:  0.1791088801270416
iteration : 10674
train acc:  0.8984375
train loss:  0.29112085700035095
train gradient:  0.09730564208183852
iteration : 10675
train acc:  0.8046875
train loss:  0.3573686480522156
train gradient:  0.1341056661859376
iteration : 10676
train acc:  0.9296875
train loss:  0.21372169256210327
train gradient:  0.1687198900916278
iteration : 10677
train acc:  0.859375
train loss:  0.30115264654159546
train gradient:  0.17866242747462707
iteration : 10678
train acc:  0.8671875
train loss:  0.36072447896003723
train gradient:  0.20687778599848375
iteration : 10679
train acc:  0.8828125
train loss:  0.2484578788280487
train gradient:  0.2396391442251734
iteration : 10680
train acc:  0.890625
train loss:  0.29074156284332275
train gradient:  0.1847310243226603
iteration : 10681
train acc:  0.8203125
train loss:  0.43076077103614807
train gradient:  0.26525566335465456
iteration : 10682
train acc:  0.875
train loss:  0.3012073040008545
train gradient:  0.11758483992298388
iteration : 10683
train acc:  0.796875
train loss:  0.41960012912750244
train gradient:  0.2175010893747308
iteration : 10684
train acc:  0.875
train loss:  0.31292060017585754
train gradient:  0.14407671229709906
iteration : 10685
train acc:  0.859375
train loss:  0.30847272276878357
train gradient:  0.16633306941800433
iteration : 10686
train acc:  0.8515625
train loss:  0.29124581813812256
train gradient:  0.12634808820168325
iteration : 10687
train acc:  0.8671875
train loss:  0.29086917638778687
train gradient:  0.32951702499111213
iteration : 10688
train acc:  0.84375
train loss:  0.28266817331314087
train gradient:  0.09356643191649215
iteration : 10689
train acc:  0.859375
train loss:  0.2876623272895813
train gradient:  0.16229449404888147
iteration : 10690
train acc:  0.8125
train loss:  0.42930203676223755
train gradient:  0.2003351465541895
iteration : 10691
train acc:  0.9140625
train loss:  0.255531370639801
train gradient:  0.18146084908673532
iteration : 10692
train acc:  0.8984375
train loss:  0.25024113059043884
train gradient:  0.114566814155237
iteration : 10693
train acc:  0.859375
train loss:  0.3173596262931824
train gradient:  0.11727588904438054
iteration : 10694
train acc:  0.8671875
train loss:  0.3022119104862213
train gradient:  0.13638526581998492
iteration : 10695
train acc:  0.890625
train loss:  0.2968502640724182
train gradient:  0.11034340403440715
iteration : 10696
train acc:  0.828125
train loss:  0.34488940238952637
train gradient:  0.1510207557742695
iteration : 10697
train acc:  0.890625
train loss:  0.28550422191619873
train gradient:  0.10191782438562039
iteration : 10698
train acc:  0.8359375
train loss:  0.3823394179344177
train gradient:  0.21839730733691784
iteration : 10699
train acc:  0.890625
train loss:  0.28192925453186035
train gradient:  0.10874884625453272
iteration : 10700
train acc:  0.8515625
train loss:  0.4088956415653229
train gradient:  0.21875429304764762
iteration : 10701
train acc:  0.84375
train loss:  0.32136163115501404
train gradient:  0.15959133040298046
iteration : 10702
train acc:  0.8671875
train loss:  0.3293616473674774
train gradient:  0.2162184095476396
iteration : 10703
train acc:  0.8515625
train loss:  0.3378963768482208
train gradient:  0.13851778279707433
iteration : 10704
train acc:  0.9375
train loss:  0.23404599726200104
train gradient:  0.06956020861846447
iteration : 10705
train acc:  0.890625
train loss:  0.2943677008152008
train gradient:  0.15757315871716426
iteration : 10706
train acc:  0.828125
train loss:  0.3974050283432007
train gradient:  0.19128742840531748
iteration : 10707
train acc:  0.9296875
train loss:  0.2135537564754486
train gradient:  0.09316866111624184
iteration : 10708
train acc:  0.8359375
train loss:  0.3527541756629944
train gradient:  0.31322161241961277
iteration : 10709
train acc:  0.84375
train loss:  0.35902488231658936
train gradient:  0.29649788018242446
iteration : 10710
train acc:  0.859375
train loss:  0.35589247941970825
train gradient:  0.16383630483873643
iteration : 10711
train acc:  0.8359375
train loss:  0.3153795599937439
train gradient:  0.1570141291206124
iteration : 10712
train acc:  0.8984375
train loss:  0.2619575262069702
train gradient:  0.08811096279125068
iteration : 10713
train acc:  0.8046875
train loss:  0.4787728190422058
train gradient:  0.1998758958311953
iteration : 10714
train acc:  0.8984375
train loss:  0.2620511054992676
train gradient:  0.10270958606195664
iteration : 10715
train acc:  0.890625
train loss:  0.26575198769569397
train gradient:  0.15260023593976374
iteration : 10716
train acc:  0.859375
train loss:  0.3165593147277832
train gradient:  0.16272459221521213
iteration : 10717
train acc:  0.8671875
train loss:  0.3399830460548401
train gradient:  0.28826703310512913
iteration : 10718
train acc:  0.8125
train loss:  0.3766499161720276
train gradient:  0.24625407833661342
iteration : 10719
train acc:  0.84375
train loss:  0.33537882566452026
train gradient:  0.13053824185692936
iteration : 10720
train acc:  0.875
train loss:  0.3331027030944824
train gradient:  0.16667763310161543
iteration : 10721
train acc:  0.875
train loss:  0.3271394968032837
train gradient:  0.1760518186630577
iteration : 10722
train acc:  0.8984375
train loss:  0.2264595925807953
train gradient:  0.10724682524226138
iteration : 10723
train acc:  0.8671875
train loss:  0.3064517378807068
train gradient:  0.13044847948125654
iteration : 10724
train acc:  0.859375
train loss:  0.31386154890060425
train gradient:  0.11254160867741668
iteration : 10725
train acc:  0.9375
train loss:  0.22503240406513214
train gradient:  0.0978751195561989
iteration : 10726
train acc:  0.9140625
train loss:  0.2280343919992447
train gradient:  0.11173170740752239
iteration : 10727
train acc:  0.8515625
train loss:  0.33322349190711975
train gradient:  0.16131142500198062
iteration : 10728
train acc:  0.8515625
train loss:  0.3305106461048126
train gradient:  0.17300000387154457
iteration : 10729
train acc:  0.9453125
train loss:  0.1783897876739502
train gradient:  0.06718970236536444
iteration : 10730
train acc:  0.84375
train loss:  0.3119271397590637
train gradient:  0.087477492731904
iteration : 10731
train acc:  0.84375
train loss:  0.3679206073284149
train gradient:  0.1605524803767805
iteration : 10732
train acc:  0.828125
train loss:  0.37400007247924805
train gradient:  0.24865232692757308
iteration : 10733
train acc:  0.890625
train loss:  0.2657546401023865
train gradient:  0.12531995639736296
iteration : 10734
train acc:  0.84375
train loss:  0.32922640442848206
train gradient:  0.16688316267079212
iteration : 10735
train acc:  0.8671875
train loss:  0.30346864461898804
train gradient:  0.1479785578202603
iteration : 10736
train acc:  0.890625
train loss:  0.27554625272750854
train gradient:  0.15310291349655786
iteration : 10737
train acc:  0.8671875
train loss:  0.29513072967529297
train gradient:  0.2005754532263771
iteration : 10738
train acc:  0.84375
train loss:  0.3732989430427551
train gradient:  0.18272981517279524
iteration : 10739
train acc:  0.8828125
train loss:  0.2759479880332947
train gradient:  0.16104667499218056
iteration : 10740
train acc:  0.8671875
train loss:  0.32326623797416687
train gradient:  0.16613680690860014
iteration : 10741
train acc:  0.859375
train loss:  0.28668156266212463
train gradient:  0.16866425232840898
iteration : 10742
train acc:  0.84375
train loss:  0.31385213136672974
train gradient:  0.16492258670175053
iteration : 10743
train acc:  0.84375
train loss:  0.3473880887031555
train gradient:  0.16125952022870232
iteration : 10744
train acc:  0.875
train loss:  0.33480459451675415
train gradient:  0.17548143502673927
iteration : 10745
train acc:  0.828125
train loss:  0.3493690490722656
train gradient:  0.21603302554287027
iteration : 10746
train acc:  0.8203125
train loss:  0.3559074401855469
train gradient:  0.1428817701323867
iteration : 10747
train acc:  0.875
train loss:  0.3135804533958435
train gradient:  0.15135876217437447
iteration : 10748
train acc:  0.9140625
train loss:  0.2877785563468933
train gradient:  0.12937916955986242
iteration : 10749
train acc:  0.890625
train loss:  0.30542925000190735
train gradient:  0.14764142942324215
iteration : 10750
train acc:  0.875
train loss:  0.3565788269042969
train gradient:  0.27534959068938675
iteration : 10751
train acc:  0.84375
train loss:  0.3441823720932007
train gradient:  0.1602432614790411
iteration : 10752
train acc:  0.8671875
train loss:  0.3559604287147522
train gradient:  0.15529600018250628
iteration : 10753
train acc:  0.84375
train loss:  0.31702524423599243
train gradient:  0.15033504116524254
iteration : 10754
train acc:  0.828125
train loss:  0.33391866087913513
train gradient:  0.16655341634895982
iteration : 10755
train acc:  0.8046875
train loss:  0.3390295207500458
train gradient:  0.19363808300661733
iteration : 10756
train acc:  0.8671875
train loss:  0.3271092474460602
train gradient:  0.12114164648962436
iteration : 10757
train acc:  0.8828125
train loss:  0.26117587089538574
train gradient:  0.13369773864026327
iteration : 10758
train acc:  0.875
train loss:  0.2772732973098755
train gradient:  0.1075904469056674
iteration : 10759
train acc:  0.890625
train loss:  0.24937182664871216
train gradient:  0.12620287985659073
iteration : 10760
train acc:  0.8828125
train loss:  0.29394617676734924
train gradient:  0.11149716946440279
iteration : 10761
train acc:  0.8671875
train loss:  0.297995924949646
train gradient:  0.16166276492417458
iteration : 10762
train acc:  0.8203125
train loss:  0.3531409204006195
train gradient:  0.13703164539263835
iteration : 10763
train acc:  0.8984375
train loss:  0.3153236210346222
train gradient:  0.15188732999892018
iteration : 10764
train acc:  0.890625
train loss:  0.2441151738166809
train gradient:  0.11414372285415891
iteration : 10765
train acc:  0.8828125
train loss:  0.28826117515563965
train gradient:  0.16610543594807442
iteration : 10766
train acc:  0.84375
train loss:  0.3348986506462097
train gradient:  0.1534106385942532
iteration : 10767
train acc:  0.84375
train loss:  0.37865501642227173
train gradient:  0.25163679452485105
iteration : 10768
train acc:  0.859375
train loss:  0.33201152086257935
train gradient:  0.19640372281702545
iteration : 10769
train acc:  0.8203125
train loss:  0.3907144069671631
train gradient:  0.21088176912033058
iteration : 10770
train acc:  0.859375
train loss:  0.3169538974761963
train gradient:  0.22874945602674596
iteration : 10771
train acc:  0.890625
train loss:  0.24017246067523956
train gradient:  0.12570548473020168
iteration : 10772
train acc:  0.8359375
train loss:  0.35810694098472595
train gradient:  0.17573055889424233
iteration : 10773
train acc:  0.875
train loss:  0.26370659470558167
train gradient:  0.12026608170522542
iteration : 10774
train acc:  0.828125
train loss:  0.3171452283859253
train gradient:  0.14324391011775972
iteration : 10775
train acc:  0.859375
train loss:  0.33122169971466064
train gradient:  0.16246481655981013
iteration : 10776
train acc:  0.875
train loss:  0.30509817600250244
train gradient:  0.15228346744732463
iteration : 10777
train acc:  0.859375
train loss:  0.3373296856880188
train gradient:  0.17072702567208614
iteration : 10778
train acc:  0.84375
train loss:  0.3622210919857025
train gradient:  0.18422237419045046
iteration : 10779
train acc:  0.8828125
train loss:  0.28079742193222046
train gradient:  0.11021659541122186
iteration : 10780
train acc:  0.890625
train loss:  0.27052173018455505
train gradient:  0.16399194887250312
iteration : 10781
train acc:  0.8359375
train loss:  0.3451843559741974
train gradient:  0.15433316264938887
iteration : 10782
train acc:  0.8671875
train loss:  0.38277924060821533
train gradient:  0.1489085386668831
iteration : 10783
train acc:  0.890625
train loss:  0.32311272621154785
train gradient:  0.20542787494798204
iteration : 10784
train acc:  0.890625
train loss:  0.264492005109787
train gradient:  0.11771174053517333
iteration : 10785
train acc:  0.8671875
train loss:  0.33443742990493774
train gradient:  0.17451428712781122
iteration : 10786
train acc:  0.8359375
train loss:  0.3622727394104004
train gradient:  0.19519248333770262
iteration : 10787
train acc:  0.890625
train loss:  0.2736285328865051
train gradient:  0.13524577565781143
iteration : 10788
train acc:  0.859375
train loss:  0.30140799283981323
train gradient:  0.13540703485252076
iteration : 10789
train acc:  0.8046875
train loss:  0.4030769467353821
train gradient:  0.32450139669215433
iteration : 10790
train acc:  0.828125
train loss:  0.4182937741279602
train gradient:  0.2785950244906553
iteration : 10791
train acc:  0.8515625
train loss:  0.38339531421661377
train gradient:  0.19675555062972122
iteration : 10792
train acc:  0.8671875
train loss:  0.27007073163986206
train gradient:  0.10502317720247506
iteration : 10793
train acc:  0.828125
train loss:  0.3505023717880249
train gradient:  0.17379620675954777
iteration : 10794
train acc:  0.828125
train loss:  0.34537458419799805
train gradient:  0.13669444821313148
iteration : 10795
train acc:  0.8671875
train loss:  0.3169936239719391
train gradient:  0.12609896875560944
iteration : 10796
train acc:  0.8671875
train loss:  0.3368162214756012
train gradient:  0.13261465654360222
iteration : 10797
train acc:  0.8125
train loss:  0.41774532198905945
train gradient:  0.22163991317117146
iteration : 10798
train acc:  0.828125
train loss:  0.40567663311958313
train gradient:  0.25340572428097907
iteration : 10799
train acc:  0.875
train loss:  0.2887028753757477
train gradient:  0.14260047388066277
iteration : 10800
train acc:  0.9140625
train loss:  0.21689997613430023
train gradient:  0.08212831058641243
iteration : 10801
train acc:  0.828125
train loss:  0.36752596497535706
train gradient:  0.1512329610185003
iteration : 10802
train acc:  0.8671875
train loss:  0.29114383459091187
train gradient:  0.14169548492768289
iteration : 10803
train acc:  0.875
train loss:  0.31571701169013977
train gradient:  0.14567779127774
iteration : 10804
train acc:  0.8359375
train loss:  0.32827144861221313
train gradient:  0.15917261998386123
iteration : 10805
train acc:  0.8359375
train loss:  0.3208448886871338
train gradient:  0.12256603862340477
iteration : 10806
train acc:  0.828125
train loss:  0.3698413074016571
train gradient:  0.2987353213493595
iteration : 10807
train acc:  0.765625
train loss:  0.40673983097076416
train gradient:  0.19920379570047989
iteration : 10808
train acc:  0.78125
train loss:  0.4110408127307892
train gradient:  0.3956283495483223
iteration : 10809
train acc:  0.828125
train loss:  0.348490446805954
train gradient:  0.1621989598461815
iteration : 10810
train acc:  0.8515625
train loss:  0.3080251216888428
train gradient:  0.16536660954524562
iteration : 10811
train acc:  0.8515625
train loss:  0.35663920640945435
train gradient:  0.16871049447047065
iteration : 10812
train acc:  0.8671875
train loss:  0.29638150334358215
train gradient:  0.12152227738150227
iteration : 10813
train acc:  0.890625
train loss:  0.2838398218154907
train gradient:  0.11006541980898053
iteration : 10814
train acc:  0.78125
train loss:  0.39823824167251587
train gradient:  0.2379052647748492
iteration : 10815
train acc:  0.7734375
train loss:  0.4538171887397766
train gradient:  0.24054566739826916
iteration : 10816
train acc:  0.8671875
train loss:  0.3349412679672241
train gradient:  0.1664313892165617
iteration : 10817
train acc:  0.8203125
train loss:  0.35494980216026306
train gradient:  0.16424068926377822
iteration : 10818
train acc:  0.8203125
train loss:  0.36120396852493286
train gradient:  0.16321072516450444
iteration : 10819
train acc:  0.890625
train loss:  0.2894301414489746
train gradient:  0.13772237978626228
iteration : 10820
train acc:  0.828125
train loss:  0.2879047393798828
train gradient:  0.10559248123597248
iteration : 10821
train acc:  0.84375
train loss:  0.34298789501190186
train gradient:  0.15080304757240603
iteration : 10822
train acc:  0.859375
train loss:  0.34208738803863525
train gradient:  0.16795165668745843
iteration : 10823
train acc:  0.8671875
train loss:  0.28405502438545227
train gradient:  0.11022783849606312
iteration : 10824
train acc:  0.9140625
train loss:  0.24384671449661255
train gradient:  0.17456673133573108
iteration : 10825
train acc:  0.890625
train loss:  0.25325876474380493
train gradient:  0.1075674707323713
iteration : 10826
train acc:  0.8828125
train loss:  0.3084767162799835
train gradient:  0.12471811062088314
iteration : 10827
train acc:  0.84375
train loss:  0.37566643953323364
train gradient:  0.26991169181010544
iteration : 10828
train acc:  0.859375
train loss:  0.26548832654953003
train gradient:  0.12346696337303956
iteration : 10829
train acc:  0.8828125
train loss:  0.28900057077407837
train gradient:  0.13224617664938668
iteration : 10830
train acc:  0.828125
train loss:  0.42967689037323
train gradient:  0.5065066333691273
iteration : 10831
train acc:  0.75
train loss:  0.5441441535949707
train gradient:  0.34881017477581894
iteration : 10832
train acc:  0.8984375
train loss:  0.2762008607387543
train gradient:  0.08115691053685617
iteration : 10833
train acc:  0.84375
train loss:  0.3501001000404358
train gradient:  0.1517119076926664
iteration : 10834
train acc:  0.8671875
train loss:  0.27658966183662415
train gradient:  0.11801035368707292
iteration : 10835
train acc:  0.8828125
train loss:  0.2553662657737732
train gradient:  0.07851780564280586
iteration : 10836
train acc:  0.828125
train loss:  0.33288174867630005
train gradient:  0.1557170262013082
iteration : 10837
train acc:  0.8203125
train loss:  0.3493826687335968
train gradient:  0.14253142116458595
iteration : 10838
train acc:  0.8984375
train loss:  0.27108991146087646
train gradient:  0.13884409291315147
iteration : 10839
train acc:  0.8828125
train loss:  0.3256624937057495
train gradient:  0.12288621831407498
iteration : 10840
train acc:  0.828125
train loss:  0.39642664790153503
train gradient:  0.16157060088726571
iteration : 10841
train acc:  0.875
train loss:  0.3024940490722656
train gradient:  0.20064852040845113
iteration : 10842
train acc:  0.859375
train loss:  0.3531385660171509
train gradient:  0.14556703689333522
iteration : 10843
train acc:  0.875
train loss:  0.30801159143447876
train gradient:  0.144026743798015
iteration : 10844
train acc:  0.8359375
train loss:  0.378132164478302
train gradient:  0.2268145689947997
iteration : 10845
train acc:  0.890625
train loss:  0.26379525661468506
train gradient:  0.11079493542606574
iteration : 10846
train acc:  0.875
train loss:  0.2676846385002136
train gradient:  0.1063377672924651
iteration : 10847
train acc:  0.8046875
train loss:  0.3730986416339874
train gradient:  0.15658436591156907
iteration : 10848
train acc:  0.8671875
train loss:  0.3208177983760834
train gradient:  0.1414330882186282
iteration : 10849
train acc:  0.859375
train loss:  0.32080090045928955
train gradient:  0.15320309750919855
iteration : 10850
train acc:  0.8203125
train loss:  0.33998942375183105
train gradient:  0.17823095931295296
iteration : 10851
train acc:  0.828125
train loss:  0.40447986125946045
train gradient:  0.23291065970458202
iteration : 10852
train acc:  0.84375
train loss:  0.34559667110443115
train gradient:  0.21658068282909607
iteration : 10853
train acc:  0.890625
train loss:  0.3073408603668213
train gradient:  0.09363507389663367
iteration : 10854
train acc:  0.8359375
train loss:  0.34100058674812317
train gradient:  0.12394994869444591
iteration : 10855
train acc:  0.9296875
train loss:  0.25177857279777527
train gradient:  0.15671893746752188
iteration : 10856
train acc:  0.875
train loss:  0.2654399871826172
train gradient:  0.10667648990942963
iteration : 10857
train acc:  0.8359375
train loss:  0.34535032510757446
train gradient:  0.14481301788858608
iteration : 10858
train acc:  0.859375
train loss:  0.38361334800720215
train gradient:  0.2143653078740537
iteration : 10859
train acc:  0.859375
train loss:  0.32271093130111694
train gradient:  0.16952620322070322
iteration : 10860
train acc:  0.8203125
train loss:  0.32801300287246704
train gradient:  0.13201572754433571
iteration : 10861
train acc:  0.84375
train loss:  0.37609848380088806
train gradient:  0.29032720984526134
iteration : 10862
train acc:  0.796875
train loss:  0.39006417989730835
train gradient:  0.256201105878175
iteration : 10863
train acc:  0.890625
train loss:  0.3075355291366577
train gradient:  0.1819460835958262
iteration : 10864
train acc:  0.90625
train loss:  0.24643096327781677
train gradient:  0.08613261793487442
iteration : 10865
train acc:  0.875
train loss:  0.26640596985816956
train gradient:  0.12299118315299788
iteration : 10866
train acc:  0.84375
train loss:  0.3272457420825958
train gradient:  0.19226160159578476
iteration : 10867
train acc:  0.9296875
train loss:  0.2486468404531479
train gradient:  0.13710932073592946
iteration : 10868
train acc:  0.796875
train loss:  0.40808597207069397
train gradient:  0.22504977470955123
iteration : 10869
train acc:  0.9375
train loss:  0.2277567982673645
train gradient:  0.10517594599078472
iteration : 10870
train acc:  0.859375
train loss:  0.38490819931030273
train gradient:  0.191133354887853
iteration : 10871
train acc:  0.8203125
train loss:  0.3573666512966156
train gradient:  0.12599158331288657
iteration : 10872
train acc:  0.8359375
train loss:  0.3454095125198364
train gradient:  0.19024619409141952
iteration : 10873
train acc:  0.8125
train loss:  0.47127676010131836
train gradient:  0.2909840011938336
iteration : 10874
train acc:  0.8125
train loss:  0.4296269416809082
train gradient:  0.21598230568149773
iteration : 10875
train acc:  0.8359375
train loss:  0.36228030920028687
train gradient:  0.17751467740092405
iteration : 10876
train acc:  0.84375
train loss:  0.3109189569950104
train gradient:  0.1673970514018665
iteration : 10877
train acc:  0.859375
train loss:  0.424249529838562
train gradient:  0.2675888270911835
iteration : 10878
train acc:  0.8359375
train loss:  0.3328566551208496
train gradient:  0.1861149337863522
iteration : 10879
train acc:  0.859375
train loss:  0.3296233117580414
train gradient:  0.1678830848631896
iteration : 10880
train acc:  0.84375
train loss:  0.32177308201789856
train gradient:  0.14784797443368575
iteration : 10881
train acc:  0.8828125
train loss:  0.3247062563896179
train gradient:  0.10358731972797611
iteration : 10882
train acc:  0.8125
train loss:  0.3452390432357788
train gradient:  0.178980383120539
iteration : 10883
train acc:  0.859375
train loss:  0.3671135902404785
train gradient:  0.16965926434300255
iteration : 10884
train acc:  0.859375
train loss:  0.3478192687034607
train gradient:  0.12812966328008688
iteration : 10885
train acc:  0.8828125
train loss:  0.25929754972457886
train gradient:  0.07284861153182079
iteration : 10886
train acc:  0.8203125
train loss:  0.3952374756336212
train gradient:  0.21359187473541008
iteration : 10887
train acc:  0.90625
train loss:  0.3040982782840729
train gradient:  0.12840945923032182
iteration : 10888
train acc:  0.796875
train loss:  0.3308698534965515
train gradient:  0.17526803119589884
iteration : 10889
train acc:  0.8125
train loss:  0.38916659355163574
train gradient:  0.19121040013727497
iteration : 10890
train acc:  0.8671875
train loss:  0.3795510530471802
train gradient:  0.20388287637998892
iteration : 10891
train acc:  0.8671875
train loss:  0.3053329885005951
train gradient:  0.12272247599885858
iteration : 10892
train acc:  0.8515625
train loss:  0.29071977734565735
train gradient:  0.11643554588040704
iteration : 10893
train acc:  0.8671875
train loss:  0.3002668023109436
train gradient:  0.12008698217488693
iteration : 10894
train acc:  0.8515625
train loss:  0.3621511459350586
train gradient:  0.19123837953941905
iteration : 10895
train acc:  0.859375
train loss:  0.3385852575302124
train gradient:  0.1647659775703439
iteration : 10896
train acc:  0.875
train loss:  0.26811009645462036
train gradient:  0.13894558720864308
iteration : 10897
train acc:  0.859375
train loss:  0.327919065952301
train gradient:  0.15035737316156095
iteration : 10898
train acc:  0.8671875
train loss:  0.2900831997394562
train gradient:  0.1623545531113938
iteration : 10899
train acc:  0.8671875
train loss:  0.30621230602264404
train gradient:  0.12286659966480222
iteration : 10900
train acc:  0.890625
train loss:  0.2676737308502197
train gradient:  0.11626435348770058
iteration : 10901
train acc:  0.828125
train loss:  0.330994188785553
train gradient:  0.15754579251269543
iteration : 10902
train acc:  0.8203125
train loss:  0.38276201486587524
train gradient:  0.18220411383743312
iteration : 10903
train acc:  0.875
train loss:  0.30541491508483887
train gradient:  0.18330242807170824
iteration : 10904
train acc:  0.859375
train loss:  0.2961122393608093
train gradient:  0.13350125430848536
iteration : 10905
train acc:  0.828125
train loss:  0.3432566523551941
train gradient:  0.12025671343009374
iteration : 10906
train acc:  0.7734375
train loss:  0.4331910312175751
train gradient:  0.22319016315206733
iteration : 10907
train acc:  0.859375
train loss:  0.3014715015888214
train gradient:  0.15104503640113282
iteration : 10908
train acc:  0.859375
train loss:  0.29856395721435547
train gradient:  0.1488568843271164
iteration : 10909
train acc:  0.921875
train loss:  0.22508995234966278
train gradient:  0.06943430609816899
iteration : 10910
train acc:  0.828125
train loss:  0.38420963287353516
train gradient:  0.18053380198518074
iteration : 10911
train acc:  0.890625
train loss:  0.3014777600765228
train gradient:  0.16588249344175904
iteration : 10912
train acc:  0.8828125
train loss:  0.26407986879348755
train gradient:  0.1360622458631966
iteration : 10913
train acc:  0.8671875
train loss:  0.36728769540786743
train gradient:  0.15703039022812892
iteration : 10914
train acc:  0.84375
train loss:  0.3604276776313782
train gradient:  0.14889950794726087
iteration : 10915
train acc:  0.8046875
train loss:  0.4267004132270813
train gradient:  0.23754676118762102
iteration : 10916
train acc:  0.8515625
train loss:  0.2924988865852356
train gradient:  0.16016461144942432
iteration : 10917
train acc:  0.921875
train loss:  0.2652784585952759
train gradient:  0.08370378699446904
iteration : 10918
train acc:  0.8203125
train loss:  0.3774688243865967
train gradient:  0.16574858408960563
iteration : 10919
train acc:  0.8125
train loss:  0.3953031301498413
train gradient:  0.2291807533466103
iteration : 10920
train acc:  0.9140625
train loss:  0.26529958844184875
train gradient:  0.1110329202117886
iteration : 10921
train acc:  0.84375
train loss:  0.31315886974334717
train gradient:  0.15073970561750938
iteration : 10922
train acc:  0.8046875
train loss:  0.3652080297470093
train gradient:  0.14433774981322928
iteration : 10923
train acc:  0.921875
train loss:  0.23043227195739746
train gradient:  0.12802696962848792
iteration : 10924
train acc:  0.8671875
train loss:  0.30379563570022583
train gradient:  0.11599090146838541
iteration : 10925
train acc:  0.859375
train loss:  0.31979987025260925
train gradient:  0.14252898863146413
iteration : 10926
train acc:  0.8828125
train loss:  0.2986016869544983
train gradient:  0.12444100229819732
iteration : 10927
train acc:  0.859375
train loss:  0.32438454031944275
train gradient:  0.14413375372144965
iteration : 10928
train acc:  0.8828125
train loss:  0.3221020996570587
train gradient:  0.1735229768312086
iteration : 10929
train acc:  0.8125
train loss:  0.4320506751537323
train gradient:  0.3028854568590902
iteration : 10930
train acc:  0.8359375
train loss:  0.3770732283592224
train gradient:  0.32696673249287933
iteration : 10931
train acc:  0.8984375
train loss:  0.29598331451416016
train gradient:  0.13287923688854203
iteration : 10932
train acc:  0.8203125
train loss:  0.3613657057285309
train gradient:  0.21266361962321378
iteration : 10933
train acc:  0.7890625
train loss:  0.400686115026474
train gradient:  0.23030852532987328
iteration : 10934
train acc:  0.8671875
train loss:  0.3341423273086548
train gradient:  0.17569913793619657
iteration : 10935
train acc:  0.890625
train loss:  0.27724307775497437
train gradient:  0.154106373141117
iteration : 10936
train acc:  0.84375
train loss:  0.37913376092910767
train gradient:  0.16527686454091017
iteration : 10937
train acc:  0.8671875
train loss:  0.3127496838569641
train gradient:  0.1300020094707962
iteration : 10938
train acc:  0.875
train loss:  0.3310130834579468
train gradient:  0.13321200979585185
iteration : 10939
train acc:  0.8046875
train loss:  0.35397404432296753
train gradient:  0.13768531489586167
iteration : 10940
train acc:  0.828125
train loss:  0.2871060371398926
train gradient:  0.14149123270513608
iteration : 10941
train acc:  0.9140625
train loss:  0.24065421521663666
train gradient:  0.11410699077799243
iteration : 10942
train acc:  0.84375
train loss:  0.31987300515174866
train gradient:  0.14430916632778454
iteration : 10943
train acc:  0.859375
train loss:  0.33251842856407166
train gradient:  0.14681669429054417
iteration : 10944
train acc:  0.796875
train loss:  0.39989566802978516
train gradient:  0.18940035942534522
iteration : 10945
train acc:  0.8359375
train loss:  0.3489535450935364
train gradient:  0.19235257401218703
iteration : 10946
train acc:  0.828125
train loss:  0.36523929238319397
train gradient:  0.21381555408747832
iteration : 10947
train acc:  0.8828125
train loss:  0.343380868434906
train gradient:  0.14009464222919893
iteration : 10948
train acc:  0.890625
train loss:  0.33737272024154663
train gradient:  0.1288759033103305
iteration : 10949
train acc:  0.84375
train loss:  0.32553544640541077
train gradient:  0.1025637003385236
iteration : 10950
train acc:  0.84375
train loss:  0.35188671946525574
train gradient:  0.15825440417297412
iteration : 10951
train acc:  0.828125
train loss:  0.4110301733016968
train gradient:  0.22053999454053733
iteration : 10952
train acc:  0.9140625
train loss:  0.295317143201828
train gradient:  0.09941128212431874
iteration : 10953
train acc:  0.84375
train loss:  0.28845638036727905
train gradient:  0.13592836167960137
iteration : 10954
train acc:  0.875
train loss:  0.3055865466594696
train gradient:  0.11132039387920162
iteration : 10955
train acc:  0.859375
train loss:  0.34107092022895813
train gradient:  0.14630095638028207
iteration : 10956
train acc:  0.890625
train loss:  0.287896990776062
train gradient:  0.11473050533711257
iteration : 10957
train acc:  0.8515625
train loss:  0.39487525820732117
train gradient:  0.21054831291380094
iteration : 10958
train acc:  0.859375
train loss:  0.2964223027229309
train gradient:  0.1282791298725423
iteration : 10959
train acc:  0.8515625
train loss:  0.3597138524055481
train gradient:  0.1660605324634962
iteration : 10960
train acc:  0.890625
train loss:  0.2951558530330658
train gradient:  0.10707443537322116
iteration : 10961
train acc:  0.8359375
train loss:  0.38385891914367676
train gradient:  0.21236707043450254
iteration : 10962
train acc:  0.8515625
train loss:  0.3409337103366852
train gradient:  0.16139249169496767
iteration : 10963
train acc:  0.875
train loss:  0.3130108118057251
train gradient:  0.1470753733932595
iteration : 10964
train acc:  0.8828125
train loss:  0.3453964293003082
train gradient:  0.18715855351052013
iteration : 10965
train acc:  0.8515625
train loss:  0.3092854917049408
train gradient:  0.15163619631933295
iteration : 10966
train acc:  0.8515625
train loss:  0.29890763759613037
train gradient:  0.09787579267743797
iteration : 10967
train acc:  0.8125
train loss:  0.35691380500793457
train gradient:  0.15791563877402526
iteration : 10968
train acc:  0.8125
train loss:  0.38284048438072205
train gradient:  0.16799138410950978
iteration : 10969
train acc:  0.84375
train loss:  0.3366559147834778
train gradient:  0.1370946267454362
iteration : 10970
train acc:  0.84375
train loss:  0.346929669380188
train gradient:  0.21449003270870595
iteration : 10971
train acc:  0.8515625
train loss:  0.33519232273101807
train gradient:  0.16187859652242265
iteration : 10972
train acc:  0.9140625
train loss:  0.26295462250709534
train gradient:  0.10661897893663484
iteration : 10973
train acc:  0.7734375
train loss:  0.46623265743255615
train gradient:  0.24056259249222745
iteration : 10974
train acc:  0.8984375
train loss:  0.32678186893463135
train gradient:  0.15934976776140505
iteration : 10975
train acc:  0.8359375
train loss:  0.33932262659072876
train gradient:  0.18701931233151398
iteration : 10976
train acc:  0.875
train loss:  0.2970402240753174
train gradient:  0.18546685787977396
iteration : 10977
train acc:  0.84375
train loss:  0.34429872035980225
train gradient:  0.13065605083095455
iteration : 10978
train acc:  0.8828125
train loss:  0.2738664150238037
train gradient:  0.13414952884991177
iteration : 10979
train acc:  0.8515625
train loss:  0.34173980355262756
train gradient:  0.2154037314224846
iteration : 10980
train acc:  0.890625
train loss:  0.26099902391433716
train gradient:  0.12780933612244755
iteration : 10981
train acc:  0.8671875
train loss:  0.358043372631073
train gradient:  0.16284953515245193
iteration : 10982
train acc:  0.8671875
train loss:  0.34603461623191833
train gradient:  0.14495238849458453
iteration : 10983
train acc:  0.8046875
train loss:  0.4208829998970032
train gradient:  0.23487715319274025
iteration : 10984
train acc:  0.8671875
train loss:  0.2828999161720276
train gradient:  0.12196367669411083
iteration : 10985
train acc:  0.8125
train loss:  0.3483268916606903
train gradient:  0.14093707874852338
iteration : 10986
train acc:  0.890625
train loss:  0.28694966435432434
train gradient:  0.12719093819237443
iteration : 10987
train acc:  0.8046875
train loss:  0.3925257623195648
train gradient:  0.2782382731355588
iteration : 10988
train acc:  0.9140625
train loss:  0.2525961995124817
train gradient:  0.09281082492131387
iteration : 10989
train acc:  0.8828125
train loss:  0.29438474774360657
train gradient:  0.14859612826238172
iteration : 10990
train acc:  0.8828125
train loss:  0.3142380714416504
train gradient:  0.17327603683793413
iteration : 10991
train acc:  0.875
train loss:  0.29541677236557007
train gradient:  0.13315972505061502
iteration : 10992
train acc:  0.90625
train loss:  0.2666495442390442
train gradient:  0.17114655993046657
iteration : 10993
train acc:  0.8359375
train loss:  0.3595258593559265
train gradient:  0.2142818343564809
iteration : 10994
train acc:  0.921875
train loss:  0.22713622450828552
train gradient:  0.0855748319249
iteration : 10995
train acc:  0.8125
train loss:  0.3904675841331482
train gradient:  0.1696922593583633
iteration : 10996
train acc:  0.90625
train loss:  0.26120084524154663
train gradient:  0.1334426577093687
iteration : 10997
train acc:  0.84375
train loss:  0.3847009539604187
train gradient:  0.24322468453048535
iteration : 10998
train acc:  0.84375
train loss:  0.34811264276504517
train gradient:  0.1468921193714553
iteration : 10999
train acc:  0.828125
train loss:  0.4338357448577881
train gradient:  0.249040385561496
iteration : 11000
train acc:  0.8359375
train loss:  0.3242560029029846
train gradient:  0.12986818447411239
iteration : 11001
train acc:  0.8515625
train loss:  0.32639750838279724
train gradient:  0.17496868689609618
iteration : 11002
train acc:  0.890625
train loss:  0.2693924307823181
train gradient:  0.10687799552219648
iteration : 11003
train acc:  0.890625
train loss:  0.27848270535469055
train gradient:  0.1347543760818829
iteration : 11004
train acc:  0.859375
train loss:  0.3356548845767975
train gradient:  0.18625776406833847
iteration : 11005
train acc:  0.859375
train loss:  0.2867468595504761
train gradient:  0.11683761324711718
iteration : 11006
train acc:  0.84375
train loss:  0.2962748110294342
train gradient:  0.10624574550909192
iteration : 11007
train acc:  0.8046875
train loss:  0.38639914989471436
train gradient:  0.1728818106399451
iteration : 11008
train acc:  0.8515625
train loss:  0.3702465891838074
train gradient:  0.18371970097830828
iteration : 11009
train acc:  0.859375
train loss:  0.3314768970012665
train gradient:  0.1525356079197907
iteration : 11010
train acc:  0.84375
train loss:  0.3333611488342285
train gradient:  0.13012128530379438
iteration : 11011
train acc:  0.7734375
train loss:  0.39909929037094116
train gradient:  0.2194298362431848
iteration : 11012
train acc:  0.8515625
train loss:  0.3769600987434387
train gradient:  0.1577552051858344
iteration : 11013
train acc:  0.7890625
train loss:  0.420197457075119
train gradient:  0.21637926042576233
iteration : 11014
train acc:  0.828125
train loss:  0.40949323773384094
train gradient:  0.19917841500169284
iteration : 11015
train acc:  0.8671875
train loss:  0.3019709289073944
train gradient:  0.13007509587392518
iteration : 11016
train acc:  0.8671875
train loss:  0.38038453459739685
train gradient:  0.12696366004279813
iteration : 11017
train acc:  0.8203125
train loss:  0.3361104726791382
train gradient:  0.18890825225515018
iteration : 11018
train acc:  0.8125
train loss:  0.4650740623474121
train gradient:  0.24456451862488895
iteration : 11019
train acc:  0.84375
train loss:  0.38931500911712646
train gradient:  0.22500995769689555
iteration : 11020
train acc:  0.90625
train loss:  0.3220025599002838
train gradient:  0.11907040800465883
iteration : 11021
train acc:  0.859375
train loss:  0.28985875844955444
train gradient:  0.12836329769032706
iteration : 11022
train acc:  0.8203125
train loss:  0.3996710181236267
train gradient:  0.2361894879277553
iteration : 11023
train acc:  0.8359375
train loss:  0.3706432580947876
train gradient:  0.2389860972233624
iteration : 11024
train acc:  0.8046875
train loss:  0.42549583315849304
train gradient:  0.18675463727754665
iteration : 11025
train acc:  0.828125
train loss:  0.3565623164176941
train gradient:  0.3357077815023529
iteration : 11026
train acc:  0.859375
train loss:  0.39630359411239624
train gradient:  0.18677857555798325
iteration : 11027
train acc:  0.8515625
train loss:  0.2919728457927704
train gradient:  0.1285724530936293
iteration : 11028
train acc:  0.8671875
train loss:  0.3422628343105316
train gradient:  0.1325136012452586
iteration : 11029
train acc:  0.8359375
train loss:  0.4090319573879242
train gradient:  0.16322023527878893
iteration : 11030
train acc:  0.8046875
train loss:  0.4488520622253418
train gradient:  0.22778549736417264
iteration : 11031
train acc:  0.890625
train loss:  0.2833477854728699
train gradient:  0.1929741555966781
iteration : 11032
train acc:  0.859375
train loss:  0.3893570899963379
train gradient:  0.18365407647135268
iteration : 11033
train acc:  0.84375
train loss:  0.3637790083885193
train gradient:  0.1649724522125251
iteration : 11034
train acc:  0.8046875
train loss:  0.3543415069580078
train gradient:  0.16927489637498264
iteration : 11035
train acc:  0.8515625
train loss:  0.2995011508464813
train gradient:  0.09955422572176591
iteration : 11036
train acc:  0.8984375
train loss:  0.2682077884674072
train gradient:  0.14625421039505465
iteration : 11037
train acc:  0.859375
train loss:  0.28842058777809143
train gradient:  0.13441547006992613
iteration : 11038
train acc:  0.8984375
train loss:  0.25993233919143677
train gradient:  0.08865999557825142
iteration : 11039
train acc:  0.8046875
train loss:  0.3631143867969513
train gradient:  0.17917980930179195
iteration : 11040
train acc:  0.84375
train loss:  0.34230294823646545
train gradient:  0.09983814297776183
iteration : 11041
train acc:  0.859375
train loss:  0.34203577041625977
train gradient:  0.21823539100592368
iteration : 11042
train acc:  0.859375
train loss:  0.2866012454032898
train gradient:  0.1600854915186859
iteration : 11043
train acc:  0.84375
train loss:  0.3302779495716095
train gradient:  0.10818065815674306
iteration : 11044
train acc:  0.8828125
train loss:  0.27308425307273865
train gradient:  0.13317163646896224
iteration : 11045
train acc:  0.8046875
train loss:  0.3840603232383728
train gradient:  0.19006249098876074
iteration : 11046
train acc:  0.875
train loss:  0.317802757024765
train gradient:  0.1735415273105518
iteration : 11047
train acc:  0.90625
train loss:  0.2640172243118286
train gradient:  0.10316814705885462
iteration : 11048
train acc:  0.84375
train loss:  0.31859537959098816
train gradient:  0.1170487928686946
iteration : 11049
train acc:  0.8515625
train loss:  0.34727901220321655
train gradient:  0.17440046706412993
iteration : 11050
train acc:  0.8984375
train loss:  0.3529506027698517
train gradient:  0.17126399589567493
iteration : 11051
train acc:  0.859375
train loss:  0.34081727266311646
train gradient:  0.13602655842013073
iteration : 11052
train acc:  0.8671875
train loss:  0.34422504901885986
train gradient:  0.2865241897427762
iteration : 11053
train acc:  0.890625
train loss:  0.24238622188568115
train gradient:  0.10519630556176955
iteration : 11054
train acc:  0.8671875
train loss:  0.2860260605812073
train gradient:  0.11463419091155831
iteration : 11055
train acc:  0.890625
train loss:  0.3036981225013733
train gradient:  0.11698087102164842
iteration : 11056
train acc:  0.8671875
train loss:  0.31342437863349915
train gradient:  0.15420486921713156
iteration : 11057
train acc:  0.8984375
train loss:  0.2875136733055115
train gradient:  0.12065867223689174
iteration : 11058
train acc:  0.890625
train loss:  0.3239663243293762
train gradient:  0.19296251353391833
iteration : 11059
train acc:  0.8515625
train loss:  0.32238173484802246
train gradient:  0.15694712012630777
iteration : 11060
train acc:  0.921875
train loss:  0.25185245275497437
train gradient:  0.12462358488486429
iteration : 11061
train acc:  0.828125
train loss:  0.37616997957229614
train gradient:  0.16755896437080237
iteration : 11062
train acc:  0.921875
train loss:  0.20952287316322327
train gradient:  0.07736554879993485
iteration : 11063
train acc:  0.8828125
train loss:  0.3359048068523407
train gradient:  0.2975332550748549
iteration : 11064
train acc:  0.859375
train loss:  0.28857481479644775
train gradient:  0.11135737400562487
iteration : 11065
train acc:  0.84375
train loss:  0.3431549072265625
train gradient:  0.11351949076275188
iteration : 11066
train acc:  0.8515625
train loss:  0.3788674771785736
train gradient:  0.19841401566238093
iteration : 11067
train acc:  0.9140625
train loss:  0.2565114498138428
train gradient:  0.39918676269632436
iteration : 11068
train acc:  0.875
train loss:  0.32119321823120117
train gradient:  0.116949043814567
iteration : 11069
train acc:  0.8515625
train loss:  0.31877365708351135
train gradient:  0.1000283234377604
iteration : 11070
train acc:  0.8828125
train loss:  0.3043282926082611
train gradient:  0.13687527595439755
iteration : 11071
train acc:  0.921875
train loss:  0.21320706605911255
train gradient:  0.0898244510441757
iteration : 11072
train acc:  0.84375
train loss:  0.3362466096878052
train gradient:  0.20894136959116177
iteration : 11073
train acc:  0.828125
train loss:  0.3923053443431854
train gradient:  0.30371882825923396
iteration : 11074
train acc:  0.8515625
train loss:  0.3553311228752136
train gradient:  0.23246523436513777
iteration : 11075
train acc:  0.84375
train loss:  0.3744127154350281
train gradient:  0.23484509037424445
iteration : 11076
train acc:  0.875
train loss:  0.39259788393974304
train gradient:  0.24178892914698172
iteration : 11077
train acc:  0.8671875
train loss:  0.33504390716552734
train gradient:  0.1727067034646297
iteration : 11078
train acc:  0.8828125
train loss:  0.28879982233047485
train gradient:  0.10465010937978478
iteration : 11079
train acc:  0.8515625
train loss:  0.47688865661621094
train gradient:  0.3232266671679009
iteration : 11080
train acc:  0.8359375
train loss:  0.3358721435070038
train gradient:  0.14829418120706114
iteration : 11081
train acc:  0.890625
train loss:  0.24301709234714508
train gradient:  0.09634474131040706
iteration : 11082
train acc:  0.8515625
train loss:  0.4251187741756439
train gradient:  0.2783805737177457
iteration : 11083
train acc:  0.8515625
train loss:  0.3121768832206726
train gradient:  0.17060073059500183
iteration : 11084
train acc:  0.7578125
train loss:  0.4380078613758087
train gradient:  0.33711082147309607
iteration : 11085
train acc:  0.8984375
train loss:  0.272695392370224
train gradient:  0.123551619021695
iteration : 11086
train acc:  0.8984375
train loss:  0.26738542318344116
train gradient:  0.14009450593636463
iteration : 11087
train acc:  0.8359375
train loss:  0.3320884704589844
train gradient:  0.1730116672246051
iteration : 11088
train acc:  0.8671875
train loss:  0.27864766120910645
train gradient:  0.10865672289214674
iteration : 11089
train acc:  0.84375
train loss:  0.3278377056121826
train gradient:  0.1721526826622305
iteration : 11090
train acc:  0.8671875
train loss:  0.31908750534057617
train gradient:  0.21102765964823178
iteration : 11091
train acc:  0.8359375
train loss:  0.30394524335861206
train gradient:  0.12515091378457377
iteration : 11092
train acc:  0.859375
train loss:  0.34646058082580566
train gradient:  0.15630579786212118
iteration : 11093
train acc:  0.84375
train loss:  0.3444228172302246
train gradient:  0.31836771035148914
iteration : 11094
train acc:  0.8359375
train loss:  0.3413199484348297
train gradient:  0.15685519040802148
iteration : 11095
train acc:  0.8984375
train loss:  0.2591439187526703
train gradient:  0.08938442352233922
iteration : 11096
train acc:  0.84375
train loss:  0.34663110971450806
train gradient:  0.3140104628698335
iteration : 11097
train acc:  0.8515625
train loss:  0.3773697316646576
train gradient:  0.2038747294262807
iteration : 11098
train acc:  0.859375
train loss:  0.3076007068157196
train gradient:  0.11508813974203042
iteration : 11099
train acc:  0.8671875
train loss:  0.3231392502784729
train gradient:  0.1812199624606316
iteration : 11100
train acc:  0.9453125
train loss:  0.21693098545074463
train gradient:  0.1112578761269444
iteration : 11101
train acc:  0.84375
train loss:  0.352649986743927
train gradient:  0.1613551965361036
iteration : 11102
train acc:  0.7890625
train loss:  0.44239237904548645
train gradient:  0.2856138912701228
iteration : 11103
train acc:  0.9140625
train loss:  0.3139435350894928
train gradient:  0.1221753690487134
iteration : 11104
train acc:  0.828125
train loss:  0.3668293356895447
train gradient:  0.1947294748216845
iteration : 11105
train acc:  0.8984375
train loss:  0.27913379669189453
train gradient:  0.17432226627125022
iteration : 11106
train acc:  0.859375
train loss:  0.33507829904556274
train gradient:  0.23080039122177426
iteration : 11107
train acc:  0.8359375
train loss:  0.3593400716781616
train gradient:  0.18129581527795696
iteration : 11108
train acc:  0.84375
train loss:  0.3275483250617981
train gradient:  0.1736905801467493
iteration : 11109
train acc:  0.859375
train loss:  0.32537293434143066
train gradient:  0.17487071595419948
iteration : 11110
train acc:  0.8828125
train loss:  0.30945277214050293
train gradient:  0.16069868417553174
iteration : 11111
train acc:  0.8671875
train loss:  0.3413558006286621
train gradient:  0.1427289072757355
iteration : 11112
train acc:  0.875
train loss:  0.2773856222629547
train gradient:  0.16974609864860757
iteration : 11113
train acc:  0.8671875
train loss:  0.3036816120147705
train gradient:  0.13793050896368225
iteration : 11114
train acc:  0.8125
train loss:  0.42898422479629517
train gradient:  0.1975049178275447
iteration : 11115
train acc:  0.890625
train loss:  0.2503509223461151
train gradient:  0.096201850182767
iteration : 11116
train acc:  0.8671875
train loss:  0.3451929986476898
train gradient:  0.21530567434553827
iteration : 11117
train acc:  0.84375
train loss:  0.3763468861579895
train gradient:  0.19221468439655803
iteration : 11118
train acc:  0.8828125
train loss:  0.29095911979675293
train gradient:  0.10486093105764036
iteration : 11119
train acc:  0.8828125
train loss:  0.35648587346076965
train gradient:  0.15609522643785953
iteration : 11120
train acc:  0.859375
train loss:  0.349735826253891
train gradient:  0.20174999783276487
iteration : 11121
train acc:  0.890625
train loss:  0.23626190423965454
train gradient:  0.08521621373049315
iteration : 11122
train acc:  0.859375
train loss:  0.3042895197868347
train gradient:  0.15665372195184868
iteration : 11123
train acc:  0.90625
train loss:  0.24481764435768127
train gradient:  0.09379324299570264
iteration : 11124
train acc:  0.8515625
train loss:  0.31634199619293213
train gradient:  0.1527615968666305
iteration : 11125
train acc:  0.8671875
train loss:  0.26782917976379395
train gradient:  0.12158105719249554
iteration : 11126
train acc:  0.8515625
train loss:  0.3083916902542114
train gradient:  0.17499972539182165
iteration : 11127
train acc:  0.8671875
train loss:  0.3267058730125427
train gradient:  0.17331242798179353
iteration : 11128
train acc:  0.8671875
train loss:  0.3159380555152893
train gradient:  0.15800015329998623
iteration : 11129
train acc:  0.8828125
train loss:  0.3032023310661316
train gradient:  0.1298211369842781
iteration : 11130
train acc:  0.84375
train loss:  0.3263632357120514
train gradient:  0.1459295570281615
iteration : 11131
train acc:  0.8203125
train loss:  0.4004911184310913
train gradient:  0.21201417036872533
iteration : 11132
train acc:  0.8515625
train loss:  0.3421592712402344
train gradient:  0.18690549920209842
iteration : 11133
train acc:  0.921875
train loss:  0.22612935304641724
train gradient:  0.12263472039935149
iteration : 11134
train acc:  0.8359375
train loss:  0.3277161717414856
train gradient:  0.14477990020325804
iteration : 11135
train acc:  0.796875
train loss:  0.41107386350631714
train gradient:  0.2259383254996769
iteration : 11136
train acc:  0.890625
train loss:  0.25586390495300293
train gradient:  0.114083821194334
iteration : 11137
train acc:  0.8359375
train loss:  0.3380550742149353
train gradient:  0.14748164332477837
iteration : 11138
train acc:  0.8671875
train loss:  0.300687313079834
train gradient:  0.12310124707402485
iteration : 11139
train acc:  0.859375
train loss:  0.30293703079223633
train gradient:  0.14223497383805178
iteration : 11140
train acc:  0.8515625
train loss:  0.3194313645362854
train gradient:  0.1733969459757011
iteration : 11141
train acc:  0.8984375
train loss:  0.28956088423728943
train gradient:  0.14222363121748238
iteration : 11142
train acc:  0.8515625
train loss:  0.3449624180793762
train gradient:  0.20143574677958057
iteration : 11143
train acc:  0.8359375
train loss:  0.32462871074676514
train gradient:  0.15370534432522442
iteration : 11144
train acc:  0.8828125
train loss:  0.27867621183395386
train gradient:  0.18471322475853297
iteration : 11145
train acc:  0.859375
train loss:  0.31133636832237244
train gradient:  0.13580941470427665
iteration : 11146
train acc:  0.8515625
train loss:  0.3291252851486206
train gradient:  0.1596970430920256
iteration : 11147
train acc:  0.921875
train loss:  0.2122591733932495
train gradient:  0.07903348098505596
iteration : 11148
train acc:  0.8515625
train loss:  0.34675127267837524
train gradient:  0.19943142406144143
iteration : 11149
train acc:  0.859375
train loss:  0.31699368357658386
train gradient:  0.1402259027431934
iteration : 11150
train acc:  0.84375
train loss:  0.29410213232040405
train gradient:  0.14911071988271368
iteration : 11151
train acc:  0.7890625
train loss:  0.4225218594074249
train gradient:  0.1964499084683591
iteration : 11152
train acc:  0.90625
train loss:  0.2037818282842636
train gradient:  0.08355435609455639
iteration : 11153
train acc:  0.890625
train loss:  0.3138793110847473
train gradient:  0.15299794754330376
iteration : 11154
train acc:  0.8125
train loss:  0.3325103521347046
train gradient:  0.1824351213587096
iteration : 11155
train acc:  0.8671875
train loss:  0.28553032875061035
train gradient:  0.15694064699084787
iteration : 11156
train acc:  0.8828125
train loss:  0.26003962755203247
train gradient:  0.14220795285144341
iteration : 11157
train acc:  0.8671875
train loss:  0.33303260803222656
train gradient:  0.25966820894580384
iteration : 11158
train acc:  0.8203125
train loss:  0.3887483477592468
train gradient:  0.16346833348059023
iteration : 11159
train acc:  0.8359375
train loss:  0.31915345788002014
train gradient:  0.17942936437087567
iteration : 11160
train acc:  0.84375
train loss:  0.3173103928565979
train gradient:  0.11432488212980091
iteration : 11161
train acc:  0.8828125
train loss:  0.3132208585739136
train gradient:  0.1991952678534733
iteration : 11162
train acc:  0.8984375
train loss:  0.2889958620071411
train gradient:  0.11440191289394719
iteration : 11163
train acc:  0.8984375
train loss:  0.24573588371276855
train gradient:  0.11596739947761736
iteration : 11164
train acc:  0.796875
train loss:  0.46684175729751587
train gradient:  0.2781658946539027
iteration : 11165
train acc:  0.8671875
train loss:  0.3127884864807129
train gradient:  0.15433805815462287
iteration : 11166
train acc:  0.875
train loss:  0.2831001281738281
train gradient:  0.17742164227737103
iteration : 11167
train acc:  0.859375
train loss:  0.28212976455688477
train gradient:  0.16518791745324676
iteration : 11168
train acc:  0.921875
train loss:  0.2434631586074829
train gradient:  0.10868186569352882
iteration : 11169
train acc:  0.8828125
train loss:  0.26441824436187744
train gradient:  0.12554605899556248
iteration : 11170
train acc:  0.875
train loss:  0.37346380949020386
train gradient:  0.14839154758231254
iteration : 11171
train acc:  0.890625
train loss:  0.2569584250450134
train gradient:  0.07851547107187407
iteration : 11172
train acc:  0.890625
train loss:  0.3090384602546692
train gradient:  0.16991045626464268
iteration : 11173
train acc:  0.9140625
train loss:  0.24922065436840057
train gradient:  0.08714694196182492
iteration : 11174
train acc:  0.921875
train loss:  0.20400533080101013
train gradient:  0.06863230492001182
iteration : 11175
train acc:  0.828125
train loss:  0.35966458916664124
train gradient:  0.17284148074657724
iteration : 11176
train acc:  0.8828125
train loss:  0.2980486750602722
train gradient:  0.12580015957047821
iteration : 11177
train acc:  0.859375
train loss:  0.3375494182109833
train gradient:  0.17963440804822012
iteration : 11178
train acc:  0.9140625
train loss:  0.23591327667236328
train gradient:  0.12945449888198768
iteration : 11179
train acc:  0.8359375
train loss:  0.31192171573638916
train gradient:  0.15667624251009396
iteration : 11180
train acc:  0.921875
train loss:  0.3017103374004364
train gradient:  0.1613059422668027
iteration : 11181
train acc:  0.8515625
train loss:  0.35681647062301636
train gradient:  0.19182917092539092
iteration : 11182
train acc:  0.8359375
train loss:  0.4216051697731018
train gradient:  0.32955021674948126
iteration : 11183
train acc:  0.890625
train loss:  0.31247538328170776
train gradient:  0.15186433624740966
iteration : 11184
train acc:  0.875
train loss:  0.2906741499900818
train gradient:  0.1595047324693627
iteration : 11185
train acc:  0.859375
train loss:  0.36292457580566406
train gradient:  0.24663209856599883
iteration : 11186
train acc:  0.890625
train loss:  0.25910696387290955
train gradient:  0.08367967244093805
iteration : 11187
train acc:  0.828125
train loss:  0.30305927991867065
train gradient:  0.18630745728270948
iteration : 11188
train acc:  0.84375
train loss:  0.3552606701850891
train gradient:  0.19903524913246026
iteration : 11189
train acc:  0.8359375
train loss:  0.37065985798835754
train gradient:  0.2199115476405421
iteration : 11190
train acc:  0.8515625
train loss:  0.38787466287612915
train gradient:  0.21018742852268674
iteration : 11191
train acc:  0.8671875
train loss:  0.31438755989074707
train gradient:  0.13820960206869026
iteration : 11192
train acc:  0.8125
train loss:  0.4198996424674988
train gradient:  0.2009758382576985
iteration : 11193
train acc:  0.828125
train loss:  0.3857332468032837
train gradient:  0.13765866253486592
iteration : 11194
train acc:  0.859375
train loss:  0.3118022382259369
train gradient:  0.1635659330758162
iteration : 11195
train acc:  0.9140625
train loss:  0.2559118866920471
train gradient:  0.12984897986135005
iteration : 11196
train acc:  0.859375
train loss:  0.36198025941848755
train gradient:  0.16555166946844474
iteration : 11197
train acc:  0.828125
train loss:  0.3735332489013672
train gradient:  0.2505839423542056
iteration : 11198
train acc:  0.859375
train loss:  0.31261107325553894
train gradient:  0.1419328500559912
iteration : 11199
train acc:  0.8515625
train loss:  0.3467123806476593
train gradient:  0.23543314405910415
iteration : 11200
train acc:  0.8515625
train loss:  0.2959437966346741
train gradient:  0.1390169804512322
iteration : 11201
train acc:  0.8828125
train loss:  0.3225098252296448
train gradient:  0.1376649880298378
iteration : 11202
train acc:  0.8828125
train loss:  0.36964529752731323
train gradient:  0.17032792641249092
iteration : 11203
train acc:  0.859375
train loss:  0.34174567461013794
train gradient:  0.19915098073453072
iteration : 11204
train acc:  0.8671875
train loss:  0.3363672196865082
train gradient:  0.14663626626603884
iteration : 11205
train acc:  0.84375
train loss:  0.3685055673122406
train gradient:  0.19371403632645162
iteration : 11206
train acc:  0.8671875
train loss:  0.3105817437171936
train gradient:  0.12819285150667786
iteration : 11207
train acc:  0.921875
train loss:  0.27034658193588257
train gradient:  0.1020569653170373
iteration : 11208
train acc:  0.875
train loss:  0.2859814465045929
train gradient:  0.1568527700263531
iteration : 11209
train acc:  0.8984375
train loss:  0.3046950697898865
train gradient:  0.16687599424236665
iteration : 11210
train acc:  0.875
train loss:  0.3194412589073181
train gradient:  0.1310211552488635
iteration : 11211
train acc:  0.84375
train loss:  0.41823330521583557
train gradient:  0.18159777523237391
iteration : 11212
train acc:  0.8359375
train loss:  0.35615527629852295
train gradient:  0.2370378531220434
iteration : 11213
train acc:  0.84375
train loss:  0.3374027907848358
train gradient:  0.20570259974362737
iteration : 11214
train acc:  0.8984375
train loss:  0.2732618451118469
train gradient:  0.10093497167191308
iteration : 11215
train acc:  0.859375
train loss:  0.3360324203968048
train gradient:  0.16539347462335807
iteration : 11216
train acc:  0.90625
train loss:  0.27797430753707886
train gradient:  0.13561728927052452
iteration : 11217
train acc:  0.828125
train loss:  0.3703782558441162
train gradient:  0.18610721788577694
iteration : 11218
train acc:  0.8359375
train loss:  0.3597991168498993
train gradient:  0.17490527215453242
iteration : 11219
train acc:  0.796875
train loss:  0.39074161648750305
train gradient:  0.15617441699410967
iteration : 11220
train acc:  0.8984375
train loss:  0.25067228078842163
train gradient:  0.1653218907184487
iteration : 11221
train acc:  0.8671875
train loss:  0.2716473937034607
train gradient:  0.11358060515629535
iteration : 11222
train acc:  0.9140625
train loss:  0.23876285552978516
train gradient:  0.13557354912379777
iteration : 11223
train acc:  0.8515625
train loss:  0.3292262554168701
train gradient:  0.24651466514463796
iteration : 11224
train acc:  0.90625
train loss:  0.2500532865524292
train gradient:  0.15481375452613838
iteration : 11225
train acc:  0.9140625
train loss:  0.2115984708070755
train gradient:  0.08352352934194653
iteration : 11226
train acc:  0.796875
train loss:  0.45815956592559814
train gradient:  0.3385017350515843
iteration : 11227
train acc:  0.8828125
train loss:  0.2667328119277954
train gradient:  0.12253827654156066
iteration : 11228
train acc:  0.859375
train loss:  0.35155022144317627
train gradient:  0.1919488016604256
iteration : 11229
train acc:  0.8671875
train loss:  0.33341002464294434
train gradient:  0.1967565275239613
iteration : 11230
train acc:  0.828125
train loss:  0.378542959690094
train gradient:  0.26333937613508485
iteration : 11231
train acc:  0.8671875
train loss:  0.32183778285980225
train gradient:  0.17897194385088855
iteration : 11232
train acc:  0.8984375
train loss:  0.2848721444606781
train gradient:  0.1279980977996994
iteration : 11233
train acc:  0.921875
train loss:  0.24131858348846436
train gradient:  0.11929101616292608
iteration : 11234
train acc:  0.8359375
train loss:  0.30509188771247864
train gradient:  0.16651132627472684
iteration : 11235
train acc:  0.8515625
train loss:  0.3523843288421631
train gradient:  0.2134796268273027
iteration : 11236
train acc:  0.859375
train loss:  0.3043106198310852
train gradient:  0.13111550185975918
iteration : 11237
train acc:  0.828125
train loss:  0.32739973068237305
train gradient:  0.17912099344392995
iteration : 11238
train acc:  0.859375
train loss:  0.3654245138168335
train gradient:  0.22828608113384125
iteration : 11239
train acc:  0.8671875
train loss:  0.31309443712234497
train gradient:  0.2521717247845789
iteration : 11240
train acc:  0.8125
train loss:  0.39444699883461
train gradient:  0.29625323957278404
iteration : 11241
train acc:  0.859375
train loss:  0.2808386981487274
train gradient:  0.17775585227881685
iteration : 11242
train acc:  0.9296875
train loss:  0.2539424002170563
train gradient:  0.07321220220072149
iteration : 11243
train acc:  0.8671875
train loss:  0.29471325874328613
train gradient:  0.12756283946422126
iteration : 11244
train acc:  0.859375
train loss:  0.35635828971862793
train gradient:  0.21958297146284672
iteration : 11245
train acc:  0.84375
train loss:  0.3654401898384094
train gradient:  0.203487227109365
iteration : 11246
train acc:  0.8515625
train loss:  0.35974109172821045
train gradient:  0.18656047359967687
iteration : 11247
train acc:  0.890625
train loss:  0.2803226709365845
train gradient:  0.11321574189384667
iteration : 11248
train acc:  0.8125
train loss:  0.4163193702697754
train gradient:  0.2627289923844986
iteration : 11249
train acc:  0.828125
train loss:  0.3554884195327759
train gradient:  0.21223450274155942
iteration : 11250
train acc:  0.8046875
train loss:  0.41164255142211914
train gradient:  0.330455108043718
iteration : 11251
train acc:  0.859375
train loss:  0.3333921432495117
train gradient:  0.12490994658147525
iteration : 11252
train acc:  0.84375
train loss:  0.3350278437137604
train gradient:  0.19219126692262448
iteration : 11253
train acc:  0.859375
train loss:  0.2762751281261444
train gradient:  0.12700142382167745
iteration : 11254
train acc:  0.84375
train loss:  0.33259299397468567
train gradient:  0.1429627872293091
iteration : 11255
train acc:  0.8125
train loss:  0.3652363419532776
train gradient:  0.14729887344254702
iteration : 11256
train acc:  0.8828125
train loss:  0.28242355585098267
train gradient:  0.1285345178199811
iteration : 11257
train acc:  0.9296875
train loss:  0.24341300129890442
train gradient:  0.14278886922406736
iteration : 11258
train acc:  0.8515625
train loss:  0.2906603515148163
train gradient:  0.13531310539272656
iteration : 11259
train acc:  0.84375
train loss:  0.3669295907020569
train gradient:  0.146579218859294
iteration : 11260
train acc:  0.8515625
train loss:  0.35224664211273193
train gradient:  0.17202404214623862
iteration : 11261
train acc:  0.8671875
train loss:  0.3431905508041382
train gradient:  0.18306090999595698
iteration : 11262
train acc:  0.859375
train loss:  0.29617244005203247
train gradient:  0.15117779406763662
iteration : 11263
train acc:  0.8984375
train loss:  0.26021891832351685
train gradient:  0.11151677720882039
iteration : 11264
train acc:  0.859375
train loss:  0.34622275829315186
train gradient:  0.1606279654158879
iteration : 11265
train acc:  0.8671875
train loss:  0.2879621386528015
train gradient:  0.10899320410905419
iteration : 11266
train acc:  0.84375
train loss:  0.3925684690475464
train gradient:  0.2773665831999098
iteration : 11267
train acc:  0.859375
train loss:  0.3399946987628937
train gradient:  0.37950860655312574
iteration : 11268
train acc:  0.8359375
train loss:  0.3338850736618042
train gradient:  0.11327155843918595
iteration : 11269
train acc:  0.8125
train loss:  0.41170549392700195
train gradient:  0.20014099868172577
iteration : 11270
train acc:  0.859375
train loss:  0.38791540265083313
train gradient:  0.28310397827923395
iteration : 11271
train acc:  0.8828125
train loss:  0.25357457995414734
train gradient:  0.14145874353080296
iteration : 11272
train acc:  0.9140625
train loss:  0.236108660697937
train gradient:  0.08206582004523964
iteration : 11273
train acc:  0.8515625
train loss:  0.3616853952407837
train gradient:  0.1814001685238483
iteration : 11274
train acc:  0.90625
train loss:  0.26320165395736694
train gradient:  0.14401748423012015
iteration : 11275
train acc:  0.8828125
train loss:  0.3019182085990906
train gradient:  0.13523920845427162
iteration : 11276
train acc:  0.8359375
train loss:  0.4115912914276123
train gradient:  0.2219670054166283
iteration : 11277
train acc:  0.8828125
train loss:  0.33934974670410156
train gradient:  0.14038305893701408
iteration : 11278
train acc:  0.8359375
train loss:  0.3466567397117615
train gradient:  0.2101060881383607
iteration : 11279
train acc:  0.8515625
train loss:  0.28061720728874207
train gradient:  0.09928556942559633
iteration : 11280
train acc:  0.84375
train loss:  0.35851892828941345
train gradient:  0.1714451271138922
iteration : 11281
train acc:  0.8515625
train loss:  0.35521015524864197
train gradient:  0.15780488301678341
iteration : 11282
train acc:  0.828125
train loss:  0.3631506562232971
train gradient:  0.15839155004361935
iteration : 11283
train acc:  0.8515625
train loss:  0.3432880640029907
train gradient:  0.1250389087763437
iteration : 11284
train acc:  0.7890625
train loss:  0.40737032890319824
train gradient:  0.2305344744427478
iteration : 11285
train acc:  0.8828125
train loss:  0.31548547744750977
train gradient:  0.112438915394609
iteration : 11286
train acc:  0.859375
train loss:  0.3686668276786804
train gradient:  0.19928518535461953
iteration : 11287
train acc:  0.828125
train loss:  0.4248051047325134
train gradient:  0.17963746684609577
iteration : 11288
train acc:  0.8359375
train loss:  0.33420348167419434
train gradient:  0.16138542088419525
iteration : 11289
train acc:  0.8359375
train loss:  0.389661580324173
train gradient:  0.22565003309742865
iteration : 11290
train acc:  0.8828125
train loss:  0.30097275972366333
train gradient:  0.1511277178545074
iteration : 11291
train acc:  0.859375
train loss:  0.2886069416999817
train gradient:  0.09658705739696156
iteration : 11292
train acc:  0.84375
train loss:  0.35886716842651367
train gradient:  0.1597360714941413
iteration : 11293
train acc:  0.8203125
train loss:  0.42977190017700195
train gradient:  0.1913854671631492
iteration : 11294
train acc:  0.8125
train loss:  0.4211554527282715
train gradient:  0.195992906820545
iteration : 11295
train acc:  0.828125
train loss:  0.29346245527267456
train gradient:  0.1567040861906913
iteration : 11296
train acc:  0.890625
train loss:  0.27013617753982544
train gradient:  0.21907676228657993
iteration : 11297
train acc:  0.8671875
train loss:  0.2918303906917572
train gradient:  0.1682004443259144
iteration : 11298
train acc:  0.859375
train loss:  0.27893128991127014
train gradient:  0.09975923547108967
iteration : 11299
train acc:  0.859375
train loss:  0.33082327246665955
train gradient:  0.14526311179928508
iteration : 11300
train acc:  0.90625
train loss:  0.3005441725254059
train gradient:  0.14597753181483297
iteration : 11301
train acc:  0.8515625
train loss:  0.3048226833343506
train gradient:  0.16923153643720545
iteration : 11302
train acc:  0.8671875
train loss:  0.34045642614364624
train gradient:  0.1375308955735562
iteration : 11303
train acc:  0.90625
train loss:  0.29079681634902954
train gradient:  0.08885266468714237
iteration : 11304
train acc:  0.8828125
train loss:  0.25341305136680603
train gradient:  0.11158420644636438
iteration : 11305
train acc:  0.8359375
train loss:  0.363347589969635
train gradient:  0.1770939243927919
iteration : 11306
train acc:  0.8515625
train loss:  0.3286382555961609
train gradient:  0.11268808854677838
iteration : 11307
train acc:  0.859375
train loss:  0.3727821111679077
train gradient:  0.13931005032420302
iteration : 11308
train acc:  0.875
train loss:  0.29066359996795654
train gradient:  0.09426299964963956
iteration : 11309
train acc:  0.875
train loss:  0.3039900064468384
train gradient:  0.13851454318756407
iteration : 11310
train acc:  0.8359375
train loss:  0.34313035011291504
train gradient:  0.18148380107206752
iteration : 11311
train acc:  0.8359375
train loss:  0.3156430721282959
train gradient:  0.14239761780840446
iteration : 11312
train acc:  0.84375
train loss:  0.32542046904563904
train gradient:  0.1286564160342879
iteration : 11313
train acc:  0.875
train loss:  0.3125320374965668
train gradient:  0.12787836745155018
iteration : 11314
train acc:  0.84375
train loss:  0.3248298764228821
train gradient:  0.11408319250246582
iteration : 11315
train acc:  0.8671875
train loss:  0.3026416301727295
train gradient:  0.13863635191950324
iteration : 11316
train acc:  0.8828125
train loss:  0.3075985312461853
train gradient:  0.11909391268265267
iteration : 11317
train acc:  0.859375
train loss:  0.33318716287612915
train gradient:  0.13899949537146986
iteration : 11318
train acc:  0.8359375
train loss:  0.3841056227684021
train gradient:  0.19928391207832433
iteration : 11319
train acc:  0.78125
train loss:  0.4112514555454254
train gradient:  0.25676236655023016
iteration : 11320
train acc:  0.90625
train loss:  0.24915622174739838
train gradient:  0.18019507248536953
iteration : 11321
train acc:  0.8515625
train loss:  0.31542709469795227
train gradient:  0.13870744295718235
iteration : 11322
train acc:  0.8671875
train loss:  0.319408655166626
train gradient:  0.1699441030047119
iteration : 11323
train acc:  0.875
train loss:  0.3007762134075165
train gradient:  0.11739993998877475
iteration : 11324
train acc:  0.8203125
train loss:  0.3698263466358185
train gradient:  0.19312495423469725
iteration : 11325
train acc:  0.828125
train loss:  0.4094432592391968
train gradient:  0.21622378512175028
iteration : 11326
train acc:  0.890625
train loss:  0.32325106859207153
train gradient:  0.1453149533878988
iteration : 11327
train acc:  0.890625
train loss:  0.2851163148880005
train gradient:  0.09124881498737926
iteration : 11328
train acc:  0.921875
train loss:  0.23145635426044464
train gradient:  0.09274536317337326
iteration : 11329
train acc:  0.8359375
train loss:  0.38233482837677
train gradient:  0.15690147264758275
iteration : 11330
train acc:  0.875
train loss:  0.28682011365890503
train gradient:  0.14412164307198516
iteration : 11331
train acc:  0.8515625
train loss:  0.3144279718399048
train gradient:  0.11455204988376035
iteration : 11332
train acc:  0.84375
train loss:  0.35277971625328064
train gradient:  0.1528314771754148
iteration : 11333
train acc:  0.8671875
train loss:  0.29286909103393555
train gradient:  0.0826385928132535
iteration : 11334
train acc:  0.8515625
train loss:  0.352672815322876
train gradient:  0.17570266852259483
iteration : 11335
train acc:  0.8984375
train loss:  0.27173805236816406
train gradient:  0.11088082609047999
iteration : 11336
train acc:  0.8515625
train loss:  0.30131494998931885
train gradient:  0.15394243589952306
iteration : 11337
train acc:  0.84375
train loss:  0.34538841247558594
train gradient:  0.21210879908666774
iteration : 11338
train acc:  0.8671875
train loss:  0.31866639852523804
train gradient:  0.14440303999970827
iteration : 11339
train acc:  0.875
train loss:  0.3387269973754883
train gradient:  0.1920616930548421
iteration : 11340
train acc:  0.875
train loss:  0.31478047370910645
train gradient:  0.11894273206018435
iteration : 11341
train acc:  0.875
train loss:  0.3117213547229767
train gradient:  0.10503768530951456
iteration : 11342
train acc:  0.8984375
train loss:  0.30938273668289185
train gradient:  0.10647324540397766
iteration : 11343
train acc:  0.8671875
train loss:  0.34465643763542175
train gradient:  0.1152784619427476
iteration : 11344
train acc:  0.8671875
train loss:  0.33471015095710754
train gradient:  0.16147493042096128
iteration : 11345
train acc:  0.875
train loss:  0.24926629662513733
train gradient:  0.1142114685245483
iteration : 11346
train acc:  0.890625
train loss:  0.2854829430580139
train gradient:  0.12997826678486335
iteration : 11347
train acc:  0.8828125
train loss:  0.25492987036705017
train gradient:  0.1048783795419232
iteration : 11348
train acc:  0.84375
train loss:  0.34226280450820923
train gradient:  0.15285987216776142
iteration : 11349
train acc:  0.84375
train loss:  0.34061917662620544
train gradient:  0.16120751596218486
iteration : 11350
train acc:  0.8203125
train loss:  0.35703516006469727
train gradient:  0.1860311143115299
iteration : 11351
train acc:  0.8515625
train loss:  0.37605997920036316
train gradient:  0.20251730807008503
iteration : 11352
train acc:  0.8828125
train loss:  0.29303741455078125
train gradient:  0.16597677380505002
iteration : 11353
train acc:  0.8203125
train loss:  0.3876851797103882
train gradient:  0.1799200628543936
iteration : 11354
train acc:  0.890625
train loss:  0.28151071071624756
train gradient:  0.1279426579237215
iteration : 11355
train acc:  0.8828125
train loss:  0.28875207901000977
train gradient:  0.14870508637126215
iteration : 11356
train acc:  0.8671875
train loss:  0.30990973114967346
train gradient:  0.12775464108371193
iteration : 11357
train acc:  0.859375
train loss:  0.33331847190856934
train gradient:  0.12791911225399327
iteration : 11358
train acc:  0.859375
train loss:  0.2995152473449707
train gradient:  0.12314185624749494
iteration : 11359
train acc:  0.8671875
train loss:  0.3173511028289795
train gradient:  0.18908981382098955
iteration : 11360
train acc:  0.8671875
train loss:  0.34697699546813965
train gradient:  0.1927084224925706
iteration : 11361
train acc:  0.828125
train loss:  0.3711707592010498
train gradient:  0.1785534801937501
iteration : 11362
train acc:  0.859375
train loss:  0.2975762188434601
train gradient:  0.1283162867109727
iteration : 11363
train acc:  0.8515625
train loss:  0.3668576776981354
train gradient:  0.1944038594832311
iteration : 11364
train acc:  0.875
train loss:  0.30345818400382996
train gradient:  0.1687112690561976
iteration : 11365
train acc:  0.8671875
train loss:  0.3067804276943207
train gradient:  0.1279965167062982
iteration : 11366
train acc:  0.8515625
train loss:  0.35301706194877625
train gradient:  0.17165075845504096
iteration : 11367
train acc:  0.8828125
train loss:  0.263928085565567
train gradient:  0.10533795791725639
iteration : 11368
train acc:  0.8359375
train loss:  0.4084653854370117
train gradient:  0.19493192034189727
iteration : 11369
train acc:  0.8046875
train loss:  0.43172550201416016
train gradient:  0.263582075568226
iteration : 11370
train acc:  0.8515625
train loss:  0.33671700954437256
train gradient:  0.15723413361832406
iteration : 11371
train acc:  0.890625
train loss:  0.2812477946281433
train gradient:  0.1520776555816416
iteration : 11372
train acc:  0.828125
train loss:  0.333876371383667
train gradient:  0.15010841315683676
iteration : 11373
train acc:  0.859375
train loss:  0.38595303893089294
train gradient:  0.17809106172764128
iteration : 11374
train acc:  0.90625
train loss:  0.324378103017807
train gradient:  0.12013806325998934
iteration : 11375
train acc:  0.8046875
train loss:  0.4157297611236572
train gradient:  0.2713135384117915
iteration : 11376
train acc:  0.84375
train loss:  0.3222196698188782
train gradient:  0.12609404138411187
iteration : 11377
train acc:  0.90625
train loss:  0.2710255980491638
train gradient:  0.08520786576975968
iteration : 11378
train acc:  0.796875
train loss:  0.38703131675720215
train gradient:  0.2303838365490271
iteration : 11379
train acc:  0.8828125
train loss:  0.2907089591026306
train gradient:  0.12117077792589852
iteration : 11380
train acc:  0.8828125
train loss:  0.3163480758666992
train gradient:  0.12133531925832196
iteration : 11381
train acc:  0.8828125
train loss:  0.27113574743270874
train gradient:  0.09970503702217112
iteration : 11382
train acc:  0.8125
train loss:  0.4358985126018524
train gradient:  0.23898682788190578
iteration : 11383
train acc:  0.8671875
train loss:  0.3193413019180298
train gradient:  0.1339479617635866
iteration : 11384
train acc:  0.8984375
train loss:  0.2897399067878723
train gradient:  0.1171659904485758
iteration : 11385
train acc:  0.8359375
train loss:  0.3266964852809906
train gradient:  0.20617681153620582
iteration : 11386
train acc:  0.8828125
train loss:  0.2752074599266052
train gradient:  0.09896347600307533
iteration : 11387
train acc:  0.8984375
train loss:  0.2589782774448395
train gradient:  0.11019044825446388
iteration : 11388
train acc:  0.875
train loss:  0.3240416347980499
train gradient:  0.1191682994426066
iteration : 11389
train acc:  0.875
train loss:  0.29152536392211914
train gradient:  0.17964350903929271
iteration : 11390
train acc:  0.8828125
train loss:  0.28600379824638367
train gradient:  0.12329849264090666
iteration : 11391
train acc:  0.890625
train loss:  0.30515995621681213
train gradient:  0.14457094316204355
iteration : 11392
train acc:  0.78125
train loss:  0.40639811754226685
train gradient:  0.2577247130200778
iteration : 11393
train acc:  0.890625
train loss:  0.2617833614349365
train gradient:  0.11051824752595248
iteration : 11394
train acc:  0.875
train loss:  0.3044819235801697
train gradient:  0.13608264587479865
iteration : 11395
train acc:  0.8828125
train loss:  0.3115394115447998
train gradient:  0.2927537694916797
iteration : 11396
train acc:  0.90625
train loss:  0.23487365245819092
train gradient:  0.08647622317827611
iteration : 11397
train acc:  0.8828125
train loss:  0.32026207447052
train gradient:  0.1326760503144698
iteration : 11398
train acc:  0.859375
train loss:  0.32136231660842896
train gradient:  0.13253663227863036
iteration : 11399
train acc:  0.8203125
train loss:  0.36178138852119446
train gradient:  0.1851996424344351
iteration : 11400
train acc:  0.8671875
train loss:  0.30700570344924927
train gradient:  0.11161502765021221
iteration : 11401
train acc:  0.890625
train loss:  0.29538848996162415
train gradient:  0.11191922544376712
iteration : 11402
train acc:  0.8671875
train loss:  0.39432162046432495
train gradient:  0.21720311727684247
iteration : 11403
train acc:  0.890625
train loss:  0.2759828269481659
train gradient:  0.09316854932876947
iteration : 11404
train acc:  0.8359375
train loss:  0.4166514575481415
train gradient:  0.2038291024707954
iteration : 11405
train acc:  0.8671875
train loss:  0.3104872405529022
train gradient:  0.11765295207440947
iteration : 11406
train acc:  0.8046875
train loss:  0.33192330598831177
train gradient:  0.20817632474626696
iteration : 11407
train acc:  0.8203125
train loss:  0.4025012254714966
train gradient:  0.26821804359277945
iteration : 11408
train acc:  0.890625
train loss:  0.30896908044815063
train gradient:  0.21072522335589275
iteration : 11409
train acc:  0.859375
train loss:  0.2838450074195862
train gradient:  0.12902018853216352
iteration : 11410
train acc:  0.8828125
train loss:  0.3096720278263092
train gradient:  0.1878521489771185
iteration : 11411
train acc:  0.90625
train loss:  0.24367448687553406
train gradient:  0.0770178785539661
iteration : 11412
train acc:  0.8984375
train loss:  0.274183452129364
train gradient:  0.15557478585476273
iteration : 11413
train acc:  0.8515625
train loss:  0.3311833143234253
train gradient:  0.13355361371019883
iteration : 11414
train acc:  0.8828125
train loss:  0.3291745185852051
train gradient:  0.16562469452758855
iteration : 11415
train acc:  0.8671875
train loss:  0.3697980046272278
train gradient:  0.16922906589240772
iteration : 11416
train acc:  0.828125
train loss:  0.3380654454231262
train gradient:  0.16863665962226487
iteration : 11417
train acc:  0.8671875
train loss:  0.35504841804504395
train gradient:  0.14115918682700188
iteration : 11418
train acc:  0.859375
train loss:  0.33086347579956055
train gradient:  0.15843502100971313
iteration : 11419
train acc:  0.8359375
train loss:  0.35042691230773926
train gradient:  0.18844701850308582
iteration : 11420
train acc:  0.859375
train loss:  0.34982556104660034
train gradient:  0.13078013362865443
iteration : 11421
train acc:  0.8359375
train loss:  0.3133242726325989
train gradient:  0.15304427562501974
iteration : 11422
train acc:  0.8984375
train loss:  0.2794371247291565
train gradient:  0.127414709453951
iteration : 11423
train acc:  0.8828125
train loss:  0.26916319131851196
train gradient:  0.11162625044210148
iteration : 11424
train acc:  0.9140625
train loss:  0.23814386129379272
train gradient:  0.09592154911653246
iteration : 11425
train acc:  0.875
train loss:  0.33818161487579346
train gradient:  0.1402049043623149
iteration : 11426
train acc:  0.8984375
train loss:  0.22281990945339203
train gradient:  0.09160868179492299
iteration : 11427
train acc:  0.796875
train loss:  0.4555172920227051
train gradient:  0.26040734083926537
iteration : 11428
train acc:  0.8125
train loss:  0.35975372791290283
train gradient:  0.1501275000493985
iteration : 11429
train acc:  0.875
train loss:  0.26991915702819824
train gradient:  0.15847535005535723
iteration : 11430
train acc:  0.828125
train loss:  0.3649364113807678
train gradient:  0.1659310911613902
iteration : 11431
train acc:  0.9140625
train loss:  0.26685094833374023
train gradient:  0.13639553010162214
iteration : 11432
train acc:  0.8359375
train loss:  0.34815436601638794
train gradient:  0.1435136858306549
iteration : 11433
train acc:  0.8515625
train loss:  0.31096723675727844
train gradient:  0.13444997298953462
iteration : 11434
train acc:  0.890625
train loss:  0.32099759578704834
train gradient:  0.12147431461859025
iteration : 11435
train acc:  0.859375
train loss:  0.3478718400001526
train gradient:  0.2105495655237733
iteration : 11436
train acc:  0.859375
train loss:  0.2794804573059082
train gradient:  0.12234129746386835
iteration : 11437
train acc:  0.875
train loss:  0.3288964629173279
train gradient:  0.14988600994954676
iteration : 11438
train acc:  0.84375
train loss:  0.36699917912483215
train gradient:  0.18078215711556742
iteration : 11439
train acc:  0.8515625
train loss:  0.3854248523712158
train gradient:  0.19989448422142445
iteration : 11440
train acc:  0.8671875
train loss:  0.3350299596786499
train gradient:  0.12920229027948316
iteration : 11441
train acc:  0.8828125
train loss:  0.3149067759513855
train gradient:  0.2225174602085789
iteration : 11442
train acc:  0.890625
train loss:  0.2934059798717499
train gradient:  0.11304242855745414
iteration : 11443
train acc:  0.8515625
train loss:  0.3272014260292053
train gradient:  0.18830602736822397
iteration : 11444
train acc:  0.9140625
train loss:  0.2938862144947052
train gradient:  0.1584274146776806
iteration : 11445
train acc:  0.8671875
train loss:  0.35825008153915405
train gradient:  0.161432257115283
iteration : 11446
train acc:  0.8125
train loss:  0.3494613766670227
train gradient:  0.1688844145472687
iteration : 11447
train acc:  0.875
train loss:  0.35426294803619385
train gradient:  0.18172985497231794
iteration : 11448
train acc:  0.875
train loss:  0.3359861969947815
train gradient:  0.1455422091829242
iteration : 11449
train acc:  0.8515625
train loss:  0.3693423867225647
train gradient:  0.2226059694770185
iteration : 11450
train acc:  0.84375
train loss:  0.30566179752349854
train gradient:  0.1261200542753286
iteration : 11451
train acc:  0.8203125
train loss:  0.3377925157546997
train gradient:  0.1532288489330238
iteration : 11452
train acc:  0.8359375
train loss:  0.38313525915145874
train gradient:  0.2232219279534881
iteration : 11453
train acc:  0.8984375
train loss:  0.28216788172721863
train gradient:  0.08379789911792394
iteration : 11454
train acc:  0.828125
train loss:  0.3267955183982849
train gradient:  0.09845059307878924
iteration : 11455
train acc:  0.8515625
train loss:  0.32950130105018616
train gradient:  0.19393870335363994
iteration : 11456
train acc:  0.8359375
train loss:  0.3830373287200928
train gradient:  0.19794828108370266
iteration : 11457
train acc:  0.859375
train loss:  0.35009562969207764
train gradient:  0.2144547831378682
iteration : 11458
train acc:  0.8203125
train loss:  0.36301177740097046
train gradient:  0.2670035543456205
iteration : 11459
train acc:  0.828125
train loss:  0.34552091360092163
train gradient:  0.17048494923362914
iteration : 11460
train acc:  0.890625
train loss:  0.26142793893814087
train gradient:  0.17773807481798767
iteration : 11461
train acc:  0.9375
train loss:  0.21291527152061462
train gradient:  0.07610900254730474
iteration : 11462
train acc:  0.859375
train loss:  0.36538007855415344
train gradient:  0.1584211546983036
iteration : 11463
train acc:  0.921875
train loss:  0.2207249402999878
train gradient:  0.10225275028299696
iteration : 11464
train acc:  0.875
train loss:  0.26490867137908936
train gradient:  0.16789986498209244
iteration : 11465
train acc:  0.859375
train loss:  0.3085804581642151
train gradient:  0.2544731109008345
iteration : 11466
train acc:  0.8828125
train loss:  0.31116870045661926
train gradient:  0.11230048115600674
iteration : 11467
train acc:  0.84375
train loss:  0.3742097020149231
train gradient:  0.23222213325554736
iteration : 11468
train acc:  0.84375
train loss:  0.36174219846725464
train gradient:  0.1504286265346307
iteration : 11469
train acc:  0.9140625
train loss:  0.2540644705295563
train gradient:  0.1100867208054473
iteration : 11470
train acc:  0.84375
train loss:  0.3182024657726288
train gradient:  0.1967046023896602
iteration : 11471
train acc:  0.8671875
train loss:  0.28466731309890747
train gradient:  0.10858535370082849
iteration : 11472
train acc:  0.8984375
train loss:  0.22164568305015564
train gradient:  0.14078087226844194
iteration : 11473
train acc:  0.8671875
train loss:  0.2808414101600647
train gradient:  0.12243512866959497
iteration : 11474
train acc:  0.84375
train loss:  0.36325228214263916
train gradient:  0.1937859267721585
iteration : 11475
train acc:  0.8671875
train loss:  0.2944283187389374
train gradient:  0.09376124449961304
iteration : 11476
train acc:  0.8046875
train loss:  0.36435478925704956
train gradient:  0.1717123650887089
iteration : 11477
train acc:  0.8203125
train loss:  0.3468519449234009
train gradient:  0.1493636400601468
iteration : 11478
train acc:  0.8515625
train loss:  0.2957639694213867
train gradient:  0.14590584464089226
iteration : 11479
train acc:  0.8515625
train loss:  0.2942749857902527
train gradient:  0.1447627326199568
iteration : 11480
train acc:  0.9296875
train loss:  0.24096640944480896
train gradient:  0.08519526750746437
iteration : 11481
train acc:  0.8671875
train loss:  0.2538280487060547
train gradient:  0.09719963801816325
iteration : 11482
train acc:  0.859375
train loss:  0.32700860500335693
train gradient:  0.24124160273392597
iteration : 11483
train acc:  0.8984375
train loss:  0.30024823546409607
train gradient:  0.13898653061718072
iteration : 11484
train acc:  0.9140625
train loss:  0.2834392786026001
train gradient:  0.19556345957241783
iteration : 11485
train acc:  0.8671875
train loss:  0.31513768434524536
train gradient:  0.2098002627699132
iteration : 11486
train acc:  0.796875
train loss:  0.49290886521339417
train gradient:  0.3766811250888876
iteration : 11487
train acc:  0.8671875
train loss:  0.41707539558410645
train gradient:  0.2121557735247364
iteration : 11488
train acc:  0.8515625
train loss:  0.311721533536911
train gradient:  0.17362411037073944
iteration : 11489
train acc:  0.90625
train loss:  0.2674708068370819
train gradient:  0.10687534316917476
iteration : 11490
train acc:  0.828125
train loss:  0.38279467821121216
train gradient:  0.15232023720190208
iteration : 11491
train acc:  0.90625
train loss:  0.24180461466312408
train gradient:  0.10803310790598443
iteration : 11492
train acc:  0.8671875
train loss:  0.3298070430755615
train gradient:  0.13732400485205487
iteration : 11493
train acc:  0.84375
train loss:  0.3833381235599518
train gradient:  0.17507566103269978
iteration : 11494
train acc:  0.921875
train loss:  0.23341014981269836
train gradient:  0.09158848633386843
iteration : 11495
train acc:  0.8828125
train loss:  0.30569177865982056
train gradient:  0.16900655370517959
iteration : 11496
train acc:  0.875
train loss:  0.3017968535423279
train gradient:  0.10740121069248205
iteration : 11497
train acc:  0.8984375
train loss:  0.316809743642807
train gradient:  0.15008520300980188
iteration : 11498
train acc:  0.84375
train loss:  0.3595688045024872
train gradient:  0.19771020389686278
iteration : 11499
train acc:  0.8828125
train loss:  0.2709396481513977
train gradient:  0.10073971690119839
iteration : 11500
train acc:  0.8828125
train loss:  0.2702588438987732
train gradient:  0.1744265521121094
iteration : 11501
train acc:  0.8515625
train loss:  0.3146294057369232
train gradient:  0.14368451939472937
iteration : 11502
train acc:  0.890625
train loss:  0.2605983316898346
train gradient:  0.13256500527567702
iteration : 11503
train acc:  0.8125
train loss:  0.3500625491142273
train gradient:  0.15456645272802258
iteration : 11504
train acc:  0.9140625
train loss:  0.19298811256885529
train gradient:  0.10490461518040423
iteration : 11505
train acc:  0.8984375
train loss:  0.24630600214004517
train gradient:  0.09380140815834949
iteration : 11506
train acc:  0.890625
train loss:  0.2684018611907959
train gradient:  0.08199310310022569
iteration : 11507
train acc:  0.7890625
train loss:  0.4080250859260559
train gradient:  0.2046761765243307
iteration : 11508
train acc:  0.8671875
train loss:  0.3443739116191864
train gradient:  0.1229509234609366
iteration : 11509
train acc:  0.8671875
train loss:  0.340533971786499
train gradient:  0.16817048904384663
iteration : 11510
train acc:  0.78125
train loss:  0.4510837197303772
train gradient:  0.2560424467623101
iteration : 11511
train acc:  0.8671875
train loss:  0.31996139883995056
train gradient:  0.1454952498761169
iteration : 11512
train acc:  0.875
train loss:  0.2889033257961273
train gradient:  0.1412514035475429
iteration : 11513
train acc:  0.8671875
train loss:  0.3008272647857666
train gradient:  0.12424778455541863
iteration : 11514
train acc:  0.859375
train loss:  0.31285473704338074
train gradient:  0.14022916438939398
iteration : 11515
train acc:  0.8671875
train loss:  0.325870156288147
train gradient:  0.15060867913495216
iteration : 11516
train acc:  0.8671875
train loss:  0.3214624226093292
train gradient:  0.20814023037403737
iteration : 11517
train acc:  0.890625
train loss:  0.31645792722702026
train gradient:  0.13617690931580825
iteration : 11518
train acc:  0.7890625
train loss:  0.42391449213027954
train gradient:  0.22350823298029016
iteration : 11519
train acc:  0.90625
train loss:  0.3301088809967041
train gradient:  0.1270972164403976
iteration : 11520
train acc:  0.8203125
train loss:  0.37079471349716187
train gradient:  0.1516521871460396
iteration : 11521
train acc:  0.7890625
train loss:  0.3636263608932495
train gradient:  0.160460973674179
iteration : 11522
train acc:  0.8125
train loss:  0.37178659439086914
train gradient:  0.2501724274833128
iteration : 11523
train acc:  0.859375
train loss:  0.2722439169883728
train gradient:  0.10928630666912353
iteration : 11524
train acc:  0.9140625
train loss:  0.22925147414207458
train gradient:  0.08575574559000093
iteration : 11525
train acc:  0.9140625
train loss:  0.27429312467575073
train gradient:  0.09731132215929177
iteration : 11526
train acc:  0.828125
train loss:  0.32208263874053955
train gradient:  0.16864503831605748
iteration : 11527
train acc:  0.859375
train loss:  0.348646342754364
train gradient:  0.16236947676279384
iteration : 11528
train acc:  0.8984375
train loss:  0.29987823963165283
train gradient:  0.141061041938358
iteration : 11529
train acc:  0.890625
train loss:  0.2939208149909973
train gradient:  0.1257795546529519
iteration : 11530
train acc:  0.8671875
train loss:  0.2677021622657776
train gradient:  0.13498372384148744
iteration : 11531
train acc:  0.8125
train loss:  0.37765738368034363
train gradient:  0.20069674308799307
iteration : 11532
train acc:  0.8125
train loss:  0.36794203519821167
train gradient:  0.22885326654401164
iteration : 11533
train acc:  0.84375
train loss:  0.3970724940299988
train gradient:  0.17434775591855356
iteration : 11534
train acc:  0.890625
train loss:  0.24005095660686493
train gradient:  0.1165509000106974
iteration : 11535
train acc:  0.8203125
train loss:  0.315690815448761
train gradient:  0.1648417741481399
iteration : 11536
train acc:  0.875
train loss:  0.3498117923736572
train gradient:  0.18638580543008545
iteration : 11537
train acc:  0.8515625
train loss:  0.31494584679603577
train gradient:  0.17049534324306265
iteration : 11538
train acc:  0.859375
train loss:  0.30558186769485474
train gradient:  0.24282579872395615
iteration : 11539
train acc:  0.890625
train loss:  0.24886630475521088
train gradient:  0.12176789368068858
iteration : 11540
train acc:  0.8828125
train loss:  0.31255462765693665
train gradient:  0.17817709270255494
iteration : 11541
train acc:  0.921875
train loss:  0.2377975881099701
train gradient:  0.1253983184434535
iteration : 11542
train acc:  0.890625
train loss:  0.28868210315704346
train gradient:  0.08996653525775644
iteration : 11543
train acc:  0.8359375
train loss:  0.3788267970085144
train gradient:  0.2214191719429288
iteration : 11544
train acc:  0.8828125
train loss:  0.2846207022666931
train gradient:  0.18436384886955942
iteration : 11545
train acc:  0.84375
train loss:  0.31828129291534424
train gradient:  0.15435066943409675
iteration : 11546
train acc:  0.875
train loss:  0.24852094054222107
train gradient:  0.12946303627955769
iteration : 11547
train acc:  0.8359375
train loss:  0.39809542894363403
train gradient:  0.2187648848708541
iteration : 11548
train acc:  0.8203125
train loss:  0.3544495105743408
train gradient:  0.16207376367148402
iteration : 11549
train acc:  0.90625
train loss:  0.2347361445426941
train gradient:  0.07738136804941587
iteration : 11550
train acc:  0.875
train loss:  0.3231351375579834
train gradient:  0.23361179923191028
iteration : 11551
train acc:  0.84375
train loss:  0.3480225205421448
train gradient:  0.174926638692816
iteration : 11552
train acc:  0.8359375
train loss:  0.3645162284374237
train gradient:  0.1409391462945481
iteration : 11553
train acc:  0.890625
train loss:  0.2874193787574768
train gradient:  0.09949702307971064
iteration : 11554
train acc:  0.890625
train loss:  0.2492729127407074
train gradient:  0.10879280758031977
iteration : 11555
train acc:  0.90625
train loss:  0.27731677889823914
train gradient:  0.09938795612712506
iteration : 11556
train acc:  0.859375
train loss:  0.33431828022003174
train gradient:  0.13889630234334033
iteration : 11557
train acc:  0.8515625
train loss:  0.34780651330947876
train gradient:  0.16628749506153867
iteration : 11558
train acc:  0.875
train loss:  0.28880906105041504
train gradient:  0.14400945046729013
iteration : 11559
train acc:  0.8359375
train loss:  0.3957814574241638
train gradient:  0.19214670309614346
iteration : 11560
train acc:  0.828125
train loss:  0.39496082067489624
train gradient:  0.22990133111394212
iteration : 11561
train acc:  0.8359375
train loss:  0.33997854590415955
train gradient:  0.13269816286475172
iteration : 11562
train acc:  0.84375
train loss:  0.3265559673309326
train gradient:  0.131859566422285
iteration : 11563
train acc:  0.8515625
train loss:  0.30697059631347656
train gradient:  0.1229851315008205
iteration : 11564
train acc:  0.8125
train loss:  0.40703505277633667
train gradient:  0.1928743538751739
iteration : 11565
train acc:  0.8984375
train loss:  0.2686161696910858
train gradient:  0.10359175374591703
iteration : 11566
train acc:  0.8359375
train loss:  0.3510800004005432
train gradient:  0.15354820275522826
iteration : 11567
train acc:  0.8984375
train loss:  0.26140379905700684
train gradient:  0.09151341281468259
iteration : 11568
train acc:  0.8046875
train loss:  0.3970060348510742
train gradient:  0.18005461766910952
iteration : 11569
train acc:  0.828125
train loss:  0.3690316677093506
train gradient:  0.1591810821109576
iteration : 11570
train acc:  0.84375
train loss:  0.3815673291683197
train gradient:  0.19539423997960934
iteration : 11571
train acc:  0.8359375
train loss:  0.3448166251182556
train gradient:  0.1684574976884807
iteration : 11572
train acc:  0.8515625
train loss:  0.3772115707397461
train gradient:  0.18210651936708
iteration : 11573
train acc:  0.8828125
train loss:  0.2866981327533722
train gradient:  0.08500087754815255
iteration : 11574
train acc:  0.8671875
train loss:  0.30064547061920166
train gradient:  0.11905447612049914
iteration : 11575
train acc:  0.8203125
train loss:  0.3502015173435211
train gradient:  0.23562454282641307
iteration : 11576
train acc:  0.828125
train loss:  0.3457539677619934
train gradient:  0.17133255023715355
iteration : 11577
train acc:  0.8671875
train loss:  0.28223347663879395
train gradient:  0.11336231281694796
iteration : 11578
train acc:  0.8671875
train loss:  0.2729957699775696
train gradient:  0.1242234767527971
iteration : 11579
train acc:  0.8828125
train loss:  0.3288365304470062
train gradient:  0.16529824898524953
iteration : 11580
train acc:  0.8203125
train loss:  0.37293219566345215
train gradient:  0.2301550630024941
iteration : 11581
train acc:  0.828125
train loss:  0.35168954730033875
train gradient:  0.1462871495974034
iteration : 11582
train acc:  0.875
train loss:  0.3096681237220764
train gradient:  0.09960006289898314
iteration : 11583
train acc:  0.828125
train loss:  0.38355904817581177
train gradient:  0.18511440885259473
iteration : 11584
train acc:  0.84375
train loss:  0.3398464322090149
train gradient:  0.12684415577890024
iteration : 11585
train acc:  0.8671875
train loss:  0.29645293951034546
train gradient:  0.10794034707393685
iteration : 11586
train acc:  0.8515625
train loss:  0.2960401177406311
train gradient:  0.15665581163023595
iteration : 11587
train acc:  0.8515625
train loss:  0.33740144968032837
train gradient:  0.14468558594049194
iteration : 11588
train acc:  0.8515625
train loss:  0.26729315519332886
train gradient:  0.09782555771085115
iteration : 11589
train acc:  0.84375
train loss:  0.32594501972198486
train gradient:  0.1339470811936581
iteration : 11590
train acc:  0.8671875
train loss:  0.28813159465789795
train gradient:  0.16082987755738185
iteration : 11591
train acc:  0.8046875
train loss:  0.3876352906227112
train gradient:  0.25700495153048386
iteration : 11592
train acc:  0.875
train loss:  0.2928977608680725
train gradient:  0.19807559708404582
iteration : 11593
train acc:  0.875
train loss:  0.3340747654438019
train gradient:  0.15356772057642132
iteration : 11594
train acc:  0.8203125
train loss:  0.4045109748840332
train gradient:  0.13171095268283625
iteration : 11595
train acc:  0.828125
train loss:  0.3500764071941376
train gradient:  0.15599033304866966
iteration : 11596
train acc:  0.859375
train loss:  0.3683171272277832
train gradient:  0.20413321072154184
iteration : 11597
train acc:  0.8515625
train loss:  0.3390744924545288
train gradient:  0.14227520522247356
iteration : 11598
train acc:  0.8515625
train loss:  0.30282264947891235
train gradient:  0.08568742017488154
iteration : 11599
train acc:  0.9140625
train loss:  0.24417564272880554
train gradient:  0.09505454728042471
iteration : 11600
train acc:  0.8828125
train loss:  0.2777043581008911
train gradient:  0.11128373755775642
iteration : 11601
train acc:  0.859375
train loss:  0.30900099873542786
train gradient:  0.11881654139907792
iteration : 11602
train acc:  0.8828125
train loss:  0.31046944856643677
train gradient:  0.12332061558712419
iteration : 11603
train acc:  0.875
train loss:  0.3034728467464447
train gradient:  0.13119621212489463
iteration : 11604
train acc:  0.8515625
train loss:  0.3357124924659729
train gradient:  0.5959053029866099
iteration : 11605
train acc:  0.84375
train loss:  0.32210245728492737
train gradient:  0.10453873691302065
iteration : 11606
train acc:  0.8046875
train loss:  0.43618494272232056
train gradient:  0.23059095951727024
iteration : 11607
train acc:  0.875
train loss:  0.28604215383529663
train gradient:  0.15352481477529945
iteration : 11608
train acc:  0.8359375
train loss:  0.3285556137561798
train gradient:  0.13630609939777444
iteration : 11609
train acc:  0.8828125
train loss:  0.299736350774765
train gradient:  0.08464047370024068
iteration : 11610
train acc:  0.875
train loss:  0.27221012115478516
train gradient:  0.1217450006577401
iteration : 11611
train acc:  0.9296875
train loss:  0.23192264139652252
train gradient:  0.12488765083042774
iteration : 11612
train acc:  0.8515625
train loss:  0.30928835272789
train gradient:  0.1830611082987192
iteration : 11613
train acc:  0.8515625
train loss:  0.32488104701042175
train gradient:  0.12913998377404354
iteration : 11614
train acc:  0.875
train loss:  0.2837773561477661
train gradient:  0.14151349753859027
iteration : 11615
train acc:  0.859375
train loss:  0.3827626705169678
train gradient:  0.2625691060438914
iteration : 11616
train acc:  0.859375
train loss:  0.32271042466163635
train gradient:  0.1565280032565755
iteration : 11617
train acc:  0.875
train loss:  0.3177494704723358
train gradient:  0.14745767389867503
iteration : 11618
train acc:  0.84375
train loss:  0.37800145149230957
train gradient:  0.16305293994426795
iteration : 11619
train acc:  0.890625
train loss:  0.30459457635879517
train gradient:  0.40269717512246145
iteration : 11620
train acc:  0.8203125
train loss:  0.3802642226219177
train gradient:  0.14995179183895224
iteration : 11621
train acc:  0.8125
train loss:  0.35269564390182495
train gradient:  0.13424304099372175
iteration : 11622
train acc:  0.8359375
train loss:  0.38439422845840454
train gradient:  0.21461577429729414
iteration : 11623
train acc:  0.828125
train loss:  0.33970940113067627
train gradient:  0.18811457599534895
iteration : 11624
train acc:  0.8515625
train loss:  0.3233891725540161
train gradient:  0.11789803674041334
iteration : 11625
train acc:  0.828125
train loss:  0.3489493131637573
train gradient:  0.1771580510340299
iteration : 11626
train acc:  0.890625
train loss:  0.2444213181734085
train gradient:  0.08993078232342182
iteration : 11627
train acc:  0.859375
train loss:  0.30526310205459595
train gradient:  0.15102951615623256
iteration : 11628
train acc:  0.8984375
train loss:  0.27507343888282776
train gradient:  0.09350019739789876
iteration : 11629
train acc:  0.890625
train loss:  0.2425214946269989
train gradient:  0.1006628486511426
iteration : 11630
train acc:  0.90625
train loss:  0.2912518084049225
train gradient:  0.09982486755064623
iteration : 11631
train acc:  0.8515625
train loss:  0.36398845911026
train gradient:  0.16688872578524988
iteration : 11632
train acc:  0.8515625
train loss:  0.34274518489837646
train gradient:  0.12018150937420277
iteration : 11633
train acc:  0.875
train loss:  0.3107811212539673
train gradient:  0.17181900279869708
iteration : 11634
train acc:  0.8671875
train loss:  0.30063632130622864
train gradient:  0.10991424767430451
iteration : 11635
train acc:  0.8671875
train loss:  0.3435925245285034
train gradient:  0.14415916661223205
iteration : 11636
train acc:  0.8359375
train loss:  0.3973589539527893
train gradient:  0.1849510840838561
iteration : 11637
train acc:  0.875
train loss:  0.41554808616638184
train gradient:  0.15134369188535107
iteration : 11638
train acc:  0.8125
train loss:  0.40361443161964417
train gradient:  0.28854917331607144
iteration : 11639
train acc:  0.859375
train loss:  0.3610273599624634
train gradient:  0.1592444448930819
iteration : 11640
train acc:  0.875
train loss:  0.3029150068759918
train gradient:  0.11297021586409181
iteration : 11641
train acc:  0.8671875
train loss:  0.31077221035957336
train gradient:  0.14038854925655841
iteration : 11642
train acc:  0.8515625
train loss:  0.3279900848865509
train gradient:  0.1489299557104682
iteration : 11643
train acc:  0.796875
train loss:  0.3715452253818512
train gradient:  0.17091488077517514
iteration : 11644
train acc:  0.8828125
train loss:  0.2885197401046753
train gradient:  0.1522274827307112
iteration : 11645
train acc:  0.9140625
train loss:  0.27385812997817993
train gradient:  0.09521235786161139
iteration : 11646
train acc:  0.7734375
train loss:  0.46465566754341125
train gradient:  0.2551985154209876
iteration : 11647
train acc:  0.828125
train loss:  0.35836371779441833
train gradient:  0.15442501945838122
iteration : 11648
train acc:  0.84375
train loss:  0.2925206124782562
train gradient:  0.15407159821598165
iteration : 11649
train acc:  0.8515625
train loss:  0.2744144797325134
train gradient:  0.1175656177430057
iteration : 11650
train acc:  0.8671875
train loss:  0.27855831384658813
train gradient:  0.10830797596673147
iteration : 11651
train acc:  0.875
train loss:  0.3059326410293579
train gradient:  0.08538236215180774
iteration : 11652
train acc:  0.875
train loss:  0.29575759172439575
train gradient:  0.08827749679800626
iteration : 11653
train acc:  0.875
train loss:  0.35548487305641174
train gradient:  0.17126459147717552
iteration : 11654
train acc:  0.8203125
train loss:  0.33771225810050964
train gradient:  0.13941377406996164
iteration : 11655
train acc:  0.8671875
train loss:  0.3232429623603821
train gradient:  0.17187759159515176
iteration : 11656
train acc:  0.859375
train loss:  0.3138453960418701
train gradient:  0.19993189915700113
iteration : 11657
train acc:  0.84375
train loss:  0.37645626068115234
train gradient:  0.21701473509136981
iteration : 11658
train acc:  0.9375
train loss:  0.2004609853029251
train gradient:  0.0836129636059661
iteration : 11659
train acc:  0.8515625
train loss:  0.3063398599624634
train gradient:  0.13673107457387124
iteration : 11660
train acc:  0.8828125
train loss:  0.270068496465683
train gradient:  0.21754216264400728
iteration : 11661
train acc:  0.8828125
train loss:  0.276854932308197
train gradient:  0.16787384745692718
iteration : 11662
train acc:  0.8671875
train loss:  0.23956768214702606
train gradient:  0.17357879770478385
iteration : 11663
train acc:  0.828125
train loss:  0.4084966778755188
train gradient:  0.18947311406989462
iteration : 11664
train acc:  0.828125
train loss:  0.3700374364852905
train gradient:  0.15964285767641695
iteration : 11665
train acc:  0.9140625
train loss:  0.25231754779815674
train gradient:  0.16154703989539082
iteration : 11666
train acc:  0.859375
train loss:  0.3991418182849884
train gradient:  0.24179620867085752
iteration : 11667
train acc:  0.8359375
train loss:  0.3181855380535126
train gradient:  0.12494418751002315
iteration : 11668
train acc:  0.8671875
train loss:  0.29937779903411865
train gradient:  0.12877158331897043
iteration : 11669
train acc:  0.78125
train loss:  0.4487003982067108
train gradient:  0.2690303126516099
iteration : 11670
train acc:  0.8359375
train loss:  0.35295549035072327
train gradient:  0.17289585977300836
iteration : 11671
train acc:  0.8671875
train loss:  0.2997957766056061
train gradient:  0.12360100108663166
iteration : 11672
train acc:  0.84375
train loss:  0.3112771213054657
train gradient:  0.11841238127554436
iteration : 11673
train acc:  0.875
train loss:  0.3138761520385742
train gradient:  0.1721941957953299
iteration : 11674
train acc:  0.828125
train loss:  0.3662831485271454
train gradient:  0.2350601456261373
iteration : 11675
train acc:  0.875
train loss:  0.3408348858356476
train gradient:  0.12239628009067198
iteration : 11676
train acc:  0.828125
train loss:  0.3683520257472992
train gradient:  0.15644111911984648
iteration : 11677
train acc:  0.890625
train loss:  0.2586863338947296
train gradient:  0.096072085552945
iteration : 11678
train acc:  0.8515625
train loss:  0.30210036039352417
train gradient:  0.17632910354623987
iteration : 11679
train acc:  0.8125
train loss:  0.35338249802589417
train gradient:  0.3820929813903173
iteration : 11680
train acc:  0.8515625
train loss:  0.32053306698799133
train gradient:  0.14245898279115693
iteration : 11681
train acc:  0.8046875
train loss:  0.47118592262268066
train gradient:  0.2914126629433785
iteration : 11682
train acc:  0.90625
train loss:  0.26577186584472656
train gradient:  0.09313465004204263
iteration : 11683
train acc:  0.84375
train loss:  0.4350854456424713
train gradient:  0.3279509352030191
iteration : 11684
train acc:  0.90625
train loss:  0.27010253071784973
train gradient:  0.14032893794365514
iteration : 11685
train acc:  0.8359375
train loss:  0.35920649766921997
train gradient:  0.18165033140452547
iteration : 11686
train acc:  0.890625
train loss:  0.316244900226593
train gradient:  0.1054938059929061
iteration : 11687
train acc:  0.84375
train loss:  0.3943428099155426
train gradient:  0.21518424036197903
iteration : 11688
train acc:  0.84375
train loss:  0.34806373715400696
train gradient:  0.21709204205092042
iteration : 11689
train acc:  0.8984375
train loss:  0.2517881393432617
train gradient:  0.09060674105500752
iteration : 11690
train acc:  0.859375
train loss:  0.352161169052124
train gradient:  0.20967194388600474
iteration : 11691
train acc:  0.8828125
train loss:  0.2844400405883789
train gradient:  0.12211430923667092
iteration : 11692
train acc:  0.8828125
train loss:  0.26202166080474854
train gradient:  0.09686178665671642
iteration : 11693
train acc:  0.859375
train loss:  0.37350916862487793
train gradient:  0.1846322204069402
iteration : 11694
train acc:  0.8515625
train loss:  0.29423096776008606
train gradient:  0.1548689136256938
iteration : 11695
train acc:  0.8359375
train loss:  0.29853129386901855
train gradient:  0.12930826359826125
iteration : 11696
train acc:  0.7734375
train loss:  0.4076771140098572
train gradient:  0.29420354449948
iteration : 11697
train acc:  0.8828125
train loss:  0.2565939426422119
train gradient:  0.12177467949239337
iteration : 11698
train acc:  0.90625
train loss:  0.30540725588798523
train gradient:  0.1953005622551537
iteration : 11699
train acc:  0.9140625
train loss:  0.25122350454330444
train gradient:  0.13038829297085472
iteration : 11700
train acc:  0.828125
train loss:  0.365852952003479
train gradient:  0.1753549209917498
iteration : 11701
train acc:  0.8203125
train loss:  0.3431313633918762
train gradient:  0.18016090714182653
iteration : 11702
train acc:  0.859375
train loss:  0.29921114444732666
train gradient:  0.1799473144422777
iteration : 11703
train acc:  0.8828125
train loss:  0.27125465869903564
train gradient:  0.10280495132381108
iteration : 11704
train acc:  0.8046875
train loss:  0.44483062624931335
train gradient:  0.19262811933149038
iteration : 11705
train acc:  0.9140625
train loss:  0.29969194531440735
train gradient:  0.1258282692910734
iteration : 11706
train acc:  0.8359375
train loss:  0.3153262734413147
train gradient:  0.15283126410755357
iteration : 11707
train acc:  0.890625
train loss:  0.2963975667953491
train gradient:  0.10462813528222902
iteration : 11708
train acc:  0.890625
train loss:  0.26732832193374634
train gradient:  0.10181277319986685
iteration : 11709
train acc:  0.84375
train loss:  0.3467312455177307
train gradient:  0.1441197168907053
iteration : 11710
train acc:  0.8125
train loss:  0.4290066957473755
train gradient:  0.20945642489503724
iteration : 11711
train acc:  0.8828125
train loss:  0.3494417369365692
train gradient:  0.1597539958657633
iteration : 11712
train acc:  0.8046875
train loss:  0.3680058419704437
train gradient:  0.13344817758338406
iteration : 11713
train acc:  0.8828125
train loss:  0.3651459813117981
train gradient:  0.18726056328252685
iteration : 11714
train acc:  0.8671875
train loss:  0.2901865839958191
train gradient:  0.09988795628538191
iteration : 11715
train acc:  0.8515625
train loss:  0.3087157607078552
train gradient:  0.13011984365982504
iteration : 11716
train acc:  0.8359375
train loss:  0.3160763680934906
train gradient:  0.16637496784034633
iteration : 11717
train acc:  0.8828125
train loss:  0.2644832134246826
train gradient:  0.14656431440652035
iteration : 11718
train acc:  0.859375
train loss:  0.31588470935821533
train gradient:  0.1279891986444371
iteration : 11719
train acc:  0.828125
train loss:  0.4030344784259796
train gradient:  0.19195097265761085
iteration : 11720
train acc:  0.8359375
train loss:  0.32465922832489014
train gradient:  0.1154681694275493
iteration : 11721
train acc:  0.8515625
train loss:  0.321658730506897
train gradient:  0.0954837569658197
iteration : 11722
train acc:  0.8828125
train loss:  0.27835866808891296
train gradient:  0.1595399719084163
iteration : 11723
train acc:  0.8828125
train loss:  0.3135570287704468
train gradient:  0.15808367153057434
iteration : 11724
train acc:  0.8203125
train loss:  0.39619141817092896
train gradient:  0.17249201076796405
iteration : 11725
train acc:  0.8203125
train loss:  0.35833537578582764
train gradient:  0.18516657429757033
iteration : 11726
train acc:  0.859375
train loss:  0.3257960081100464
train gradient:  0.12882029362513836
iteration : 11727
train acc:  0.890625
train loss:  0.258083701133728
train gradient:  0.11984134921934637
iteration : 11728
train acc:  0.8359375
train loss:  0.34933149814605713
train gradient:  0.26164583916619955
iteration : 11729
train acc:  0.8671875
train loss:  0.288993775844574
train gradient:  0.12375440030774529
iteration : 11730
train acc:  0.8359375
train loss:  0.31261885166168213
train gradient:  0.10680559395142192
iteration : 11731
train acc:  0.84375
train loss:  0.34902891516685486
train gradient:  0.206307281084659
iteration : 11732
train acc:  0.78125
train loss:  0.45342835783958435
train gradient:  0.2709648694448979
iteration : 11733
train acc:  0.84375
train loss:  0.37391582131385803
train gradient:  0.22649354431652136
iteration : 11734
train acc:  0.8828125
train loss:  0.30812615156173706
train gradient:  0.1486941213159534
iteration : 11735
train acc:  0.8515625
train loss:  0.3575518727302551
train gradient:  0.2680235694489202
iteration : 11736
train acc:  0.859375
train loss:  0.29975074529647827
train gradient:  0.13944815877758251
iteration : 11737
train acc:  0.8984375
train loss:  0.350522518157959
train gradient:  0.1495722320392234
iteration : 11738
train acc:  0.90625
train loss:  0.24820394814014435
train gradient:  0.13365920523220676
iteration : 11739
train acc:  0.8125
train loss:  0.3655439615249634
train gradient:  0.20900123394852227
iteration : 11740
train acc:  0.875
train loss:  0.2808789312839508
train gradient:  0.1488851964222021
iteration : 11741
train acc:  0.859375
train loss:  0.34087976813316345
train gradient:  0.14076349978998748
iteration : 11742
train acc:  0.890625
train loss:  0.34334465861320496
train gradient:  0.17616758534551272
iteration : 11743
train acc:  0.859375
train loss:  0.3497321605682373
train gradient:  0.1529693818118095
iteration : 11744
train acc:  0.890625
train loss:  0.26244598627090454
train gradient:  0.1289208160365462
iteration : 11745
train acc:  0.859375
train loss:  0.3137738108634949
train gradient:  0.1401631592863521
iteration : 11746
train acc:  0.8671875
train loss:  0.3230239748954773
train gradient:  0.14353745576942156
iteration : 11747
train acc:  0.8359375
train loss:  0.35231196880340576
train gradient:  0.10607581689606156
iteration : 11748
train acc:  0.8828125
train loss:  0.2885016202926636
train gradient:  0.14237208002862162
iteration : 11749
train acc:  0.8671875
train loss:  0.3240543007850647
train gradient:  0.11338326067281039
iteration : 11750
train acc:  0.875
train loss:  0.2718827724456787
train gradient:  0.10496435982811574
iteration : 11751
train acc:  0.875
train loss:  0.38740551471710205
train gradient:  0.15286616685971743
iteration : 11752
train acc:  0.890625
train loss:  0.2664691209793091
train gradient:  0.13688256983311453
iteration : 11753
train acc:  0.8515625
train loss:  0.3312987983226776
train gradient:  0.10080799545395543
iteration : 11754
train acc:  0.8515625
train loss:  0.31653210520744324
train gradient:  0.1025762373364798
iteration : 11755
train acc:  0.8359375
train loss:  0.3614925444126129
train gradient:  0.21571448683712718
iteration : 11756
train acc:  0.859375
train loss:  0.29959651827812195
train gradient:  0.1387568186910723
iteration : 11757
train acc:  0.890625
train loss:  0.2740287482738495
train gradient:  0.1271974201038497
iteration : 11758
train acc:  0.9140625
train loss:  0.31276756525039673
train gradient:  0.18208755740425087
iteration : 11759
train acc:  0.84375
train loss:  0.32526466250419617
train gradient:  0.1269984143998786
iteration : 11760
train acc:  0.828125
train loss:  0.37037986516952515
train gradient:  0.19034766565236214
iteration : 11761
train acc:  0.875
train loss:  0.3109448254108429
train gradient:  0.10842235018152191
iteration : 11762
train acc:  0.8359375
train loss:  0.2855900526046753
train gradient:  0.35330845568008
iteration : 11763
train acc:  0.84375
train loss:  0.33689069747924805
train gradient:  0.14754649659086838
iteration : 11764
train acc:  0.796875
train loss:  0.3983232378959656
train gradient:  0.24050795687029244
iteration : 11765
train acc:  0.84375
train loss:  0.3511393070220947
train gradient:  0.14969163596177965
iteration : 11766
train acc:  0.8671875
train loss:  0.3278648555278778
train gradient:  0.13855441945214825
iteration : 11767
train acc:  0.84375
train loss:  0.3824731707572937
train gradient:  0.1715721073048563
iteration : 11768
train acc:  0.7890625
train loss:  0.4128738045692444
train gradient:  0.2388786442784461
iteration : 11769
train acc:  0.8359375
train loss:  0.381171852350235
train gradient:  0.16853974335870808
iteration : 11770
train acc:  0.828125
train loss:  0.3133491277694702
train gradient:  0.12824343036969288
iteration : 11771
train acc:  0.8828125
train loss:  0.3237314224243164
train gradient:  0.17333875325829695
iteration : 11772
train acc:  0.90625
train loss:  0.24982094764709473
train gradient:  0.0690819947886337
iteration : 11773
train acc:  0.8125
train loss:  0.36126551032066345
train gradient:  0.1273517947956776
iteration : 11774
train acc:  0.8203125
train loss:  0.48437395691871643
train gradient:  0.30676136182117186
iteration : 11775
train acc:  0.8671875
train loss:  0.28149592876434326
train gradient:  0.08365824985545381
iteration : 11776
train acc:  0.84375
train loss:  0.3385286033153534
train gradient:  0.13701987789466336
iteration : 11777
train acc:  0.8671875
train loss:  0.32680732011795044
train gradient:  0.1361996467595627
iteration : 11778
train acc:  0.8515625
train loss:  0.35980552434921265
train gradient:  0.16617100126671208
iteration : 11779
train acc:  0.8203125
train loss:  0.38006117939949036
train gradient:  0.12449380633414707
iteration : 11780
train acc:  0.875
train loss:  0.30799275636672974
train gradient:  0.10761401199736197
iteration : 11781
train acc:  0.890625
train loss:  0.3000970780849457
train gradient:  0.12661858700213058
iteration : 11782
train acc:  0.8203125
train loss:  0.3200068473815918
train gradient:  0.19563247409371998
iteration : 11783
train acc:  0.90625
train loss:  0.23900511860847473
train gradient:  0.14623647268864431
iteration : 11784
train acc:  0.8125
train loss:  0.32808759808540344
train gradient:  0.13229390995726825
iteration : 11785
train acc:  0.890625
train loss:  0.3026190400123596
train gradient:  0.14373619624557085
iteration : 11786
train acc:  0.7890625
train loss:  0.3922582268714905
train gradient:  0.20902315055046028
iteration : 11787
train acc:  0.84375
train loss:  0.35204780101776123
train gradient:  0.15872053145267398
iteration : 11788
train acc:  0.859375
train loss:  0.3044745922088623
train gradient:  0.12935358179875583
iteration : 11789
train acc:  0.859375
train loss:  0.3273000419139862
train gradient:  0.2253873471070177
iteration : 11790
train acc:  0.9140625
train loss:  0.28753674030303955
train gradient:  0.15457694738057792
iteration : 11791
train acc:  0.859375
train loss:  0.32498764991760254
train gradient:  0.15615762815735867
iteration : 11792
train acc:  0.890625
train loss:  0.2771158516407013
train gradient:  0.15796557481294987
iteration : 11793
train acc:  0.8046875
train loss:  0.39284083247184753
train gradient:  0.2056479779158819
iteration : 11794
train acc:  0.8828125
train loss:  0.27614980936050415
train gradient:  0.11882057343261147
iteration : 11795
train acc:  0.8515625
train loss:  0.3703904151916504
train gradient:  0.1703999570696612
iteration : 11796
train acc:  0.8828125
train loss:  0.29737845063209534
train gradient:  0.1628396172955476
iteration : 11797
train acc:  0.8984375
train loss:  0.23956727981567383
train gradient:  0.08208430875571074
iteration : 11798
train acc:  0.8515625
train loss:  0.3388340473175049
train gradient:  0.14577527242253596
iteration : 11799
train acc:  0.859375
train loss:  0.33462366461753845
train gradient:  0.11504106819018035
iteration : 11800
train acc:  0.859375
train loss:  0.34102049469947815
train gradient:  0.11564172833152844
iteration : 11801
train acc:  0.921875
train loss:  0.21288007497787476
train gradient:  0.07022002819305234
iteration : 11802
train acc:  0.8515625
train loss:  0.34719088673591614
train gradient:  0.15266660789276645
iteration : 11803
train acc:  0.8359375
train loss:  0.3766845464706421
train gradient:  0.15887780820603686
iteration : 11804
train acc:  0.859375
train loss:  0.282955139875412
train gradient:  0.12233466709176313
iteration : 11805
train acc:  0.875
train loss:  0.32893577218055725
train gradient:  0.1582885027317107
iteration : 11806
train acc:  0.84375
train loss:  0.3455982208251953
train gradient:  0.1643374971473887
iteration : 11807
train acc:  0.859375
train loss:  0.33880850672721863
train gradient:  0.14328748284804163
iteration : 11808
train acc:  0.890625
train loss:  0.2823304831981659
train gradient:  0.1224504058250888
iteration : 11809
train acc:  0.8984375
train loss:  0.304401695728302
train gradient:  0.1486463351982913
iteration : 11810
train acc:  0.84375
train loss:  0.37234461307525635
train gradient:  0.13295476189458738
iteration : 11811
train acc:  0.828125
train loss:  0.3786667287349701
train gradient:  0.20686181629503003
iteration : 11812
train acc:  0.84375
train loss:  0.3462178111076355
train gradient:  0.1398459159431279
iteration : 11813
train acc:  0.8671875
train loss:  0.29024267196655273
train gradient:  0.12085664508915443
iteration : 11814
train acc:  0.859375
train loss:  0.3656386137008667
train gradient:  0.184616366521479
iteration : 11815
train acc:  0.8359375
train loss:  0.33434125781059265
train gradient:  0.15680975561653585
iteration : 11816
train acc:  0.875
train loss:  0.38317766785621643
train gradient:  0.14578095679137304
iteration : 11817
train acc:  0.8671875
train loss:  0.31745007634162903
train gradient:  0.12031837851931561
iteration : 11818
train acc:  0.8984375
train loss:  0.2705748677253723
train gradient:  0.09223286281469702
iteration : 11819
train acc:  0.828125
train loss:  0.349852979183197
train gradient:  0.19207416604499472
iteration : 11820
train acc:  0.8359375
train loss:  0.35976237058639526
train gradient:  0.29966174350904495
iteration : 11821
train acc:  0.828125
train loss:  0.34250548481941223
train gradient:  0.17611916886195522
iteration : 11822
train acc:  0.78125
train loss:  0.47094982862472534
train gradient:  0.3730655959531972
iteration : 11823
train acc:  0.828125
train loss:  0.32521897554397583
train gradient:  0.1808695016718806
iteration : 11824
train acc:  0.8203125
train loss:  0.3992396295070648
train gradient:  0.18346232306187898
iteration : 11825
train acc:  0.8828125
train loss:  0.29999712109565735
train gradient:  0.10537139454820389
iteration : 11826
train acc:  0.828125
train loss:  0.3658110499382019
train gradient:  0.17944597646558103
iteration : 11827
train acc:  0.84375
train loss:  0.36031651496887207
train gradient:  0.11624963599855784
iteration : 11828
train acc:  0.8828125
train loss:  0.3202579915523529
train gradient:  0.1643933334007858
iteration : 11829
train acc:  0.875
train loss:  0.3039610981941223
train gradient:  0.2761340101830194
iteration : 11830
train acc:  0.9140625
train loss:  0.2899959683418274
train gradient:  0.19946616279513418
iteration : 11831
train acc:  0.8359375
train loss:  0.35171616077423096
train gradient:  0.1802385829123372
iteration : 11832
train acc:  0.890625
train loss:  0.318716824054718
train gradient:  0.16730746801707
iteration : 11833
train acc:  0.875
train loss:  0.2889418601989746
train gradient:  0.0999779702139162
iteration : 11834
train acc:  0.828125
train loss:  0.32413250207901
train gradient:  0.17062133161639437
iteration : 11835
train acc:  0.875
train loss:  0.33159613609313965
train gradient:  0.10382268304941054
iteration : 11836
train acc:  0.8515625
train loss:  0.3524894118309021
train gradient:  0.14463133429321273
iteration : 11837
train acc:  0.8828125
train loss:  0.3155861496925354
train gradient:  0.10628501323448093
iteration : 11838
train acc:  0.875
train loss:  0.3477730453014374
train gradient:  0.12723274396512657
iteration : 11839
train acc:  0.84375
train loss:  0.36560294032096863
train gradient:  0.22100752178289906
iteration : 11840
train acc:  0.8515625
train loss:  0.31928426027297974
train gradient:  0.1109301478197984
iteration : 11841
train acc:  0.8671875
train loss:  0.3156518340110779
train gradient:  0.13877079543357462
iteration : 11842
train acc:  0.8359375
train loss:  0.3916803002357483
train gradient:  0.2413736470185897
iteration : 11843
train acc:  0.875
train loss:  0.23899324238300323
train gradient:  0.11972794475050763
iteration : 11844
train acc:  0.875
train loss:  0.32225823402404785
train gradient:  0.12161540556855126
iteration : 11845
train acc:  0.875
train loss:  0.27913975715637207
train gradient:  0.10111116566395362
iteration : 11846
train acc:  0.890625
train loss:  0.28435707092285156
train gradient:  0.1319009316425449
iteration : 11847
train acc:  0.828125
train loss:  0.345651775598526
train gradient:  0.13098089478680913
iteration : 11848
train acc:  0.84375
train loss:  0.33837711811065674
train gradient:  0.164771865039404
iteration : 11849
train acc:  0.890625
train loss:  0.2996046543121338
train gradient:  0.18473425541422903
iteration : 11850
train acc:  0.8671875
train loss:  0.2857744097709656
train gradient:  0.12051358204887584
iteration : 11851
train acc:  0.859375
train loss:  0.280680775642395
train gradient:  0.14191651537036465
iteration : 11852
train acc:  0.9296875
train loss:  0.23517777025699615
train gradient:  0.09414721718284286
iteration : 11853
train acc:  0.84375
train loss:  0.3237709701061249
train gradient:  0.18309009030552803
iteration : 11854
train acc:  0.8125
train loss:  0.43130454421043396
train gradient:  0.21148591526013466
iteration : 11855
train acc:  0.890625
train loss:  0.3327219784259796
train gradient:  0.18210253271139865
iteration : 11856
train acc:  0.875
train loss:  0.33007895946502686
train gradient:  0.2162827003282461
iteration : 11857
train acc:  0.890625
train loss:  0.26967644691467285
train gradient:  0.11445809399099942
iteration : 11858
train acc:  0.859375
train loss:  0.3350021243095398
train gradient:  0.1361906729049533
iteration : 11859
train acc:  0.8125
train loss:  0.40818312764167786
train gradient:  0.2599702329061192
iteration : 11860
train acc:  0.890625
train loss:  0.24401505291461945
train gradient:  0.08676755009356082
iteration : 11861
train acc:  0.8125
train loss:  0.3656194806098938
train gradient:  0.24011343116876122
iteration : 11862
train acc:  0.8515625
train loss:  0.37461525201797485
train gradient:  0.14837416925421318
iteration : 11863
train acc:  0.84375
train loss:  0.3945460915565491
train gradient:  0.18100838787448453
iteration : 11864
train acc:  0.828125
train loss:  0.38901352882385254
train gradient:  0.17518497511916833
iteration : 11865
train acc:  0.8828125
train loss:  0.3015359342098236
train gradient:  0.12033588798901579
iteration : 11866
train acc:  0.8359375
train loss:  0.4447762668132782
train gradient:  0.19455431598469683
iteration : 11867
train acc:  0.921875
train loss:  0.23341389000415802
train gradient:  0.1256933453896722
iteration : 11868
train acc:  0.8515625
train loss:  0.28117603063583374
train gradient:  0.09615648416433964
iteration : 11869
train acc:  0.859375
train loss:  0.29135990142822266
train gradient:  0.13943231688710067
iteration : 11870
train acc:  0.8515625
train loss:  0.33757320046424866
train gradient:  0.1403626887033633
iteration : 11871
train acc:  0.875
train loss:  0.3573932945728302
train gradient:  0.18981099971006232
iteration : 11872
train acc:  0.8671875
train loss:  0.30304789543151855
train gradient:  0.13287828181335862
iteration : 11873
train acc:  0.8515625
train loss:  0.3198818266391754
train gradient:  0.12105111669424103
iteration : 11874
train acc:  0.890625
train loss:  0.2672026753425598
train gradient:  0.13039319665290477
iteration : 11875
train acc:  0.828125
train loss:  0.36174821853637695
train gradient:  0.17297153118907288
iteration : 11876
train acc:  0.90625
train loss:  0.2375471144914627
train gradient:  0.07337709757842742
iteration : 11877
train acc:  0.8671875
train loss:  0.3563944101333618
train gradient:  0.2345231937560658
iteration : 11878
train acc:  0.8046875
train loss:  0.35539573431015015
train gradient:  0.14354976710154915
iteration : 11879
train acc:  0.8984375
train loss:  0.3048175573348999
train gradient:  0.1308989921865546
iteration : 11880
train acc:  0.859375
train loss:  0.33511704206466675
train gradient:  0.18355745984025562
iteration : 11881
train acc:  0.875
train loss:  0.300394207239151
train gradient:  0.13792705376850922
iteration : 11882
train acc:  0.90625
train loss:  0.2456909716129303
train gradient:  0.1831841259390204
iteration : 11883
train acc:  0.8671875
train loss:  0.3246117830276489
train gradient:  0.1265087454961531
iteration : 11884
train acc:  0.8828125
train loss:  0.32714569568634033
train gradient:  0.16413295363982333
iteration : 11885
train acc:  0.8203125
train loss:  0.3673481345176697
train gradient:  0.2229274646613571
iteration : 11886
train acc:  0.8984375
train loss:  0.315252423286438
train gradient:  0.11167705174396553
iteration : 11887
train acc:  0.828125
train loss:  0.375121533870697
train gradient:  0.21188759401537266
iteration : 11888
train acc:  0.796875
train loss:  0.38512715697288513
train gradient:  0.20260160458594806
iteration : 11889
train acc:  0.875
train loss:  0.33138737082481384
train gradient:  0.14256024565497677
iteration : 11890
train acc:  0.859375
train loss:  0.29044967889785767
train gradient:  0.11952542712287059
iteration : 11891
train acc:  0.84375
train loss:  0.32586783170700073
train gradient:  0.14321020589384176
iteration : 11892
train acc:  0.84375
train loss:  0.34054774045944214
train gradient:  0.1461140005178903
iteration : 11893
train acc:  0.8828125
train loss:  0.3222755789756775
train gradient:  0.17342097888619895
iteration : 11894
train acc:  0.8828125
train loss:  0.2707657217979431
train gradient:  0.11571618019204272
iteration : 11895
train acc:  0.875
train loss:  0.35594838857650757
train gradient:  0.1950463373465805
iteration : 11896
train acc:  0.8515625
train loss:  0.4027149975299835
train gradient:  0.20224793338073638
iteration : 11897
train acc:  0.8671875
train loss:  0.29750126600265503
train gradient:  0.12192082114936767
iteration : 11898
train acc:  0.84375
train loss:  0.2736317813396454
train gradient:  0.13999695710810636
iteration : 11899
train acc:  0.8125
train loss:  0.44270163774490356
train gradient:  0.21145360884644038
iteration : 11900
train acc:  0.8671875
train loss:  0.3611863851547241
train gradient:  0.2604216815850389
iteration : 11901
train acc:  0.8359375
train loss:  0.3504623770713806
train gradient:  0.20710012846291365
iteration : 11902
train acc:  0.8984375
train loss:  0.2742130160331726
train gradient:  0.11819085219095891
iteration : 11903
train acc:  0.8984375
train loss:  0.27152496576309204
train gradient:  0.14041326308238752
iteration : 11904
train acc:  0.8515625
train loss:  0.30080553889274597
train gradient:  0.12622366214820424
iteration : 11905
train acc:  0.8125
train loss:  0.3923126757144928
train gradient:  0.20998450739814153
iteration : 11906
train acc:  0.8671875
train loss:  0.3019997179508209
train gradient:  0.12587164552065708
iteration : 11907
train acc:  0.875
train loss:  0.3361172676086426
train gradient:  0.11387276427757476
iteration : 11908
train acc:  0.9140625
train loss:  0.20705023407936096
train gradient:  0.06899376369802918
iteration : 11909
train acc:  0.8515625
train loss:  0.35739636421203613
train gradient:  0.18318691933026865
iteration : 11910
train acc:  0.8515625
train loss:  0.3155026435852051
train gradient:  0.15951529202255355
iteration : 11911
train acc:  0.875
train loss:  0.33229511976242065
train gradient:  0.19685835059670115
iteration : 11912
train acc:  0.84375
train loss:  0.33434587717056274
train gradient:  0.13348712356212517
iteration : 11913
train acc:  0.828125
train loss:  0.31136682629585266
train gradient:  0.16816355432734198
iteration : 11914
train acc:  0.8125
train loss:  0.3394741117954254
train gradient:  0.22620224047171505
iteration : 11915
train acc:  0.90625
train loss:  0.26815295219421387
train gradient:  0.1253798862573336
iteration : 11916
train acc:  0.90625
train loss:  0.295510470867157
train gradient:  0.10513903495321605
iteration : 11917
train acc:  0.7734375
train loss:  0.4754525423049927
train gradient:  0.25652142353207114
iteration : 11918
train acc:  0.828125
train loss:  0.3847404718399048
train gradient:  0.2709767026792428
iteration : 11919
train acc:  0.828125
train loss:  0.34876447916030884
train gradient:  0.17852469570507623
iteration : 11920
train acc:  0.875
train loss:  0.2780757248401642
train gradient:  0.09225957315149236
iteration : 11921
train acc:  0.875
train loss:  0.29860642552375793
train gradient:  0.14381810109466184
iteration : 11922
train acc:  0.875
train loss:  0.304623544216156
train gradient:  0.19445070087917019
iteration : 11923
train acc:  0.8671875
train loss:  0.32453685998916626
train gradient:  0.16359488817805046
iteration : 11924
train acc:  0.890625
train loss:  0.2606407701969147
train gradient:  0.12001409942761303
iteration : 11925
train acc:  0.8515625
train loss:  0.33373764157295227
train gradient:  0.14252677121197907
iteration : 11926
train acc:  0.859375
train loss:  0.27595680952072144
train gradient:  0.10240588701377978
iteration : 11927
train acc:  0.859375
train loss:  0.2986757159233093
train gradient:  0.11565097870132492
iteration : 11928
train acc:  0.859375
train loss:  0.3512449264526367
train gradient:  0.23880509363187896
iteration : 11929
train acc:  0.875
train loss:  0.29890841245651245
train gradient:  0.16040363885463632
iteration : 11930
train acc:  0.8125
train loss:  0.33840474486351013
train gradient:  0.15317242951704388
iteration : 11931
train acc:  0.8671875
train loss:  0.334037184715271
train gradient:  0.14666686299665502
iteration : 11932
train acc:  0.8984375
train loss:  0.2846437096595764
train gradient:  0.13918317672510083
iteration : 11933
train acc:  0.875
train loss:  0.29191553592681885
train gradient:  0.11543471784415132
iteration : 11934
train acc:  0.9296875
train loss:  0.17763502895832062
train gradient:  0.08293569958344726
iteration : 11935
train acc:  0.90625
train loss:  0.285999059677124
train gradient:  0.12080232673378288
iteration : 11936
train acc:  0.875
train loss:  0.33695530891418457
train gradient:  0.14075457697025295
iteration : 11937
train acc:  0.8515625
train loss:  0.3728032112121582
train gradient:  0.1801319952618424
iteration : 11938
train acc:  0.7734375
train loss:  0.4281105101108551
train gradient:  0.1862333011325848
iteration : 11939
train acc:  0.8671875
train loss:  0.300853431224823
train gradient:  0.1184050306591029
iteration : 11940
train acc:  0.8359375
train loss:  0.35287731885910034
train gradient:  0.15633949563607238
iteration : 11941
train acc:  0.8515625
train loss:  0.299078106880188
train gradient:  0.1185331901173502
iteration : 11942
train acc:  0.890625
train loss:  0.2847261428833008
train gradient:  0.11425891913337081
iteration : 11943
train acc:  0.8828125
train loss:  0.3141449987888336
train gradient:  0.20505923727521963
iteration : 11944
train acc:  0.8359375
train loss:  0.37101417779922485
train gradient:  0.20930425815138445
iteration : 11945
train acc:  0.859375
train loss:  0.3264636695384979
train gradient:  0.1687389404917116
iteration : 11946
train acc:  0.8046875
train loss:  0.3708115220069885
train gradient:  0.15624965702993682
iteration : 11947
train acc:  0.84375
train loss:  0.33542048931121826
train gradient:  0.1633622867468946
iteration : 11948
train acc:  0.8671875
train loss:  0.3236270248889923
train gradient:  0.28989388558382584
iteration : 11949
train acc:  0.8984375
train loss:  0.22299683094024658
train gradient:  0.07373676198822535
iteration : 11950
train acc:  0.84375
train loss:  0.2910640239715576
train gradient:  0.1328508578024057
iteration : 11951
train acc:  0.8203125
train loss:  0.40428483486175537
train gradient:  0.17725539834822018
iteration : 11952
train acc:  0.90625
train loss:  0.23458436131477356
train gradient:  0.09776658940493103
iteration : 11953
train acc:  0.859375
train loss:  0.3215349018573761
train gradient:  0.2177919500173111
iteration : 11954
train acc:  0.8984375
train loss:  0.2529817819595337
train gradient:  0.12275874741480172
iteration : 11955
train acc:  0.8125
train loss:  0.4013993740081787
train gradient:  0.23040133292554332
iteration : 11956
train acc:  0.8125
train loss:  0.40290117263793945
train gradient:  0.3475565029418399
iteration : 11957
train acc:  0.8515625
train loss:  0.3362145721912384
train gradient:  0.13915633504839012
iteration : 11958
train acc:  0.890625
train loss:  0.27631640434265137
train gradient:  0.1079725461422252
iteration : 11959
train acc:  0.875
train loss:  0.29780256748199463
train gradient:  0.1198455500804281
iteration : 11960
train acc:  0.8515625
train loss:  0.34196335077285767
train gradient:  0.16072513299964616
iteration : 11961
train acc:  0.859375
train loss:  0.3362393081188202
train gradient:  0.1784307117649772
iteration : 11962
train acc:  0.8828125
train loss:  0.2774183452129364
train gradient:  0.13142762704731864
iteration : 11963
train acc:  0.9140625
train loss:  0.261160671710968
train gradient:  0.10319903488121601
iteration : 11964
train acc:  0.7890625
train loss:  0.37916800379753113
train gradient:  0.1888808218077554
iteration : 11965
train acc:  0.8671875
train loss:  0.3106195032596588
train gradient:  0.1621420371614986
iteration : 11966
train acc:  0.8203125
train loss:  0.3904891014099121
train gradient:  0.26093179258546484
iteration : 11967
train acc:  0.875
train loss:  0.24709399044513702
train gradient:  0.09450780790819112
iteration : 11968
train acc:  0.8828125
train loss:  0.3208443522453308
train gradient:  0.1174092358455362
iteration : 11969
train acc:  0.8828125
train loss:  0.33282792568206787
train gradient:  0.158775478194062
iteration : 11970
train acc:  0.8828125
train loss:  0.27329710125923157
train gradient:  0.11504550694160598
iteration : 11971
train acc:  0.8203125
train loss:  0.3726332187652588
train gradient:  0.14358156617728876
iteration : 11972
train acc:  0.8984375
train loss:  0.2997124195098877
train gradient:  0.13492828495151388
iteration : 11973
train acc:  0.84375
train loss:  0.3147691488265991
train gradient:  0.13585903851744352
iteration : 11974
train acc:  0.8828125
train loss:  0.32350611686706543
train gradient:  0.13525272921102216
iteration : 11975
train acc:  0.8203125
train loss:  0.366894394159317
train gradient:  0.14048704483599522
iteration : 11976
train acc:  0.890625
train loss:  0.32310691475868225
train gradient:  0.1549068040191786
iteration : 11977
train acc:  0.8671875
train loss:  0.30987080931663513
train gradient:  0.1577060176107491
iteration : 11978
train acc:  0.8515625
train loss:  0.35228753089904785
train gradient:  0.17498663226136002
iteration : 11979
train acc:  0.8515625
train loss:  0.3387148976325989
train gradient:  0.14135485376232637
iteration : 11980
train acc:  0.90625
train loss:  0.24631410837173462
train gradient:  0.12055823290306233
iteration : 11981
train acc:  0.8828125
train loss:  0.2543746829032898
train gradient:  0.08007470837732511
iteration : 11982
train acc:  0.859375
train loss:  0.37092235684394836
train gradient:  0.22365095638986127
iteration : 11983
train acc:  0.8359375
train loss:  0.3688042163848877
train gradient:  0.19898296138968924
iteration : 11984
train acc:  0.8515625
train loss:  0.29329729080200195
train gradient:  0.11402386152856266
iteration : 11985
train acc:  0.828125
train loss:  0.33812761306762695
train gradient:  0.1773562207187891
iteration : 11986
train acc:  0.84375
train loss:  0.3144383430480957
train gradient:  0.1143814948718975
iteration : 11987
train acc:  0.8515625
train loss:  0.3105301260948181
train gradient:  0.11311711032802885
iteration : 11988
train acc:  0.8984375
train loss:  0.29668349027633667
train gradient:  0.09957408988395666
iteration : 11989
train acc:  0.859375
train loss:  0.2676096260547638
train gradient:  0.100754539837579
iteration : 11990
train acc:  0.859375
train loss:  0.35312914848327637
train gradient:  0.14220906421231794
iteration : 11991
train acc:  0.890625
train loss:  0.3086320161819458
train gradient:  0.138819980387305
iteration : 11992
train acc:  0.8671875
train loss:  0.29811543226242065
train gradient:  0.13036345009682376
iteration : 11993
train acc:  0.8984375
train loss:  0.25578609108924866
train gradient:  0.07695766334956664
iteration : 11994
train acc:  0.84375
train loss:  0.32564687728881836
train gradient:  0.17351895934084585
iteration : 11995
train acc:  0.84375
train loss:  0.3961881101131439
train gradient:  0.1643056466201149
iteration : 11996
train acc:  0.859375
train loss:  0.33345216512680054
train gradient:  0.1502050915934042
iteration : 11997
train acc:  0.875
train loss:  0.2975582182407379
train gradient:  0.1320502626410618
iteration : 11998
train acc:  0.8046875
train loss:  0.40196114778518677
train gradient:  0.1620764968566149
iteration : 11999
train acc:  0.8515625
train loss:  0.3560880124568939
train gradient:  0.2040879846698728
iteration : 12000
train acc:  0.90625
train loss:  0.2433747947216034
train gradient:  0.07884191844661825
iteration : 12001
train acc:  0.890625
train loss:  0.2935716509819031
train gradient:  0.13334554914349167
iteration : 12002
train acc:  0.875
train loss:  0.27745094895362854
train gradient:  0.12532272272233946
iteration : 12003
train acc:  0.859375
train loss:  0.3212091326713562
train gradient:  0.09755520574804674
iteration : 12004
train acc:  0.90625
train loss:  0.23357036709785461
train gradient:  0.11977266569833438
iteration : 12005
train acc:  0.875
train loss:  0.26841121912002563
train gradient:  0.10928622066795578
iteration : 12006
train acc:  0.875
train loss:  0.34017330408096313
train gradient:  0.17241560756345442
iteration : 12007
train acc:  0.84375
train loss:  0.35231298208236694
train gradient:  0.1317012164989888
iteration : 12008
train acc:  0.890625
train loss:  0.3055195212364197
train gradient:  0.12558352438095205
iteration : 12009
train acc:  0.90625
train loss:  0.2670314908027649
train gradient:  0.10581775397761072
iteration : 12010
train acc:  0.875
train loss:  0.3265231251716614
train gradient:  0.13632467459292547
iteration : 12011
train acc:  0.8984375
train loss:  0.2517535388469696
train gradient:  0.10580959154141067
iteration : 12012
train acc:  0.8828125
train loss:  0.29401957988739014
train gradient:  0.21074515592514992
iteration : 12013
train acc:  0.8203125
train loss:  0.381736695766449
train gradient:  0.16524483510501764
iteration : 12014
train acc:  0.9140625
train loss:  0.228939950466156
train gradient:  0.08970153510022512
iteration : 12015
train acc:  0.921875
train loss:  0.20533180236816406
train gradient:  0.06442284679995124
iteration : 12016
train acc:  0.875
train loss:  0.2808234691619873
train gradient:  0.13206978158103455
iteration : 12017
train acc:  0.84375
train loss:  0.3585783541202545
train gradient:  0.2781561269220161
iteration : 12018
train acc:  0.84375
train loss:  0.356264591217041
train gradient:  0.190157244209581
iteration : 12019
train acc:  0.84375
train loss:  0.3637673258781433
train gradient:  0.14481950422163725
iteration : 12020
train acc:  0.8984375
train loss:  0.2853624224662781
train gradient:  0.10202782948279193
iteration : 12021
train acc:  0.8203125
train loss:  0.38297972083091736
train gradient:  0.23200156836543007
iteration : 12022
train acc:  0.9140625
train loss:  0.2762230634689331
train gradient:  0.11937413117862533
iteration : 12023
train acc:  0.8359375
train loss:  0.34501969814300537
train gradient:  0.1536032303170844
iteration : 12024
train acc:  0.8671875
train loss:  0.31374865770339966
train gradient:  0.10430296787981738
iteration : 12025
train acc:  0.890625
train loss:  0.3353099226951599
train gradient:  0.1408328521476752
iteration : 12026
train acc:  0.8828125
train loss:  0.2735482454299927
train gradient:  0.09585848680451832
iteration : 12027
train acc:  0.8125
train loss:  0.38418078422546387
train gradient:  0.316561210162607
iteration : 12028
train acc:  0.8515625
train loss:  0.3049120306968689
train gradient:  0.12208136602130015
iteration : 12029
train acc:  0.90625
train loss:  0.2796160578727722
train gradient:  0.1290985822958418
iteration : 12030
train acc:  0.7890625
train loss:  0.382345050573349
train gradient:  0.2542419941412136
iteration : 12031
train acc:  0.8359375
train loss:  0.3226458728313446
train gradient:  0.14418185776145187
iteration : 12032
train acc:  0.84375
train loss:  0.3659900426864624
train gradient:  0.21665174178403457
iteration : 12033
train acc:  0.859375
train loss:  0.3511064052581787
train gradient:  0.1539894681616406
iteration : 12034
train acc:  0.890625
train loss:  0.2794800400733948
train gradient:  0.11432419268182785
iteration : 12035
train acc:  0.890625
train loss:  0.24768909811973572
train gradient:  0.0946892524278484
iteration : 12036
train acc:  0.875
train loss:  0.3148457109928131
train gradient:  0.16994629056219962
iteration : 12037
train acc:  0.875
train loss:  0.2833942174911499
train gradient:  0.14886455102963322
iteration : 12038
train acc:  0.9140625
train loss:  0.32110798358917236
train gradient:  0.15019812459748522
iteration : 12039
train acc:  0.8359375
train loss:  0.34118348360061646
train gradient:  0.21035546675065958
iteration : 12040
train acc:  0.875
train loss:  0.27917706966400146
train gradient:  0.11284777089855376
iteration : 12041
train acc:  0.875
train loss:  0.27619802951812744
train gradient:  0.0950739534193905
iteration : 12042
train acc:  0.875
train loss:  0.2904317378997803
train gradient:  0.1140727791409257
iteration : 12043
train acc:  0.890625
train loss:  0.28781718015670776
train gradient:  0.12587286931873076
iteration : 12044
train acc:  0.8671875
train loss:  0.32175999879837036
train gradient:  0.13831491310023633
iteration : 12045
train acc:  0.8359375
train loss:  0.3913826048374176
train gradient:  0.2628338288003042
iteration : 12046
train acc:  0.8515625
train loss:  0.3341076672077179
train gradient:  0.19112883969382388
iteration : 12047
train acc:  0.8828125
train loss:  0.3212928771972656
train gradient:  0.16955084957642647
iteration : 12048
train acc:  0.921875
train loss:  0.2287861555814743
train gradient:  0.07482284789658135
iteration : 12049
train acc:  0.859375
train loss:  0.30280113220214844
train gradient:  0.14635612558938355
iteration : 12050
train acc:  0.8515625
train loss:  0.2929099500179291
train gradient:  0.09862777234168056
iteration : 12051
train acc:  0.8671875
train loss:  0.29710859060287476
train gradient:  0.1296201625078031
iteration : 12052
train acc:  0.8828125
train loss:  0.30026841163635254
train gradient:  0.12656610709708016
iteration : 12053
train acc:  0.84375
train loss:  0.3505963087081909
train gradient:  0.17862777773966684
iteration : 12054
train acc:  0.8671875
train loss:  0.3104366660118103
train gradient:  0.12023441858366563
iteration : 12055
train acc:  0.8828125
train loss:  0.3094899654388428
train gradient:  0.10715412562331221
iteration : 12056
train acc:  0.859375
train loss:  0.35123521089553833
train gradient:  0.1890809418510905
iteration : 12057
train acc:  0.8828125
train loss:  0.29936933517456055
train gradient:  0.1280844299828488
iteration : 12058
train acc:  0.84375
train loss:  0.330284059047699
train gradient:  0.1898581112719135
iteration : 12059
train acc:  0.875
train loss:  0.2801450788974762
train gradient:  0.09168305676353655
iteration : 12060
train acc:  0.9140625
train loss:  0.2026161253452301
train gradient:  0.05469500900319134
iteration : 12061
train acc:  0.828125
train loss:  0.32567155361175537
train gradient:  0.13283650873750472
iteration : 12062
train acc:  0.8828125
train loss:  0.3636731207370758
train gradient:  0.18969205701580738
iteration : 12063
train acc:  0.90625
train loss:  0.2414294332265854
train gradient:  0.08650140207802937
iteration : 12064
train acc:  0.828125
train loss:  0.3939293920993805
train gradient:  0.24332233219192223
iteration : 12065
train acc:  0.8828125
train loss:  0.2971474230289459
train gradient:  0.11720180590009414
iteration : 12066
train acc:  0.8359375
train loss:  0.3147987127304077
train gradient:  0.13596130453374863
iteration : 12067
train acc:  0.8984375
train loss:  0.24753686785697937
train gradient:  0.06956475111932303
iteration : 12068
train acc:  0.828125
train loss:  0.43378937244415283
train gradient:  0.2113688302246701
iteration : 12069
train acc:  0.84375
train loss:  0.32816261053085327
train gradient:  0.1618292110539358
iteration : 12070
train acc:  0.875
train loss:  0.3105495572090149
train gradient:  0.19606203011878537
iteration : 12071
train acc:  0.8515625
train loss:  0.34255942702293396
train gradient:  0.16753314267496616
iteration : 12072
train acc:  0.828125
train loss:  0.3845546841621399
train gradient:  0.17965670007764312
iteration : 12073
train acc:  0.8515625
train loss:  0.3940342664718628
train gradient:  0.23404529029163407
iteration : 12074
train acc:  0.90625
train loss:  0.232977032661438
train gradient:  0.1052489605596562
iteration : 12075
train acc:  0.890625
train loss:  0.2549844980239868
train gradient:  0.10590106331083393
iteration : 12076
train acc:  0.9140625
train loss:  0.27193909883499146
train gradient:  0.08825356301119792
iteration : 12077
train acc:  0.84375
train loss:  0.3445778489112854
train gradient:  0.1426742627012664
iteration : 12078
train acc:  0.875
train loss:  0.3274857997894287
train gradient:  0.14139643247022393
iteration : 12079
train acc:  0.890625
train loss:  0.2613084316253662
train gradient:  0.0919938846438929
iteration : 12080
train acc:  0.8671875
train loss:  0.2971263527870178
train gradient:  0.12310576212686683
iteration : 12081
train acc:  0.875
train loss:  0.27667728066444397
train gradient:  0.11982375625344503
iteration : 12082
train acc:  0.8359375
train loss:  0.3930884599685669
train gradient:  0.2835177388178175
iteration : 12083
train acc:  0.8046875
train loss:  0.3889343738555908
train gradient:  0.2894054598554135
iteration : 12084
train acc:  0.859375
train loss:  0.3562578558921814
train gradient:  0.13205440251917366
iteration : 12085
train acc:  0.8515625
train loss:  0.35782110691070557
train gradient:  0.3069504836777927
iteration : 12086
train acc:  0.8515625
train loss:  0.3433288335800171
train gradient:  0.15482657834872116
iteration : 12087
train acc:  0.9140625
train loss:  0.2511321008205414
train gradient:  0.10359535563369136
iteration : 12088
train acc:  0.7578125
train loss:  0.5009315013885498
train gradient:  0.28485304964471364
iteration : 12089
train acc:  0.90625
train loss:  0.26224440336227417
train gradient:  0.09074045473643495
iteration : 12090
train acc:  0.8125
train loss:  0.3753173351287842
train gradient:  0.18895700220752837
iteration : 12091
train acc:  0.8515625
train loss:  0.2995142340660095
train gradient:  0.1686391953486972
iteration : 12092
train acc:  0.859375
train loss:  0.3625674843788147
train gradient:  0.19234446335915314
iteration : 12093
train acc:  0.859375
train loss:  0.3126225769519806
train gradient:  0.2646090430170775
iteration : 12094
train acc:  0.8046875
train loss:  0.38075709342956543
train gradient:  0.1895408980443386
iteration : 12095
train acc:  0.7734375
train loss:  0.4644721448421478
train gradient:  0.27355948187542445
iteration : 12096
train acc:  0.890625
train loss:  0.25430816411972046
train gradient:  0.10036088355377208
iteration : 12097
train acc:  0.9140625
train loss:  0.2322605848312378
train gradient:  0.11125661302822502
iteration : 12098
train acc:  0.8671875
train loss:  0.36465853452682495
train gradient:  0.1437559721247786
iteration : 12099
train acc:  0.84375
train loss:  0.32695844769477844
train gradient:  0.2427684271727411
iteration : 12100
train acc:  0.859375
train loss:  0.270084410905838
train gradient:  0.12663170804595614
iteration : 12101
train acc:  0.921875
train loss:  0.2774352729320526
train gradient:  0.12368771016832258
iteration : 12102
train acc:  0.859375
train loss:  0.303703635931015
train gradient:  0.10729778801893226
iteration : 12103
train acc:  0.8671875
train loss:  0.3173345923423767
train gradient:  0.1377718744966861
iteration : 12104
train acc:  0.8359375
train loss:  0.3070816397666931
train gradient:  0.14896818841456239
iteration : 12105
train acc:  0.8203125
train loss:  0.33146071434020996
train gradient:  0.22226349024042968
iteration : 12106
train acc:  0.90625
train loss:  0.25622129440307617
train gradient:  0.09404448172330719
iteration : 12107
train acc:  0.8515625
train loss:  0.3287976384162903
train gradient:  0.12279821796126078
iteration : 12108
train acc:  0.8671875
train loss:  0.3202950954437256
train gradient:  0.10838732086869751
iteration : 12109
train acc:  0.875
train loss:  0.3160804510116577
train gradient:  0.11868026860853445
iteration : 12110
train acc:  0.84375
train loss:  0.33900707960128784
train gradient:  0.14523365182404877
iteration : 12111
train acc:  0.84375
train loss:  0.33234018087387085
train gradient:  0.13020515207118788
iteration : 12112
train acc:  0.859375
train loss:  0.3045802712440491
train gradient:  0.1156656870039456
iteration : 12113
train acc:  0.8828125
train loss:  0.2927756905555725
train gradient:  0.12573382941729624
iteration : 12114
train acc:  0.8671875
train loss:  0.34626689553260803
train gradient:  0.14652526131182436
iteration : 12115
train acc:  0.90625
train loss:  0.2575609087944031
train gradient:  0.09324691401910645
iteration : 12116
train acc:  0.8359375
train loss:  0.36468809843063354
train gradient:  0.1911133293259884
iteration : 12117
train acc:  0.9296875
train loss:  0.2739763855934143
train gradient:  0.09061475850345098
iteration : 12118
train acc:  0.859375
train loss:  0.3896564543247223
train gradient:  0.15687193454826692
iteration : 12119
train acc:  0.8828125
train loss:  0.3141888380050659
train gradient:  0.11929471032212849
iteration : 12120
train acc:  0.90625
train loss:  0.23817189037799835
train gradient:  0.09644424390460885
iteration : 12121
train acc:  0.859375
train loss:  0.3770008683204651
train gradient:  0.15772591537899927
iteration : 12122
train acc:  0.875
train loss:  0.27284523844718933
train gradient:  0.1388946393731404
iteration : 12123
train acc:  0.796875
train loss:  0.41962742805480957
train gradient:  0.17384709096671613
iteration : 12124
train acc:  0.875
train loss:  0.30464884638786316
train gradient:  0.09663210335796582
iteration : 12125
train acc:  0.90625
train loss:  0.2535261809825897
train gradient:  0.15158102317413238
iteration : 12126
train acc:  0.8046875
train loss:  0.3813570737838745
train gradient:  0.3271122438682707
iteration : 12127
train acc:  0.84375
train loss:  0.30084002017974854
train gradient:  0.11301167546289335
iteration : 12128
train acc:  0.8984375
train loss:  0.23906433582305908
train gradient:  0.12335939129556171
iteration : 12129
train acc:  0.859375
train loss:  0.31835681200027466
train gradient:  0.11831604966349456
iteration : 12130
train acc:  0.8359375
train loss:  0.3738136887550354
train gradient:  0.1790597416797443
iteration : 12131
train acc:  0.84375
train loss:  0.2860755920410156
train gradient:  0.14669933272226351
iteration : 12132
train acc:  0.8359375
train loss:  0.3592890501022339
train gradient:  0.19818149156646198
iteration : 12133
train acc:  0.8671875
train loss:  0.3097041845321655
train gradient:  0.11875176095574964
iteration : 12134
train acc:  0.859375
train loss:  0.2848206162452698
train gradient:  0.13429721095801525
iteration : 12135
train acc:  0.859375
train loss:  0.2939026355743408
train gradient:  0.11175008781762362
iteration : 12136
train acc:  0.90625
train loss:  0.25650089979171753
train gradient:  0.10407277422435515
iteration : 12137
train acc:  0.8828125
train loss:  0.31470146775245667
train gradient:  0.1078076222547553
iteration : 12138
train acc:  0.90625
train loss:  0.25285232067108154
train gradient:  0.0752754203055021
iteration : 12139
train acc:  0.90625
train loss:  0.2647761106491089
train gradient:  0.15458187313202731
iteration : 12140
train acc:  0.890625
train loss:  0.2546152174472809
train gradient:  0.07451045439454465
iteration : 12141
train acc:  0.8984375
train loss:  0.2870820164680481
train gradient:  0.1295260856969383
iteration : 12142
train acc:  0.84375
train loss:  0.3466441333293915
train gradient:  0.22290482995377797
iteration : 12143
train acc:  0.828125
train loss:  0.3655135929584503
train gradient:  0.16209685794381265
iteration : 12144
train acc:  0.859375
train loss:  0.30820176005363464
train gradient:  0.13238716407682738
iteration : 12145
train acc:  0.9140625
train loss:  0.2685440480709076
train gradient:  0.10106634289839354
iteration : 12146
train acc:  0.90625
train loss:  0.28508415818214417
train gradient:  0.18147221837261546
iteration : 12147
train acc:  0.859375
train loss:  0.3387633264064789
train gradient:  0.17806639843983155
iteration : 12148
train acc:  0.8671875
train loss:  0.3335357904434204
train gradient:  0.13204757388575933
iteration : 12149
train acc:  0.8671875
train loss:  0.2565402388572693
train gradient:  0.10776468480867464
iteration : 12150
train acc:  0.8359375
train loss:  0.3443511724472046
train gradient:  0.19828036450326858
iteration : 12151
train acc:  0.8984375
train loss:  0.32039347290992737
train gradient:  0.19412642222594198
iteration : 12152
train acc:  0.875
train loss:  0.30477598309516907
train gradient:  0.11773699256275401
iteration : 12153
train acc:  0.8515625
train loss:  0.2966143488883972
train gradient:  0.1402445911613829
iteration : 12154
train acc:  0.8828125
train loss:  0.2901986241340637
train gradient:  0.1229764597082938
iteration : 12155
train acc:  0.875
train loss:  0.2772037386894226
train gradient:  0.10627516334833542
iteration : 12156
train acc:  0.90625
train loss:  0.26388949155807495
train gradient:  0.12392955997447347
iteration : 12157
train acc:  0.890625
train loss:  0.3036796450614929
train gradient:  0.15078633523567891
iteration : 12158
train acc:  0.90625
train loss:  0.29560935497283936
train gradient:  0.12153473669989592
iteration : 12159
train acc:  0.78125
train loss:  0.4181232452392578
train gradient:  0.33816079391423437
iteration : 12160
train acc:  0.828125
train loss:  0.40766796469688416
train gradient:  0.23931399317799562
iteration : 12161
train acc:  0.890625
train loss:  0.27960413694381714
train gradient:  0.1382847977532004
iteration : 12162
train acc:  0.859375
train loss:  0.28525906801223755
train gradient:  0.13130304719870603
iteration : 12163
train acc:  0.90625
train loss:  0.23343580961227417
train gradient:  0.10940815134063511
iteration : 12164
train acc:  0.828125
train loss:  0.36288607120513916
train gradient:  0.24112409400029197
iteration : 12165
train acc:  0.890625
train loss:  0.24845993518829346
train gradient:  0.08984283576804167
iteration : 12166
train acc:  0.8671875
train loss:  0.30881261825561523
train gradient:  0.1425603663508149
iteration : 12167
train acc:  0.8203125
train loss:  0.35833507776260376
train gradient:  0.2469929341800427
iteration : 12168
train acc:  0.828125
train loss:  0.36363205313682556
train gradient:  0.1644098678947288
iteration : 12169
train acc:  0.875
train loss:  0.2660406231880188
train gradient:  0.4368176816996889
iteration : 12170
train acc:  0.875
train loss:  0.280502587556839
train gradient:  0.09583735553051231
iteration : 12171
train acc:  0.875
train loss:  0.31288179755210876
train gradient:  0.12259794861237751
iteration : 12172
train acc:  0.859375
train loss:  0.33361610770225525
train gradient:  0.14586769485898246
iteration : 12173
train acc:  0.84375
train loss:  0.3127138614654541
train gradient:  0.12820153763345218
iteration : 12174
train acc:  0.8203125
train loss:  0.4367688298225403
train gradient:  0.2286197489021347
iteration : 12175
train acc:  0.8828125
train loss:  0.3316118121147156
train gradient:  0.2036298044212138
iteration : 12176
train acc:  0.8671875
train loss:  0.27085211873054504
train gradient:  0.10528843303593709
iteration : 12177
train acc:  0.84375
train loss:  0.35964956879615784
train gradient:  0.13254051592922028
iteration : 12178
train acc:  0.84375
train loss:  0.3445906639099121
train gradient:  0.16707852852722443
iteration : 12179
train acc:  0.8515625
train loss:  0.3378421664237976
train gradient:  0.15142320770980267
iteration : 12180
train acc:  0.84375
train loss:  0.3494170308113098
train gradient:  0.14448330702008544
iteration : 12181
train acc:  0.8828125
train loss:  0.31380972266197205
train gradient:  0.19017833024847836
iteration : 12182
train acc:  0.875
train loss:  0.25886714458465576
train gradient:  0.10676230784289523
iteration : 12183
train acc:  0.859375
train loss:  0.3204176425933838
train gradient:  0.13559059267220014
iteration : 12184
train acc:  0.84375
train loss:  0.35638415813446045
train gradient:  0.1389544225718432
iteration : 12185
train acc:  0.8203125
train loss:  0.3617420494556427
train gradient:  0.20070640704426762
iteration : 12186
train acc:  0.8515625
train loss:  0.32256001234054565
train gradient:  0.19301278664643778
iteration : 12187
train acc:  0.890625
train loss:  0.2571844756603241
train gradient:  0.1093771125551476
iteration : 12188
train acc:  0.84375
train loss:  0.30299821496009827
train gradient:  0.16303908596135858
iteration : 12189
train acc:  0.8984375
train loss:  0.27841687202453613
train gradient:  0.1684699610835345
iteration : 12190
train acc:  0.875
train loss:  0.2947273850440979
train gradient:  0.1279240708975129
iteration : 12191
train acc:  0.90625
train loss:  0.2989661395549774
train gradient:  0.12856486068490783
iteration : 12192
train acc:  0.890625
train loss:  0.28622040152549744
train gradient:  0.1478229796733142
iteration : 12193
train acc:  0.8828125
train loss:  0.3249269127845764
train gradient:  0.1743752135212048
iteration : 12194
train acc:  0.859375
train loss:  0.37594538927078247
train gradient:  0.1819812587079384
iteration : 12195
train acc:  0.875
train loss:  0.2969217896461487
train gradient:  0.17339464376497793
iteration : 12196
train acc:  0.8828125
train loss:  0.26108425855636597
train gradient:  0.1383741415079554
iteration : 12197
train acc:  0.859375
train loss:  0.2776115834712982
train gradient:  0.11551770886834674
iteration : 12198
train acc:  0.9296875
train loss:  0.2552897036075592
train gradient:  0.08527614952561419
iteration : 12199
train acc:  0.8828125
train loss:  0.2392040193080902
train gradient:  0.14258904435983827
iteration : 12200
train acc:  0.8671875
train loss:  0.34343987703323364
train gradient:  0.22920026426088302
iteration : 12201
train acc:  0.875
train loss:  0.338718980550766
train gradient:  0.16362962465400444
iteration : 12202
train acc:  0.9296875
train loss:  0.2215770035982132
train gradient:  0.09175401820755244
iteration : 12203
train acc:  0.8671875
train loss:  0.28871750831604004
train gradient:  0.0925093981967141
iteration : 12204
train acc:  0.8828125
train loss:  0.3545214831829071
train gradient:  0.1835511775930187
iteration : 12205
train acc:  0.8828125
train loss:  0.3162645101547241
train gradient:  0.16942650241353885
iteration : 12206
train acc:  0.7890625
train loss:  0.3809581398963928
train gradient:  0.1980511596473243
iteration : 12207
train acc:  0.8515625
train loss:  0.35825639963150024
train gradient:  0.192071338594762
iteration : 12208
train acc:  0.859375
train loss:  0.40389928221702576
train gradient:  0.3657509188927601
iteration : 12209
train acc:  0.84375
train loss:  0.3629266023635864
train gradient:  0.18726692227395575
iteration : 12210
train acc:  0.84375
train loss:  0.39124035835266113
train gradient:  0.23018111678689646
iteration : 12211
train acc:  0.8671875
train loss:  0.3141457438468933
train gradient:  0.13319643163067246
iteration : 12212
train acc:  0.8515625
train loss:  0.32579153776168823
train gradient:  0.20365571512105618
iteration : 12213
train acc:  0.890625
train loss:  0.26302066445350647
train gradient:  0.10397054268416632
iteration : 12214
train acc:  0.796875
train loss:  0.452240914106369
train gradient:  0.27821786024786255
iteration : 12215
train acc:  0.859375
train loss:  0.289665162563324
train gradient:  0.13889898202379578
iteration : 12216
train acc:  0.890625
train loss:  0.23378747701644897
train gradient:  0.12087174764403869
iteration : 12217
train acc:  0.796875
train loss:  0.414482980966568
train gradient:  0.24831008292807222
iteration : 12218
train acc:  0.796875
train loss:  0.4074908494949341
train gradient:  0.27399908234455833
iteration : 12219
train acc:  0.859375
train loss:  0.28478705883026123
train gradient:  0.13904097599457443
iteration : 12220
train acc:  0.8515625
train loss:  0.3756803870201111
train gradient:  0.1777673587156336
iteration : 12221
train acc:  0.84375
train loss:  0.374581515789032
train gradient:  0.1610568934140694
iteration : 12222
train acc:  0.875
train loss:  0.33228999376296997
train gradient:  0.13404185049231998
iteration : 12223
train acc:  0.890625
train loss:  0.33100438117980957
train gradient:  0.22247089948639173
iteration : 12224
train acc:  0.8984375
train loss:  0.30741018056869507
train gradient:  0.14465758978091184
iteration : 12225
train acc:  0.8046875
train loss:  0.3390692174434662
train gradient:  0.12126613007718587
iteration : 12226
train acc:  0.8515625
train loss:  0.3036063611507416
train gradient:  0.11373369517040322
iteration : 12227
train acc:  0.8359375
train loss:  0.33514922857284546
train gradient:  0.15178019199183335
iteration : 12228
train acc:  0.890625
train loss:  0.30328369140625
train gradient:  0.11271785659588159
iteration : 12229
train acc:  0.859375
train loss:  0.3899475336074829
train gradient:  0.2417162694409165
iteration : 12230
train acc:  0.890625
train loss:  0.25945335626602173
train gradient:  0.10579163182019527
iteration : 12231
train acc:  0.8125
train loss:  0.35594460368156433
train gradient:  0.15613249643170085
iteration : 12232
train acc:  0.8828125
train loss:  0.27274221181869507
train gradient:  0.10841652902116043
iteration : 12233
train acc:  0.8515625
train loss:  0.35665395855903625
train gradient:  0.1905556025081111
iteration : 12234
train acc:  0.8671875
train loss:  0.3404703140258789
train gradient:  0.14009192123892306
iteration : 12235
train acc:  0.8671875
train loss:  0.3372134566307068
train gradient:  0.15173891587915556
iteration : 12236
train acc:  0.8671875
train loss:  0.34336403012275696
train gradient:  0.1644726773908839
iteration : 12237
train acc:  0.8984375
train loss:  0.26904013752937317
train gradient:  0.22878392322149538
iteration : 12238
train acc:  0.8828125
train loss:  0.30661070346832275
train gradient:  0.12403247893557452
iteration : 12239
train acc:  0.875
train loss:  0.3008729815483093
train gradient:  0.10355427565329606
iteration : 12240
train acc:  0.875
train loss:  0.2848687767982483
train gradient:  0.1406683474648326
iteration : 12241
train acc:  0.8515625
train loss:  0.3577338457107544
train gradient:  0.1846464931737371
iteration : 12242
train acc:  0.859375
train loss:  0.307378888130188
train gradient:  0.14545747655380212
iteration : 12243
train acc:  0.84375
train loss:  0.3589816093444824
train gradient:  0.14817886215904236
iteration : 12244
train acc:  0.8203125
train loss:  0.33001697063446045
train gradient:  0.1967407359087433
iteration : 12245
train acc:  0.8515625
train loss:  0.3484104573726654
train gradient:  0.13447501973420695
iteration : 12246
train acc:  0.8359375
train loss:  0.3531656563282013
train gradient:  0.18579837486360662
iteration : 12247
train acc:  0.8671875
train loss:  0.31250956654548645
train gradient:  0.12133652372367193
iteration : 12248
train acc:  0.9140625
train loss:  0.2386726289987564
train gradient:  0.10801854907065024
iteration : 12249
train acc:  0.7890625
train loss:  0.4069233536720276
train gradient:  0.18396626206368663
iteration : 12250
train acc:  0.8515625
train loss:  0.31882762908935547
train gradient:  0.15486436340677454
iteration : 12251
train acc:  0.8359375
train loss:  0.3347877264022827
train gradient:  0.15566618009274502
iteration : 12252
train acc:  0.8671875
train loss:  0.3525047302246094
train gradient:  0.1467795463474567
iteration : 12253
train acc:  0.8828125
train loss:  0.3655620515346527
train gradient:  0.15395760192420874
iteration : 12254
train acc:  0.859375
train loss:  0.31589534878730774
train gradient:  0.1128374987721452
iteration : 12255
train acc:  0.890625
train loss:  0.25181546807289124
train gradient:  0.06749052312361101
iteration : 12256
train acc:  0.859375
train loss:  0.33949708938598633
train gradient:  0.17693807915108148
iteration : 12257
train acc:  0.7890625
train loss:  0.40791237354278564
train gradient:  0.18039747848779744
iteration : 12258
train acc:  0.8203125
train loss:  0.38691920042037964
train gradient:  0.1381402327035951
iteration : 12259
train acc:  0.8828125
train loss:  0.2852993905544281
train gradient:  0.09291859177541598
iteration : 12260
train acc:  0.84375
train loss:  0.321790874004364
train gradient:  0.17268894059134893
iteration : 12261
train acc:  0.875
train loss:  0.3193410336971283
train gradient:  0.14334816089483843
iteration : 12262
train acc:  0.859375
train loss:  0.3247549533843994
train gradient:  0.15068254922009197
iteration : 12263
train acc:  0.8671875
train loss:  0.2697659730911255
train gradient:  0.109911668638967
iteration : 12264
train acc:  0.8828125
train loss:  0.2758552134037018
train gradient:  0.10552667949939894
iteration : 12265
train acc:  0.8515625
train loss:  0.26947471499443054
train gradient:  0.17917540491814404
iteration : 12266
train acc:  0.8828125
train loss:  0.29259881377220154
train gradient:  0.10317230970563586
iteration : 12267
train acc:  0.8828125
train loss:  0.29109933972358704
train gradient:  0.08521032552574936
iteration : 12268
train acc:  0.8671875
train loss:  0.371168851852417
train gradient:  0.1489995059591483
iteration : 12269
train acc:  0.875
train loss:  0.3082275986671448
train gradient:  0.11939147209113428
iteration : 12270
train acc:  0.8984375
train loss:  0.2722938060760498
train gradient:  0.0993057918989503
iteration : 12271
train acc:  0.78125
train loss:  0.3991526961326599
train gradient:  0.1961292230840272
iteration : 12272
train acc:  0.9140625
train loss:  0.2715606093406677
train gradient:  0.14612436805437248
iteration : 12273
train acc:  0.875
train loss:  0.33290034532546997
train gradient:  0.18691024122520375
iteration : 12274
train acc:  0.8828125
train loss:  0.2820587754249573
train gradient:  0.12265250165946724
iteration : 12275
train acc:  0.875
train loss:  0.2879752516746521
train gradient:  0.132033714553202
iteration : 12276
train acc:  0.8984375
train loss:  0.2716643512248993
train gradient:  0.0945552860655087
iteration : 12277
train acc:  0.859375
train loss:  0.36588621139526367
train gradient:  0.21393111836880352
iteration : 12278
train acc:  0.8515625
train loss:  0.34012389183044434
train gradient:  0.17115674794233532
iteration : 12279
train acc:  0.875
train loss:  0.326033353805542
train gradient:  0.1154015304188747
iteration : 12280
train acc:  0.8515625
train loss:  0.3206060230731964
train gradient:  0.1320559988658438
iteration : 12281
train acc:  0.828125
train loss:  0.3409544825553894
train gradient:  0.14564580491483872
iteration : 12282
train acc:  0.8046875
train loss:  0.37263596057891846
train gradient:  0.15813518434807367
iteration : 12283
train acc:  0.765625
train loss:  0.39066046476364136
train gradient:  0.20998876875550074
iteration : 12284
train acc:  0.8515625
train loss:  0.2648323178291321
train gradient:  0.09936098892653279
iteration : 12285
train acc:  0.8203125
train loss:  0.3333447277545929
train gradient:  0.19685393081740132
iteration : 12286
train acc:  0.8828125
train loss:  0.3380764424800873
train gradient:  0.1524206065357985
iteration : 12287
train acc:  0.8359375
train loss:  0.34704315662384033
train gradient:  0.1558564748120364
iteration : 12288
train acc:  0.859375
train loss:  0.33152416348457336
train gradient:  0.1132946782147424
iteration : 12289
train acc:  0.859375
train loss:  0.31166788935661316
train gradient:  0.20062778057588382
iteration : 12290
train acc:  0.8984375
train loss:  0.28443795442581177
train gradient:  0.13234890232153645
iteration : 12291
train acc:  0.8828125
train loss:  0.25123870372772217
train gradient:  0.10652341418636016
iteration : 12292
train acc:  0.84375
train loss:  0.2767052948474884
train gradient:  0.15132244878739703
iteration : 12293
train acc:  0.8984375
train loss:  0.24580498039722443
train gradient:  0.0900280526818021
iteration : 12294
train acc:  0.8828125
train loss:  0.28167298436164856
train gradient:  0.15433021755267873
iteration : 12295
train acc:  0.859375
train loss:  0.33649566769599915
train gradient:  0.17470116407783445
iteration : 12296
train acc:  0.875
train loss:  0.26991933584213257
train gradient:  0.11560125218478425
iteration : 12297
train acc:  0.8515625
train loss:  0.35870665311813354
train gradient:  0.13070127897699402
iteration : 12298
train acc:  0.9140625
train loss:  0.22562691569328308
train gradient:  0.07152100658736939
iteration : 12299
train acc:  0.84375
train loss:  0.31668949127197266
train gradient:  0.14955000270942798
iteration : 12300
train acc:  0.828125
train loss:  0.3492182195186615
train gradient:  0.24020945986667372
iteration : 12301
train acc:  0.8359375
train loss:  0.386318176984787
train gradient:  0.17338610030676038
iteration : 12302
train acc:  0.8125
train loss:  0.3923353850841522
train gradient:  0.18735030673809688
iteration : 12303
train acc:  0.921875
train loss:  0.21886751055717468
train gradient:  0.0792104386277336
iteration : 12304
train acc:  0.859375
train loss:  0.2727053165435791
train gradient:  0.125910429067738
iteration : 12305
train acc:  0.8203125
train loss:  0.32926487922668457
train gradient:  0.16125273469252593
iteration : 12306
train acc:  0.8828125
train loss:  0.33162885904312134
train gradient:  0.1454992761139784
iteration : 12307
train acc:  0.875
train loss:  0.32286378741264343
train gradient:  0.11673900889970619
iteration : 12308
train acc:  0.875
train loss:  0.25317123532295227
train gradient:  0.11376611494672895
iteration : 12309
train acc:  0.75
train loss:  0.47557464241981506
train gradient:  0.2365997911742096
iteration : 12310
train acc:  0.875
train loss:  0.3070196807384491
train gradient:  0.12738620149194133
iteration : 12311
train acc:  0.84375
train loss:  0.3282243013381958
train gradient:  0.14446014608316235
iteration : 12312
train acc:  0.8203125
train loss:  0.34300559759140015
train gradient:  0.14309213184412636
iteration : 12313
train acc:  0.875
train loss:  0.28205764293670654
train gradient:  0.14052461720924592
iteration : 12314
train acc:  0.8984375
train loss:  0.24026064574718475
train gradient:  0.08009643075862032
iteration : 12315
train acc:  0.859375
train loss:  0.3212995231151581
train gradient:  0.14057327655412857
iteration : 12316
train acc:  0.8828125
train loss:  0.3005462884902954
train gradient:  0.14584696686461213
iteration : 12317
train acc:  0.8203125
train loss:  0.34491151571273804
train gradient:  0.10805471431848053
iteration : 12318
train acc:  0.8671875
train loss:  0.3228268325328827
train gradient:  0.1311943874707582
iteration : 12319
train acc:  0.9453125
train loss:  0.21006473898887634
train gradient:  0.07370058263755684
iteration : 12320
train acc:  0.84375
train loss:  0.29843470454216003
train gradient:  0.13659694218831914
iteration : 12321
train acc:  0.84375
train loss:  0.3230002522468567
train gradient:  0.12322919806371246
iteration : 12322
train acc:  0.828125
train loss:  0.38082635402679443
train gradient:  0.18747592796754947
iteration : 12323
train acc:  0.84375
train loss:  0.33774611353874207
train gradient:  0.12166924661112179
iteration : 12324
train acc:  0.828125
train loss:  0.3525432348251343
train gradient:  0.14021739699993274
iteration : 12325
train acc:  0.859375
train loss:  0.3294928967952728
train gradient:  0.17144552756294934
iteration : 12326
train acc:  0.8671875
train loss:  0.3240634500980377
train gradient:  0.14658920476763163
iteration : 12327
train acc:  0.828125
train loss:  0.3607827425003052
train gradient:  0.1624321899006796
iteration : 12328
train acc:  0.8359375
train loss:  0.3393271863460541
train gradient:  0.12015041618491444
iteration : 12329
train acc:  0.859375
train loss:  0.3515123724937439
train gradient:  0.16882411459061247
iteration : 12330
train acc:  0.890625
train loss:  0.25888386368751526
train gradient:  0.09392444762064668
iteration : 12331
train acc:  0.859375
train loss:  0.30432480573654175
train gradient:  0.11204153404054215
iteration : 12332
train acc:  0.8046875
train loss:  0.3265514373779297
train gradient:  0.1672313719324911
iteration : 12333
train acc:  0.8359375
train loss:  0.346637099981308
train gradient:  0.14995512489966065
iteration : 12334
train acc:  0.921875
train loss:  0.2480708360671997
train gradient:  0.09868517536181141
iteration : 12335
train acc:  0.8828125
train loss:  0.27782851457595825
train gradient:  0.10912607321162447
iteration : 12336
train acc:  0.84375
train loss:  0.2983660101890564
train gradient:  0.14003073341298777
iteration : 12337
train acc:  0.8359375
train loss:  0.3291245698928833
train gradient:  0.12496693745527969
iteration : 12338
train acc:  0.828125
train loss:  0.39933106303215027
train gradient:  0.20934458832773806
iteration : 12339
train acc:  0.859375
train loss:  0.38311564922332764
train gradient:  0.14327188003955263
iteration : 12340
train acc:  0.828125
train loss:  0.41345128417015076
train gradient:  0.19826017272101348
iteration : 12341
train acc:  0.828125
train loss:  0.3622714877128601
train gradient:  0.1814213693771799
iteration : 12342
train acc:  0.8515625
train loss:  0.3034513592720032
train gradient:  0.14530178930155158
iteration : 12343
train acc:  0.859375
train loss:  0.30538037419319153
train gradient:  0.09756943832825357
iteration : 12344
train acc:  0.8515625
train loss:  0.38001033663749695
train gradient:  0.2356536299927362
iteration : 12345
train acc:  0.890625
train loss:  0.2701167166233063
train gradient:  0.09202949369624264
iteration : 12346
train acc:  0.8671875
train loss:  0.29707100987434387
train gradient:  0.10617821945294466
iteration : 12347
train acc:  0.84375
train loss:  0.37897688150405884
train gradient:  0.17559216840257258
iteration : 12348
train acc:  0.8359375
train loss:  0.2791524827480316
train gradient:  0.12641759655652512
iteration : 12349
train acc:  0.859375
train loss:  0.3312411308288574
train gradient:  0.1311824427306438
iteration : 12350
train acc:  0.875
train loss:  0.3491330146789551
train gradient:  0.19198738274276017
iteration : 12351
train acc:  0.828125
train loss:  0.35726937651634216
train gradient:  0.2451922265554216
iteration : 12352
train acc:  0.8515625
train loss:  0.3604712188243866
train gradient:  0.1671184964359118
iteration : 12353
train acc:  0.875
train loss:  0.3606351315975189
train gradient:  0.13634869220368417
iteration : 12354
train acc:  0.859375
train loss:  0.3255992829799652
train gradient:  0.14324227192853517
iteration : 12355
train acc:  0.84375
train loss:  0.32951438426971436
train gradient:  0.13851808799998683
iteration : 12356
train acc:  0.859375
train loss:  0.3710641860961914
train gradient:  0.24379053188461164
iteration : 12357
train acc:  0.8203125
train loss:  0.4001794159412384
train gradient:  0.1979023551939254
iteration : 12358
train acc:  0.859375
train loss:  0.30203762650489807
train gradient:  0.1076330291815524
iteration : 12359
train acc:  0.921875
train loss:  0.27561241388320923
train gradient:  0.09226091221213843
iteration : 12360
train acc:  0.796875
train loss:  0.46891799569129944
train gradient:  0.2712659867904862
iteration : 12361
train acc:  0.8671875
train loss:  0.31951403617858887
train gradient:  0.18595858924108916
iteration : 12362
train acc:  0.8671875
train loss:  0.36272838711738586
train gradient:  0.2372042919291505
iteration : 12363
train acc:  0.7890625
train loss:  0.4317227005958557
train gradient:  0.28614586118079693
iteration : 12364
train acc:  0.8125
train loss:  0.40642252564430237
train gradient:  0.21607606422724154
iteration : 12365
train acc:  0.90625
train loss:  0.25501886010169983
train gradient:  0.08066256841149136
iteration : 12366
train acc:  0.8828125
train loss:  0.3134914040565491
train gradient:  0.13345392262679034
iteration : 12367
train acc:  0.90625
train loss:  0.2691071629524231
train gradient:  0.13579729638746008
iteration : 12368
train acc:  0.8515625
train loss:  0.37132108211517334
train gradient:  0.17320055285559144
iteration : 12369
train acc:  0.90625
train loss:  0.29743126034736633
train gradient:  0.12208057950141134
iteration : 12370
train acc:  0.875
train loss:  0.28891459107398987
train gradient:  0.14034195360597315
iteration : 12371
train acc:  0.875
train loss:  0.3052297830581665
train gradient:  0.1205522459479251
iteration : 12372
train acc:  0.8671875
train loss:  0.3315434455871582
train gradient:  0.15042177605906415
iteration : 12373
train acc:  0.8515625
train loss:  0.3155193328857422
train gradient:  0.1418044168100282
iteration : 12374
train acc:  0.875
train loss:  0.26439133286476135
train gradient:  0.12149971405579393
iteration : 12375
train acc:  0.890625
train loss:  0.3311206102371216
train gradient:  0.17717333485753872
iteration : 12376
train acc:  0.84375
train loss:  0.3384062647819519
train gradient:  0.21206563196547773
iteration : 12377
train acc:  0.8046875
train loss:  0.4180014729499817
train gradient:  0.23152840972759964
iteration : 12378
train acc:  0.8984375
train loss:  0.27171221375465393
train gradient:  0.12904497900040185
iteration : 12379
train acc:  0.890625
train loss:  0.2600664794445038
train gradient:  0.11488773175275387
iteration : 12380
train acc:  0.859375
train loss:  0.3033910393714905
train gradient:  0.1941846101209313
iteration : 12381
train acc:  0.890625
train loss:  0.31395837664604187
train gradient:  0.11638207068136877
iteration : 12382
train acc:  0.859375
train loss:  0.28377044200897217
train gradient:  0.1112444087494382
iteration : 12383
train acc:  0.8203125
train loss:  0.44272708892822266
train gradient:  0.21084404998637266
iteration : 12384
train acc:  0.8203125
train loss:  0.4318605065345764
train gradient:  0.2358595529410346
iteration : 12385
train acc:  0.890625
train loss:  0.23054219782352448
train gradient:  0.0974138179845464
iteration : 12386
train acc:  0.875
train loss:  0.29507380723953247
train gradient:  0.10709184837091837
iteration : 12387
train acc:  0.8359375
train loss:  0.4045664966106415
train gradient:  0.18077576815055416
iteration : 12388
train acc:  0.796875
train loss:  0.39681050181388855
train gradient:  0.20947898407822033
iteration : 12389
train acc:  0.8359375
train loss:  0.3342812955379486
train gradient:  0.12724897945017039
iteration : 12390
train acc:  0.8671875
train loss:  0.307626873254776
train gradient:  0.1724139205315682
iteration : 12391
train acc:  0.8515625
train loss:  0.31133830547332764
train gradient:  0.16136671061247157
iteration : 12392
train acc:  0.90625
train loss:  0.2928016185760498
train gradient:  0.09158393319492478
iteration : 12393
train acc:  0.859375
train loss:  0.3255475163459778
train gradient:  0.1763950218800461
iteration : 12394
train acc:  0.8046875
train loss:  0.3931541442871094
train gradient:  0.2241794155540478
iteration : 12395
train acc:  0.859375
train loss:  0.33001554012298584
train gradient:  0.2697971972278028
iteration : 12396
train acc:  0.796875
train loss:  0.4202122986316681
train gradient:  0.23176634350640204
iteration : 12397
train acc:  0.875
train loss:  0.28335845470428467
train gradient:  0.11133399423788722
iteration : 12398
train acc:  0.84375
train loss:  0.364424467086792
train gradient:  0.215269955696343
iteration : 12399
train acc:  0.8125
train loss:  0.35402214527130127
train gradient:  0.2160101727991365
iteration : 12400
train acc:  0.8984375
train loss:  0.2837141752243042
train gradient:  0.13584180930252293
iteration : 12401
train acc:  0.875
train loss:  0.34217631816864014
train gradient:  0.14207107796044435
iteration : 12402
train acc:  0.8671875
train loss:  0.2875838577747345
train gradient:  0.12845580454362354
iteration : 12403
train acc:  0.859375
train loss:  0.2990657091140747
train gradient:  0.13291292582688036
iteration : 12404
train acc:  0.8359375
train loss:  0.3599260449409485
train gradient:  0.1834581154291289
iteration : 12405
train acc:  0.8671875
train loss:  0.29354724287986755
train gradient:  0.11418888096522885
iteration : 12406
train acc:  0.8671875
train loss:  0.2755769193172455
train gradient:  0.11516404426578095
iteration : 12407
train acc:  0.8515625
train loss:  0.30277058482170105
train gradient:  0.11180476181758131
iteration : 12408
train acc:  0.8671875
train loss:  0.2873852849006653
train gradient:  0.1382018701954006
iteration : 12409
train acc:  0.90625
train loss:  0.25194138288497925
train gradient:  0.07568036042738507
iteration : 12410
train acc:  0.78125
train loss:  0.3643186092376709
train gradient:  0.19167030499210977
iteration : 12411
train acc:  0.859375
train loss:  0.3166258633136749
train gradient:  0.15427887578182783
iteration : 12412
train acc:  0.859375
train loss:  0.3308718800544739
train gradient:  0.20188825024087603
iteration : 12413
train acc:  0.84375
train loss:  0.3582192063331604
train gradient:  0.19207150949511137
iteration : 12414
train acc:  0.828125
train loss:  0.3120591938495636
train gradient:  0.11655337221813049
iteration : 12415
train acc:  0.875
train loss:  0.30902454257011414
train gradient:  0.12126258031849509
iteration : 12416
train acc:  0.8671875
train loss:  0.28859996795654297
train gradient:  0.13115623860821485
iteration : 12417
train acc:  0.8671875
train loss:  0.31728458404541016
train gradient:  0.21852153406700847
iteration : 12418
train acc:  0.859375
train loss:  0.2566641569137573
train gradient:  0.13545481377684254
iteration : 12419
train acc:  0.8359375
train loss:  0.29789453744888306
train gradient:  0.17306631627130037
iteration : 12420
train acc:  0.8515625
train loss:  0.30919989943504333
train gradient:  0.15707362651638002
iteration : 12421
train acc:  0.890625
train loss:  0.3175937533378601
train gradient:  0.1333407165748806
iteration : 12422
train acc:  0.875
train loss:  0.28940466046333313
train gradient:  0.11195533871141612
iteration : 12423
train acc:  0.84375
train loss:  0.32628294825553894
train gradient:  0.1366059052418354
iteration : 12424
train acc:  0.8515625
train loss:  0.354573130607605
train gradient:  0.13839415367899227
iteration : 12425
train acc:  0.859375
train loss:  0.2892119288444519
train gradient:  0.12620186594135627
iteration : 12426
train acc:  0.890625
train loss:  0.2893163859844208
train gradient:  0.11064343194661874
iteration : 12427
train acc:  0.8671875
train loss:  0.3235618472099304
train gradient:  0.13273581455391698
iteration : 12428
train acc:  0.8515625
train loss:  0.3143507242202759
train gradient:  0.12431937877526408
iteration : 12429
train acc:  0.8671875
train loss:  0.3049270808696747
train gradient:  0.1184086029371681
iteration : 12430
train acc:  0.875
train loss:  0.34832996129989624
train gradient:  0.16637536713119944
iteration : 12431
train acc:  0.8515625
train loss:  0.35413289070129395
train gradient:  0.22124796063209662
iteration : 12432
train acc:  0.8125
train loss:  0.4156348407268524
train gradient:  0.2054641208262629
iteration : 12433
train acc:  0.828125
train loss:  0.3935922682285309
train gradient:  0.3451648160924061
iteration : 12434
train acc:  0.875
train loss:  0.29632771015167236
train gradient:  0.09589103975768186
iteration : 12435
train acc:  0.890625
train loss:  0.33260250091552734
train gradient:  0.1350627292176923
iteration : 12436
train acc:  0.8359375
train loss:  0.3253573179244995
train gradient:  0.1488197855521386
iteration : 12437
train acc:  0.8984375
train loss:  0.2956692576408386
train gradient:  0.1419853121927928
iteration : 12438
train acc:  0.796875
train loss:  0.40606585144996643
train gradient:  0.3099122581829039
iteration : 12439
train acc:  0.8125
train loss:  0.39609605073928833
train gradient:  0.2512568569047264
iteration : 12440
train acc:  0.875
train loss:  0.2702242136001587
train gradient:  0.08620225000707843
iteration : 12441
train acc:  0.828125
train loss:  0.33953940868377686
train gradient:  0.23359877947995744
iteration : 12442
train acc:  0.8515625
train loss:  0.3254558742046356
train gradient:  0.15783480176993328
iteration : 12443
train acc:  0.8828125
train loss:  0.2393200397491455
train gradient:  0.09263694625339129
iteration : 12444
train acc:  0.8671875
train loss:  0.3471492826938629
train gradient:  0.14867856260327322
iteration : 12445
train acc:  0.8828125
train loss:  0.29632002115249634
train gradient:  0.1349795638494658
iteration : 12446
train acc:  0.796875
train loss:  0.3534358739852905
train gradient:  0.1461796047703857
iteration : 12447
train acc:  0.8671875
train loss:  0.34124594926834106
train gradient:  0.10582505423882915
iteration : 12448
train acc:  0.8515625
train loss:  0.3549293279647827
train gradient:  0.14258577175094178
iteration : 12449
train acc:  0.8515625
train loss:  0.31774094700813293
train gradient:  0.19937028622683467
iteration : 12450
train acc:  0.875
train loss:  0.33600544929504395
train gradient:  0.13634668339103406
iteration : 12451
train acc:  0.8359375
train loss:  0.3662218451499939
train gradient:  0.2386229654040946
iteration : 12452
train acc:  0.828125
train loss:  0.42021891474723816
train gradient:  0.22179300715937894
iteration : 12453
train acc:  0.828125
train loss:  0.3941945433616638
train gradient:  0.1758629696306879
iteration : 12454
train acc:  0.8359375
train loss:  0.30406081676483154
train gradient:  0.1491565184722839
iteration : 12455
train acc:  0.8515625
train loss:  0.32755234837532043
train gradient:  0.1429935227646994
iteration : 12456
train acc:  0.8203125
train loss:  0.3698612153530121
train gradient:  0.17115272153023192
iteration : 12457
train acc:  0.8671875
train loss:  0.3746408522129059
train gradient:  0.17733148255632947
iteration : 12458
train acc:  0.875
train loss:  0.2862357497215271
train gradient:  0.19497190754005475
iteration : 12459
train acc:  0.8359375
train loss:  0.37233930826187134
train gradient:  0.18826688380245812
iteration : 12460
train acc:  0.84375
train loss:  0.325615257024765
train gradient:  0.13284130621320095
iteration : 12461
train acc:  0.8515625
train loss:  0.3287340998649597
train gradient:  0.158924632922673
iteration : 12462
train acc:  0.90625
train loss:  0.2873571217060089
train gradient:  0.10515681841761966
iteration : 12463
train acc:  0.8203125
train loss:  0.33034271001815796
train gradient:  0.1494328649776535
iteration : 12464
train acc:  0.9296875
train loss:  0.2364964783191681
train gradient:  0.10592975368650324
iteration : 12465
train acc:  0.8515625
train loss:  0.2865504026412964
train gradient:  0.13240275966717358
iteration : 12466
train acc:  0.8515625
train loss:  0.3187749683856964
train gradient:  0.11986609897663503
iteration : 12467
train acc:  0.8828125
train loss:  0.2672349810600281
train gradient:  0.10046834381972497
iteration : 12468
train acc:  0.7890625
train loss:  0.4081883132457733
train gradient:  0.25580643261950287
iteration : 12469
train acc:  0.859375
train loss:  0.35644522309303284
train gradient:  0.19796847274866208
iteration : 12470
train acc:  0.8984375
train loss:  0.27763229608535767
train gradient:  0.11239802511944577
iteration : 12471
train acc:  0.8984375
train loss:  0.2638256847858429
train gradient:  0.10745056581745867
iteration : 12472
train acc:  0.8359375
train loss:  0.3073030114173889
train gradient:  0.1810232975739349
iteration : 12473
train acc:  0.828125
train loss:  0.38411158323287964
train gradient:  0.18660896506554778
iteration : 12474
train acc:  0.8828125
train loss:  0.27817171812057495
train gradient:  0.1187613166834662
iteration : 12475
train acc:  0.9296875
train loss:  0.21537946164608002
train gradient:  0.06590123130900336
iteration : 12476
train acc:  0.8046875
train loss:  0.3690497577190399
train gradient:  0.17457725314140793
iteration : 12477
train acc:  0.828125
train loss:  0.2858133912086487
train gradient:  0.15598802312361174
iteration : 12478
train acc:  0.8203125
train loss:  0.3552117347717285
train gradient:  0.16033796380088522
iteration : 12479
train acc:  0.875
train loss:  0.308729350566864
train gradient:  0.16459618632604547
iteration : 12480
train acc:  0.875
train loss:  0.3059936761856079
train gradient:  0.12942961012596482
iteration : 12481
train acc:  0.84375
train loss:  0.32371431589126587
train gradient:  0.15966193487781924
iteration : 12482
train acc:  0.875
train loss:  0.3311699628829956
train gradient:  0.21901298579196518
iteration : 12483
train acc:  0.890625
train loss:  0.2520347237586975
train gradient:  0.08138589960937875
iteration : 12484
train acc:  0.90625
train loss:  0.2819559574127197
train gradient:  0.13584256536026168
iteration : 12485
train acc:  0.8359375
train loss:  0.3454139828681946
train gradient:  0.15824184029447505
iteration : 12486
train acc:  0.8515625
train loss:  0.35919472575187683
train gradient:  0.1575517891057756
iteration : 12487
train acc:  0.90625
train loss:  0.26706039905548096
train gradient:  0.09547170647710602
iteration : 12488
train acc:  0.8828125
train loss:  0.261617511510849
train gradient:  0.1815564601455415
iteration : 12489
train acc:  0.8671875
train loss:  0.31073707342147827
train gradient:  0.23953977851270855
iteration : 12490
train acc:  0.8515625
train loss:  0.311562180519104
train gradient:  0.1523817296001138
iteration : 12491
train acc:  0.8984375
train loss:  0.25462764501571655
train gradient:  0.1429650310734541
iteration : 12492
train acc:  0.8203125
train loss:  0.36999934911727905
train gradient:  0.18922753054230154
iteration : 12493
train acc:  0.8359375
train loss:  0.40124163031578064
train gradient:  0.2738667060094855
iteration : 12494
train acc:  0.8515625
train loss:  0.4351499378681183
train gradient:  0.24859919492833688
iteration : 12495
train acc:  0.875
train loss:  0.3336218595504761
train gradient:  0.1224064511311814
iteration : 12496
train acc:  0.8515625
train loss:  0.3448413014411926
train gradient:  0.16170855430013933
iteration : 12497
train acc:  0.8515625
train loss:  0.29395824670791626
train gradient:  0.11981391856240063
iteration : 12498
train acc:  0.828125
train loss:  0.3264685571193695
train gradient:  0.147082243995578
iteration : 12499
train acc:  0.875
train loss:  0.2969507873058319
train gradient:  0.1420382989513856
iteration : 12500
train acc:  0.859375
train loss:  0.3881767690181732
train gradient:  0.2344829854395477
iteration : 12501
train acc:  0.859375
train loss:  0.3303510546684265
train gradient:  0.15492785411378285
iteration : 12502
train acc:  0.90625
train loss:  0.2753140926361084
train gradient:  0.11706902540022142
iteration : 12503
train acc:  0.890625
train loss:  0.28529757261276245
train gradient:  0.12222559478957568
iteration : 12504
train acc:  0.875
train loss:  0.28137052059173584
train gradient:  0.12417267129303776
iteration : 12505
train acc:  0.890625
train loss:  0.26787781715393066
train gradient:  0.07923064821501098
iteration : 12506
train acc:  0.875
train loss:  0.29126155376434326
train gradient:  0.17436024852840026
iteration : 12507
train acc:  0.875
train loss:  0.2808813154697418
train gradient:  0.09337559860341138
iteration : 12508
train acc:  0.890625
train loss:  0.2419896423816681
train gradient:  0.0867529635072078
iteration : 12509
train acc:  0.8046875
train loss:  0.3583749830722809
train gradient:  0.24656248859227253
iteration : 12510
train acc:  0.8359375
train loss:  0.3443656265735626
train gradient:  0.1380120708442774
iteration : 12511
train acc:  0.890625
train loss:  0.30249565839767456
train gradient:  0.1925718325636725
iteration : 12512
train acc:  0.90625
train loss:  0.2568112015724182
train gradient:  0.11582630346512401
iteration : 12513
train acc:  0.8515625
train loss:  0.32729440927505493
train gradient:  0.11618725749161349
iteration : 12514
train acc:  0.8515625
train loss:  0.35171306133270264
train gradient:  0.1542162310892452
iteration : 12515
train acc:  0.8671875
train loss:  0.3251761794090271
train gradient:  0.1324861152018439
iteration : 12516
train acc:  0.84375
train loss:  0.3709488809108734
train gradient:  0.22078753127941167
iteration : 12517
train acc:  0.8984375
train loss:  0.26487997174263
train gradient:  0.0974890651267902
iteration : 12518
train acc:  0.828125
train loss:  0.33466681838035583
train gradient:  0.14928662801288525
iteration : 12519
train acc:  0.84375
train loss:  0.33069032430648804
train gradient:  0.1128626494701467
iteration : 12520
train acc:  0.8671875
train loss:  0.28173255920410156
train gradient:  0.12125093951111222
iteration : 12521
train acc:  0.84375
train loss:  0.2839864492416382
train gradient:  0.10131075722734978
iteration : 12522
train acc:  0.84375
train loss:  0.34711915254592896
train gradient:  0.19495629474963233
iteration : 12523
train acc:  0.8984375
train loss:  0.24781355261802673
train gradient:  0.12775501864834815
iteration : 12524
train acc:  0.8359375
train loss:  0.39124223589897156
train gradient:  0.20874028033653935
iteration : 12525
train acc:  0.859375
train loss:  0.32450392842292786
train gradient:  0.13475806443082516
iteration : 12526
train acc:  0.875
train loss:  0.2768992781639099
train gradient:  0.16635112621971
iteration : 12527
train acc:  0.84375
train loss:  0.31674033403396606
train gradient:  0.1800419345421326
iteration : 12528
train acc:  0.8515625
train loss:  0.29725080728530884
train gradient:  0.12393907234887298
iteration : 12529
train acc:  0.8671875
train loss:  0.32333266735076904
train gradient:  0.21727551082730284
iteration : 12530
train acc:  0.8828125
train loss:  0.30658048391342163
train gradient:  0.13508689138960778
iteration : 12531
train acc:  0.890625
train loss:  0.24715076386928558
train gradient:  0.1034971971696389
iteration : 12532
train acc:  0.78125
train loss:  0.40862712264060974
train gradient:  0.22617181895051874
iteration : 12533
train acc:  0.875
train loss:  0.27995792031288147
train gradient:  0.13826187888887814
iteration : 12534
train acc:  0.828125
train loss:  0.3529195189476013
train gradient:  0.20810959062375056
iteration : 12535
train acc:  0.875
train loss:  0.32689064741134644
train gradient:  0.16975170393609182
iteration : 12536
train acc:  0.84375
train loss:  0.3554806411266327
train gradient:  0.17164000538530835
iteration : 12537
train acc:  0.859375
train loss:  0.3320552110671997
train gradient:  0.15350390052839116
iteration : 12538
train acc:  0.8671875
train loss:  0.2924486994743347
train gradient:  0.1397775858807659
iteration : 12539
train acc:  0.84375
train loss:  0.29686203598976135
train gradient:  0.1364963171170889
iteration : 12540
train acc:  0.828125
train loss:  0.40998560190200806
train gradient:  0.21463013273609402
iteration : 12541
train acc:  0.875
train loss:  0.326229453086853
train gradient:  0.1241907893879243
iteration : 12542
train acc:  0.8984375
train loss:  0.24567511677742004
train gradient:  0.07654267424132183
iteration : 12543
train acc:  0.8359375
train loss:  0.4035058319568634
train gradient:  0.1918638110658231
iteration : 12544
train acc:  0.796875
train loss:  0.3805161714553833
train gradient:  0.18491379035285982
iteration : 12545
train acc:  0.8515625
train loss:  0.3429827392101288
train gradient:  0.2424262765331756
iteration : 12546
train acc:  0.9296875
train loss:  0.22437143325805664
train gradient:  0.08425170319086112
iteration : 12547
train acc:  0.8359375
train loss:  0.40521863102912903
train gradient:  0.23991433669157647
iteration : 12548
train acc:  0.890625
train loss:  0.2506425082683563
train gradient:  0.15276669544933696
iteration : 12549
train acc:  0.921875
train loss:  0.22664913535118103
train gradient:  0.07831565740359475
iteration : 12550
train acc:  0.8984375
train loss:  0.23454320430755615
train gradient:  0.10709436314478532
iteration : 12551
train acc:  0.875
train loss:  0.33611610531806946
train gradient:  0.14832291384899698
iteration : 12552
train acc:  0.921875
train loss:  0.26835474371910095
train gradient:  0.1085252257269222
iteration : 12553
train acc:  0.8828125
train loss:  0.32106727361679077
train gradient:  0.18034332324409147
iteration : 12554
train acc:  0.875
train loss:  0.27559739351272583
train gradient:  0.1747419199442446
iteration : 12555
train acc:  0.8125
train loss:  0.3667932450771332
train gradient:  0.17665438239951992
iteration : 12556
train acc:  0.8515625
train loss:  0.3564724922180176
train gradient:  0.18690572513358128
iteration : 12557
train acc:  0.875
train loss:  0.31287768483161926
train gradient:  0.12109568278384623
iteration : 12558
train acc:  0.8828125
train loss:  0.3292146325111389
train gradient:  0.14432952669043408
iteration : 12559
train acc:  0.875
train loss:  0.2857305109500885
train gradient:  0.1388671592813315
iteration : 12560
train acc:  0.9296875
train loss:  0.24820557236671448
train gradient:  0.07963960329773762
iteration : 12561
train acc:  0.859375
train loss:  0.3089413642883301
train gradient:  0.15697970112489829
iteration : 12562
train acc:  0.84375
train loss:  0.3452450633049011
train gradient:  0.17338341255003403
iteration : 12563
train acc:  0.84375
train loss:  0.3078460991382599
train gradient:  0.13470346442662723
iteration : 12564
train acc:  0.8671875
train loss:  0.3265814781188965
train gradient:  0.13380749338418735
iteration : 12565
train acc:  0.7890625
train loss:  0.498776376247406
train gradient:  0.2728954277995894
iteration : 12566
train acc:  0.875
train loss:  0.2643340826034546
train gradient:  0.10355092758105233
iteration : 12567
train acc:  0.875
train loss:  0.32405415177345276
train gradient:  0.12856050387347917
iteration : 12568
train acc:  0.7890625
train loss:  0.42427539825439453
train gradient:  0.21356748891980815
iteration : 12569
train acc:  0.8515625
train loss:  0.3254404067993164
train gradient:  0.19432888034606638
iteration : 12570
train acc:  0.890625
train loss:  0.3036826252937317
train gradient:  0.16938154156843116
iteration : 12571
train acc:  0.859375
train loss:  0.3496253788471222
train gradient:  0.21855395371782443
iteration : 12572
train acc:  0.875
train loss:  0.26587221026420593
train gradient:  0.10020271762407576
iteration : 12573
train acc:  0.8515625
train loss:  0.3450928330421448
train gradient:  0.21350754953902618
iteration : 12574
train acc:  0.8515625
train loss:  0.2819865942001343
train gradient:  0.14545756603252596
iteration : 12575
train acc:  0.84375
train loss:  0.32084229588508606
train gradient:  0.14431775459622442
iteration : 12576
train acc:  0.8828125
train loss:  0.29761236906051636
train gradient:  0.119750139160805
iteration : 12577
train acc:  0.8828125
train loss:  0.25222334265708923
train gradient:  0.09946951931299704
iteration : 12578
train acc:  0.9296875
train loss:  0.24754972755908966
train gradient:  0.11395808597818036
iteration : 12579
train acc:  0.828125
train loss:  0.4530688226222992
train gradient:  0.26771907067964884
iteration : 12580
train acc:  0.875
train loss:  0.32214683294296265
train gradient:  0.20623396221732648
iteration : 12581
train acc:  0.828125
train loss:  0.34829503297805786
train gradient:  0.1867026890992183
iteration : 12582
train acc:  0.890625
train loss:  0.2817782759666443
train gradient:  0.1181104372863029
iteration : 12583
train acc:  0.890625
train loss:  0.31552714109420776
train gradient:  0.15786574459183841
iteration : 12584
train acc:  0.8828125
train loss:  0.2709464430809021
train gradient:  0.1497289269733555
iteration : 12585
train acc:  0.921875
train loss:  0.229722797870636
train gradient:  0.14138341794192766
iteration : 12586
train acc:  0.921875
train loss:  0.2937138080596924
train gradient:  0.11748217901372876
iteration : 12587
train acc:  0.9140625
train loss:  0.2644178867340088
train gradient:  0.1577892869711
iteration : 12588
train acc:  0.8203125
train loss:  0.3680725395679474
train gradient:  0.20011216632762963
iteration : 12589
train acc:  0.8515625
train loss:  0.29724517464637756
train gradient:  0.14244463671298832
iteration : 12590
train acc:  0.875
train loss:  0.2556281089782715
train gradient:  0.10306090659240626
iteration : 12591
train acc:  0.8671875
train loss:  0.34175583720207214
train gradient:  0.1786400198263321
iteration : 12592
train acc:  0.8359375
train loss:  0.35291287302970886
train gradient:  0.2616388466069296
iteration : 12593
train acc:  0.890625
train loss:  0.252591073513031
train gradient:  0.09868590727959359
iteration : 12594
train acc:  0.8515625
train loss:  0.3170222342014313
train gradient:  0.15561850008268324
iteration : 12595
train acc:  0.828125
train loss:  0.3393518328666687
train gradient:  0.21737850406909698
iteration : 12596
train acc:  0.8359375
train loss:  0.3710862398147583
train gradient:  0.2048656629053467
iteration : 12597
train acc:  0.875
train loss:  0.32788360118865967
train gradient:  0.19579415581057086
iteration : 12598
train acc:  0.8046875
train loss:  0.37809890508651733
train gradient:  0.20922942814453252
iteration : 12599
train acc:  0.8671875
train loss:  0.25548255443573
train gradient:  0.17392770073831537
iteration : 12600
train acc:  0.8671875
train loss:  0.28369462490081787
train gradient:  0.16755805148151612
iteration : 12601
train acc:  0.890625
train loss:  0.3182850480079651
train gradient:  0.17046313977945313
iteration : 12602
train acc:  0.8828125
train loss:  0.31667080521583557
train gradient:  0.14408583341601428
iteration : 12603
train acc:  0.8125
train loss:  0.3809598684310913
train gradient:  0.2273582190758135
iteration : 12604
train acc:  0.828125
train loss:  0.3457619547843933
train gradient:  0.24693957418098178
iteration : 12605
train acc:  0.84375
train loss:  0.3073376417160034
train gradient:  0.13551103904292344
iteration : 12606
train acc:  0.8984375
train loss:  0.22756806015968323
train gradient:  0.11169932363778404
iteration : 12607
train acc:  0.875
train loss:  0.3135421574115753
train gradient:  0.1699932172376045
iteration : 12608
train acc:  0.8515625
train loss:  0.338470995426178
train gradient:  0.13824891046505677
iteration : 12609
train acc:  0.828125
train loss:  0.36480554938316345
train gradient:  0.2014430426767469
iteration : 12610
train acc:  0.8515625
train loss:  0.36316967010498047
train gradient:  0.23883578158878327
iteration : 12611
train acc:  0.8203125
train loss:  0.39987194538116455
train gradient:  0.23466269869041434
iteration : 12612
train acc:  0.890625
train loss:  0.30457258224487305
train gradient:  0.12357163081226853
iteration : 12613
train acc:  0.875
train loss:  0.2857024073600769
train gradient:  0.12026487453283673
iteration : 12614
train acc:  0.8828125
train loss:  0.304332971572876
train gradient:  0.1514375243357003
iteration : 12615
train acc:  0.9140625
train loss:  0.27797991037368774
train gradient:  0.25609993889915256
iteration : 12616
train acc:  0.796875
train loss:  0.3796129524707794
train gradient:  0.25444759024374486
iteration : 12617
train acc:  0.828125
train loss:  0.36456647515296936
train gradient:  0.14950957795431785
iteration : 12618
train acc:  0.828125
train loss:  0.3801906406879425
train gradient:  0.20641308488948729
iteration : 12619
train acc:  0.84375
train loss:  0.3237822651863098
train gradient:  0.16345921407253627
iteration : 12620
train acc:  0.859375
train loss:  0.4093111753463745
train gradient:  0.21597208361553316
iteration : 12621
train acc:  0.8671875
train loss:  0.3014150857925415
train gradient:  0.15421302355456262
iteration : 12622
train acc:  0.9609375
train loss:  0.2099795639514923
train gradient:  0.09248950346633901
iteration : 12623
train acc:  0.921875
train loss:  0.2320048213005066
train gradient:  0.123306252779661
iteration : 12624
train acc:  0.90625
train loss:  0.2825947403907776
train gradient:  0.1044420914779123
iteration : 12625
train acc:  0.90625
train loss:  0.2911847233772278
train gradient:  0.13624652163393172
iteration : 12626
train acc:  0.890625
train loss:  0.2560845613479614
train gradient:  0.11067968916363957
iteration : 12627
train acc:  0.8984375
train loss:  0.3044767379760742
train gradient:  0.13694933523002445
iteration : 12628
train acc:  0.890625
train loss:  0.2657138407230377
train gradient:  0.09875506511692295
iteration : 12629
train acc:  0.9140625
train loss:  0.23993931710720062
train gradient:  0.09593079256968327
iteration : 12630
train acc:  0.828125
train loss:  0.3583337664604187
train gradient:  0.19292007546775447
iteration : 12631
train acc:  0.8359375
train loss:  0.32712507247924805
train gradient:  0.14528221915973244
iteration : 12632
train acc:  0.8671875
train loss:  0.2617446780204773
train gradient:  0.15062889089672166
iteration : 12633
train acc:  0.859375
train loss:  0.321938157081604
train gradient:  0.12369497454290364
iteration : 12634
train acc:  0.8671875
train loss:  0.29581838846206665
train gradient:  0.1299971399766467
iteration : 12635
train acc:  0.828125
train loss:  0.3849164843559265
train gradient:  0.1635721207020731
iteration : 12636
train acc:  0.828125
train loss:  0.3952205777168274
train gradient:  0.27541386328716877
iteration : 12637
train acc:  0.8828125
train loss:  0.31481924653053284
train gradient:  0.12160883197858438
iteration : 12638
train acc:  0.84375
train loss:  0.36333000659942627
train gradient:  0.27370829137820396
iteration : 12639
train acc:  0.859375
train loss:  0.3024594783782959
train gradient:  0.16678038466576342
iteration : 12640
train acc:  0.8984375
train loss:  0.24441803991794586
train gradient:  0.11147156416887434
iteration : 12641
train acc:  0.890625
train loss:  0.25943198800086975
train gradient:  0.1124799911797177
iteration : 12642
train acc:  0.9140625
train loss:  0.25602930784225464
train gradient:  0.11642322642988583
iteration : 12643
train acc:  0.828125
train loss:  0.4050130844116211
train gradient:  0.22385167348759671
iteration : 12644
train acc:  0.875
train loss:  0.39337724447250366
train gradient:  0.24537479420038238
iteration : 12645
train acc:  0.84375
train loss:  0.3318546414375305
train gradient:  0.1389652107691274
iteration : 12646
train acc:  0.8203125
train loss:  0.3624591827392578
train gradient:  0.1259924302894585
iteration : 12647
train acc:  0.8671875
train loss:  0.3087911009788513
train gradient:  0.13810901669218698
iteration : 12648
train acc:  0.8359375
train loss:  0.35854196548461914
train gradient:  0.30689265156458745
iteration : 12649
train acc:  0.8515625
train loss:  0.3067176342010498
train gradient:  0.15935972365188833
iteration : 12650
train acc:  0.859375
train loss:  0.3527986407279968
train gradient:  0.24346386865874883
iteration : 12651
train acc:  0.84375
train loss:  0.3623553514480591
train gradient:  0.16881720534841632
iteration : 12652
train acc:  0.8515625
train loss:  0.3183521032333374
train gradient:  0.16860734326400872
iteration : 12653
train acc:  0.90625
train loss:  0.22138988971710205
train gradient:  0.09017481193448006
iteration : 12654
train acc:  0.8984375
train loss:  0.22811706364154816
train gradient:  0.10848180260303052
iteration : 12655
train acc:  0.859375
train loss:  0.3017570376396179
train gradient:  0.13301627883212466
iteration : 12656
train acc:  0.8359375
train loss:  0.37445005774497986
train gradient:  0.20803637162902958
iteration : 12657
train acc:  0.8203125
train loss:  0.3682815134525299
train gradient:  0.16412775783495745
iteration : 12658
train acc:  0.875
train loss:  0.26547130942344666
train gradient:  0.1439675116614955
iteration : 12659
train acc:  0.828125
train loss:  0.3038064241409302
train gradient:  0.12068283653268223
iteration : 12660
train acc:  0.9296875
train loss:  0.21636873483657837
train gradient:  0.068201281713516
iteration : 12661
train acc:  0.8671875
train loss:  0.30555421113967896
train gradient:  0.2969256614166651
iteration : 12662
train acc:  0.875
train loss:  0.324998140335083
train gradient:  0.11581657466775942
iteration : 12663
train acc:  0.8671875
train loss:  0.35433658957481384
train gradient:  0.18257031336417107
iteration : 12664
train acc:  0.8828125
train loss:  0.34375250339508057
train gradient:  0.15793242723058243
iteration : 12665
train acc:  0.90625
train loss:  0.2523433268070221
train gradient:  0.10052519618728925
iteration : 12666
train acc:  0.8828125
train loss:  0.2840602993965149
train gradient:  0.1277610881641121
iteration : 12667
train acc:  0.8828125
train loss:  0.32119739055633545
train gradient:  0.14143383020813885
iteration : 12668
train acc:  0.8984375
train loss:  0.2439647614955902
train gradient:  0.12950474598065953
iteration : 12669
train acc:  0.8984375
train loss:  0.23844027519226074
train gradient:  0.08221791242900177
iteration : 12670
train acc:  0.875
train loss:  0.3492007255554199
train gradient:  0.19636494389947337
iteration : 12671
train acc:  0.9140625
train loss:  0.2500527799129486
train gradient:  0.15736392770017088
iteration : 12672
train acc:  0.921875
train loss:  0.20879289507865906
train gradient:  0.0816905268506124
iteration : 12673
train acc:  0.875
train loss:  0.3438769578933716
train gradient:  0.15504741069953387
iteration : 12674
train acc:  0.78125
train loss:  0.5164700150489807
train gradient:  0.37575539855181794
iteration : 12675
train acc:  0.90625
train loss:  0.26089906692504883
train gradient:  0.0784433439224201
iteration : 12676
train acc:  0.8671875
train loss:  0.3561103940010071
train gradient:  0.26555170626666186
iteration : 12677
train acc:  0.828125
train loss:  0.309772789478302
train gradient:  0.1453681229787268
iteration : 12678
train acc:  0.8671875
train loss:  0.29484957456588745
train gradient:  0.15464521375120752
iteration : 12679
train acc:  0.890625
train loss:  0.28327929973602295
train gradient:  0.12383599292519284
iteration : 12680
train acc:  0.859375
train loss:  0.31258851289749146
train gradient:  0.14231952650282664
iteration : 12681
train acc:  0.9140625
train loss:  0.22500285506248474
train gradient:  0.08591275341001293
iteration : 12682
train acc:  0.8671875
train loss:  0.294816792011261
train gradient:  0.18203500224076735
iteration : 12683
train acc:  0.796875
train loss:  0.39257073402404785
train gradient:  0.1891474398243326
iteration : 12684
train acc:  0.859375
train loss:  0.3307434618473053
train gradient:  0.13603198892976945
iteration : 12685
train acc:  0.8671875
train loss:  0.3359931409358978
train gradient:  0.20536973692567723
iteration : 12686
train acc:  0.84375
train loss:  0.27686822414398193
train gradient:  0.1872672082524809
iteration : 12687
train acc:  0.84375
train loss:  0.32132643461227417
train gradient:  0.14619208200799025
iteration : 12688
train acc:  0.9140625
train loss:  0.2556559443473816
train gradient:  0.11204302297728931
iteration : 12689
train acc:  0.859375
train loss:  0.3235166668891907
train gradient:  0.20067841060566338
iteration : 12690
train acc:  0.8515625
train loss:  0.40209507942199707
train gradient:  0.35279173224279053
iteration : 12691
train acc:  0.828125
train loss:  0.3886360228061676
train gradient:  0.19237373809138836
iteration : 12692
train acc:  0.890625
train loss:  0.2898312509059906
train gradient:  0.14934302702473137
iteration : 12693
train acc:  0.875
train loss:  0.2606281042098999
train gradient:  0.09914817910843496
iteration : 12694
train acc:  0.84375
train loss:  0.36093729734420776
train gradient:  0.20288650720855864
iteration : 12695
train acc:  0.9296875
train loss:  0.2441204935312271
train gradient:  0.08625095120857863
iteration : 12696
train acc:  0.8515625
train loss:  0.31834715604782104
train gradient:  0.1780617016931655
iteration : 12697
train acc:  0.8671875
train loss:  0.32021021842956543
train gradient:  0.1238262177932304
iteration : 12698
train acc:  0.859375
train loss:  0.3601919710636139
train gradient:  0.2737412620502774
iteration : 12699
train acc:  0.8125
train loss:  0.43609127402305603
train gradient:  0.22854754075584904
iteration : 12700
train acc:  0.8828125
train loss:  0.2642374634742737
train gradient:  0.16885799920748573
iteration : 12701
train acc:  0.8828125
train loss:  0.29831358790397644
train gradient:  0.14472526907411068
iteration : 12702
train acc:  0.859375
train loss:  0.3597818911075592
train gradient:  0.20052626504642596
iteration : 12703
train acc:  0.8984375
train loss:  0.2343878448009491
train gradient:  0.10177700058428385
iteration : 12704
train acc:  0.875
train loss:  0.29431211948394775
train gradient:  0.16362624567496326
iteration : 12705
train acc:  0.859375
train loss:  0.29417532682418823
train gradient:  0.10769603316068223
iteration : 12706
train acc:  0.859375
train loss:  0.2981974482536316
train gradient:  0.1023484534532206
iteration : 12707
train acc:  0.8125
train loss:  0.450490802526474
train gradient:  0.26162976321582815
iteration : 12708
train acc:  0.8671875
train loss:  0.32512229681015015
train gradient:  0.20614449151404407
iteration : 12709
train acc:  0.8203125
train loss:  0.42490458488464355
train gradient:  0.21212622699609357
iteration : 12710
train acc:  0.828125
train loss:  0.3919004499912262
train gradient:  0.18335404312526793
iteration : 12711
train acc:  0.8671875
train loss:  0.36993253231048584
train gradient:  0.11382811008064633
iteration : 12712
train acc:  0.8828125
train loss:  0.2527785003185272
train gradient:  0.08644137963403009
iteration : 12713
train acc:  0.828125
train loss:  0.29615670442581177
train gradient:  0.12807012297892884
iteration : 12714
train acc:  0.8828125
train loss:  0.26672977209091187
train gradient:  0.10254776379609026
iteration : 12715
train acc:  0.8984375
train loss:  0.2848528325557709
train gradient:  0.09152016052567004
iteration : 12716
train acc:  0.84375
train loss:  0.3159319758415222
train gradient:  0.15657124078561194
iteration : 12717
train acc:  0.890625
train loss:  0.2685074806213379
train gradient:  0.09800012983027678
iteration : 12718
train acc:  0.859375
train loss:  0.27351516485214233
train gradient:  0.1154095478453192
iteration : 12719
train acc:  0.84375
train loss:  0.3788180351257324
train gradient:  0.21146717975132792
iteration : 12720
train acc:  0.875
train loss:  0.25420647859573364
train gradient:  0.13319486813395573
iteration : 12721
train acc:  0.859375
train loss:  0.3334246873855591
train gradient:  0.11641300879159085
iteration : 12722
train acc:  0.890625
train loss:  0.31228578090667725
train gradient:  0.1347453433654125
iteration : 12723
train acc:  0.8046875
train loss:  0.36985543370246887
train gradient:  0.16446336121568889
iteration : 12724
train acc:  0.84375
train loss:  0.3001672625541687
train gradient:  0.1563543714531425
iteration : 12725
train acc:  0.84375
train loss:  0.36475855112075806
train gradient:  0.16347338620597426
iteration : 12726
train acc:  0.890625
train loss:  0.2907716631889343
train gradient:  0.13036975554434688
iteration : 12727
train acc:  0.875
train loss:  0.27404794096946716
train gradient:  0.12124836163864375
iteration : 12728
train acc:  0.8359375
train loss:  0.3883504271507263
train gradient:  0.22802554448576362
iteration : 12729
train acc:  0.8671875
train loss:  0.3201247751712799
train gradient:  0.15867355391559096
iteration : 12730
train acc:  0.8828125
train loss:  0.30777469277381897
train gradient:  0.11511435937226078
iteration : 12731
train acc:  0.859375
train loss:  0.38649505376815796
train gradient:  0.2024520221292082
iteration : 12732
train acc:  0.890625
train loss:  0.2834981679916382
train gradient:  0.16193281214917069
iteration : 12733
train acc:  0.890625
train loss:  0.26753708720207214
train gradient:  0.1307634160392826
iteration : 12734
train acc:  0.84375
train loss:  0.31088608503341675
train gradient:  0.24976792753331684
iteration : 12735
train acc:  0.84375
train loss:  0.37151557207107544
train gradient:  0.1780981670975535
iteration : 12736
train acc:  0.8984375
train loss:  0.27214789390563965
train gradient:  0.10901809348903932
iteration : 12737
train acc:  0.8125
train loss:  0.4178884029388428
train gradient:  0.29231657981620435
iteration : 12738
train acc:  0.8515625
train loss:  0.32506343722343445
train gradient:  0.22446224847929977
iteration : 12739
train acc:  0.875
train loss:  0.3797890543937683
train gradient:  0.18562804964759533
iteration : 12740
train acc:  0.8671875
train loss:  0.34234732389450073
train gradient:  0.1958187069922977
iteration : 12741
train acc:  0.8359375
train loss:  0.35029059648513794
train gradient:  0.17708701197683097
iteration : 12742
train acc:  0.8515625
train loss:  0.33700326085090637
train gradient:  0.11788569889314846
iteration : 12743
train acc:  0.84375
train loss:  0.329414963722229
train gradient:  0.17812799942064472
iteration : 12744
train acc:  0.8671875
train loss:  0.2913764715194702
train gradient:  0.1420278793440087
iteration : 12745
train acc:  0.921875
train loss:  0.24695616960525513
train gradient:  0.1341902411206487
iteration : 12746
train acc:  0.8671875
train loss:  0.290162056684494
train gradient:  0.11845023745829258
iteration : 12747
train acc:  0.8515625
train loss:  0.3389005661010742
train gradient:  0.17502899058159682
iteration : 12748
train acc:  0.921875
train loss:  0.22112324833869934
train gradient:  0.0879431396518743
iteration : 12749
train acc:  0.84375
train loss:  0.29809707403182983
train gradient:  0.13053954066286638
iteration : 12750
train acc:  0.84375
train loss:  0.38302215933799744
train gradient:  0.19511763205926022
iteration : 12751
train acc:  0.8515625
train loss:  0.3378211259841919
train gradient:  0.14863463997335363
iteration : 12752
train acc:  0.828125
train loss:  0.38194918632507324
train gradient:  0.18630582380877594
iteration : 12753
train acc:  0.8203125
train loss:  0.4055328369140625
train gradient:  0.27029951770437344
iteration : 12754
train acc:  0.890625
train loss:  0.27188581228256226
train gradient:  0.08354900322838178
iteration : 12755
train acc:  0.875
train loss:  0.3257894814014435
train gradient:  0.10812971573512471
iteration : 12756
train acc:  0.8515625
train loss:  0.3646310865879059
train gradient:  0.18096673393926588
iteration : 12757
train acc:  0.8359375
train loss:  0.2890530824661255
train gradient:  0.20430545703263087
iteration : 12758
train acc:  0.8125
train loss:  0.37602561712265015
train gradient:  0.19757623248449407
iteration : 12759
train acc:  0.8828125
train loss:  0.3054686188697815
train gradient:  0.16295168929609039
iteration : 12760
train acc:  0.875
train loss:  0.34006965160369873
train gradient:  0.13226611114062897
iteration : 12761
train acc:  0.8671875
train loss:  0.2821449637413025
train gradient:  0.10265556290423632
iteration : 12762
train acc:  0.8828125
train loss:  0.3180485665798187
train gradient:  0.1385996732551446
iteration : 12763
train acc:  0.8671875
train loss:  0.3130201995372772
train gradient:  0.13709789308265377
iteration : 12764
train acc:  0.859375
train loss:  0.340350478887558
train gradient:  0.19650830708819184
iteration : 12765
train acc:  0.890625
train loss:  0.28589534759521484
train gradient:  0.09493049800615186
iteration : 12766
train acc:  0.8515625
train loss:  0.296847939491272
train gradient:  0.10871254738601933
iteration : 12767
train acc:  0.8515625
train loss:  0.3269367218017578
train gradient:  0.15617592101468797
iteration : 12768
train acc:  0.8515625
train loss:  0.30038946866989136
train gradient:  0.11123848764002152
iteration : 12769
train acc:  0.8125
train loss:  0.36080092191696167
train gradient:  0.1340352242392876
iteration : 12770
train acc:  0.84375
train loss:  0.3577401638031006
train gradient:  0.15857364481613948
iteration : 12771
train acc:  0.8515625
train loss:  0.30502596497535706
train gradient:  0.1693249799104986
iteration : 12772
train acc:  0.8828125
train loss:  0.2860069274902344
train gradient:  0.12484878672103368
iteration : 12773
train acc:  0.890625
train loss:  0.3055897355079651
train gradient:  0.12344444311846967
iteration : 12774
train acc:  0.859375
train loss:  0.2935258746147156
train gradient:  0.17039633993398257
iteration : 12775
train acc:  0.8671875
train loss:  0.34797704219818115
train gradient:  0.13834209103769474
iteration : 12776
train acc:  0.890625
train loss:  0.28204113245010376
train gradient:  0.14036917857065173
iteration : 12777
train acc:  0.8671875
train loss:  0.3477269411087036
train gradient:  0.13319941476398928
iteration : 12778
train acc:  0.8671875
train loss:  0.2615475356578827
train gradient:  0.11293834009574545
iteration : 12779
train acc:  0.8515625
train loss:  0.29929232597351074
train gradient:  0.11636820759547464
iteration : 12780
train acc:  0.890625
train loss:  0.32534998655319214
train gradient:  0.17979954099577386
iteration : 12781
train acc:  0.8671875
train loss:  0.2568904161453247
train gradient:  0.0828869866897805
iteration : 12782
train acc:  0.890625
train loss:  0.28872451186180115
train gradient:  0.1079349931351027
iteration : 12783
train acc:  0.8828125
train loss:  0.30395522713661194
train gradient:  0.11410248586615057
iteration : 12784
train acc:  0.8515625
train loss:  0.28442487120628357
train gradient:  0.15196432041708335
iteration : 12785
train acc:  0.8984375
train loss:  0.25848445296287537
train gradient:  0.09139388242022176
iteration : 12786
train acc:  0.890625
train loss:  0.2691058814525604
train gradient:  0.11176080738998262
iteration : 12787
train acc:  0.859375
train loss:  0.3247547149658203
train gradient:  0.1568856985381454
iteration : 12788
train acc:  0.90625
train loss:  0.2314559519290924
train gradient:  0.111157581605961
iteration : 12789
train acc:  0.859375
train loss:  0.2643356919288635
train gradient:  0.08674055822948597
iteration : 12790
train acc:  0.8671875
train loss:  0.29448845982551575
train gradient:  0.10337130643345635
iteration : 12791
train acc:  0.8359375
train loss:  0.3796102702617645
train gradient:  0.1634557322956483
iteration : 12792
train acc:  0.8671875
train loss:  0.3302484154701233
train gradient:  0.1650241969184104
iteration : 12793
train acc:  0.90625
train loss:  0.2340073585510254
train gradient:  0.11208462462441815
iteration : 12794
train acc:  0.90625
train loss:  0.2538938820362091
train gradient:  0.0934924880095677
iteration : 12795
train acc:  0.8671875
train loss:  0.30788660049438477
train gradient:  0.11177321491093654
iteration : 12796
train acc:  0.90625
train loss:  0.29922235012054443
train gradient:  0.14849886499801057
iteration : 12797
train acc:  0.875
train loss:  0.28379732370376587
train gradient:  0.174761207883104
iteration : 12798
train acc:  0.84375
train loss:  0.3552958071231842
train gradient:  0.15120133657258789
iteration : 12799
train acc:  0.8984375
train loss:  0.253726989030838
train gradient:  0.14438148790120048
iteration : 12800
train acc:  0.859375
train loss:  0.2678445875644684
train gradient:  0.14452345361515612
iteration : 12801
train acc:  0.8203125
train loss:  0.3887562155723572
train gradient:  0.19179039476423265
iteration : 12802
train acc:  0.8828125
train loss:  0.2582581639289856
train gradient:  0.08635941488462906
iteration : 12803
train acc:  0.84375
train loss:  0.3654119372367859
train gradient:  0.19050261057008375
iteration : 12804
train acc:  0.8671875
train loss:  0.3246309161186218
train gradient:  0.16183451298629714
iteration : 12805
train acc:  0.90625
train loss:  0.27301645278930664
train gradient:  0.11371243718548121
iteration : 12806
train acc:  0.8671875
train loss:  0.27394208312034607
train gradient:  0.15641965686079856
iteration : 12807
train acc:  0.84375
train loss:  0.31616485118865967
train gradient:  0.14072750383096694
iteration : 12808
train acc:  0.890625
train loss:  0.2705504596233368
train gradient:  0.13319373430975903
iteration : 12809
train acc:  0.875
train loss:  0.25698351860046387
train gradient:  0.1280779119350962
iteration : 12810
train acc:  0.8515625
train loss:  0.3259350061416626
train gradient:  0.1757983260518841
iteration : 12811
train acc:  0.8515625
train loss:  0.3482130765914917
train gradient:  0.15982995065874894
iteration : 12812
train acc:  0.8828125
train loss:  0.2874801456928253
train gradient:  0.12120082563527632
iteration : 12813
train acc:  0.8515625
train loss:  0.32267993688583374
train gradient:  0.13422272355777998
iteration : 12814
train acc:  0.8671875
train loss:  0.3525438904762268
train gradient:  0.20471478495444928
iteration : 12815
train acc:  0.828125
train loss:  0.3437070846557617
train gradient:  0.2207892572179983
iteration : 12816
train acc:  0.8046875
train loss:  0.38688230514526367
train gradient:  0.19771044517146102
iteration : 12817
train acc:  0.8671875
train loss:  0.31576335430145264
train gradient:  0.19105995033619227
iteration : 12818
train acc:  0.875
train loss:  0.2683965563774109
train gradient:  0.13903956492479091
iteration : 12819
train acc:  0.84375
train loss:  0.314243346452713
train gradient:  0.16650123727330762
iteration : 12820
train acc:  0.8515625
train loss:  0.3076273798942566
train gradient:  0.1356733485991377
iteration : 12821
train acc:  0.875
train loss:  0.2558886706829071
train gradient:  0.12790481230282694
iteration : 12822
train acc:  0.84375
train loss:  0.31776851415634155
train gradient:  0.15977189804368372
iteration : 12823
train acc:  0.8828125
train loss:  0.35195687413215637
train gradient:  0.2379232357108766
iteration : 12824
train acc:  0.828125
train loss:  0.3568155765533447
train gradient:  0.16617387011604196
iteration : 12825
train acc:  0.8828125
train loss:  0.28594860434532166
train gradient:  0.13692183689883353
iteration : 12826
train acc:  0.8515625
train loss:  0.29939115047454834
train gradient:  0.1393756682310619
iteration : 12827
train acc:  0.90625
train loss:  0.27014169096946716
train gradient:  0.11673797988054162
iteration : 12828
train acc:  0.828125
train loss:  0.3809720575809479
train gradient:  0.21659453624482047
iteration : 12829
train acc:  0.875
train loss:  0.2941601872444153
train gradient:  0.15443833279542685
iteration : 12830
train acc:  0.859375
train loss:  0.3013809621334076
train gradient:  0.2220569308497603
iteration : 12831
train acc:  0.9140625
train loss:  0.24097497761249542
train gradient:  0.11806564706282645
iteration : 12832
train acc:  0.8203125
train loss:  0.38581031560897827
train gradient:  0.1572700280218596
iteration : 12833
train acc:  0.890625
train loss:  0.2541337013244629
train gradient:  0.14208768905593977
iteration : 12834
train acc:  0.875
train loss:  0.2800860106945038
train gradient:  0.11290321535884787
iteration : 12835
train acc:  0.859375
train loss:  0.3942204713821411
train gradient:  0.1787126175489141
iteration : 12836
train acc:  0.875
train loss:  0.2737383246421814
train gradient:  0.1262747423797687
iteration : 12837
train acc:  0.8828125
train loss:  0.3243201971054077
train gradient:  0.20715593032504123
iteration : 12838
train acc:  0.875
train loss:  0.2740677297115326
train gradient:  0.10965807028840248
iteration : 12839
train acc:  0.875
train loss:  0.27873802185058594
train gradient:  0.12145381027166033
iteration : 12840
train acc:  0.8984375
train loss:  0.22765269875526428
train gradient:  0.12628420689407033
iteration : 12841
train acc:  0.8203125
train loss:  0.37486061453819275
train gradient:  0.1673298075298943
iteration : 12842
train acc:  0.8515625
train loss:  0.32139405608177185
train gradient:  0.1399944789050233
iteration : 12843
train acc:  0.796875
train loss:  0.4433028995990753
train gradient:  0.21540262536749857
iteration : 12844
train acc:  0.8515625
train loss:  0.35607481002807617
train gradient:  0.2660988919396052
iteration : 12845
train acc:  0.78125
train loss:  0.39747053384780884
train gradient:  0.19603833066849347
iteration : 12846
train acc:  0.8671875
train loss:  0.2843582034111023
train gradient:  0.11532698587279881
iteration : 12847
train acc:  0.8671875
train loss:  0.35171937942504883
train gradient:  0.20910734876221232
iteration : 12848
train acc:  0.90625
train loss:  0.28226238489151
train gradient:  0.2097058321422757
iteration : 12849
train acc:  0.8828125
train loss:  0.2791065573692322
train gradient:  0.1328354720200714
iteration : 12850
train acc:  0.84375
train loss:  0.3648744225502014
train gradient:  0.20347083992420786
iteration : 12851
train acc:  0.8359375
train loss:  0.3532654643058777
train gradient:  0.23317418531367817
iteration : 12852
train acc:  0.828125
train loss:  0.34139055013656616
train gradient:  0.1836244502543357
iteration : 12853
train acc:  0.8359375
train loss:  0.38739117980003357
train gradient:  0.23617325996662597
iteration : 12854
train acc:  0.8515625
train loss:  0.32659631967544556
train gradient:  0.18281032357819255
iteration : 12855
train acc:  0.890625
train loss:  0.30982375144958496
train gradient:  0.16191116870787034
iteration : 12856
train acc:  0.8125
train loss:  0.3748503625392914
train gradient:  0.3582838094465524
iteration : 12857
train acc:  0.8515625
train loss:  0.35633617639541626
train gradient:  0.17061430003097655
iteration : 12858
train acc:  0.8515625
train loss:  0.3070904612541199
train gradient:  0.1292293073951855
iteration : 12859
train acc:  0.90625
train loss:  0.29930806159973145
train gradient:  0.13505231995120404
iteration : 12860
train acc:  0.921875
train loss:  0.2407674789428711
train gradient:  0.12479481540086364
iteration : 12861
train acc:  0.9140625
train loss:  0.2552151679992676
train gradient:  0.12076390148864646
iteration : 12862
train acc:  0.8828125
train loss:  0.31167659163475037
train gradient:  0.13174395607130607
iteration : 12863
train acc:  0.875
train loss:  0.24139779806137085
train gradient:  0.09733685955935699
iteration : 12864
train acc:  0.875
train loss:  0.3231355845928192
train gradient:  0.22166559862892998
iteration : 12865
train acc:  0.8515625
train loss:  0.37066203355789185
train gradient:  0.15818474544459865
iteration : 12866
train acc:  0.8359375
train loss:  0.4020906090736389
train gradient:  0.23645209172302317
iteration : 12867
train acc:  0.859375
train loss:  0.3351430594921112
train gradient:  0.193801943056591
iteration : 12868
train acc:  0.921875
train loss:  0.24089518189430237
train gradient:  0.10090183114674356
iteration : 12869
train acc:  0.859375
train loss:  0.30495837330818176
train gradient:  0.17396267531260765
iteration : 12870
train acc:  0.859375
train loss:  0.3332587480545044
train gradient:  0.12215256591118509
iteration : 12871
train acc:  0.90625
train loss:  0.2514798045158386
train gradient:  0.09918510798802044
iteration : 12872
train acc:  0.859375
train loss:  0.37219423055648804
train gradient:  0.23920433725275486
iteration : 12873
train acc:  0.875
train loss:  0.274430513381958
train gradient:  0.12120116621182596
iteration : 12874
train acc:  0.859375
train loss:  0.3175368905067444
train gradient:  0.1810106127139397
iteration : 12875
train acc:  0.8359375
train loss:  0.4091636538505554
train gradient:  0.3349444946487703
iteration : 12876
train acc:  0.828125
train loss:  0.34199702739715576
train gradient:  0.17585553918638352
iteration : 12877
train acc:  0.8984375
train loss:  0.23355063796043396
train gradient:  0.0802835397585678
iteration : 12878
train acc:  0.9140625
train loss:  0.26661455631256104
train gradient:  0.11044929780295397
iteration : 12879
train acc:  0.8203125
train loss:  0.38477563858032227
train gradient:  0.19855882979825637
iteration : 12880
train acc:  0.8125
train loss:  0.3546016812324524
train gradient:  0.15735031770981667
iteration : 12881
train acc:  0.875
train loss:  0.28323549032211304
train gradient:  0.1050138077459187
iteration : 12882
train acc:  0.8515625
train loss:  0.2993108928203583
train gradient:  0.12124553048272466
iteration : 12883
train acc:  0.8828125
train loss:  0.29982632398605347
train gradient:  0.11014215034872768
iteration : 12884
train acc:  0.828125
train loss:  0.29942333698272705
train gradient:  0.14703836375931834
iteration : 12885
train acc:  0.8828125
train loss:  0.2777560353279114
train gradient:  0.1167050565251615
iteration : 12886
train acc:  0.8828125
train loss:  0.2780640721321106
train gradient:  0.13353222702448211
iteration : 12887
train acc:  0.796875
train loss:  0.4349837303161621
train gradient:  0.2507045404455899
iteration : 12888
train acc:  0.859375
train loss:  0.369790256023407
train gradient:  0.1968679128186739
iteration : 12889
train acc:  0.8984375
train loss:  0.28978970646858215
train gradient:  0.17426440870354748
iteration : 12890
train acc:  0.8515625
train loss:  0.34917449951171875
train gradient:  0.15215344990531857
iteration : 12891
train acc:  0.90625
train loss:  0.23762555420398712
train gradient:  0.10091178557405994
iteration : 12892
train acc:  0.8828125
train loss:  0.3353942632675171
train gradient:  0.1598095366251629
iteration : 12893
train acc:  0.859375
train loss:  0.31061995029449463
train gradient:  0.14955734162555606
iteration : 12894
train acc:  0.8984375
train loss:  0.312838613986969
train gradient:  0.1785112941779551
iteration : 12895
train acc:  0.8828125
train loss:  0.3507952094078064
train gradient:  0.17229889755236194
iteration : 12896
train acc:  0.828125
train loss:  0.3661477565765381
train gradient:  0.22599744942105987
iteration : 12897
train acc:  0.8828125
train loss:  0.29900383949279785
train gradient:  0.12430109459809886
iteration : 12898
train acc:  0.8828125
train loss:  0.30417126417160034
train gradient:  0.13022258286008628
iteration : 12899
train acc:  0.8984375
train loss:  0.285316526889801
train gradient:  0.09246517444264866
iteration : 12900
train acc:  0.828125
train loss:  0.335227906703949
train gradient:  0.1880495035880816
iteration : 12901
train acc:  0.90625
train loss:  0.2255917191505432
train gradient:  0.15029656728137009
iteration : 12902
train acc:  0.9140625
train loss:  0.27237528562545776
train gradient:  0.11582457312007167
iteration : 12903
train acc:  0.859375
train loss:  0.3476729691028595
train gradient:  0.20308979254854098
iteration : 12904
train acc:  0.8515625
train loss:  0.3663770854473114
train gradient:  0.17213417902389933
iteration : 12905
train acc:  0.859375
train loss:  0.26119211316108704
train gradient:  0.09225828138215521
iteration : 12906
train acc:  0.84375
train loss:  0.3247970640659332
train gradient:  0.19355763594805014
iteration : 12907
train acc:  0.859375
train loss:  0.3123478591442108
train gradient:  0.15039781528204138
iteration : 12908
train acc:  0.796875
train loss:  0.424838662147522
train gradient:  0.2364626418913682
iteration : 12909
train acc:  0.875
train loss:  0.2907957434654236
train gradient:  0.12196211275876073
iteration : 12910
train acc:  0.859375
train loss:  0.32068759202957153
train gradient:  0.1370181918197157
iteration : 12911
train acc:  0.90625
train loss:  0.22302331030368805
train gradient:  0.10527863175202794
iteration : 12912
train acc:  0.8359375
train loss:  0.3028460741043091
train gradient:  0.11994106533287369
iteration : 12913
train acc:  0.859375
train loss:  0.3111390173435211
train gradient:  0.12905271244459687
iteration : 12914
train acc:  0.875
train loss:  0.3348071277141571
train gradient:  0.10303267096935166
iteration : 12915
train acc:  0.890625
train loss:  0.3223206400871277
train gradient:  0.180453632569169
iteration : 12916
train acc:  0.8046875
train loss:  0.33230382204055786
train gradient:  0.15106797371597305
iteration : 12917
train acc:  0.7890625
train loss:  0.38897448778152466
train gradient:  0.23007335889423655
iteration : 12918
train acc:  0.859375
train loss:  0.33275264501571655
train gradient:  0.1742016990487328
iteration : 12919
train acc:  0.890625
train loss:  0.304065465927124
train gradient:  0.17375984088595808
iteration : 12920
train acc:  0.8671875
train loss:  0.2753555178642273
train gradient:  0.10958754890730561
iteration : 12921
train acc:  0.8828125
train loss:  0.3331810235977173
train gradient:  0.20380718486748897
iteration : 12922
train acc:  0.8515625
train loss:  0.4125530421733856
train gradient:  0.2475618262319012
iteration : 12923
train acc:  0.8671875
train loss:  0.28369754552841187
train gradient:  0.06851756886049792
iteration : 12924
train acc:  0.796875
train loss:  0.43896546959877014
train gradient:  0.27817649995783417
iteration : 12925
train acc:  0.8515625
train loss:  0.35998237133026123
train gradient:  0.1662174153659682
iteration : 12926
train acc:  0.8671875
train loss:  0.3061233460903168
train gradient:  0.16342401346582155
iteration : 12927
train acc:  0.8828125
train loss:  0.24818992614746094
train gradient:  0.11814015097213773
iteration : 12928
train acc:  0.875
train loss:  0.30510568618774414
train gradient:  0.1760199127416307
iteration : 12929
train acc:  0.859375
train loss:  0.32157042622566223
train gradient:  0.11747132260231745
iteration : 12930
train acc:  0.875
train loss:  0.31397294998168945
train gradient:  0.15572285932369953
iteration : 12931
train acc:  0.8515625
train loss:  0.2918005585670471
train gradient:  0.10580523433141695
iteration : 12932
train acc:  0.90625
train loss:  0.2736639976501465
train gradient:  0.1632345989075048
iteration : 12933
train acc:  0.8828125
train loss:  0.30161619186401367
train gradient:  0.11970400423011777
iteration : 12934
train acc:  0.8828125
train loss:  0.27530139684677124
train gradient:  0.0901072750910558
iteration : 12935
train acc:  0.8515625
train loss:  0.32036465406417847
train gradient:  0.12597062395514197
iteration : 12936
train acc:  0.84375
train loss:  0.33457061648368835
train gradient:  0.11999118358205627
iteration : 12937
train acc:  0.890625
train loss:  0.27268970012664795
train gradient:  0.09409418052873852
iteration : 12938
train acc:  0.921875
train loss:  0.22467009723186493
train gradient:  0.0882018024016861
iteration : 12939
train acc:  0.890625
train loss:  0.2654978632926941
train gradient:  0.10229933814452115
iteration : 12940
train acc:  0.859375
train loss:  0.3778112232685089
train gradient:  0.18107142978530397
iteration : 12941
train acc:  0.859375
train loss:  0.29048043489456177
train gradient:  0.12180221690795774
iteration : 12942
train acc:  0.8671875
train loss:  0.31271564960479736
train gradient:  0.1559896175897679
iteration : 12943
train acc:  0.875
train loss:  0.34325528144836426
train gradient:  0.18150642025739044
iteration : 12944
train acc:  0.8359375
train loss:  0.3764841556549072
train gradient:  0.192725748224105
iteration : 12945
train acc:  0.78125
train loss:  0.4344976544380188
train gradient:  0.31006380256811944
iteration : 12946
train acc:  0.8515625
train loss:  0.3334881663322449
train gradient:  0.16642006849532617
iteration : 12947
train acc:  0.859375
train loss:  0.31020307540893555
train gradient:  0.23699560616825713
iteration : 12948
train acc:  0.8125
train loss:  0.3996155560016632
train gradient:  0.20537775314500326
iteration : 12949
train acc:  0.84375
train loss:  0.30032482743263245
train gradient:  0.10645322673957956
iteration : 12950
train acc:  0.8671875
train loss:  0.32092219591140747
train gradient:  0.1356520889688515
iteration : 12951
train acc:  0.8515625
train loss:  0.2949301600456238
train gradient:  0.10682077404815303
iteration : 12952
train acc:  0.8203125
train loss:  0.3470657467842102
train gradient:  0.2884033550748631
iteration : 12953
train acc:  0.859375
train loss:  0.2864096760749817
train gradient:  0.14138514735204047
iteration : 12954
train acc:  0.84375
train loss:  0.35673654079437256
train gradient:  0.15029584444487304
iteration : 12955
train acc:  0.875
train loss:  0.3216151297092438
train gradient:  0.14638829495532069
iteration : 12956
train acc:  0.875
train loss:  0.29618948698043823
train gradient:  0.11864131178336908
iteration : 12957
train acc:  0.9140625
train loss:  0.252802312374115
train gradient:  0.12126663729109799
iteration : 12958
train acc:  0.8984375
train loss:  0.31941449642181396
train gradient:  0.1164878566924394
iteration : 12959
train acc:  0.8828125
train loss:  0.2626184821128845
train gradient:  0.11646968520481528
iteration : 12960
train acc:  0.828125
train loss:  0.34709981083869934
train gradient:  0.1654254361827229
iteration : 12961
train acc:  0.8203125
train loss:  0.3639327585697174
train gradient:  0.14171647017538455
iteration : 12962
train acc:  0.8984375
train loss:  0.2762880325317383
train gradient:  0.11288326791346805
iteration : 12963
train acc:  0.890625
train loss:  0.2290562391281128
train gradient:  0.08718240824019104
iteration : 12964
train acc:  0.859375
train loss:  0.31177130341529846
train gradient:  0.1803394112444626
iteration : 12965
train acc:  0.875
train loss:  0.3091006875038147
train gradient:  0.18674302286327535
iteration : 12966
train acc:  0.8828125
train loss:  0.2658913731575012
train gradient:  0.10907957390510468
iteration : 12967
train acc:  0.8828125
train loss:  0.24926763772964478
train gradient:  0.0873393156558985
iteration : 12968
train acc:  0.8671875
train loss:  0.29177096486091614
train gradient:  0.12904270922145533
iteration : 12969
train acc:  0.875
train loss:  0.29771074652671814
train gradient:  0.11207493985074618
iteration : 12970
train acc:  0.875
train loss:  0.23845292627811432
train gradient:  0.11334116714358858
iteration : 12971
train acc:  0.90625
train loss:  0.21744059026241302
train gradient:  0.07831593223760952
iteration : 12972
train acc:  0.859375
train loss:  0.2986041009426117
train gradient:  0.15922066072704694
iteration : 12973
train acc:  0.875
train loss:  0.35051149129867554
train gradient:  0.16994655640913725
iteration : 12974
train acc:  0.796875
train loss:  0.4473438858985901
train gradient:  0.2536719482130099
iteration : 12975
train acc:  0.890625
train loss:  0.28509095311164856
train gradient:  0.14503803757201547
iteration : 12976
train acc:  0.7890625
train loss:  0.4604122042655945
train gradient:  0.24234642918276275
iteration : 12977
train acc:  0.890625
train loss:  0.25978606939315796
train gradient:  0.13001995994056847
iteration : 12978
train acc:  0.8359375
train loss:  0.3358810544013977
train gradient:  0.1811928386147127
iteration : 12979
train acc:  0.8671875
train loss:  0.31514161825180054
train gradient:  0.16428864616513728
iteration : 12980
train acc:  0.84375
train loss:  0.3672305643558502
train gradient:  0.2638031403858153
iteration : 12981
train acc:  0.8203125
train loss:  0.42170003056526184
train gradient:  0.2314046955512749
iteration : 12982
train acc:  0.8515625
train loss:  0.3141297996044159
train gradient:  0.12226424223474137
iteration : 12983
train acc:  0.859375
train loss:  0.31463855504989624
train gradient:  0.11578485627632709
iteration : 12984
train acc:  0.890625
train loss:  0.2678492069244385
train gradient:  0.10818057729719767
iteration : 12985
train acc:  0.875
train loss:  0.3363548815250397
train gradient:  0.27933266293854975
iteration : 12986
train acc:  0.8984375
train loss:  0.2486455738544464
train gradient:  0.10628813374565796
iteration : 12987
train acc:  0.8828125
train loss:  0.28491726517677307
train gradient:  0.10867386621440277
iteration : 12988
train acc:  0.84375
train loss:  0.31325671076774597
train gradient:  0.14599886180973126
iteration : 12989
train acc:  0.8359375
train loss:  0.2884180247783661
train gradient:  0.08886102214017548
iteration : 12990
train acc:  0.8203125
train loss:  0.3202117681503296
train gradient:  0.16847118691872365
iteration : 12991
train acc:  0.828125
train loss:  0.37379854917526245
train gradient:  0.2199250582085443
iteration : 12992
train acc:  0.8203125
train loss:  0.34679150581359863
train gradient:  0.14901301089274788
iteration : 12993
train acc:  0.8828125
train loss:  0.344411700963974
train gradient:  0.16788590339844658
iteration : 12994
train acc:  0.828125
train loss:  0.35568851232528687
train gradient:  0.19605292028059054
iteration : 12995
train acc:  0.84375
train loss:  0.35004162788391113
train gradient:  0.24785181311953286
iteration : 12996
train acc:  0.8125
train loss:  0.36934709548950195
train gradient:  0.18782919043222954
iteration : 12997
train acc:  0.828125
train loss:  0.3013979196548462
train gradient:  0.16699876859775337
iteration : 12998
train acc:  0.890625
train loss:  0.3534417748451233
train gradient:  0.15635890272284397
iteration : 12999
train acc:  0.9140625
train loss:  0.29019466042518616
train gradient:  0.09990138692141948
iteration : 13000
train acc:  0.8671875
train loss:  0.31059855222702026
train gradient:  0.12250486827009589
iteration : 13001
train acc:  0.828125
train loss:  0.3503221869468689
train gradient:  0.20607511596874173
iteration : 13002
train acc:  0.8984375
train loss:  0.24027219414710999
train gradient:  0.13501876116455996
iteration : 13003
train acc:  0.8125
train loss:  0.34657472372055054
train gradient:  0.16997716860939166
iteration : 13004
train acc:  0.8828125
train loss:  0.28391051292419434
train gradient:  0.1441639116612823
iteration : 13005
train acc:  0.921875
train loss:  0.24854935705661774
train gradient:  0.08626696022478654
iteration : 13006
train acc:  0.921875
train loss:  0.2391507923603058
train gradient:  0.09599302511092438
iteration : 13007
train acc:  0.8828125
train loss:  0.27239006757736206
train gradient:  0.14667724006608707
iteration : 13008
train acc:  0.890625
train loss:  0.3191799521446228
train gradient:  0.11861289506013467
iteration : 13009
train acc:  0.9375
train loss:  0.22241798043251038
train gradient:  0.08337984878517325
iteration : 13010
train acc:  0.875
train loss:  0.27811679244041443
train gradient:  0.07418295173435156
iteration : 13011
train acc:  0.8828125
train loss:  0.3272923231124878
train gradient:  0.1396627748212041
iteration : 13012
train acc:  0.9140625
train loss:  0.2792375385761261
train gradient:  0.1090070886614697
iteration : 13013
train acc:  0.8046875
train loss:  0.3971232771873474
train gradient:  0.272653878159167
iteration : 13014
train acc:  0.8359375
train loss:  0.3168730139732361
train gradient:  0.12251114669632975
iteration : 13015
train acc:  0.8359375
train loss:  0.3559441864490509
train gradient:  0.1581634788818314
iteration : 13016
train acc:  0.8359375
train loss:  0.350147008895874
train gradient:  0.1780847413617906
iteration : 13017
train acc:  0.8203125
train loss:  0.3838129937648773
train gradient:  0.18369833974825203
iteration : 13018
train acc:  0.890625
train loss:  0.26025521755218506
train gradient:  0.0959586095543948
iteration : 13019
train acc:  0.875
train loss:  0.24159905314445496
train gradient:  0.1089900900931942
iteration : 13020
train acc:  0.859375
train loss:  0.397259920835495
train gradient:  0.17983606320789938
iteration : 13021
train acc:  0.84375
train loss:  0.31190961599349976
train gradient:  0.14951261795363555
iteration : 13022
train acc:  0.890625
train loss:  0.30285656452178955
train gradient:  0.2575389063002644
iteration : 13023
train acc:  0.859375
train loss:  0.3286798596382141
train gradient:  0.14705136708952374
iteration : 13024
train acc:  0.84375
train loss:  0.3181087374687195
train gradient:  0.10310947143050378
iteration : 13025
train acc:  0.90625
train loss:  0.31395602226257324
train gradient:  0.12119537673742468
iteration : 13026
train acc:  0.859375
train loss:  0.28050097823143005
train gradient:  0.17458091307662077
iteration : 13027
train acc:  0.8828125
train loss:  0.26199081540107727
train gradient:  0.11398838938995456
iteration : 13028
train acc:  0.8359375
train loss:  0.3534524440765381
train gradient:  0.20645092648945873
iteration : 13029
train acc:  0.8125
train loss:  0.4188219904899597
train gradient:  0.3239795132434675
iteration : 13030
train acc:  0.8359375
train loss:  0.3851844072341919
train gradient:  0.25194928436739145
iteration : 13031
train acc:  0.8828125
train loss:  0.2891765236854553
train gradient:  0.13412324304881684
iteration : 13032
train acc:  0.8671875
train loss:  0.3059931993484497
train gradient:  0.11826308246932347
iteration : 13033
train acc:  0.890625
train loss:  0.26409977674484253
train gradient:  0.15554506188113978
iteration : 13034
train acc:  0.8359375
train loss:  0.36291995644569397
train gradient:  0.19691730270162633
iteration : 13035
train acc:  0.8828125
train loss:  0.29813680052757263
train gradient:  0.11789471430860402
iteration : 13036
train acc:  0.8515625
train loss:  0.34804266691207886
train gradient:  0.19745539240588375
iteration : 13037
train acc:  0.8828125
train loss:  0.30533120036125183
train gradient:  0.09318392543420224
iteration : 13038
train acc:  0.796875
train loss:  0.4097697138786316
train gradient:  0.19859672730656253
iteration : 13039
train acc:  0.890625
train loss:  0.28024184703826904
train gradient:  0.10439884349642237
iteration : 13040
train acc:  0.8984375
train loss:  0.2766251564025879
train gradient:  0.08984047451489227
iteration : 13041
train acc:  0.875
train loss:  0.366838276386261
train gradient:  0.1494950471049964
iteration : 13042
train acc:  0.8828125
train loss:  0.2917652130126953
train gradient:  0.10348540704810509
iteration : 13043
train acc:  0.90625
train loss:  0.2757014036178589
train gradient:  0.08711697904654864
iteration : 13044
train acc:  0.84375
train loss:  0.3208882510662079
train gradient:  0.12005648998737019
iteration : 13045
train acc:  0.890625
train loss:  0.2685457766056061
train gradient:  0.10304629032748834
iteration : 13046
train acc:  0.8671875
train loss:  0.34982335567474365
train gradient:  0.12501824381948995
iteration : 13047
train acc:  0.8515625
train loss:  0.2962172031402588
train gradient:  0.10092256629682204
iteration : 13048
train acc:  0.828125
train loss:  0.3811676502227783
train gradient:  0.3852636846336417
iteration : 13049
train acc:  0.90625
train loss:  0.26234591007232666
train gradient:  0.11583563540821054
iteration : 13050
train acc:  0.859375
train loss:  0.34684038162231445
train gradient:  0.14875171982485905
iteration : 13051
train acc:  0.828125
train loss:  0.4030079245567322
train gradient:  0.17891503253288993
iteration : 13052
train acc:  0.8359375
train loss:  0.3235999345779419
train gradient:  0.11909354886416587
iteration : 13053
train acc:  0.8984375
train loss:  0.24977900087833405
train gradient:  0.09777540218921536
iteration : 13054
train acc:  0.859375
train loss:  0.31480568647384644
train gradient:  0.15819051206556908
iteration : 13055
train acc:  0.8984375
train loss:  0.2786839008331299
train gradient:  0.11847288340744483
iteration : 13056
train acc:  0.8828125
train loss:  0.28456389904022217
train gradient:  0.11448827410145464
iteration : 13057
train acc:  0.8828125
train loss:  0.27323055267333984
train gradient:  0.1331892335546499
iteration : 13058
train acc:  0.890625
train loss:  0.26810240745544434
train gradient:  0.11657006705638229
iteration : 13059
train acc:  0.8515625
train loss:  0.3792504668235779
train gradient:  0.20719111515650723
iteration : 13060
train acc:  0.90625
train loss:  0.26057255268096924
train gradient:  0.07718931452747496
iteration : 13061
train acc:  0.8515625
train loss:  0.4350268542766571
train gradient:  0.24844475377315428
iteration : 13062
train acc:  0.875
train loss:  0.2856926918029785
train gradient:  0.10861295350047277
iteration : 13063
train acc:  0.8828125
train loss:  0.32180511951446533
train gradient:  0.11963548103493776
iteration : 13064
train acc:  0.8203125
train loss:  0.30241209268569946
train gradient:  0.1750043005145821
iteration : 13065
train acc:  0.8046875
train loss:  0.41933393478393555
train gradient:  0.23479601258988164
iteration : 13066
train acc:  0.8515625
train loss:  0.3023867607116699
train gradient:  0.1221312192819004
iteration : 13067
train acc:  0.8984375
train loss:  0.25908586382865906
train gradient:  0.10072086399843863
iteration : 13068
train acc:  0.8125
train loss:  0.3860926628112793
train gradient:  0.23699233458421043
iteration : 13069
train acc:  0.8515625
train loss:  0.26723819971084595
train gradient:  0.11811191134210397
iteration : 13070
train acc:  0.828125
train loss:  0.3483175039291382
train gradient:  0.2147243418143117
iteration : 13071
train acc:  0.8671875
train loss:  0.3325870633125305
train gradient:  0.13166348682563198
iteration : 13072
train acc:  0.890625
train loss:  0.3247259259223938
train gradient:  0.1554783322327406
iteration : 13073
train acc:  0.875
train loss:  0.3259691596031189
train gradient:  0.1402369542694789
iteration : 13074
train acc:  0.875
train loss:  0.282400518655777
train gradient:  0.10806930663887607
iteration : 13075
train acc:  0.875
train loss:  0.2721644639968872
train gradient:  0.10590269241307568
iteration : 13076
train acc:  0.8671875
train loss:  0.28100812435150146
train gradient:  0.12483305276246931
iteration : 13077
train acc:  0.84375
train loss:  0.35438036918640137
train gradient:  0.11660089552078676
iteration : 13078
train acc:  0.8671875
train loss:  0.34269770979881287
train gradient:  0.13320503675901518
iteration : 13079
train acc:  0.8984375
train loss:  0.30769652128219604
train gradient:  0.1372120134966152
iteration : 13080
train acc:  0.890625
train loss:  0.26193612813949585
train gradient:  0.10130120979611389
iteration : 13081
train acc:  0.859375
train loss:  0.3249295651912689
train gradient:  0.17488427669335704
iteration : 13082
train acc:  0.859375
train loss:  0.3296152949333191
train gradient:  0.13093155646163673
iteration : 13083
train acc:  0.859375
train loss:  0.2649330496788025
train gradient:  0.07581461055876282
iteration : 13084
train acc:  0.8359375
train loss:  0.36140182614326477
train gradient:  0.12870533914544152
iteration : 13085
train acc:  0.8359375
train loss:  0.3567081391811371
train gradient:  0.29662443040547265
iteration : 13086
train acc:  0.8359375
train loss:  0.3028658628463745
train gradient:  0.15833718631534577
iteration : 13087
train acc:  0.8125
train loss:  0.3949555456638336
train gradient:  0.1782068372702752
iteration : 13088
train acc:  0.8359375
train loss:  0.4404921531677246
train gradient:  0.2146087987941913
iteration : 13089
train acc:  0.90625
train loss:  0.2495115101337433
train gradient:  0.05977620598959613
iteration : 13090
train acc:  0.8828125
train loss:  0.29719775915145874
train gradient:  0.11810014195651285
iteration : 13091
train acc:  0.8515625
train loss:  0.34397709369659424
train gradient:  0.14452350179718548
iteration : 13092
train acc:  0.8046875
train loss:  0.33765116333961487
train gradient:  0.1165067802933321
iteration : 13093
train acc:  0.796875
train loss:  0.41909468173980713
train gradient:  0.17390319671897664
iteration : 13094
train acc:  0.8671875
train loss:  0.3061222434043884
train gradient:  0.1353836750842033
iteration : 13095
train acc:  0.875
train loss:  0.33075615763664246
train gradient:  0.12481708675777037
iteration : 13096
train acc:  0.8828125
train loss:  0.26369863748550415
train gradient:  0.0701861890753631
iteration : 13097
train acc:  0.8828125
train loss:  0.2864460051059723
train gradient:  0.12566093107112863
iteration : 13098
train acc:  0.8359375
train loss:  0.3100084960460663
train gradient:  0.119137163489966
iteration : 13099
train acc:  0.84375
train loss:  0.3741845488548279
train gradient:  0.19814787715345356
iteration : 13100
train acc:  0.8828125
train loss:  0.24930045008659363
train gradient:  0.07907607461084953
iteration : 13101
train acc:  0.8671875
train loss:  0.2614724636077881
train gradient:  0.10359074685876872
iteration : 13102
train acc:  0.828125
train loss:  0.35661160945892334
train gradient:  0.1832779717421451
iteration : 13103
train acc:  0.90625
train loss:  0.28596627712249756
train gradient:  0.08686515508789808
iteration : 13104
train acc:  0.828125
train loss:  0.36821305751800537
train gradient:  0.19792266029978434
iteration : 13105
train acc:  0.859375
train loss:  0.3325479328632355
train gradient:  0.133553000181233
iteration : 13106
train acc:  0.828125
train loss:  0.3678281605243683
train gradient:  0.172564867506113
iteration : 13107
train acc:  0.8671875
train loss:  0.2733229100704193
train gradient:  0.0877976409516761
iteration : 13108
train acc:  0.9140625
train loss:  0.26350557804107666
train gradient:  0.11003348549150634
iteration : 13109
train acc:  0.8359375
train loss:  0.3451949954032898
train gradient:  0.14719158081166012
iteration : 13110
train acc:  0.8515625
train loss:  0.2916947901248932
train gradient:  0.11587820723396865
iteration : 13111
train acc:  0.84375
train loss:  0.3217129409313202
train gradient:  0.16134752805787927
iteration : 13112
train acc:  0.84375
train loss:  0.31980252265930176
train gradient:  0.13264299838350146
iteration : 13113
train acc:  0.875
train loss:  0.32840418815612793
train gradient:  0.0998430221237799
iteration : 13114
train acc:  0.8828125
train loss:  0.2741265892982483
train gradient:  0.15754634769012948
iteration : 13115
train acc:  0.84375
train loss:  0.38175666332244873
train gradient:  0.17205005318564412
iteration : 13116
train acc:  0.8984375
train loss:  0.24038897454738617
train gradient:  0.16368222175090125
iteration : 13117
train acc:  0.921875
train loss:  0.19986355304718018
train gradient:  0.06100848691929407
iteration : 13118
train acc:  0.890625
train loss:  0.2578202486038208
train gradient:  0.0872756124364917
iteration : 13119
train acc:  0.8359375
train loss:  0.38845282793045044
train gradient:  0.21297575378357322
iteration : 13120
train acc:  0.8359375
train loss:  0.38288068771362305
train gradient:  0.20636564517163336
iteration : 13121
train acc:  0.8671875
train loss:  0.2585189938545227
train gradient:  0.15411490358682395
iteration : 13122
train acc:  0.8671875
train loss:  0.33352524042129517
train gradient:  0.19349192904284657
iteration : 13123
train acc:  0.828125
train loss:  0.364995539188385
train gradient:  0.17054923490238025
iteration : 13124
train acc:  0.8515625
train loss:  0.3274186849594116
train gradient:  0.11842369412705023
iteration : 13125
train acc:  0.84375
train loss:  0.3405449688434601
train gradient:  0.24791660396928192
iteration : 13126
train acc:  0.8203125
train loss:  0.38307154178619385
train gradient:  0.20525523991171085
iteration : 13127
train acc:  0.8828125
train loss:  0.2560257911682129
train gradient:  0.12827047793372712
iteration : 13128
train acc:  0.8515625
train loss:  0.32829564809799194
train gradient:  0.14717062761612654
iteration : 13129
train acc:  0.8125
train loss:  0.43921273946762085
train gradient:  0.36902717241621125
iteration : 13130
train acc:  0.8671875
train loss:  0.2900550365447998
train gradient:  0.16118507104258012
iteration : 13131
train acc:  0.921875
train loss:  0.23138797283172607
train gradient:  0.1954296203772422
iteration : 13132
train acc:  0.8515625
train loss:  0.32505062222480774
train gradient:  0.15516583916572085
iteration : 13133
train acc:  0.8671875
train loss:  0.3387158513069153
train gradient:  0.12641886712453096
iteration : 13134
train acc:  0.875
train loss:  0.3016321361064911
train gradient:  0.1310030554368328
iteration : 13135
train acc:  0.890625
train loss:  0.26955804228782654
train gradient:  0.11717707469536526
iteration : 13136
train acc:  0.84375
train loss:  0.357666015625
train gradient:  0.20205357775050048
iteration : 13137
train acc:  0.875
train loss:  0.30726081132888794
train gradient:  0.12210516535624936
iteration : 13138
train acc:  0.90625
train loss:  0.2383529543876648
train gradient:  0.12442519901599398
iteration : 13139
train acc:  0.8515625
train loss:  0.3385886549949646
train gradient:  0.20199210886509122
iteration : 13140
train acc:  0.84375
train loss:  0.3129851520061493
train gradient:  0.1286456749729578
iteration : 13141
train acc:  0.8359375
train loss:  0.45080089569091797
train gradient:  0.2306421128187401
iteration : 13142
train acc:  0.875
train loss:  0.30351656675338745
train gradient:  0.09077851859881227
iteration : 13143
train acc:  0.8046875
train loss:  0.38470080494880676
train gradient:  0.2803558298343639
iteration : 13144
train acc:  0.8359375
train loss:  0.4510546326637268
train gradient:  0.2713941900392282
iteration : 13145
train acc:  0.8515625
train loss:  0.30150383710861206
train gradient:  0.15309559660725539
iteration : 13146
train acc:  0.84375
train loss:  0.2793336510658264
train gradient:  0.11991752984351438
iteration : 13147
train acc:  0.859375
train loss:  0.33387088775634766
train gradient:  0.1597767423448626
iteration : 13148
train acc:  0.8671875
train loss:  0.3207396864891052
train gradient:  0.2426463675874324
iteration : 13149
train acc:  0.8359375
train loss:  0.2933972477912903
train gradient:  0.09800415443538381
iteration : 13150
train acc:  0.859375
train loss:  0.33906808495521545
train gradient:  0.13802871634973607
iteration : 13151
train acc:  0.84375
train loss:  0.34080541133880615
train gradient:  0.18272977187523648
iteration : 13152
train acc:  0.8828125
train loss:  0.28023746609687805
train gradient:  0.13658384479712932
iteration : 13153
train acc:  0.8984375
train loss:  0.23966597020626068
train gradient:  0.11367071898484866
iteration : 13154
train acc:  0.859375
train loss:  0.2852846384048462
train gradient:  0.14694346004558878
iteration : 13155
train acc:  0.8515625
train loss:  0.31698381900787354
train gradient:  0.12380152464705804
iteration : 13156
train acc:  0.8203125
train loss:  0.35851147770881653
train gradient:  0.1610565085578127
iteration : 13157
train acc:  0.8125
train loss:  0.37242478132247925
train gradient:  0.21449755357342662
iteration : 13158
train acc:  0.8359375
train loss:  0.30467164516448975
train gradient:  0.10254951632249129
iteration : 13159
train acc:  0.8828125
train loss:  0.31526535749435425
train gradient:  0.1332280694477656
iteration : 13160
train acc:  0.84375
train loss:  0.32562732696533203
train gradient:  0.12434638755950193
iteration : 13161
train acc:  0.8828125
train loss:  0.2856125235557556
train gradient:  0.09342401598665181
iteration : 13162
train acc:  0.84375
train loss:  0.4108020067214966
train gradient:  0.25635781123710727
iteration : 13163
train acc:  0.84375
train loss:  0.34212905168533325
train gradient:  0.1551869070387096
iteration : 13164
train acc:  0.875
train loss:  0.32851743698120117
train gradient:  0.13380123174266723
iteration : 13165
train acc:  0.890625
train loss:  0.3211832046508789
train gradient:  0.11606148868317284
iteration : 13166
train acc:  0.8671875
train loss:  0.3069688081741333
train gradient:  0.14359059885448422
iteration : 13167
train acc:  0.9140625
train loss:  0.2433544099330902
train gradient:  0.0940745235861895
iteration : 13168
train acc:  0.859375
train loss:  0.3775382339954376
train gradient:  0.16617635186231233
iteration : 13169
train acc:  0.859375
train loss:  0.33110636472702026
train gradient:  0.15095743161419334
iteration : 13170
train acc:  0.8515625
train loss:  0.3537812829017639
train gradient:  0.13367851872703426
iteration : 13171
train acc:  0.8125
train loss:  0.37803733348846436
train gradient:  0.1817532306747444
iteration : 13172
train acc:  0.796875
train loss:  0.44970905780792236
train gradient:  0.1907875556127187
iteration : 13173
train acc:  0.8671875
train loss:  0.2876659631729126
train gradient:  0.09605654704438514
iteration : 13174
train acc:  0.8828125
train loss:  0.2599829435348511
train gradient:  0.08147562078008663
iteration : 13175
train acc:  0.8671875
train loss:  0.2874014973640442
train gradient:  0.10130763212348597
iteration : 13176
train acc:  0.8515625
train loss:  0.3474957048892975
train gradient:  0.1613746564888508
iteration : 13177
train acc:  0.8515625
train loss:  0.34404057264328003
train gradient:  0.14337506589812718
iteration : 13178
train acc:  0.8828125
train loss:  0.334463894367218
train gradient:  0.14528867080486113
iteration : 13179
train acc:  0.8515625
train loss:  0.3715289533138275
train gradient:  0.222563259008919
iteration : 13180
train acc:  0.8515625
train loss:  0.3519306778907776
train gradient:  0.13667120958267284
iteration : 13181
train acc:  0.8828125
train loss:  0.2964329123497009
train gradient:  0.14445443217970597
iteration : 13182
train acc:  0.8671875
train loss:  0.322886198759079
train gradient:  0.13499555790677248
iteration : 13183
train acc:  0.84375
train loss:  0.3462625741958618
train gradient:  0.1529744321887357
iteration : 13184
train acc:  0.796875
train loss:  0.45858001708984375
train gradient:  0.2594486253725153
iteration : 13185
train acc:  0.875
train loss:  0.2987475097179413
train gradient:  0.14642024623821623
iteration : 13186
train acc:  0.9296875
train loss:  0.22537600994110107
train gradient:  0.06442365582972691
iteration : 13187
train acc:  0.8203125
train loss:  0.39179226756095886
train gradient:  0.1781344164951602
iteration : 13188
train acc:  0.8515625
train loss:  0.2995564341545105
train gradient:  0.13160291019817572
iteration : 13189
train acc:  0.921875
train loss:  0.2585127353668213
train gradient:  0.097330746101883
iteration : 13190
train acc:  0.8671875
train loss:  0.32897520065307617
train gradient:  0.1378477611549363
iteration : 13191
train acc:  0.8359375
train loss:  0.3476773500442505
train gradient:  0.17890257753847977
iteration : 13192
train acc:  0.8671875
train loss:  0.30088746547698975
train gradient:  0.1390024478284539
iteration : 13193
train acc:  0.8828125
train loss:  0.2649582028388977
train gradient:  0.09027458146071811
iteration : 13194
train acc:  0.9140625
train loss:  0.26200491189956665
train gradient:  0.14178556595462663
iteration : 13195
train acc:  0.921875
train loss:  0.2593458890914917
train gradient:  0.11739587153049622
iteration : 13196
train acc:  0.8828125
train loss:  0.28510066866874695
train gradient:  0.09240298824525414
iteration : 13197
train acc:  0.859375
train loss:  0.3266013264656067
train gradient:  0.12690506368687293
iteration : 13198
train acc:  0.890625
train loss:  0.28447413444519043
train gradient:  0.1228790373455827
iteration : 13199
train acc:  0.8984375
train loss:  0.2816935181617737
train gradient:  0.12186172554717162
iteration : 13200
train acc:  0.875
train loss:  0.34914034605026245
train gradient:  0.1365071640867219
iteration : 13201
train acc:  0.8671875
train loss:  0.31448549032211304
train gradient:  0.13874844262823444
iteration : 13202
train acc:  0.890625
train loss:  0.34014415740966797
train gradient:  0.14406726630204678
iteration : 13203
train acc:  0.8828125
train loss:  0.2816508412361145
train gradient:  0.15157243750530283
iteration : 13204
train acc:  0.8359375
train loss:  0.29762279987335205
train gradient:  0.09763500233967697
iteration : 13205
train acc:  0.8515625
train loss:  0.30754488706588745
train gradient:  0.15771954754766793
iteration : 13206
train acc:  0.8828125
train loss:  0.2886679172515869
train gradient:  0.10072296000639423
iteration : 13207
train acc:  0.875
train loss:  0.31561213731765747
train gradient:  0.12199965937164706
iteration : 13208
train acc:  0.8203125
train loss:  0.32753992080688477
train gradient:  0.15716449865267174
iteration : 13209
train acc:  0.859375
train loss:  0.27743399143218994
train gradient:  0.08752453060385307
iteration : 13210
train acc:  0.8515625
train loss:  0.28284451365470886
train gradient:  0.15627576461186798
iteration : 13211
train acc:  0.84375
train loss:  0.31659379601478577
train gradient:  0.1802085148201493
iteration : 13212
train acc:  0.796875
train loss:  0.3742053508758545
train gradient:  0.13433939592314123
iteration : 13213
train acc:  0.8984375
train loss:  0.27629953622817993
train gradient:  0.09568682054203749
iteration : 13214
train acc:  0.8828125
train loss:  0.3524066209793091
train gradient:  0.1576064986779187
iteration : 13215
train acc:  0.875
train loss:  0.30106544494628906
train gradient:  0.14889929314541217
iteration : 13216
train acc:  0.8203125
train loss:  0.3819328844547272
train gradient:  0.16791920684785683
iteration : 13217
train acc:  0.828125
train loss:  0.3443455696105957
train gradient:  0.15954607689498176
iteration : 13218
train acc:  0.9296875
train loss:  0.2524404525756836
train gradient:  0.14342865251102838
iteration : 13219
train acc:  0.9296875
train loss:  0.24565044045448303
train gradient:  0.08527815769663855
iteration : 13220
train acc:  0.84375
train loss:  0.3992638885974884
train gradient:  0.1822725687429273
iteration : 13221
train acc:  0.84375
train loss:  0.3375067710876465
train gradient:  0.10569185620225834
iteration : 13222
train acc:  0.859375
train loss:  0.34991925954818726
train gradient:  0.12468813795501651
iteration : 13223
train acc:  0.9140625
train loss:  0.2054939866065979
train gradient:  0.0760667688051899
iteration : 13224
train acc:  0.90625
train loss:  0.23218131065368652
train gradient:  0.1410491941996651
iteration : 13225
train acc:  0.84375
train loss:  0.3642781376838684
train gradient:  0.23181762084970564
iteration : 13226
train acc:  0.828125
train loss:  0.3897286653518677
train gradient:  0.14134846686878977
iteration : 13227
train acc:  0.90625
train loss:  0.3097372055053711
train gradient:  0.09151471142807897
iteration : 13228
train acc:  0.8984375
train loss:  0.2558683156967163
train gradient:  0.1017034606499817
iteration : 13229
train acc:  0.8203125
train loss:  0.37978070974349976
train gradient:  0.16537133609457952
iteration : 13230
train acc:  0.875
train loss:  0.30305370688438416
train gradient:  0.11475399345532804
iteration : 13231
train acc:  0.859375
train loss:  0.3605070412158966
train gradient:  0.14445458714254392
iteration : 13232
train acc:  0.84375
train loss:  0.31890469789505005
train gradient:  0.16911782760451483
iteration : 13233
train acc:  0.859375
train loss:  0.31295260787010193
train gradient:  0.09608706858323594
iteration : 13234
train acc:  0.8515625
train loss:  0.35174134373664856
train gradient:  0.12284486898934419
iteration : 13235
train acc:  0.8515625
train loss:  0.3241698741912842
train gradient:  0.1631723718303258
iteration : 13236
train acc:  0.8046875
train loss:  0.3364245891571045
train gradient:  0.22165561856085964
iteration : 13237
train acc:  0.8203125
train loss:  0.4234459400177002
train gradient:  0.2397604758407908
iteration : 13238
train acc:  0.8828125
train loss:  0.2980027198791504
train gradient:  0.10288723661620333
iteration : 13239
train acc:  0.828125
train loss:  0.3636054992675781
train gradient:  0.18648731375491398
iteration : 13240
train acc:  0.8671875
train loss:  0.2789805829524994
train gradient:  0.11517215214172385
iteration : 13241
train acc:  0.8515625
train loss:  0.3184834420681
train gradient:  0.15405172551620624
iteration : 13242
train acc:  0.8203125
train loss:  0.32752397656440735
train gradient:  0.12275233037625712
iteration : 13243
train acc:  0.8828125
train loss:  0.32782626152038574
train gradient:  0.24505901191035612
iteration : 13244
train acc:  0.890625
train loss:  0.2874639630317688
train gradient:  0.14926941086595613
iteration : 13245
train acc:  0.84375
train loss:  0.32277825474739075
train gradient:  0.14057461743328398
iteration : 13246
train acc:  0.890625
train loss:  0.26196587085723877
train gradient:  0.08990455434224073
iteration : 13247
train acc:  0.84375
train loss:  0.3173106610774994
train gradient:  0.110490179837136
iteration : 13248
train acc:  0.8671875
train loss:  0.2960478663444519
train gradient:  0.08180270741062123
iteration : 13249
train acc:  0.84375
train loss:  0.3529849052429199
train gradient:  0.1809216016198389
iteration : 13250
train acc:  0.90625
train loss:  0.2618626356124878
train gradient:  0.10868297153534721
iteration : 13251
train acc:  0.84375
train loss:  0.33608880639076233
train gradient:  0.12602750765515003
iteration : 13252
train acc:  0.8359375
train loss:  0.39485037326812744
train gradient:  0.22288696747158399
iteration : 13253
train acc:  0.875
train loss:  0.2819821536540985
train gradient:  0.19956089710558744
iteration : 13254
train acc:  0.78125
train loss:  0.48427289724349976
train gradient:  0.2580860623118213
iteration : 13255
train acc:  0.8359375
train loss:  0.3524574041366577
train gradient:  0.2008408109012388
iteration : 13256
train acc:  0.7890625
train loss:  0.3786146640777588
train gradient:  0.18308045404955825
iteration : 13257
train acc:  0.890625
train loss:  0.3345317542552948
train gradient:  0.15151519717703127
iteration : 13258
train acc:  0.8671875
train loss:  0.33519160747528076
train gradient:  0.1538687655800937
iteration : 13259
train acc:  0.890625
train loss:  0.25521212816238403
train gradient:  0.07402368809534438
iteration : 13260
train acc:  0.859375
train loss:  0.292648583650589
train gradient:  0.16084380267602016
iteration : 13261
train acc:  0.8359375
train loss:  0.32873257994651794
train gradient:  0.1616178767763084
iteration : 13262
train acc:  0.875
train loss:  0.2828543186187744
train gradient:  0.1388717783531872
iteration : 13263
train acc:  0.84375
train loss:  0.3067309856414795
train gradient:  0.24514205168182363
iteration : 13264
train acc:  0.890625
train loss:  0.2545125484466553
train gradient:  0.08403915268429021
iteration : 13265
train acc:  0.90625
train loss:  0.2708062529563904
train gradient:  0.10655664315080565
iteration : 13266
train acc:  0.90625
train loss:  0.2755664885044098
train gradient:  0.1579187989837249
iteration : 13267
train acc:  0.8359375
train loss:  0.28792300820350647
train gradient:  0.07969742747733695
iteration : 13268
train acc:  0.8671875
train loss:  0.28084367513656616
train gradient:  0.0993106756542555
iteration : 13269
train acc:  0.8046875
train loss:  0.3971705734729767
train gradient:  0.1910686878556006
iteration : 13270
train acc:  0.765625
train loss:  0.4254904091358185
train gradient:  0.17349824256803226
iteration : 13271
train acc:  0.859375
train loss:  0.32525187730789185
train gradient:  0.17339009322444798
iteration : 13272
train acc:  0.90625
train loss:  0.2548987865447998
train gradient:  0.10308839279037978
iteration : 13273
train acc:  0.890625
train loss:  0.270329087972641
train gradient:  0.09315090738994443
iteration : 13274
train acc:  0.921875
train loss:  0.2237890064716339
train gradient:  0.0870739773246951
iteration : 13275
train acc:  0.890625
train loss:  0.25983619689941406
train gradient:  0.09756460360539423
iteration : 13276
train acc:  0.8984375
train loss:  0.3091888427734375
train gradient:  0.10345621891469386
iteration : 13277
train acc:  0.84375
train loss:  0.3313233256340027
train gradient:  0.10396897704510726
iteration : 13278
train acc:  0.8359375
train loss:  0.31015297770500183
train gradient:  0.14190554968346938
iteration : 13279
train acc:  0.8671875
train loss:  0.3326212763786316
train gradient:  0.13131079422150418
iteration : 13280
train acc:  0.8828125
train loss:  0.31458088755607605
train gradient:  0.10461508827035086
iteration : 13281
train acc:  0.8125
train loss:  0.38721001148223877
train gradient:  0.18360259330799816
iteration : 13282
train acc:  0.890625
train loss:  0.3099568486213684
train gradient:  0.14006167354180835
iteration : 13283
train acc:  0.8359375
train loss:  0.3081267178058624
train gradient:  0.13226815057464036
iteration : 13284
train acc:  0.8984375
train loss:  0.2572636604309082
train gradient:  0.10864401771143882
iteration : 13285
train acc:  0.8046875
train loss:  0.41271156072616577
train gradient:  0.16327044602796414
iteration : 13286
train acc:  0.9140625
train loss:  0.22406664490699768
train gradient:  0.07092582677884894
iteration : 13287
train acc:  0.8515625
train loss:  0.3239084482192993
train gradient:  0.13985318436439115
iteration : 13288
train acc:  0.8671875
train loss:  0.3340967893600464
train gradient:  0.14334791610964256
iteration : 13289
train acc:  0.875
train loss:  0.2843678593635559
train gradient:  0.0733196223354077
iteration : 13290
train acc:  0.8671875
train loss:  0.33980226516723633
train gradient:  0.2215399314478692
iteration : 13291
train acc:  0.875
train loss:  0.29230836033821106
train gradient:  0.1728336402955654
iteration : 13292
train acc:  0.828125
train loss:  0.35294175148010254
train gradient:  0.1902186784309518
iteration : 13293
train acc:  0.859375
train loss:  0.34878629446029663
train gradient:  0.12073834461249815
iteration : 13294
train acc:  0.8828125
train loss:  0.3006967306137085
train gradient:  0.11351005108855013
iteration : 13295
train acc:  0.875
train loss:  0.2918907403945923
train gradient:  0.12842415063349177
iteration : 13296
train acc:  0.8515625
train loss:  0.37683236598968506
train gradient:  0.1258375627682681
iteration : 13297
train acc:  0.8359375
train loss:  0.39858517050743103
train gradient:  0.19154158218588735
iteration : 13298
train acc:  0.8125
train loss:  0.3773661255836487
train gradient:  0.2021452133877956
iteration : 13299
train acc:  0.828125
train loss:  0.31290730834007263
train gradient:  0.18524569068259739
iteration : 13300
train acc:  0.90625
train loss:  0.2190741002559662
train gradient:  0.0777449364966947
iteration : 13301
train acc:  0.875
train loss:  0.30653148889541626
train gradient:  0.1465318819488856
iteration : 13302
train acc:  0.859375
train loss:  0.33835479617118835
train gradient:  0.13490515662474006
iteration : 13303
train acc:  0.796875
train loss:  0.4270365238189697
train gradient:  0.2573631593218123
iteration : 13304
train acc:  0.8515625
train loss:  0.2711556553840637
train gradient:  0.11830514264465702
iteration : 13305
train acc:  0.890625
train loss:  0.2692522406578064
train gradient:  0.09354049897380558
iteration : 13306
train acc:  0.875
train loss:  0.30184128880500793
train gradient:  0.13057222390290676
iteration : 13307
train acc:  0.8671875
train loss:  0.3798375129699707
train gradient:  0.19157960261945173
iteration : 13308
train acc:  0.90625
train loss:  0.2778998613357544
train gradient:  0.11794587151669825
iteration : 13309
train acc:  0.8203125
train loss:  0.4415934681892395
train gradient:  0.3568295389561816
iteration : 13310
train acc:  0.84375
train loss:  0.3476409912109375
train gradient:  0.13961076492751764
iteration : 13311
train acc:  0.8828125
train loss:  0.2865736484527588
train gradient:  0.09344917483807988
iteration : 13312
train acc:  0.890625
train loss:  0.27561017870903015
train gradient:  0.12660611878722952
iteration : 13313
train acc:  0.8359375
train loss:  0.3406488299369812
train gradient:  0.21080426725209755
iteration : 13314
train acc:  0.828125
train loss:  0.39192044734954834
train gradient:  0.20399627171460366
iteration : 13315
train acc:  0.84375
train loss:  0.32070744037628174
train gradient:  0.13010123331550394
iteration : 13316
train acc:  0.828125
train loss:  0.4225206971168518
train gradient:  0.23571981744174675
iteration : 13317
train acc:  0.8984375
train loss:  0.2956983745098114
train gradient:  0.14775411864119772
iteration : 13318
train acc:  0.875
train loss:  0.26871800422668457
train gradient:  0.09972747035177003
iteration : 13319
train acc:  0.859375
train loss:  0.31421297788619995
train gradient:  0.1789211189337278
iteration : 13320
train acc:  0.828125
train loss:  0.34813162684440613
train gradient:  0.1947882184848547
iteration : 13321
train acc:  0.8828125
train loss:  0.32947707176208496
train gradient:  0.1272066060974746
iteration : 13322
train acc:  0.8515625
train loss:  0.30268940329551697
train gradient:  0.10739998564986541
iteration : 13323
train acc:  0.8359375
train loss:  0.40032094717025757
train gradient:  0.16619485953329277
iteration : 13324
train acc:  0.875
train loss:  0.28406667709350586
train gradient:  0.13711525176155867
iteration : 13325
train acc:  0.859375
train loss:  0.3017011880874634
train gradient:  0.15546163510374414
iteration : 13326
train acc:  0.859375
train loss:  0.3248935341835022
train gradient:  0.1886383735070526
iteration : 13327
train acc:  0.8828125
train loss:  0.31214016675949097
train gradient:  0.1173557430564276
iteration : 13328
train acc:  0.890625
train loss:  0.261204332113266
train gradient:  0.09278330789626582
iteration : 13329
train acc:  0.8828125
train loss:  0.2945546805858612
train gradient:  0.12023578123919353
iteration : 13330
train acc:  0.8359375
train loss:  0.39706844091415405
train gradient:  0.17165601299749836
iteration : 13331
train acc:  0.78125
train loss:  0.399067759513855
train gradient:  0.21330857175765608
iteration : 13332
train acc:  0.859375
train loss:  0.34821832180023193
train gradient:  0.1047940898072341
iteration : 13333
train acc:  0.890625
train loss:  0.32107317447662354
train gradient:  0.1143456624574992
iteration : 13334
train acc:  0.890625
train loss:  0.2900334298610687
train gradient:  0.15447236146262555
iteration : 13335
train acc:  0.8515625
train loss:  0.3513752818107605
train gradient:  0.1323811172650428
iteration : 13336
train acc:  0.859375
train loss:  0.3524925410747528
train gradient:  0.21097074561893528
iteration : 13337
train acc:  0.8671875
train loss:  0.3418535590171814
train gradient:  0.16325846416709455
iteration : 13338
train acc:  0.875
train loss:  0.31999218463897705
train gradient:  0.11840085253530329
iteration : 13339
train acc:  0.828125
train loss:  0.3596838712692261
train gradient:  0.15753024596139303
iteration : 13340
train acc:  0.8515625
train loss:  0.47356998920440674
train gradient:  0.3133103743429143
iteration : 13341
train acc:  0.859375
train loss:  0.3198777735233307
train gradient:  0.10931889087193844
iteration : 13342
train acc:  0.8828125
train loss:  0.3072451949119568
train gradient:  0.11034284628723992
iteration : 13343
train acc:  0.84375
train loss:  0.372994601726532
train gradient:  0.1481626094551103
iteration : 13344
train acc:  0.890625
train loss:  0.26005423069000244
train gradient:  0.09033965079513245
iteration : 13345
train acc:  0.8359375
train loss:  0.3047329783439636
train gradient:  0.11938770987020271
iteration : 13346
train acc:  0.8515625
train loss:  0.33416804671287537
train gradient:  0.13653680015156827
iteration : 13347
train acc:  0.8984375
train loss:  0.2991119623184204
train gradient:  0.13896849810425677
iteration : 13348
train acc:  0.890625
train loss:  0.2910499572753906
train gradient:  0.09566600597089794
iteration : 13349
train acc:  0.84375
train loss:  0.309714674949646
train gradient:  0.13594208537407218
iteration : 13350
train acc:  0.828125
train loss:  0.34417057037353516
train gradient:  0.11848818387490906
iteration : 13351
train acc:  0.8984375
train loss:  0.2735283076763153
train gradient:  0.09162186597367894
iteration : 13352
train acc:  0.859375
train loss:  0.2898271977901459
train gradient:  0.07974927439536807
iteration : 13353
train acc:  0.859375
train loss:  0.31782281398773193
train gradient:  0.14440688689444664
iteration : 13354
train acc:  0.859375
train loss:  0.3234620690345764
train gradient:  0.13283014491901962
iteration : 13355
train acc:  0.8125
train loss:  0.37514185905456543
train gradient:  0.18264444156568788
iteration : 13356
train acc:  0.8515625
train loss:  0.3304107189178467
train gradient:  0.20067620564098498
iteration : 13357
train acc:  0.859375
train loss:  0.3116611838340759
train gradient:  0.10952061558812559
iteration : 13358
train acc:  0.8515625
train loss:  0.3339202105998993
train gradient:  0.22851984266377662
iteration : 13359
train acc:  0.84375
train loss:  0.4046167731285095
train gradient:  0.2224967399608349
iteration : 13360
train acc:  0.8828125
train loss:  0.3477593660354614
train gradient:  0.16795253904939045
iteration : 13361
train acc:  0.8671875
train loss:  0.3414691090583801
train gradient:  0.08727450877940059
iteration : 13362
train acc:  0.84375
train loss:  0.33271849155426025
train gradient:  0.13669238564849143
iteration : 13363
train acc:  0.859375
train loss:  0.37549686431884766
train gradient:  0.2042470722655879
iteration : 13364
train acc:  0.90625
train loss:  0.2454044073820114
train gradient:  0.0740557130001365
iteration : 13365
train acc:  0.90625
train loss:  0.23751908540725708
train gradient:  0.09370383241545899
iteration : 13366
train acc:  0.8125
train loss:  0.3378165364265442
train gradient:  0.14475603064669518
iteration : 13367
train acc:  0.828125
train loss:  0.3733346462249756
train gradient:  0.16171192119212252
iteration : 13368
train acc:  0.875
train loss:  0.35668495297431946
train gradient:  0.19266119139134918
iteration : 13369
train acc:  0.875
train loss:  0.2873288691043854
train gradient:  0.1413150503152063
iteration : 13370
train acc:  0.8203125
train loss:  0.4381762146949768
train gradient:  0.17671979774054514
iteration : 13371
train acc:  0.8515625
train loss:  0.29438963532447815
train gradient:  0.11897235374331053
iteration : 13372
train acc:  0.8671875
train loss:  0.34850049018859863
train gradient:  0.150793822858924
iteration : 13373
train acc:  0.859375
train loss:  0.3109535872936249
train gradient:  0.11889750194942562
iteration : 13374
train acc:  0.84375
train loss:  0.3692896068096161
train gradient:  0.16831887942712612
iteration : 13375
train acc:  0.8359375
train loss:  0.3337561786174774
train gradient:  0.11909932523443056
iteration : 13376
train acc:  0.8359375
train loss:  0.34722405672073364
train gradient:  0.1758559560305119
iteration : 13377
train acc:  0.8671875
train loss:  0.3013673424720764
train gradient:  0.11937095789171004
iteration : 13378
train acc:  0.875
train loss:  0.292462557554245
train gradient:  0.1242850795413539
iteration : 13379
train acc:  0.890625
train loss:  0.2714708745479584
train gradient:  0.1313844117844036
iteration : 13380
train acc:  0.9140625
train loss:  0.31606051325798035
train gradient:  0.13108497338205538
iteration : 13381
train acc:  0.875
train loss:  0.27709048986434937
train gradient:  0.08976594794640437
iteration : 13382
train acc:  0.859375
train loss:  0.3099815249443054
train gradient:  0.14422797014193417
iteration : 13383
train acc:  0.875
train loss:  0.32910430431365967
train gradient:  0.1341166443121245
iteration : 13384
train acc:  0.8359375
train loss:  0.38172852993011475
train gradient:  0.1809324663031999
iteration : 13385
train acc:  0.8671875
train loss:  0.353241503238678
train gradient:  0.17690249251453072
iteration : 13386
train acc:  0.7890625
train loss:  0.45722150802612305
train gradient:  0.217798376714387
iteration : 13387
train acc:  0.890625
train loss:  0.3245668411254883
train gradient:  0.12008681747284226
iteration : 13388
train acc:  0.8203125
train loss:  0.36421290040016174
train gradient:  0.1804593317210474
iteration : 13389
train acc:  0.9140625
train loss:  0.28356456756591797
train gradient:  0.12558300184693072
iteration : 13390
train acc:  0.859375
train loss:  0.2950471341609955
train gradient:  0.11883332398644708
iteration : 13391
train acc:  0.875
train loss:  0.31770089268684387
train gradient:  0.1310064305162432
iteration : 13392
train acc:  0.8359375
train loss:  0.3611312508583069
train gradient:  0.1554776390440083
iteration : 13393
train acc:  0.8203125
train loss:  0.3826374411582947
train gradient:  0.22790779318581014
iteration : 13394
train acc:  0.8515625
train loss:  0.29900503158569336
train gradient:  0.12823994899403612
iteration : 13395
train acc:  0.8515625
train loss:  0.3134309649467468
train gradient:  0.14407153728818897
iteration : 13396
train acc:  0.890625
train loss:  0.28579455614089966
train gradient:  0.10067172154428161
iteration : 13397
train acc:  0.8671875
train loss:  0.3113434910774231
train gradient:  0.11577373522232287
iteration : 13398
train acc:  0.9296875
train loss:  0.22609037160873413
train gradient:  0.08776990245124197
iteration : 13399
train acc:  0.8828125
train loss:  0.2842676341533661
train gradient:  0.11540701875911495
iteration : 13400
train acc:  0.828125
train loss:  0.40405264496803284
train gradient:  0.19410753580736095
iteration : 13401
train acc:  0.875
train loss:  0.2766597867012024
train gradient:  0.11574744431753768
iteration : 13402
train acc:  0.8359375
train loss:  0.3178698420524597
train gradient:  0.12178132861148733
iteration : 13403
train acc:  0.890625
train loss:  0.3265896439552307
train gradient:  0.16199879544586854
iteration : 13404
train acc:  0.8359375
train loss:  0.3954921066761017
train gradient:  0.1320942983277316
iteration : 13405
train acc:  0.8828125
train loss:  0.33978092670440674
train gradient:  0.1736071928954191
iteration : 13406
train acc:  0.8828125
train loss:  0.26327162981033325
train gradient:  0.08671481407182455
iteration : 13407
train acc:  0.8203125
train loss:  0.3546372056007385
train gradient:  0.1784039699539713
iteration : 13408
train acc:  0.8984375
train loss:  0.29546546936035156
train gradient:  0.11677337750660098
iteration : 13409
train acc:  0.8671875
train loss:  0.32129746675491333
train gradient:  0.09945355612075753
iteration : 13410
train acc:  0.8359375
train loss:  0.3695218563079834
train gradient:  0.176442622180895
iteration : 13411
train acc:  0.796875
train loss:  0.41379979252815247
train gradient:  0.19994312345894955
iteration : 13412
train acc:  0.859375
train loss:  0.3325880467891693
train gradient:  0.12776447378045674
iteration : 13413
train acc:  0.90625
train loss:  0.2833605408668518
train gradient:  0.1833156173078492
iteration : 13414
train acc:  0.890625
train loss:  0.22256138920783997
train gradient:  0.10593511168196644
iteration : 13415
train acc:  0.84375
train loss:  0.3136160671710968
train gradient:  0.13578328812098583
iteration : 13416
train acc:  0.859375
train loss:  0.31814268231391907
train gradient:  0.11723136021233332
iteration : 13417
train acc:  0.8984375
train loss:  0.26612144708633423
train gradient:  0.09575209934914487
iteration : 13418
train acc:  0.890625
train loss:  0.29304492473602295
train gradient:  0.11311415084849122
iteration : 13419
train acc:  0.8671875
train loss:  0.34385910630226135
train gradient:  0.13547438984472104
iteration : 13420
train acc:  0.8515625
train loss:  0.29779428243637085
train gradient:  0.11659646826797095
iteration : 13421
train acc:  0.8359375
train loss:  0.36319413781166077
train gradient:  0.15622608182893913
iteration : 13422
train acc:  0.8671875
train loss:  0.3175424337387085
train gradient:  0.13860892668706026
iteration : 13423
train acc:  0.859375
train loss:  0.3160203695297241
train gradient:  0.10158409745036022
iteration : 13424
train acc:  0.875
train loss:  0.2602543532848358
train gradient:  0.10577674432357036
iteration : 13425
train acc:  0.8984375
train loss:  0.25659507513046265
train gradient:  0.16125235572630536
iteration : 13426
train acc:  0.828125
train loss:  0.37836891412734985
train gradient:  0.20000303260273167
iteration : 13427
train acc:  0.8359375
train loss:  0.3644733130931854
train gradient:  0.1788406137849127
iteration : 13428
train acc:  0.8515625
train loss:  0.31697043776512146
train gradient:  0.19367021383195862
iteration : 13429
train acc:  0.9140625
train loss:  0.2521955668926239
train gradient:  0.1477222140457415
iteration : 13430
train acc:  0.8671875
train loss:  0.2970403730869293
train gradient:  0.13132587049735844
iteration : 13431
train acc:  0.8671875
train loss:  0.30537092685699463
train gradient:  0.12097379934745332
iteration : 13432
train acc:  0.7890625
train loss:  0.43665027618408203
train gradient:  0.30545410858592154
iteration : 13433
train acc:  0.8515625
train loss:  0.3302035927772522
train gradient:  0.12707998639383852
iteration : 13434
train acc:  0.8515625
train loss:  0.3337864577770233
train gradient:  0.14653168410502815
iteration : 13435
train acc:  0.875
train loss:  0.2994442582130432
train gradient:  0.11073333410404596
iteration : 13436
train acc:  0.8125
train loss:  0.4252334535121918
train gradient:  0.2179636842999304
iteration : 13437
train acc:  0.90625
train loss:  0.2610012888908386
train gradient:  0.07950373689384724
iteration : 13438
train acc:  0.859375
train loss:  0.2815351188182831
train gradient:  0.07827957881209324
iteration : 13439
train acc:  0.8671875
train loss:  0.39653682708740234
train gradient:  0.16783409781714584
iteration : 13440
train acc:  0.875
train loss:  0.2896343469619751
train gradient:  0.07221669690851353
iteration : 13441
train acc:  0.8359375
train loss:  0.303383469581604
train gradient:  0.12095362358887622
iteration : 13442
train acc:  0.828125
train loss:  0.317471444606781
train gradient:  0.1456018319948859
iteration : 13443
train acc:  0.859375
train loss:  0.3275759816169739
train gradient:  0.16684199607069888
iteration : 13444
train acc:  0.8359375
train loss:  0.37048593163490295
train gradient:  0.24904113686383061
iteration : 13445
train acc:  0.8515625
train loss:  0.31329113245010376
train gradient:  0.08867024019057025
iteration : 13446
train acc:  0.8984375
train loss:  0.30654340982437134
train gradient:  0.10941368563398908
iteration : 13447
train acc:  0.8515625
train loss:  0.32259130477905273
train gradient:  0.11256096429720894
iteration : 13448
train acc:  0.8515625
train loss:  0.2911054491996765
train gradient:  0.1052335766875456
iteration : 13449
train acc:  0.921875
train loss:  0.22667598724365234
train gradient:  0.0897487740728004
iteration : 13450
train acc:  0.859375
train loss:  0.33714044094085693
train gradient:  0.1614650931335761
iteration : 13451
train acc:  0.8828125
train loss:  0.3371936082839966
train gradient:  0.12517070003007222
iteration : 13452
train acc:  0.8984375
train loss:  0.2429930865764618
train gradient:  0.08889203314421998
iteration : 13453
train acc:  0.859375
train loss:  0.3653292655944824
train gradient:  0.14360710877306715
iteration : 13454
train acc:  0.84375
train loss:  0.34282374382019043
train gradient:  0.14217450484843713
iteration : 13455
train acc:  0.890625
train loss:  0.2823335826396942
train gradient:  0.07495281288070542
iteration : 13456
train acc:  0.8828125
train loss:  0.29444077610969543
train gradient:  0.11826930292740333
iteration : 13457
train acc:  0.8828125
train loss:  0.2876613736152649
train gradient:  0.08434172180301094
iteration : 13458
train acc:  0.8359375
train loss:  0.3504527807235718
train gradient:  0.17679741261388798
iteration : 13459
train acc:  0.8515625
train loss:  0.34635746479034424
train gradient:  0.1753027088986025
iteration : 13460
train acc:  0.828125
train loss:  0.3349129557609558
train gradient:  0.1897615563482617
iteration : 13461
train acc:  0.765625
train loss:  0.41674286127090454
train gradient:  0.240914832103662
iteration : 13462
train acc:  0.8671875
train loss:  0.28933244943618774
train gradient:  0.0946224179044028
iteration : 13463
train acc:  0.8671875
train loss:  0.3072866201400757
train gradient:  0.18397378957663785
iteration : 13464
train acc:  0.890625
train loss:  0.2831127345561981
train gradient:  0.08210775501779081
iteration : 13465
train acc:  0.8984375
train loss:  0.2829989194869995
train gradient:  0.15619832439917206
iteration : 13466
train acc:  0.90625
train loss:  0.2348441481590271
train gradient:  0.06550192826841264
iteration : 13467
train acc:  0.9296875
train loss:  0.18436214327812195
train gradient:  0.053133383729127
iteration : 13468
train acc:  0.8671875
train loss:  0.33111682534217834
train gradient:  0.18395742721333605
iteration : 13469
train acc:  0.8515625
train loss:  0.3352872431278229
train gradient:  0.1532374851734109
iteration : 13470
train acc:  0.890625
train loss:  0.27797389030456543
train gradient:  0.11105991445222076
iteration : 13471
train acc:  0.859375
train loss:  0.29305508732795715
train gradient:  0.1252325249942948
iteration : 13472
train acc:  0.8671875
train loss:  0.2979893088340759
train gradient:  0.12166754958033182
iteration : 13473
train acc:  0.84375
train loss:  0.3158596158027649
train gradient:  0.13494610991306394
iteration : 13474
train acc:  0.890625
train loss:  0.2803047001361847
train gradient:  0.12073350483496718
iteration : 13475
train acc:  0.828125
train loss:  0.3997184634208679
train gradient:  0.14424599863049753
iteration : 13476
train acc:  0.890625
train loss:  0.25057369470596313
train gradient:  0.07742538408186767
iteration : 13477
train acc:  0.875
train loss:  0.26418155431747437
train gradient:  0.10108324905436077
iteration : 13478
train acc:  0.828125
train loss:  0.31887978315353394
train gradient:  0.19139253410640772
iteration : 13479
train acc:  0.8359375
train loss:  0.33544671535491943
train gradient:  0.17641666131799585
iteration : 13480
train acc:  0.8984375
train loss:  0.25939321517944336
train gradient:  0.11198204309223111
iteration : 13481
train acc:  0.875
train loss:  0.2752828001976013
train gradient:  0.11243705842453586
iteration : 13482
train acc:  0.875
train loss:  0.27302128076553345
train gradient:  0.08744208743797612
iteration : 13483
train acc:  0.875
train loss:  0.28506219387054443
train gradient:  0.09419814447996666
iteration : 13484
train acc:  0.8515625
train loss:  0.3595358729362488
train gradient:  0.13425394818899603
iteration : 13485
train acc:  0.8203125
train loss:  0.3660874366760254
train gradient:  0.16691182015111247
iteration : 13486
train acc:  0.890625
train loss:  0.35130345821380615
train gradient:  0.15754261711416573
iteration : 13487
train acc:  0.8359375
train loss:  0.3346331715583801
train gradient:  0.13585293411984592
iteration : 13488
train acc:  0.8671875
train loss:  0.28598129749298096
train gradient:  0.10175311088335937
iteration : 13489
train acc:  0.875
train loss:  0.30202731490135193
train gradient:  0.09094263225771418
iteration : 13490
train acc:  0.875
train loss:  0.29407799243927
train gradient:  0.10428641456613437
iteration : 13491
train acc:  0.8359375
train loss:  0.3573249578475952
train gradient:  0.22081617686480387
iteration : 13492
train acc:  0.875
train loss:  0.29538413882255554
train gradient:  0.13434545395897562
iteration : 13493
train acc:  0.9140625
train loss:  0.24671199917793274
train gradient:  0.08986995746084762
iteration : 13494
train acc:  0.8359375
train loss:  0.2986162006855011
train gradient:  0.1397287551112918
iteration : 13495
train acc:  0.8203125
train loss:  0.365186870098114
train gradient:  0.17964221653766002
iteration : 13496
train acc:  0.859375
train loss:  0.3357539772987366
train gradient:  0.17067214848046575
iteration : 13497
train acc:  0.8671875
train loss:  0.29376429319381714
train gradient:  0.11717513331739396
iteration : 13498
train acc:  0.8359375
train loss:  0.3413870334625244
train gradient:  0.13390108717956856
iteration : 13499
train acc:  0.875
train loss:  0.2662472724914551
train gradient:  0.12498735548472573
iteration : 13500
train acc:  0.90625
train loss:  0.2671051323413849
train gradient:  0.10206534558918953
iteration : 13501
train acc:  0.8515625
train loss:  0.3190849721431732
train gradient:  0.10418971955126698
iteration : 13502
train acc:  0.9140625
train loss:  0.23107731342315674
train gradient:  0.10144909651437163
iteration : 13503
train acc:  0.8671875
train loss:  0.3114287257194519
train gradient:  0.18578098565238113
iteration : 13504
train acc:  0.84375
train loss:  0.30988720059394836
train gradient:  0.1738365753095376
iteration : 13505
train acc:  0.921875
train loss:  0.24998076260089874
train gradient:  0.1069599992895956
iteration : 13506
train acc:  0.84375
train loss:  0.31833916902542114
train gradient:  0.14416106209404494
iteration : 13507
train acc:  0.8671875
train loss:  0.3324633836746216
train gradient:  0.15364068013338833
iteration : 13508
train acc:  0.8125
train loss:  0.4047453999519348
train gradient:  0.3204576631993732
iteration : 13509
train acc:  0.8828125
train loss:  0.2535134255886078
train gradient:  0.09175656336339937
iteration : 13510
train acc:  0.8359375
train loss:  0.3646863102912903
train gradient:  0.14792239873744262
iteration : 13511
train acc:  0.828125
train loss:  0.35600849986076355
train gradient:  0.19161116622619068
iteration : 13512
train acc:  0.9140625
train loss:  0.27141690254211426
train gradient:  0.11704621043737702
iteration : 13513
train acc:  0.90625
train loss:  0.2599203586578369
train gradient:  0.09934135068384445
iteration : 13514
train acc:  0.8671875
train loss:  0.3317578434944153
train gradient:  0.1287568640865023
iteration : 13515
train acc:  0.875
train loss:  0.28274086117744446
train gradient:  0.16209486336949525
iteration : 13516
train acc:  0.8671875
train loss:  0.3368498384952545
train gradient:  0.24325894113747454
iteration : 13517
train acc:  0.859375
train loss:  0.3362119197845459
train gradient:  0.14248093621526348
iteration : 13518
train acc:  0.875
train loss:  0.29130488634109497
train gradient:  0.13530765006916773
iteration : 13519
train acc:  0.8828125
train loss:  0.2619493901729584
train gradient:  0.1261078309739201
iteration : 13520
train acc:  0.8203125
train loss:  0.41065168380737305
train gradient:  0.22495760728387348
iteration : 13521
train acc:  0.8671875
train loss:  0.27761000394821167
train gradient:  0.09227375812838823
iteration : 13522
train acc:  0.8203125
train loss:  0.34631532430648804
train gradient:  0.18662101443698717
iteration : 13523
train acc:  0.828125
train loss:  0.3807717561721802
train gradient:  0.2650861871054303
iteration : 13524
train acc:  0.84375
train loss:  0.349520206451416
train gradient:  0.15215236940063398
iteration : 13525
train acc:  0.875
train loss:  0.323070228099823
train gradient:  0.13648366238977616
iteration : 13526
train acc:  0.8984375
train loss:  0.31170204281806946
train gradient:  0.12804873274586343
iteration : 13527
train acc:  0.8125
train loss:  0.43004804849624634
train gradient:  0.31531139154959187
iteration : 13528
train acc:  0.8828125
train loss:  0.26625195145606995
train gradient:  0.12658939456103313
iteration : 13529
train acc:  0.796875
train loss:  0.4360077977180481
train gradient:  0.25263573121718547
iteration : 13530
train acc:  0.8828125
train loss:  0.27890443801879883
train gradient:  0.13446110049319526
iteration : 13531
train acc:  0.921875
train loss:  0.27057334780693054
train gradient:  0.1147246836871143
iteration : 13532
train acc:  0.9140625
train loss:  0.3020861744880676
train gradient:  0.16592894613711506
iteration : 13533
train acc:  0.8828125
train loss:  0.29752448201179504
train gradient:  0.16287229267894093
iteration : 13534
train acc:  0.8828125
train loss:  0.2888490557670593
train gradient:  0.09346649251919328
iteration : 13535
train acc:  0.90625
train loss:  0.2430027276277542
train gradient:  0.10229229199920033
iteration : 13536
train acc:  0.8984375
train loss:  0.229408860206604
train gradient:  0.07826230019789779
iteration : 13537
train acc:  0.8828125
train loss:  0.3143206834793091
train gradient:  0.1061513746274072
iteration : 13538
train acc:  0.859375
train loss:  0.3300873935222626
train gradient:  0.13932182238608642
iteration : 13539
train acc:  0.8671875
train loss:  0.35821542143821716
train gradient:  0.1285827622656429
iteration : 13540
train acc:  0.8515625
train loss:  0.2724643349647522
train gradient:  0.09640758874771771
iteration : 13541
train acc:  0.828125
train loss:  0.38637012243270874
train gradient:  0.2173588639604253
iteration : 13542
train acc:  0.8515625
train loss:  0.2837437391281128
train gradient:  0.13810630446355687
iteration : 13543
train acc:  0.875
train loss:  0.2694847881793976
train gradient:  0.11271703630604551
iteration : 13544
train acc:  0.859375
train loss:  0.30845391750335693
train gradient:  0.19601101993714676
iteration : 13545
train acc:  0.8984375
train loss:  0.2469768226146698
train gradient:  0.11312594190255004
iteration : 13546
train acc:  0.8984375
train loss:  0.27741390466690063
train gradient:  0.13297295691393865
iteration : 13547
train acc:  0.8671875
train loss:  0.30126768350601196
train gradient:  0.12720258144672392
iteration : 13548
train acc:  0.8671875
train loss:  0.296225905418396
train gradient:  0.13947557402830985
iteration : 13549
train acc:  0.921875
train loss:  0.1922074407339096
train gradient:  0.07452175455265687
iteration : 13550
train acc:  0.8515625
train loss:  0.3152068257331848
train gradient:  0.14100822995308027
iteration : 13551
train acc:  0.8671875
train loss:  0.3459126353263855
train gradient:  0.12485555102110728
iteration : 13552
train acc:  0.890625
train loss:  0.2836962938308716
train gradient:  0.1409184533365262
iteration : 13553
train acc:  0.828125
train loss:  0.3891521990299225
train gradient:  0.20381856645194307
iteration : 13554
train acc:  0.8515625
train loss:  0.3398376405239105
train gradient:  0.14121100967785152
iteration : 13555
train acc:  0.859375
train loss:  0.3353980779647827
train gradient:  0.15777340906958695
iteration : 13556
train acc:  0.90625
train loss:  0.21424013376235962
train gradient:  0.1748240022396408
iteration : 13557
train acc:  0.8828125
train loss:  0.2951815724372864
train gradient:  0.13788274168523595
iteration : 13558
train acc:  0.8828125
train loss:  0.270649254322052
train gradient:  0.11355971111719519
iteration : 13559
train acc:  0.8125
train loss:  0.3925481140613556
train gradient:  0.26301218194841
iteration : 13560
train acc:  0.890625
train loss:  0.28455525636672974
train gradient:  0.11124558613662301
iteration : 13561
train acc:  0.90625
train loss:  0.25611230731010437
train gradient:  0.1250048153108473
iteration : 13562
train acc:  0.875
train loss:  0.33778515458106995
train gradient:  0.13272648462718195
iteration : 13563
train acc:  0.8828125
train loss:  0.3545970320701599
train gradient:  0.13794944336192708
iteration : 13564
train acc:  0.9140625
train loss:  0.2435397505760193
train gradient:  0.09451008009341032
iteration : 13565
train acc:  0.875
train loss:  0.27251213788986206
train gradient:  0.11871043469508857
iteration : 13566
train acc:  0.8515625
train loss:  0.29247161746025085
train gradient:  0.28059725303920824
iteration : 13567
train acc:  0.84375
train loss:  0.3604191541671753
train gradient:  0.1808530090535584
iteration : 13568
train acc:  0.8984375
train loss:  0.28692400455474854
train gradient:  0.10901042041725496
iteration : 13569
train acc:  0.890625
train loss:  0.27080851793289185
train gradient:  0.11370463764445025
iteration : 13570
train acc:  0.890625
train loss:  0.32002219557762146
train gradient:  0.10991604188092188
iteration : 13571
train acc:  0.828125
train loss:  0.3247712552547455
train gradient:  0.13857385378283377
iteration : 13572
train acc:  0.890625
train loss:  0.25929486751556396
train gradient:  0.11000690957648344
iteration : 13573
train acc:  0.890625
train loss:  0.24854013323783875
train gradient:  0.1385391282607631
iteration : 13574
train acc:  0.9296875
train loss:  0.28575819730758667
train gradient:  0.1941750746559946
iteration : 13575
train acc:  0.8515625
train loss:  0.2925460934638977
train gradient:  0.23229847001882262
iteration : 13576
train acc:  0.859375
train loss:  0.3465522229671478
train gradient:  0.15121193842458194
iteration : 13577
train acc:  0.8671875
train loss:  0.3359456956386566
train gradient:  0.21325764905439742
iteration : 13578
train acc:  0.8359375
train loss:  0.3306034803390503
train gradient:  0.16860585879450193
iteration : 13579
train acc:  0.84375
train loss:  0.40469270944595337
train gradient:  0.2998187322216227
iteration : 13580
train acc:  0.8515625
train loss:  0.3286324739456177
train gradient:  0.15922364540517264
iteration : 13581
train acc:  0.8203125
train loss:  0.3465832471847534
train gradient:  0.15523155662824012
iteration : 13582
train acc:  0.796875
train loss:  0.45468294620513916
train gradient:  0.23049218308396252
iteration : 13583
train acc:  0.8671875
train loss:  0.2972787022590637
train gradient:  0.09357127234074965
iteration : 13584
train acc:  0.8515625
train loss:  0.4321877062320709
train gradient:  0.2596756037324905
iteration : 13585
train acc:  0.8984375
train loss:  0.29292625188827515
train gradient:  0.10980089938709227
iteration : 13586
train acc:  0.8984375
train loss:  0.258087694644928
train gradient:  0.09104434498433277
iteration : 13587
train acc:  0.9296875
train loss:  0.21863818168640137
train gradient:  0.12893949207205757
iteration : 13588
train acc:  0.8984375
train loss:  0.26017147302627563
train gradient:  0.0760008131791427
iteration : 13589
train acc:  0.875
train loss:  0.31521379947662354
train gradient:  0.1520220055152807
iteration : 13590
train acc:  0.875
train loss:  0.29321038722991943
train gradient:  0.16746542441885495
iteration : 13591
train acc:  0.8125
train loss:  0.3188202977180481
train gradient:  0.1545724693453086
iteration : 13592
train acc:  0.890625
train loss:  0.2903931140899658
train gradient:  0.1527348860126841
iteration : 13593
train acc:  0.875
train loss:  0.29471418261528015
train gradient:  0.09269083393511335
iteration : 13594
train acc:  0.8828125
train loss:  0.3252963423728943
train gradient:  0.14565078836133613
iteration : 13595
train acc:  0.8984375
train loss:  0.2864215075969696
train gradient:  0.13023162172751668
iteration : 13596
train acc:  0.875
train loss:  0.307853639125824
train gradient:  0.13447587178059364
iteration : 13597
train acc:  0.8359375
train loss:  0.3202453851699829
train gradient:  0.16365715633504418
iteration : 13598
train acc:  0.8125
train loss:  0.36741316318511963
train gradient:  0.15953614229371432
iteration : 13599
train acc:  0.8828125
train loss:  0.2722576856613159
train gradient:  0.08833111265819364
iteration : 13600
train acc:  0.8203125
train loss:  0.4358242154121399
train gradient:  0.2116254417827661
iteration : 13601
train acc:  0.8515625
train loss:  0.3463321924209595
train gradient:  0.12972064075220868
iteration : 13602
train acc:  0.8046875
train loss:  0.3891758620738983
train gradient:  0.2009678402228231
iteration : 13603
train acc:  0.8203125
train loss:  0.3933178186416626
train gradient:  0.22428584345025415
iteration : 13604
train acc:  0.8984375
train loss:  0.27130451798439026
train gradient:  0.08583571223660885
iteration : 13605
train acc:  0.875
train loss:  0.28579503297805786
train gradient:  0.12974118764500486
iteration : 13606
train acc:  0.875
train loss:  0.2930501103401184
train gradient:  0.15176456242654282
iteration : 13607
train acc:  0.9140625
train loss:  0.25908422470092773
train gradient:  0.09912188676236522
iteration : 13608
train acc:  0.8203125
train loss:  0.37321025133132935
train gradient:  0.1710392785865407
iteration : 13609
train acc:  0.875
train loss:  0.2806737720966339
train gradient:  0.10353730641384433
iteration : 13610
train acc:  0.9140625
train loss:  0.20296335220336914
train gradient:  0.081569231815892
iteration : 13611
train acc:  0.84375
train loss:  0.32908448576927185
train gradient:  0.1764282342818695
iteration : 13612
train acc:  0.8828125
train loss:  0.2666204869747162
train gradient:  0.09605947362981
iteration : 13613
train acc:  0.8203125
train loss:  0.34441953897476196
train gradient:  0.12683540213841193
iteration : 13614
train acc:  0.859375
train loss:  0.3140459656715393
train gradient:  0.18848989012406492
iteration : 13615
train acc:  0.8359375
train loss:  0.40278512239456177
train gradient:  0.19028356707030375
iteration : 13616
train acc:  0.8984375
train loss:  0.2976677715778351
train gradient:  0.15866754540119016
iteration : 13617
train acc:  0.890625
train loss:  0.3327326476573944
train gradient:  0.15650878711756178
iteration : 13618
train acc:  0.8828125
train loss:  0.2932027578353882
train gradient:  0.18737726678461936
iteration : 13619
train acc:  0.8984375
train loss:  0.26702067255973816
train gradient:  0.15072783183677801
iteration : 13620
train acc:  0.84375
train loss:  0.3500938415527344
train gradient:  0.15261067992566862
iteration : 13621
train acc:  0.8828125
train loss:  0.3010174036026001
train gradient:  0.12845997878783638
iteration : 13622
train acc:  0.828125
train loss:  0.3434247672557831
train gradient:  0.24874838799246266
iteration : 13623
train acc:  0.890625
train loss:  0.24823319911956787
train gradient:  0.11951717991478829
iteration : 13624
train acc:  0.8671875
train loss:  0.29677218198776245
train gradient:  0.10743244371302156
iteration : 13625
train acc:  0.8671875
train loss:  0.26886454224586487
train gradient:  0.15243609959899848
iteration : 13626
train acc:  0.8515625
train loss:  0.3360179364681244
train gradient:  0.11067842398433791
iteration : 13627
train acc:  0.8203125
train loss:  0.41683101654052734
train gradient:  0.21238280429378065
iteration : 13628
train acc:  0.8984375
train loss:  0.2979356050491333
train gradient:  0.13621072663597306
iteration : 13629
train acc:  0.84375
train loss:  0.3277304768562317
train gradient:  0.17702120287241283
iteration : 13630
train acc:  0.859375
train loss:  0.2922043204307556
train gradient:  0.2386314907303772
iteration : 13631
train acc:  0.8203125
train loss:  0.36097270250320435
train gradient:  0.28478254384259494
iteration : 13632
train acc:  0.9140625
train loss:  0.28755152225494385
train gradient:  0.1345310588280879
iteration : 13633
train acc:  0.7890625
train loss:  0.4136357605457306
train gradient:  0.2629628566869323
iteration : 13634
train acc:  0.8828125
train loss:  0.28613799810409546
train gradient:  0.11840960682400106
iteration : 13635
train acc:  0.8359375
train loss:  0.3573372960090637
train gradient:  0.1962680015702091
iteration : 13636
train acc:  0.8671875
train loss:  0.32161545753479004
train gradient:  0.148314922136699
iteration : 13637
train acc:  0.84375
train loss:  0.3769075870513916
train gradient:  0.2439103786560174
iteration : 13638
train acc:  0.890625
train loss:  0.28735101222991943
train gradient:  0.14776820701133445
iteration : 13639
train acc:  0.8125
train loss:  0.33330559730529785
train gradient:  0.14603401971855234
iteration : 13640
train acc:  0.796875
train loss:  0.4641086757183075
train gradient:  0.2660534072257954
iteration : 13641
train acc:  0.8671875
train loss:  0.29148387908935547
train gradient:  0.1207042816094345
iteration : 13642
train acc:  0.8671875
train loss:  0.31986716389656067
train gradient:  0.10616029592915494
iteration : 13643
train acc:  0.8671875
train loss:  0.3183543086051941
train gradient:  0.11944171392942866
iteration : 13644
train acc:  0.8515625
train loss:  0.36427414417266846
train gradient:  0.14052874720800934
iteration : 13645
train acc:  0.8515625
train loss:  0.30090153217315674
train gradient:  0.14012570588094503
iteration : 13646
train acc:  0.9296875
train loss:  0.22513101994991302
train gradient:  0.09277686565096796
iteration : 13647
train acc:  0.875
train loss:  0.31079933047294617
train gradient:  0.0909653882997903
iteration : 13648
train acc:  0.921875
train loss:  0.20635627210140228
train gradient:  0.0888569026772311
iteration : 13649
train acc:  0.8515625
train loss:  0.32021021842956543
train gradient:  0.18058377771081485
iteration : 13650
train acc:  0.84375
train loss:  0.3326820135116577
train gradient:  0.19170284733240656
iteration : 13651
train acc:  0.8359375
train loss:  0.33094725012779236
train gradient:  0.11612370013357856
iteration : 13652
train acc:  0.9140625
train loss:  0.21830591559410095
train gradient:  0.08839997912991758
iteration : 13653
train acc:  0.8984375
train loss:  0.3166806697845459
train gradient:  0.10652191290230816
iteration : 13654
train acc:  0.8671875
train loss:  0.26487481594085693
train gradient:  0.10227951658265613
iteration : 13655
train acc:  0.8828125
train loss:  0.33643579483032227
train gradient:  0.15339735794863907
iteration : 13656
train acc:  0.90625
train loss:  0.2439572811126709
train gradient:  0.09093128475594264
iteration : 13657
train acc:  0.8203125
train loss:  0.3435364365577698
train gradient:  0.19549085033424674
iteration : 13658
train acc:  0.859375
train loss:  0.32509249448776245
train gradient:  0.16701074012441852
iteration : 13659
train acc:  0.8515625
train loss:  0.32074612379074097
train gradient:  0.14142754592993179
iteration : 13660
train acc:  0.8359375
train loss:  0.3106786906719208
train gradient:  0.11297305024641678
iteration : 13661
train acc:  0.8125
train loss:  0.4440898895263672
train gradient:  0.18905145432274245
iteration : 13662
train acc:  0.8359375
train loss:  0.37379950284957886
train gradient:  0.16178593371780647
iteration : 13663
train acc:  0.8046875
train loss:  0.35316041111946106
train gradient:  0.2056629256230419
iteration : 13664
train acc:  0.828125
train loss:  0.3645343780517578
train gradient:  0.1599031639627433
iteration : 13665
train acc:  0.84375
train loss:  0.3411772847175598
train gradient:  0.16294626270506082
iteration : 13666
train acc:  0.90625
train loss:  0.32842040061950684
train gradient:  0.17361320524371748
iteration : 13667
train acc:  0.859375
train loss:  0.2758179306983948
train gradient:  0.08227312111921994
iteration : 13668
train acc:  0.8515625
train loss:  0.3790813684463501
train gradient:  0.20826931408986285
iteration : 13669
train acc:  0.84375
train loss:  0.33843642473220825
train gradient:  0.16415924520452035
iteration : 13670
train acc:  0.859375
train loss:  0.28929466009140015
train gradient:  0.13036650745060263
iteration : 13671
train acc:  0.9296875
train loss:  0.24581429362297058
train gradient:  0.08685498697079871
iteration : 13672
train acc:  0.8359375
train loss:  0.35235345363616943
train gradient:  0.15521307634165
iteration : 13673
train acc:  0.8359375
train loss:  0.3926519751548767
train gradient:  0.24184286647715328
iteration : 13674
train acc:  0.890625
train loss:  0.2767009735107422
train gradient:  0.1013051442691542
iteration : 13675
train acc:  0.859375
train loss:  0.3371620178222656
train gradient:  0.12739012551850937
iteration : 13676
train acc:  0.8515625
train loss:  0.2870636582374573
train gradient:  0.12773900002648847
iteration : 13677
train acc:  0.875
train loss:  0.26056820154190063
train gradient:  0.10066423534555481
iteration : 13678
train acc:  0.921875
train loss:  0.24245673418045044
train gradient:  0.07712558749146645
iteration : 13679
train acc:  0.8671875
train loss:  0.29035234451293945
train gradient:  0.09743472802869778
iteration : 13680
train acc:  0.8828125
train loss:  0.31368690729141235
train gradient:  0.17701627911984114
iteration : 13681
train acc:  0.859375
train loss:  0.27938002347946167
train gradient:  0.21297037358335463
iteration : 13682
train acc:  0.875
train loss:  0.28722941875457764
train gradient:  0.12278807235295752
iteration : 13683
train acc:  0.8984375
train loss:  0.31097832322120667
train gradient:  0.1845626199212763
iteration : 13684
train acc:  0.859375
train loss:  0.3304734230041504
train gradient:  0.15415019958320036
iteration : 13685
train acc:  0.8515625
train loss:  0.31475695967674255
train gradient:  0.1383621176591926
iteration : 13686
train acc:  0.890625
train loss:  0.2816355228424072
train gradient:  0.12560535874474849
iteration : 13687
train acc:  0.8828125
train loss:  0.24400171637535095
train gradient:  0.08587551874847224
iteration : 13688
train acc:  0.9296875
train loss:  0.2463606297969818
train gradient:  0.13441772229920412
iteration : 13689
train acc:  0.8828125
train loss:  0.29573580622673035
train gradient:  0.23960292769218253
iteration : 13690
train acc:  0.9140625
train loss:  0.229637011885643
train gradient:  0.09727446348913234
iteration : 13691
train acc:  0.8515625
train loss:  0.3209657073020935
train gradient:  0.13330239946351957
iteration : 13692
train acc:  0.8671875
train loss:  0.27311480045318604
train gradient:  0.10032627785290195
iteration : 13693
train acc:  0.8984375
train loss:  0.25808703899383545
train gradient:  0.11374575388173118
iteration : 13694
train acc:  0.890625
train loss:  0.26516470313072205
train gradient:  0.13713437395107142
iteration : 13695
train acc:  0.84375
train loss:  0.27902039885520935
train gradient:  0.1288621223531729
iteration : 13696
train acc:  0.8828125
train loss:  0.3145708739757538
train gradient:  0.13700042300031162
iteration : 13697
train acc:  0.84375
train loss:  0.3536102771759033
train gradient:  0.18462726199278923
iteration : 13698
train acc:  0.8828125
train loss:  0.2883533239364624
train gradient:  0.11375696981106224
iteration : 13699
train acc:  0.828125
train loss:  0.35265475511550903
train gradient:  0.14304046637754375
iteration : 13700
train acc:  0.8203125
train loss:  0.4161773920059204
train gradient:  0.3298499673632454
iteration : 13701
train acc:  0.8671875
train loss:  0.3385351598262787
train gradient:  0.16392110653097797
iteration : 13702
train acc:  0.8671875
train loss:  0.28094482421875
train gradient:  0.15073609945673955
iteration : 13703
train acc:  0.8984375
train loss:  0.24564425647258759
train gradient:  0.0780279442751958
iteration : 13704
train acc:  0.859375
train loss:  0.34229329228401184
train gradient:  0.1785628032744943
iteration : 13705
train acc:  0.8828125
train loss:  0.26772910356521606
train gradient:  0.13099670249572185
iteration : 13706
train acc:  0.890625
train loss:  0.25604429841041565
train gradient:  0.11853920223220034
iteration : 13707
train acc:  0.8515625
train loss:  0.33640986680984497
train gradient:  0.21158279982300024
iteration : 13708
train acc:  0.9140625
train loss:  0.27403682470321655
train gradient:  0.17579588571448834
iteration : 13709
train acc:  0.859375
train loss:  0.3020443916320801
train gradient:  0.13476154180724548
iteration : 13710
train acc:  0.890625
train loss:  0.25840485095977783
train gradient:  0.1535164191675601
iteration : 13711
train acc:  0.8515625
train loss:  0.35868483781814575
train gradient:  0.21011437551938872
iteration : 13712
train acc:  0.8828125
train loss:  0.2618919014930725
train gradient:  0.12551592365152983
iteration : 13713
train acc:  0.875
train loss:  0.31513339281082153
train gradient:  0.14067084752785997
iteration : 13714
train acc:  0.875
train loss:  0.2567659616470337
train gradient:  0.14659831351813787
iteration : 13715
train acc:  0.84375
train loss:  0.3153267204761505
train gradient:  0.28161388283509947
iteration : 13716
train acc:  0.8359375
train loss:  0.3631911873817444
train gradient:  0.15639184513096682
iteration : 13717
train acc:  0.859375
train loss:  0.2909766733646393
train gradient:  0.07564903483673587
iteration : 13718
train acc:  0.890625
train loss:  0.27728697657585144
train gradient:  0.131375698509987
iteration : 13719
train acc:  0.875
train loss:  0.25996893644332886
train gradient:  0.13885479374060816
iteration : 13720
train acc:  0.890625
train loss:  0.32680386304855347
train gradient:  0.12603539465695798
iteration : 13721
train acc:  0.890625
train loss:  0.25153854489326477
train gradient:  0.08887545159790204
iteration : 13722
train acc:  0.8828125
train loss:  0.30380961298942566
train gradient:  0.12999059805837454
iteration : 13723
train acc:  0.84375
train loss:  0.3294532895088196
train gradient:  0.15582731144616485
iteration : 13724
train acc:  0.90625
train loss:  0.23932626843452454
train gradient:  0.11718325528199139
iteration : 13725
train acc:  0.8671875
train loss:  0.28219732642173767
train gradient:  0.10239239779595374
iteration : 13726
train acc:  0.828125
train loss:  0.4496825337409973
train gradient:  0.29988254102097134
iteration : 13727
train acc:  0.859375
train loss:  0.3193763792514801
train gradient:  0.12321249903802865
iteration : 13728
train acc:  0.8515625
train loss:  0.3042542338371277
train gradient:  0.15156961804474448
iteration : 13729
train acc:  0.8984375
train loss:  0.30800163745880127
train gradient:  0.10467988650366415
iteration : 13730
train acc:  0.9140625
train loss:  0.20589014887809753
train gradient:  0.08716060965878993
iteration : 13731
train acc:  0.8203125
train loss:  0.38067302107810974
train gradient:  0.2290463833104438
iteration : 13732
train acc:  0.8671875
train loss:  0.26539182662963867
train gradient:  0.09050650659051895
iteration : 13733
train acc:  0.875
train loss:  0.2995014786720276
train gradient:  0.17061118402507314
iteration : 13734
train acc:  0.8515625
train loss:  0.33078432083129883
train gradient:  0.1684096240535292
iteration : 13735
train acc:  0.84375
train loss:  0.39257678389549255
train gradient:  0.1700120076240818
iteration : 13736
train acc:  0.875
train loss:  0.31610992550849915
train gradient:  0.1145008006213491
iteration : 13737
train acc:  0.84375
train loss:  0.3187108635902405
train gradient:  0.20970483579689753
iteration : 13738
train acc:  0.8515625
train loss:  0.3959067463874817
train gradient:  0.17615268083645635
iteration : 13739
train acc:  0.84375
train loss:  0.30706506967544556
train gradient:  0.1221023765047227
iteration : 13740
train acc:  0.90625
train loss:  0.21863052248954773
train gradient:  0.09453561280099047
iteration : 13741
train acc:  0.8828125
train loss:  0.3369700610637665
train gradient:  0.33536546218018104
iteration : 13742
train acc:  0.8828125
train loss:  0.2745034694671631
train gradient:  0.09209511280632779
iteration : 13743
train acc:  0.890625
train loss:  0.2697995901107788
train gradient:  0.09364703883128277
iteration : 13744
train acc:  0.8671875
train loss:  0.29535019397735596
train gradient:  0.14473329198060622
iteration : 13745
train acc:  0.859375
train loss:  0.3792487382888794
train gradient:  0.2041284481521664
iteration : 13746
train acc:  0.859375
train loss:  0.30430760979652405
train gradient:  0.12268184160328964
iteration : 13747
train acc:  0.890625
train loss:  0.2378166913986206
train gradient:  0.071796883660928
iteration : 13748
train acc:  0.8203125
train loss:  0.44311487674713135
train gradient:  0.2512196196121173
iteration : 13749
train acc:  0.890625
train loss:  0.28036612272262573
train gradient:  0.10556515234786802
iteration : 13750
train acc:  0.8515625
train loss:  0.303161084651947
train gradient:  0.13005025500591083
iteration : 13751
train acc:  0.8984375
train loss:  0.2537190318107605
train gradient:  0.1433014480147881
iteration : 13752
train acc:  0.890625
train loss:  0.2908039093017578
train gradient:  0.1301393059328193
iteration : 13753
train acc:  0.84375
train loss:  0.3560461401939392
train gradient:  0.16946016767748923
iteration : 13754
train acc:  0.8515625
train loss:  0.3269469439983368
train gradient:  0.17277948574516616
iteration : 13755
train acc:  0.875
train loss:  0.3576304316520691
train gradient:  0.18747574004038334
iteration : 13756
train acc:  0.8203125
train loss:  0.3865577280521393
train gradient:  0.2855556843389049
iteration : 13757
train acc:  0.875
train loss:  0.27352845668792725
train gradient:  0.1057521964155675
iteration : 13758
train acc:  0.8984375
train loss:  0.312325656414032
train gradient:  0.12361398018061272
iteration : 13759
train acc:  0.8125
train loss:  0.39296960830688477
train gradient:  0.1694962002467851
iteration : 13760
train acc:  0.8515625
train loss:  0.39628851413726807
train gradient:  0.15943686335657326
iteration : 13761
train acc:  0.890625
train loss:  0.22990813851356506
train gradient:  0.09616319638605295
iteration : 13762
train acc:  0.8984375
train loss:  0.23146401345729828
train gradient:  0.08595749013110592
iteration : 13763
train acc:  0.8828125
train loss:  0.33491402864456177
train gradient:  0.13829974430075792
iteration : 13764
train acc:  0.8359375
train loss:  0.3232848644256592
train gradient:  0.14048422552090603
iteration : 13765
train acc:  0.8125
train loss:  0.39148545265197754
train gradient:  0.257638225332079
iteration : 13766
train acc:  0.8515625
train loss:  0.3011731207370758
train gradient:  0.11343692926950046
iteration : 13767
train acc:  0.8671875
train loss:  0.3399941325187683
train gradient:  0.19637316441814706
iteration : 13768
train acc:  0.890625
train loss:  0.24588294327259064
train gradient:  0.14613948956703493
iteration : 13769
train acc:  0.8203125
train loss:  0.36439794301986694
train gradient:  0.17694541727235624
iteration : 13770
train acc:  0.875
train loss:  0.26883357763290405
train gradient:  0.14308001026670586
iteration : 13771
train acc:  0.875
train loss:  0.2982865571975708
train gradient:  0.12892270884363816
iteration : 13772
train acc:  0.875
train loss:  0.31558358669281006
train gradient:  0.14409915156792044
iteration : 13773
train acc:  0.859375
train loss:  0.3608066439628601
train gradient:  0.17202380674005407
iteration : 13774
train acc:  0.8125
train loss:  0.3823535442352295
train gradient:  0.2473085564161248
iteration : 13775
train acc:  0.859375
train loss:  0.3714503049850464
train gradient:  0.1509941874251331
iteration : 13776
train acc:  0.8828125
train loss:  0.3045503497123718
train gradient:  0.13081156049144044
iteration : 13777
train acc:  0.8359375
train loss:  0.36564600467681885
train gradient:  0.1337180870664675
iteration : 13778
train acc:  0.8515625
train loss:  0.33196061849594116
train gradient:  0.16618577661537717
iteration : 13779
train acc:  0.8515625
train loss:  0.32865726947784424
train gradient:  0.1684059754506976
iteration : 13780
train acc:  0.9296875
train loss:  0.25054383277893066
train gradient:  0.11112731535905188
iteration : 13781
train acc:  0.8828125
train loss:  0.29409927129745483
train gradient:  0.10742287899019311
iteration : 13782
train acc:  0.859375
train loss:  0.3345089256763458
train gradient:  0.10833106014981177
iteration : 13783
train acc:  0.8125
train loss:  0.3446122407913208
train gradient:  0.19156398038931244
iteration : 13784
train acc:  0.875
train loss:  0.3025550842285156
train gradient:  0.11319845179733411
iteration : 13785
train acc:  0.8203125
train loss:  0.3679138720035553
train gradient:  0.22286345897714732
iteration : 13786
train acc:  0.84375
train loss:  0.3073579668998718
train gradient:  0.12316102880903425
iteration : 13787
train acc:  0.875
train loss:  0.27615636587142944
train gradient:  0.06907159866867951
iteration : 13788
train acc:  0.8828125
train loss:  0.28814637660980225
train gradient:  0.09832417114642215
iteration : 13789
train acc:  0.84375
train loss:  0.3512192964553833
train gradient:  0.19845857953566037
iteration : 13790
train acc:  0.859375
train loss:  0.2718261480331421
train gradient:  0.15422576892227383
iteration : 13791
train acc:  0.875
train loss:  0.35320162773132324
train gradient:  0.16902834985474058
iteration : 13792
train acc:  0.875
train loss:  0.27207130193710327
train gradient:  0.08506310719908372
iteration : 13793
train acc:  0.828125
train loss:  0.4140568971633911
train gradient:  0.15208605003045295
iteration : 13794
train acc:  0.890625
train loss:  0.303783655166626
train gradient:  0.13483908793798477
iteration : 13795
train acc:  0.8984375
train loss:  0.2204456329345703
train gradient:  0.06616416797140147
iteration : 13796
train acc:  0.859375
train loss:  0.33263689279556274
train gradient:  0.1825004913248147
iteration : 13797
train acc:  0.875
train loss:  0.3305208384990692
train gradient:  0.12143024774534578
iteration : 13798
train acc:  0.8671875
train loss:  0.35313084721565247
train gradient:  0.12060146395409634
iteration : 13799
train acc:  0.90625
train loss:  0.22624295949935913
train gradient:  0.08279280963997367
iteration : 13800
train acc:  0.875
train loss:  0.2806459665298462
train gradient:  0.11960701395407931
iteration : 13801
train acc:  0.875
train loss:  0.28273963928222656
train gradient:  0.07947878294899187
iteration : 13802
train acc:  0.796875
train loss:  0.3697129487991333
train gradient:  0.17281377354267072
iteration : 13803
train acc:  0.8984375
train loss:  0.2304367870092392
train gradient:  0.09105245416362448
iteration : 13804
train acc:  0.890625
train loss:  0.2918011546134949
train gradient:  0.1165904988817568
iteration : 13805
train acc:  0.890625
train loss:  0.26920416951179504
train gradient:  0.11677676932817699
iteration : 13806
train acc:  0.8671875
train loss:  0.28821641206741333
train gradient:  0.10167163924632906
iteration : 13807
train acc:  0.859375
train loss:  0.2964910566806793
train gradient:  0.12109843353834376
iteration : 13808
train acc:  0.859375
train loss:  0.2878262996673584
train gradient:  0.16183963859367112
iteration : 13809
train acc:  0.859375
train loss:  0.36890098452568054
train gradient:  0.18230923067145682
iteration : 13810
train acc:  0.8828125
train loss:  0.2923687696456909
train gradient:  0.11968668435828382
iteration : 13811
train acc:  0.875
train loss:  0.33314821124076843
train gradient:  0.15416828484630277
iteration : 13812
train acc:  0.875
train loss:  0.30607953667640686
train gradient:  0.14873770412593512
iteration : 13813
train acc:  0.890625
train loss:  0.26566487550735474
train gradient:  0.12062867974876192
iteration : 13814
train acc:  0.90625
train loss:  0.24846026301383972
train gradient:  0.08101138667609806
iteration : 13815
train acc:  0.8671875
train loss:  0.3159703016281128
train gradient:  0.11013717285147923
iteration : 13816
train acc:  0.8515625
train loss:  0.307781845331192
train gradient:  0.13457269211586992
iteration : 13817
train acc:  0.859375
train loss:  0.2820967435836792
train gradient:  0.1356121542536975
iteration : 13818
train acc:  0.890625
train loss:  0.2741850018501282
train gradient:  0.10247495061933849
iteration : 13819
train acc:  0.8515625
train loss:  0.4072973132133484
train gradient:  0.23593224713449093
iteration : 13820
train acc:  0.859375
train loss:  0.2902308702468872
train gradient:  0.09281010408702885
iteration : 13821
train acc:  0.828125
train loss:  0.33875441551208496
train gradient:  0.14650086091640183
iteration : 13822
train acc:  0.859375
train loss:  0.29810628294944763
train gradient:  0.10617752361716462
iteration : 13823
train acc:  0.8359375
train loss:  0.3661835491657257
train gradient:  0.17492278042790066
iteration : 13824
train acc:  0.8984375
train loss:  0.333268404006958
train gradient:  0.19838692077843734
iteration : 13825
train acc:  0.875
train loss:  0.2574824094772339
train gradient:  0.208599473361267
iteration : 13826
train acc:  0.8671875
train loss:  0.3060053586959839
train gradient:  0.12481597731223353
iteration : 13827
train acc:  0.84375
train loss:  0.37238609790802
train gradient:  0.19154594022412919
iteration : 13828
train acc:  0.796875
train loss:  0.39039042592048645
train gradient:  0.2224529645553036
iteration : 13829
train acc:  0.8359375
train loss:  0.3309967517852783
train gradient:  0.13562492896349712
iteration : 13830
train acc:  0.8828125
train loss:  0.29715558886528015
train gradient:  0.13023011542568397
iteration : 13831
train acc:  0.8515625
train loss:  0.37890148162841797
train gradient:  0.21837570627886466
iteration : 13832
train acc:  0.8515625
train loss:  0.33751484751701355
train gradient:  0.16798582188730332
iteration : 13833
train acc:  0.875
train loss:  0.3177966773509979
train gradient:  0.14196916941089796
iteration : 13834
train acc:  0.8515625
train loss:  0.31289732456207275
train gradient:  0.11547840019225238
iteration : 13835
train acc:  0.859375
train loss:  0.318519651889801
train gradient:  0.11977236132848708
iteration : 13836
train acc:  0.875
train loss:  0.2762218117713928
train gradient:  0.18389117153344509
iteration : 13837
train acc:  0.8828125
train loss:  0.2578897476196289
train gradient:  0.10121530478362356
iteration : 13838
train acc:  0.8828125
train loss:  0.237942636013031
train gradient:  0.06594352232588814
iteration : 13839
train acc:  0.859375
train loss:  0.2811359167098999
train gradient:  0.0739324137829488
iteration : 13840
train acc:  0.84375
train loss:  0.29765084385871887
train gradient:  0.12875258099630246
iteration : 13841
train acc:  0.8515625
train loss:  0.3280501067638397
train gradient:  0.13193588072620116
iteration : 13842
train acc:  0.8046875
train loss:  0.4133298993110657
train gradient:  0.1897271150466522
iteration : 13843
train acc:  0.875
train loss:  0.26749080419540405
train gradient:  0.09793263813014454
iteration : 13844
train acc:  0.84375
train loss:  0.32836586236953735
train gradient:  0.1526321118674218
iteration : 13845
train acc:  0.7890625
train loss:  0.39186882972717285
train gradient:  0.1825409566057085
iteration : 13846
train acc:  0.8203125
train loss:  0.3210950493812561
train gradient:  0.12328740928505443
iteration : 13847
train acc:  0.8125
train loss:  0.36181575059890747
train gradient:  0.18915302948648444
iteration : 13848
train acc:  0.90625
train loss:  0.2634044289588928
train gradient:  0.10161396295701404
iteration : 13849
train acc:  0.84375
train loss:  0.3847091495990753
train gradient:  0.2526198228040294
iteration : 13850
train acc:  0.859375
train loss:  0.2982563376426697
train gradient:  0.08586064996156731
iteration : 13851
train acc:  0.90625
train loss:  0.2707599997520447
train gradient:  0.0957356516184893
iteration : 13852
train acc:  0.8828125
train loss:  0.24037575721740723
train gradient:  0.1282903918512181
iteration : 13853
train acc:  0.828125
train loss:  0.38366881012916565
train gradient:  0.2851886114793875
iteration : 13854
train acc:  0.890625
train loss:  0.28696209192276
train gradient:  0.14396133074069106
iteration : 13855
train acc:  0.84375
train loss:  0.3740765452384949
train gradient:  0.15629497074868298
iteration : 13856
train acc:  0.875
train loss:  0.3190222978591919
train gradient:  0.10282627583714864
iteration : 13857
train acc:  0.890625
train loss:  0.28588205575942993
train gradient:  0.12067708740795251
iteration : 13858
train acc:  0.8828125
train loss:  0.2950373888015747
train gradient:  0.1275979607563155
iteration : 13859
train acc:  0.859375
train loss:  0.32766321301460266
train gradient:  0.10747352013698434
iteration : 13860
train acc:  0.8515625
train loss:  0.3208047151565552
train gradient:  0.17666724959168523
iteration : 13861
train acc:  0.875
train loss:  0.2892683148384094
train gradient:  0.12068216644039442
iteration : 13862
train acc:  0.8359375
train loss:  0.34287190437316895
train gradient:  0.27525509622077826
iteration : 13863
train acc:  0.8828125
train loss:  0.3448171317577362
train gradient:  0.12994443731675098
iteration : 13864
train acc:  0.921875
train loss:  0.27301865816116333
train gradient:  0.1207327071306671
iteration : 13865
train acc:  0.84375
train loss:  0.3008420765399933
train gradient:  0.12486431945490409
iteration : 13866
train acc:  0.859375
train loss:  0.31346142292022705
train gradient:  0.11700093879763486
iteration : 13867
train acc:  0.84375
train loss:  0.311185747385025
train gradient:  0.1405792505357305
iteration : 13868
train acc:  0.8984375
train loss:  0.2891755700111389
train gradient:  0.1619254541128363
iteration : 13869
train acc:  0.859375
train loss:  0.35490089654922485
train gradient:  0.19765981560029453
iteration : 13870
train acc:  0.8359375
train loss:  0.35688382387161255
train gradient:  0.17783278936051763
iteration : 13871
train acc:  0.8671875
train loss:  0.31380176544189453
train gradient:  0.1565473866252187
iteration : 13872
train acc:  0.84375
train loss:  0.333072304725647
train gradient:  0.1620720024384127
iteration : 13873
train acc:  0.8515625
train loss:  0.3045804500579834
train gradient:  0.1048456880383681
iteration : 13874
train acc:  0.875
train loss:  0.29835301637649536
train gradient:  0.09268250242677169
iteration : 13875
train acc:  0.8828125
train loss:  0.3223240375518799
train gradient:  0.19666327864459315
iteration : 13876
train acc:  0.890625
train loss:  0.2791982591152191
train gradient:  0.11158358597884827
iteration : 13877
train acc:  0.8828125
train loss:  0.29414302110671997
train gradient:  0.0987127675882504
iteration : 13878
train acc:  0.84375
train loss:  0.349234402179718
train gradient:  0.1958167777057939
iteration : 13879
train acc:  0.8359375
train loss:  0.39898061752319336
train gradient:  0.19595531025088636
iteration : 13880
train acc:  0.859375
train loss:  0.29780247807502747
train gradient:  0.13220040874736327
iteration : 13881
train acc:  0.8359375
train loss:  0.3390812873840332
train gradient:  0.1434510607200985
iteration : 13882
train acc:  0.8984375
train loss:  0.25300100445747375
train gradient:  0.1394141791204316
iteration : 13883
train acc:  0.8828125
train loss:  0.23774658143520355
train gradient:  0.07623623545033845
iteration : 13884
train acc:  0.8359375
train loss:  0.38315632939338684
train gradient:  0.19504928678909267
iteration : 13885
train acc:  0.890625
train loss:  0.26455962657928467
train gradient:  0.10070278902337415
iteration : 13886
train acc:  0.8515625
train loss:  0.3815760612487793
train gradient:  0.23094589011626784
iteration : 13887
train acc:  0.8984375
train loss:  0.3037167489528656
train gradient:  0.09518419616425307
iteration : 13888
train acc:  0.8984375
train loss:  0.24094533920288086
train gradient:  0.09861006417274831
iteration : 13889
train acc:  0.8984375
train loss:  0.24322551488876343
train gradient:  0.10097932472648895
iteration : 13890
train acc:  0.84375
train loss:  0.4005163908004761
train gradient:  0.15877150209808147
iteration : 13891
train acc:  0.8515625
train loss:  0.3322467505931854
train gradient:  0.12853677915338468
iteration : 13892
train acc:  0.9140625
train loss:  0.27031370997428894
train gradient:  0.10095712590639723
iteration : 13893
train acc:  0.8046875
train loss:  0.3784927725791931
train gradient:  0.17204551473296542
iteration : 13894
train acc:  0.8125
train loss:  0.345126211643219
train gradient:  0.1207876074397774
iteration : 13895
train acc:  0.8671875
train loss:  0.285597026348114
train gradient:  0.09804514143840361
iteration : 13896
train acc:  0.8515625
train loss:  0.367706298828125
train gradient:  0.17696168163263765
iteration : 13897
train acc:  0.8515625
train loss:  0.3148350119590759
train gradient:  0.1353352496727559
iteration : 13898
train acc:  0.8984375
train loss:  0.300376296043396
train gradient:  0.12200430156423678
iteration : 13899
train acc:  0.875
train loss:  0.24778273701667786
train gradient:  0.09973072935739999
iteration : 13900
train acc:  0.84375
train loss:  0.40383464097976685
train gradient:  0.1810289381027199
iteration : 13901
train acc:  0.8828125
train loss:  0.33565062284469604
train gradient:  0.13652769322530348
iteration : 13902
train acc:  0.9140625
train loss:  0.23000003397464752
train gradient:  0.08868771207741546
iteration : 13903
train acc:  0.8828125
train loss:  0.30967941880226135
train gradient:  0.09116287148337128
iteration : 13904
train acc:  0.875
train loss:  0.3379223942756653
train gradient:  0.10467724336823975
iteration : 13905
train acc:  0.8984375
train loss:  0.2722369432449341
train gradient:  0.09296574589335713
iteration : 13906
train acc:  0.859375
train loss:  0.3019082546234131
train gradient:  0.09889736927341065
iteration : 13907
train acc:  0.8828125
train loss:  0.3051708936691284
train gradient:  0.13108794587298794
iteration : 13908
train acc:  0.8046875
train loss:  0.37539181113243103
train gradient:  0.1805693102637623
iteration : 13909
train acc:  0.8671875
train loss:  0.2795702815055847
train gradient:  0.08576975134971171
iteration : 13910
train acc:  0.8671875
train loss:  0.3048078417778015
train gradient:  0.14049041128486872
iteration : 13911
train acc:  0.8203125
train loss:  0.35384401679039
train gradient:  0.16270585569849552
iteration : 13912
train acc:  0.84375
train loss:  0.34157419204711914
train gradient:  0.2017286835739298
iteration : 13913
train acc:  0.8671875
train loss:  0.27307501435279846
train gradient:  0.09446663050272243
iteration : 13914
train acc:  0.875
train loss:  0.28681159019470215
train gradient:  0.11842620266373666
iteration : 13915
train acc:  0.84375
train loss:  0.32731449604034424
train gradient:  0.11536777825704535
iteration : 13916
train acc:  0.875
train loss:  0.33603012561798096
train gradient:  0.14882402586004898
iteration : 13917
train acc:  0.828125
train loss:  0.41298460960388184
train gradient:  0.18576746481810977
iteration : 13918
train acc:  0.8671875
train loss:  0.2694706916809082
train gradient:  0.06589384315108673
iteration : 13919
train acc:  0.875
train loss:  0.26498672366142273
train gradient:  0.08114531783011732
iteration : 13920
train acc:  0.859375
train loss:  0.32766544818878174
train gradient:  0.11829554958034733
iteration : 13921
train acc:  0.8984375
train loss:  0.24017813801765442
train gradient:  0.09161607589797036
iteration : 13922
train acc:  0.8671875
train loss:  0.3381151258945465
train gradient:  0.1493528622387968
iteration : 13923
train acc:  0.8359375
train loss:  0.32463955879211426
train gradient:  0.12547698589042172
iteration : 13924
train acc:  0.8046875
train loss:  0.4617060422897339
train gradient:  0.28511351475035773
iteration : 13925
train acc:  0.875
train loss:  0.3047648072242737
train gradient:  0.12462565493939218
iteration : 13926
train acc:  0.8359375
train loss:  0.3809637129306793
train gradient:  0.15081641808642948
iteration : 13927
train acc:  0.828125
train loss:  0.37451958656311035
train gradient:  0.15579859742482782
iteration : 13928
train acc:  0.84375
train loss:  0.37856438755989075
train gradient:  0.18856034898838014
iteration : 13929
train acc:  0.8359375
train loss:  0.3628731667995453
train gradient:  0.2269262905784914
iteration : 13930
train acc:  0.875
train loss:  0.24155724048614502
train gradient:  0.09175473257130898
iteration : 13931
train acc:  0.828125
train loss:  0.33827096223831177
train gradient:  0.159718346116013
iteration : 13932
train acc:  0.8671875
train loss:  0.3177649676799774
train gradient:  0.12310422861923023
iteration : 13933
train acc:  0.8828125
train loss:  0.3194698691368103
train gradient:  0.09944992954210212
iteration : 13934
train acc:  0.8828125
train loss:  0.29279816150665283
train gradient:  0.12291498642678461
iteration : 13935
train acc:  0.84375
train loss:  0.33937832713127136
train gradient:  0.14041812274212206
iteration : 13936
train acc:  0.890625
train loss:  0.24102424085140228
train gradient:  0.08850083770551351
iteration : 13937
train acc:  0.796875
train loss:  0.43060189485549927
train gradient:  0.24085750825899221
iteration : 13938
train acc:  0.890625
train loss:  0.2581773102283478
train gradient:  0.1373740991344664
iteration : 13939
train acc:  0.796875
train loss:  0.424465537071228
train gradient:  0.23917562866562647
iteration : 13940
train acc:  0.8671875
train loss:  0.2792555093765259
train gradient:  0.10328210181048622
iteration : 13941
train acc:  0.8671875
train loss:  0.35622987151145935
train gradient:  0.1326684937085586
iteration : 13942
train acc:  0.8828125
train loss:  0.27818939089775085
train gradient:  0.1047955466170563
iteration : 13943
train acc:  0.8515625
train loss:  0.39146846532821655
train gradient:  0.22509422156499515
iteration : 13944
train acc:  0.9140625
train loss:  0.2293338179588318
train gradient:  0.09643276188255288
iteration : 13945
train acc:  0.859375
train loss:  0.32933714985847473
train gradient:  0.15306617600829736
iteration : 13946
train acc:  0.859375
train loss:  0.29681575298309326
train gradient:  0.14089429842111917
iteration : 13947
train acc:  0.8359375
train loss:  0.39176684617996216
train gradient:  0.21913236424740473
iteration : 13948
train acc:  0.84375
train loss:  0.39241307973861694
train gradient:  0.14808649272002047
iteration : 13949
train acc:  0.8515625
train loss:  0.3218544125556946
train gradient:  0.17243423852117123
iteration : 13950
train acc:  0.828125
train loss:  0.38799071311950684
train gradient:  0.2242817015637859
iteration : 13951
train acc:  0.84375
train loss:  0.3334999680519104
train gradient:  0.11662818060672685
iteration : 13952
train acc:  0.8046875
train loss:  0.40900731086730957
train gradient:  0.21988058846104874
iteration : 13953
train acc:  0.875
train loss:  0.3152748942375183
train gradient:  0.11541133996176328
iteration : 13954
train acc:  0.859375
train loss:  0.3219950795173645
train gradient:  0.16368334577642327
iteration : 13955
train acc:  0.8984375
train loss:  0.2720532715320587
train gradient:  0.10106757258108008
iteration : 13956
train acc:  0.8984375
train loss:  0.24803423881530762
train gradient:  0.09621566143269542
iteration : 13957
train acc:  0.8671875
train loss:  0.3274112343788147
train gradient:  0.13628214190681967
iteration : 13958
train acc:  0.890625
train loss:  0.3780030608177185
train gradient:  0.1660115316593407
iteration : 13959
train acc:  0.8515625
train loss:  0.2941272258758545
train gradient:  0.08439428351951864
iteration : 13960
train acc:  0.890625
train loss:  0.2892749607563019
train gradient:  0.09301614995051935
iteration : 13961
train acc:  0.8984375
train loss:  0.2395676076412201
train gradient:  0.07036851033231045
iteration : 13962
train acc:  0.8359375
train loss:  0.38521867990493774
train gradient:  0.2666859895341973
iteration : 13963
train acc:  0.84375
train loss:  0.3440669775009155
train gradient:  0.14471149673437866
iteration : 13964
train acc:  0.859375
train loss:  0.33764514327049255
train gradient:  0.16701183220114646
iteration : 13965
train acc:  0.8125
train loss:  0.3758937120437622
train gradient:  0.13562877378577992
iteration : 13966
train acc:  0.90625
train loss:  0.2671162784099579
train gradient:  0.1053543119902967
iteration : 13967
train acc:  0.890625
train loss:  0.3324219584465027
train gradient:  0.18998978183362508
iteration : 13968
train acc:  0.8125
train loss:  0.3432040214538574
train gradient:  0.12424789654226641
iteration : 13969
train acc:  0.875
train loss:  0.35415956377983093
train gradient:  0.14288623267308415
iteration : 13970
train acc:  0.859375
train loss:  0.2886132597923279
train gradient:  0.13334772529638922
iteration : 13971
train acc:  0.9375
train loss:  0.254162460565567
train gradient:  0.12439833470705498
iteration : 13972
train acc:  0.8125
train loss:  0.3477718234062195
train gradient:  0.13685759032861977
iteration : 13973
train acc:  0.90625
train loss:  0.2414107322692871
train gradient:  0.09843365836721304
iteration : 13974
train acc:  0.8671875
train loss:  0.2617206871509552
train gradient:  0.11474518061919571
iteration : 13975
train acc:  0.8203125
train loss:  0.40468841791152954
train gradient:  0.22635824534710589
iteration : 13976
train acc:  0.8515625
train loss:  0.32464486360549927
train gradient:  0.12462378840597557
iteration : 13977
train acc:  0.8828125
train loss:  0.2988002896308899
train gradient:  0.10037551807522092
iteration : 13978
train acc:  0.859375
train loss:  0.38584864139556885
train gradient:  0.20522586731984935
iteration : 13979
train acc:  0.828125
train loss:  0.39137589931488037
train gradient:  0.13100386614620635
iteration : 13980
train acc:  0.8515625
train loss:  0.36040371656417847
train gradient:  0.14623595935727685
iteration : 13981
train acc:  0.875
train loss:  0.3140087127685547
train gradient:  0.24505511347562298
iteration : 13982
train acc:  0.8671875
train loss:  0.31111693382263184
train gradient:  0.11522113215159388
iteration : 13983
train acc:  0.796875
train loss:  0.38701754808425903
train gradient:  0.24058565369490395
iteration : 13984
train acc:  0.8984375
train loss:  0.2723284065723419
train gradient:  0.09237107000987994
iteration : 13985
train acc:  0.8359375
train loss:  0.32247090339660645
train gradient:  0.18024037936097076
iteration : 13986
train acc:  0.8671875
train loss:  0.2781871259212494
train gradient:  0.09686445018400526
iteration : 13987
train acc:  0.84375
train loss:  0.30666083097457886
train gradient:  0.14634396356527607
iteration : 13988
train acc:  0.8828125
train loss:  0.284894734621048
train gradient:  0.11043578587315918
iteration : 13989
train acc:  0.875
train loss:  0.30008912086486816
train gradient:  0.11150826226724804
iteration : 13990
train acc:  0.875
train loss:  0.2652633488178253
train gradient:  0.10877821847564718
iteration : 13991
train acc:  0.8359375
train loss:  0.3678814172744751
train gradient:  0.14703633673294092
iteration : 13992
train acc:  0.8828125
train loss:  0.2961856722831726
train gradient:  0.11405061393669463
iteration : 13993
train acc:  0.875
train loss:  0.27130234241485596
train gradient:  0.11453480674667178
iteration : 13994
train acc:  0.859375
train loss:  0.31897228956222534
train gradient:  0.1431131591333319
iteration : 13995
train acc:  0.859375
train loss:  0.3345365822315216
train gradient:  0.11679181370053451
iteration : 13996
train acc:  0.9140625
train loss:  0.2725016176700592
train gradient:  0.11608186176391339
iteration : 13997
train acc:  0.8359375
train loss:  0.42172324657440186
train gradient:  0.2131604263250401
iteration : 13998
train acc:  0.8359375
train loss:  0.3746427595615387
train gradient:  0.18659955604825068
iteration : 13999
train acc:  0.875
train loss:  0.2968994975090027
train gradient:  0.1496287584366141
iteration : 14000
train acc:  0.875
train loss:  0.30520811676979065
train gradient:  0.18966712932134555
iteration : 14001
train acc:  0.8359375
train loss:  0.2959127128124237
train gradient:  0.08964926379432923
iteration : 14002
train acc:  0.921875
train loss:  0.258078008890152
train gradient:  0.07726941796922814
iteration : 14003
train acc:  0.8828125
train loss:  0.27909916639328003
train gradient:  0.11044512081393844
iteration : 14004
train acc:  0.8125
train loss:  0.404043972492218
train gradient:  0.17366536346026046
iteration : 14005
train acc:  0.890625
train loss:  0.2655939757823944
train gradient:  0.10446045876267984
iteration : 14006
train acc:  0.8671875
train loss:  0.3121407628059387
train gradient:  0.1393526194039549
iteration : 14007
train acc:  0.828125
train loss:  0.3632854223251343
train gradient:  0.16675865631445652
iteration : 14008
train acc:  0.8828125
train loss:  0.2684149742126465
train gradient:  0.093065839963628
iteration : 14009
train acc:  0.796875
train loss:  0.41099315881729126
train gradient:  0.2668188365476045
iteration : 14010
train acc:  0.8515625
train loss:  0.37077072262763977
train gradient:  0.15604987433234324
iteration : 14011
train acc:  0.8984375
train loss:  0.27324342727661133
train gradient:  0.09096438559205627
iteration : 14012
train acc:  0.90625
train loss:  0.2479592263698578
train gradient:  0.09468336678976408
iteration : 14013
train acc:  0.7890625
train loss:  0.4168245196342468
train gradient:  0.20349244597842142
iteration : 14014
train acc:  0.875
train loss:  0.35175496339797974
train gradient:  0.16208047495262795
iteration : 14015
train acc:  0.828125
train loss:  0.3519638478755951
train gradient:  0.19885745426249757
iteration : 14016
train acc:  0.84375
train loss:  0.32640424370765686
train gradient:  0.24818672077326753
iteration : 14017
train acc:  0.8359375
train loss:  0.3507501184940338
train gradient:  0.11086821998770707
iteration : 14018
train acc:  0.90625
train loss:  0.23422615230083466
train gradient:  0.08777878821717228
iteration : 14019
train acc:  0.8671875
train loss:  0.2957797050476074
train gradient:  0.10661578924613913
iteration : 14020
train acc:  0.84375
train loss:  0.3085585832595825
train gradient:  0.15458482671263052
iteration : 14021
train acc:  0.8125
train loss:  0.40542906522750854
train gradient:  0.24513888730376368
iteration : 14022
train acc:  0.890625
train loss:  0.23888471722602844
train gradient:  0.09006650557608599
iteration : 14023
train acc:  0.875
train loss:  0.3007763624191284
train gradient:  0.11164763053942835
iteration : 14024
train acc:  0.828125
train loss:  0.37548142671585083
train gradient:  0.2249286364906495
iteration : 14025
train acc:  0.8359375
train loss:  0.3521634638309479
train gradient:  0.1325274539034868
iteration : 14026
train acc:  0.8671875
train loss:  0.33347493410110474
train gradient:  0.18460643596865436
iteration : 14027
train acc:  0.828125
train loss:  0.393060564994812
train gradient:  0.24119911048451093
iteration : 14028
train acc:  0.8984375
train loss:  0.25935879349708557
train gradient:  0.09451462944916458
iteration : 14029
train acc:  0.8671875
train loss:  0.3295789957046509
train gradient:  0.14177776330158587
iteration : 14030
train acc:  0.8203125
train loss:  0.3405907154083252
train gradient:  0.16232423566143814
iteration : 14031
train acc:  0.8828125
train loss:  0.34264081716537476
train gradient:  0.19592872289469013
iteration : 14032
train acc:  0.890625
train loss:  0.30141735076904297
train gradient:  0.11474380737305015
iteration : 14033
train acc:  0.90625
train loss:  0.2868449091911316
train gradient:  0.10573322363666234
iteration : 14034
train acc:  0.84375
train loss:  0.3786647915840149
train gradient:  0.15757258885123043
iteration : 14035
train acc:  0.8984375
train loss:  0.2738298773765564
train gradient:  0.09017243464384686
iteration : 14036
train acc:  0.8828125
train loss:  0.2641468644142151
train gradient:  0.0963303101080897
iteration : 14037
train acc:  0.8515625
train loss:  0.38475871086120605
train gradient:  0.18129146338581542
iteration : 14038
train acc:  0.8671875
train loss:  0.334221214056015
train gradient:  0.1319732271689954
iteration : 14039
train acc:  0.859375
train loss:  0.3626856207847595
train gradient:  0.14950287447343513
iteration : 14040
train acc:  0.8515625
train loss:  0.32725954055786133
train gradient:  0.14220770319078155
iteration : 14041
train acc:  0.8046875
train loss:  0.35904327034950256
train gradient:  0.17353418166085718
iteration : 14042
train acc:  0.8828125
train loss:  0.26921287178993225
train gradient:  0.10116281349798185
iteration : 14043
train acc:  0.875
train loss:  0.2690505385398865
train gradient:  0.08652466579766428
iteration : 14044
train acc:  0.8359375
train loss:  0.3438442349433899
train gradient:  0.14027827963802258
iteration : 14045
train acc:  0.8671875
train loss:  0.3322291672229767
train gradient:  0.166828622108823
iteration : 14046
train acc:  0.8671875
train loss:  0.3174290955066681
train gradient:  0.11745997260037222
iteration : 14047
train acc:  0.921875
train loss:  0.23101559281349182
train gradient:  0.08628823188447599
iteration : 14048
train acc:  0.875
train loss:  0.2604309618473053
train gradient:  0.08329357913905569
iteration : 14049
train acc:  0.9140625
train loss:  0.2395085245370865
train gradient:  0.0939460246141994
iteration : 14050
train acc:  0.9140625
train loss:  0.22834020853042603
train gradient:  0.09980855067336722
iteration : 14051
train acc:  0.8515625
train loss:  0.31698668003082275
train gradient:  0.08080958694009502
iteration : 14052
train acc:  0.8828125
train loss:  0.28607702255249023
train gradient:  0.09205328121555405
iteration : 14053
train acc:  0.828125
train loss:  0.41263967752456665
train gradient:  0.22184246153675333
iteration : 14054
train acc:  0.8359375
train loss:  0.30918675661087036
train gradient:  0.09585732770002789
iteration : 14055
train acc:  0.8671875
train loss:  0.33230060338974
train gradient:  0.13777458319943736
iteration : 14056
train acc:  0.8359375
train loss:  0.3567025661468506
train gradient:  0.21605565031683233
iteration : 14057
train acc:  0.8671875
train loss:  0.2970826029777527
train gradient:  0.10621690797866866
iteration : 14058
train acc:  0.8671875
train loss:  0.2913522720336914
train gradient:  0.10604660751286449
iteration : 14059
train acc:  0.8671875
train loss:  0.3344666361808777
train gradient:  0.11981611606908135
iteration : 14060
train acc:  0.8515625
train loss:  0.3452033996582031
train gradient:  0.22342950726540317
iteration : 14061
train acc:  0.9140625
train loss:  0.23746486008167267
train gradient:  0.07241366516856425
iteration : 14062
train acc:  0.84375
train loss:  0.3776344954967499
train gradient:  0.19360539977958557
iteration : 14063
train acc:  0.84375
train loss:  0.34544238448143005
train gradient:  0.14519271826446573
iteration : 14064
train acc:  0.9140625
train loss:  0.23196664452552795
train gradient:  0.0881838584213374
iteration : 14065
train acc:  0.84375
train loss:  0.34151649475097656
train gradient:  0.11563961357040524
iteration : 14066
train acc:  0.8515625
train loss:  0.3640051484107971
train gradient:  0.17112578887041083
iteration : 14067
train acc:  0.90625
train loss:  0.2730737328529358
train gradient:  0.16305716957177963
iteration : 14068
train acc:  0.8671875
train loss:  0.3099653720855713
train gradient:  0.14499276350578735
iteration : 14069
train acc:  0.890625
train loss:  0.265326589345932
train gradient:  0.10728969813149815
iteration : 14070
train acc:  0.8515625
train loss:  0.294158935546875
train gradient:  0.11600568474979778
iteration : 14071
train acc:  0.84375
train loss:  0.33504432439804077
train gradient:  0.12683445303432336
iteration : 14072
train acc:  0.875
train loss:  0.32116714119911194
train gradient:  0.14263226676133228
iteration : 14073
train acc:  0.8515625
train loss:  0.34788694977760315
train gradient:  0.12215436478410668
iteration : 14074
train acc:  0.8203125
train loss:  0.36392420530319214
train gradient:  0.21644093263821101
iteration : 14075
train acc:  0.78125
train loss:  0.47013187408447266
train gradient:  0.25216101944923097
iteration : 14076
train acc:  0.8828125
train loss:  0.27702757716178894
train gradient:  0.10493792952146963
iteration : 14077
train acc:  0.8203125
train loss:  0.35532867908477783
train gradient:  0.13045514765417004
iteration : 14078
train acc:  0.8671875
train loss:  0.27339768409729004
train gradient:  0.09148707974880887
iteration : 14079
train acc:  0.8359375
train loss:  0.38137438893318176
train gradient:  0.16947641497250077
iteration : 14080
train acc:  0.8828125
train loss:  0.2903329133987427
train gradient:  0.11504250465839816
iteration : 14081
train acc:  0.8046875
train loss:  0.3846803903579712
train gradient:  0.16521196712103187
iteration : 14082
train acc:  0.828125
train loss:  0.384731650352478
train gradient:  0.17107505444779714
iteration : 14083
train acc:  0.8671875
train loss:  0.3214949071407318
train gradient:  0.12016532888445655
iteration : 14084
train acc:  0.8671875
train loss:  0.30130553245544434
train gradient:  0.0988926904444005
iteration : 14085
train acc:  0.84375
train loss:  0.33861440420150757
train gradient:  0.16439269618395375
iteration : 14086
train acc:  0.8671875
train loss:  0.31213778257369995
train gradient:  0.1368439437370211
iteration : 14087
train acc:  0.875
train loss:  0.3205101191997528
train gradient:  0.11316520011823394
iteration : 14088
train acc:  0.8671875
train loss:  0.2941882610321045
train gradient:  0.1219075532898478
iteration : 14089
train acc:  0.8671875
train loss:  0.30941253900527954
train gradient:  0.11793997210269018
iteration : 14090
train acc:  0.796875
train loss:  0.4077067971229553
train gradient:  0.23818104856634742
iteration : 14091
train acc:  0.875
train loss:  0.27498459815979004
train gradient:  0.08367260384186018
iteration : 14092
train acc:  0.8671875
train loss:  0.3021267056465149
train gradient:  0.11061049590899602
iteration : 14093
train acc:  0.8984375
train loss:  0.31103742122650146
train gradient:  0.10878621408607185
iteration : 14094
train acc:  0.859375
train loss:  0.31020036339759827
train gradient:  0.10668449061466928
iteration : 14095
train acc:  0.8984375
train loss:  0.2807075083255768
train gradient:  0.0762573854688309
iteration : 14096
train acc:  0.8515625
train loss:  0.3547714352607727
train gradient:  0.16918971041530356
iteration : 14097
train acc:  0.84375
train loss:  0.3636302351951599
train gradient:  0.13391234683779596
iteration : 14098
train acc:  0.8515625
train loss:  0.3965311050415039
train gradient:  0.1330524143314441
iteration : 14099
train acc:  0.8671875
train loss:  0.33695995807647705
train gradient:  0.12729864207229796
iteration : 14100
train acc:  0.9296875
train loss:  0.23448926210403442
train gradient:  0.09027150614602725
iteration : 14101
train acc:  0.8515625
train loss:  0.31158486008644104
train gradient:  0.11741336932742685
iteration : 14102
train acc:  0.875
train loss:  0.2615034282207489
train gradient:  0.06382455503254443
iteration : 14103
train acc:  0.8671875
train loss:  0.26320815086364746
train gradient:  0.09682134453264919
iteration : 14104
train acc:  0.8203125
train loss:  0.34429121017456055
train gradient:  0.154397854384811
iteration : 14105
train acc:  0.8125
train loss:  0.36541682481765747
train gradient:  0.24328097745023775
iteration : 14106
train acc:  0.828125
train loss:  0.3175133168697357
train gradient:  0.12601234776687487
iteration : 14107
train acc:  0.8203125
train loss:  0.3326065242290497
train gradient:  0.1379122303153335
iteration : 14108
train acc:  0.875
train loss:  0.2545086443424225
train gradient:  0.12646801772664243
iteration : 14109
train acc:  0.8984375
train loss:  0.25701624155044556
train gradient:  0.08239590489780733
iteration : 14110
train acc:  0.828125
train loss:  0.35412442684173584
train gradient:  0.16525028159072178
iteration : 14111
train acc:  0.828125
train loss:  0.3332168459892273
train gradient:  0.1062667258159527
iteration : 14112
train acc:  0.90625
train loss:  0.2889834940433502
train gradient:  0.12038386934714532
iteration : 14113
train acc:  0.7734375
train loss:  0.39420005679130554
train gradient:  0.257497133684689
iteration : 14114
train acc:  0.859375
train loss:  0.3280843496322632
train gradient:  0.17536760682178879
iteration : 14115
train acc:  0.9140625
train loss:  0.25146445631980896
train gradient:  0.0947387643353778
iteration : 14116
train acc:  0.875
train loss:  0.30565688014030457
train gradient:  0.14716317086179423
iteration : 14117
train acc:  0.90625
train loss:  0.25304922461509705
train gradient:  0.1270322644108508
iteration : 14118
train acc:  0.84375
train loss:  0.2948739528656006
train gradient:  0.10412806410249406
iteration : 14119
train acc:  0.8828125
train loss:  0.2592509388923645
train gradient:  0.06961844688819853
iteration : 14120
train acc:  0.8046875
train loss:  0.3859625458717346
train gradient:  0.17399813610866993
iteration : 14121
train acc:  0.875
train loss:  0.3220739960670471
train gradient:  0.19479203172743784
iteration : 14122
train acc:  0.828125
train loss:  0.3731740117073059
train gradient:  0.15001091209122053
iteration : 14123
train acc:  0.875
train loss:  0.3340873122215271
train gradient:  0.12426364327945243
iteration : 14124
train acc:  0.796875
train loss:  0.3463647663593292
train gradient:  0.18702413770632592
iteration : 14125
train acc:  0.8984375
train loss:  0.23826146125793457
train gradient:  0.07294702975692512
iteration : 14126
train acc:  0.8359375
train loss:  0.3282844126224518
train gradient:  0.13355913924850538
iteration : 14127
train acc:  0.8671875
train loss:  0.25804126262664795
train gradient:  0.0838907045696694
iteration : 14128
train acc:  0.8671875
train loss:  0.34610670804977417
train gradient:  0.17644485647398617
iteration : 14129
train acc:  0.8046875
train loss:  0.39536574482917786
train gradient:  0.17016323468786976
iteration : 14130
train acc:  0.8125
train loss:  0.3651376962661743
train gradient:  0.19863005922660804
iteration : 14131
train acc:  0.859375
train loss:  0.33658990263938904
train gradient:  0.1694943074901203
iteration : 14132
train acc:  0.8359375
train loss:  0.35367661714553833
train gradient:  0.19338510582814278
iteration : 14133
train acc:  0.8671875
train loss:  0.33528560400009155
train gradient:  0.18014263875885428
iteration : 14134
train acc:  0.875
train loss:  0.3056713342666626
train gradient:  0.13819035856304696
iteration : 14135
train acc:  0.8203125
train loss:  0.35138362646102905
train gradient:  0.22087140628233562
iteration : 14136
train acc:  0.890625
train loss:  0.3174784481525421
train gradient:  0.0945958823240146
iteration : 14137
train acc:  0.890625
train loss:  0.2577219605445862
train gradient:  0.08896299530418415
iteration : 14138
train acc:  0.8359375
train loss:  0.33960360288619995
train gradient:  0.15576382158265578
iteration : 14139
train acc:  0.9296875
train loss:  0.21559199690818787
train gradient:  0.08892306883997173
iteration : 14140
train acc:  0.8984375
train loss:  0.2653762698173523
train gradient:  0.12069767489139473
iteration : 14141
train acc:  0.875
train loss:  0.2779160737991333
train gradient:  0.09516377889814846
iteration : 14142
train acc:  0.9140625
train loss:  0.27090924978256226
train gradient:  0.11567121159355438
iteration : 14143
train acc:  0.8515625
train loss:  0.2946939170360565
train gradient:  0.08299115363650204
iteration : 14144
train acc:  0.875
train loss:  0.2650502324104309
train gradient:  0.1114912316137403
iteration : 14145
train acc:  0.8828125
train loss:  0.3294193744659424
train gradient:  0.17247788455071997
iteration : 14146
train acc:  0.859375
train loss:  0.30850881338119507
train gradient:  0.1262426099490655
iteration : 14147
train acc:  0.859375
train loss:  0.388294517993927
train gradient:  0.2022342095567612
iteration : 14148
train acc:  0.8671875
train loss:  0.28518229722976685
train gradient:  0.09452091151930224
iteration : 14149
train acc:  0.8515625
train loss:  0.3724656105041504
train gradient:  0.19064401107818835
iteration : 14150
train acc:  0.84375
train loss:  0.33751413226127625
train gradient:  0.1768445861623359
iteration : 14151
train acc:  0.8671875
train loss:  0.29514575004577637
train gradient:  0.08632858675440379
iteration : 14152
train acc:  0.8828125
train loss:  0.24438047409057617
train gradient:  0.10222478634326508
iteration : 14153
train acc:  0.828125
train loss:  0.36598822474479675
train gradient:  0.16191101240314493
iteration : 14154
train acc:  0.8359375
train loss:  0.3910785913467407
train gradient:  0.16561570542905701
iteration : 14155
train acc:  0.8515625
train loss:  0.3397325277328491
train gradient:  0.10307734955591584
iteration : 14156
train acc:  0.8515625
train loss:  0.31501710414886475
train gradient:  0.13910111759069946
iteration : 14157
train acc:  0.84375
train loss:  0.37671422958374023
train gradient:  0.1970215136305164
iteration : 14158
train acc:  0.8359375
train loss:  0.3632727861404419
train gradient:  0.1312498002494309
iteration : 14159
train acc:  0.875
train loss:  0.32697370648384094
train gradient:  0.13401398288064
iteration : 14160
train acc:  0.859375
train loss:  0.33698853850364685
train gradient:  0.1331653930535251
iteration : 14161
train acc:  0.875
train loss:  0.31758779287338257
train gradient:  0.18497445533350756
iteration : 14162
train acc:  0.8828125
train loss:  0.30653563141822815
train gradient:  0.08998962297813597
iteration : 14163
train acc:  0.890625
train loss:  0.29188165068626404
train gradient:  0.08339942418573827
iteration : 14164
train acc:  0.8984375
train loss:  0.29058122634887695
train gradient:  0.11562856849566631
iteration : 14165
train acc:  0.84375
train loss:  0.3008155822753906
train gradient:  0.1325071206438176
iteration : 14166
train acc:  0.8828125
train loss:  0.2592523694038391
train gradient:  0.08721750042650098
iteration : 14167
train acc:  0.8984375
train loss:  0.2521111071109772
train gradient:  0.07097834207631791
iteration : 14168
train acc:  0.8671875
train loss:  0.3368227779865265
train gradient:  0.12993105123093318
iteration : 14169
train acc:  0.859375
train loss:  0.34268707036972046
train gradient:  0.14888242464945178
iteration : 14170
train acc:  0.8359375
train loss:  0.3578503727912903
train gradient:  0.15279176591780996
iteration : 14171
train acc:  0.84375
train loss:  0.35495203733444214
train gradient:  0.13243006689240575
iteration : 14172
train acc:  0.8671875
train loss:  0.285871684551239
train gradient:  0.09001993704225342
iteration : 14173
train acc:  0.8828125
train loss:  0.35809826850891113
train gradient:  0.14197073418035697
iteration : 14174
train acc:  0.8984375
train loss:  0.2979070544242859
train gradient:  0.08570591360050818
iteration : 14175
train acc:  0.8984375
train loss:  0.2529466450214386
train gradient:  0.07418930780356489
iteration : 14176
train acc:  0.875
train loss:  0.29502594470977783
train gradient:  0.08280703401153834
iteration : 14177
train acc:  0.8671875
train loss:  0.3199193477630615
train gradient:  0.12295424771327168
iteration : 14178
train acc:  0.875
train loss:  0.27683305740356445
train gradient:  0.13406099831079568
iteration : 14179
train acc:  0.859375
train loss:  0.2824106812477112
train gradient:  0.08241196580537485
iteration : 14180
train acc:  0.8984375
train loss:  0.28012287616729736
train gradient:  0.07912878197541444
iteration : 14181
train acc:  0.8359375
train loss:  0.37090957164764404
train gradient:  0.13608817899948278
iteration : 14182
train acc:  0.8984375
train loss:  0.2902272343635559
train gradient:  0.09609597877915059
iteration : 14183
train acc:  0.859375
train loss:  0.309642493724823
train gradient:  0.12019778369143015
iteration : 14184
train acc:  0.859375
train loss:  0.2973248362541199
train gradient:  0.13072764342564946
iteration : 14185
train acc:  0.8359375
train loss:  0.3678187131881714
train gradient:  0.14220907505546898
iteration : 14186
train acc:  0.7222222222222222
train loss:  0.5234276056289673
train gradient:  3.294657203143749
val acc:  0.8662204644559375
val f1:  0.8674756891424897
val confusion matrix:  [[84484 14126]
 [12258 86352]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.9296875
train loss:  0.27375009655952454
train gradient:  0.10009298678435564
iteration : 1
train acc:  0.875
train loss:  0.3117949962615967
train gradient:  0.11615722626426045
iteration : 2
train acc:  0.8671875
train loss:  0.3329949676990509
train gradient:  0.13154125109040116
iteration : 3
train acc:  0.875
train loss:  0.34935927391052246
train gradient:  0.17248024884685004
iteration : 4
train acc:  0.8515625
train loss:  0.36672860383987427
train gradient:  0.20808517874552407
iteration : 5
train acc:  0.8515625
train loss:  0.2995598614215851
train gradient:  0.12434125511528481
iteration : 6
train acc:  0.859375
train loss:  0.25440713763237
train gradient:  0.09301600073152949
iteration : 7
train acc:  0.796875
train loss:  0.48345470428466797
train gradient:  0.2578416351839231
iteration : 8
train acc:  0.8203125
train loss:  0.3228294849395752
train gradient:  0.15120865026385838
iteration : 9
train acc:  0.8671875
train loss:  0.3223249316215515
train gradient:  0.13320642718169443
iteration : 10
train acc:  0.8515625
train loss:  0.33127209544181824
train gradient:  0.1490148210059306
iteration : 11
train acc:  0.875
train loss:  0.2850273847579956
train gradient:  0.12186852191105162
iteration : 12
train acc:  0.8515625
train loss:  0.3238796293735504
train gradient:  0.15560526257525747
iteration : 13
train acc:  0.8671875
train loss:  0.33183297514915466
train gradient:  0.1612815568240008
iteration : 14
train acc:  0.8671875
train loss:  0.3298156261444092
train gradient:  0.11963988998210079
iteration : 15
train acc:  0.875
train loss:  0.34269610047340393
train gradient:  0.10054441204348923
iteration : 16
train acc:  0.84375
train loss:  0.3919059634208679
train gradient:  0.17987549737096442
iteration : 17
train acc:  0.8046875
train loss:  0.4019182324409485
train gradient:  0.14672980716766404
iteration : 18
train acc:  0.8984375
train loss:  0.24279308319091797
train gradient:  0.10057957266857856
iteration : 19
train acc:  0.8359375
train loss:  0.31079867482185364
train gradient:  0.09800919210996198
iteration : 20
train acc:  0.859375
train loss:  0.29937559366226196
train gradient:  0.12362725973202643
iteration : 21
train acc:  0.8671875
train loss:  0.292156457901001
train gradient:  0.18341030438550543
iteration : 22
train acc:  0.890625
train loss:  0.284895658493042
train gradient:  0.13777951554270795
iteration : 23
train acc:  0.8984375
train loss:  0.24639944732189178
train gradient:  0.08468590057514228
iteration : 24
train acc:  0.8671875
train loss:  0.2895950973033905
train gradient:  0.16183214306598787
iteration : 25
train acc:  0.84375
train loss:  0.3476434350013733
train gradient:  0.10423285483685617
iteration : 26
train acc:  0.875
train loss:  0.25788256525993347
train gradient:  0.09913229831126057
iteration : 27
train acc:  0.921875
train loss:  0.20993837714195251
train gradient:  0.09061170270517938
iteration : 28
train acc:  0.8203125
train loss:  0.3519793450832367
train gradient:  0.14739955245701392
iteration : 29
train acc:  0.8671875
train loss:  0.305423378944397
train gradient:  0.09179159702214351
iteration : 30
train acc:  0.8359375
train loss:  0.4114660322666168
train gradient:  0.22884415573828198
iteration : 31
train acc:  0.8828125
train loss:  0.30022603273391724
train gradient:  0.1570902198034998
iteration : 32
train acc:  0.8203125
train loss:  0.3649035096168518
train gradient:  0.1578207066441915
iteration : 33
train acc:  0.8359375
train loss:  0.3516702353954315
train gradient:  0.14757692204487066
iteration : 34
train acc:  0.8828125
train loss:  0.2852933406829834
train gradient:  0.11751599502373355
iteration : 35
train acc:  0.84375
train loss:  0.3682079315185547
train gradient:  0.18513657946836293
iteration : 36
train acc:  0.9140625
train loss:  0.25728917121887207
train gradient:  0.08867145274876694
iteration : 37
train acc:  0.875
train loss:  0.3021286129951477
train gradient:  0.13644729693979307
iteration : 38
train acc:  0.875
train loss:  0.29000356793403625
train gradient:  0.10801402222007203
iteration : 39
train acc:  0.890625
train loss:  0.2727442979812622
train gradient:  0.1520243243231016
iteration : 40
train acc:  0.8359375
train loss:  0.3444381356239319
train gradient:  0.15436501736044522
iteration : 41
train acc:  0.796875
train loss:  0.38101041316986084
train gradient:  0.20355160204135925
iteration : 42
train acc:  0.8828125
train loss:  0.2788304090499878
train gradient:  0.09922247731277177
iteration : 43
train acc:  0.84375
train loss:  0.35610848665237427
train gradient:  0.17778067466350372
iteration : 44
train acc:  0.8515625
train loss:  0.3491814434528351
train gradient:  0.15138627854687103
iteration : 45
train acc:  0.7890625
train loss:  0.4518558979034424
train gradient:  0.4284951869094318
iteration : 46
train acc:  0.9296875
train loss:  0.264178991317749
train gradient:  0.10281058417788065
iteration : 47
train acc:  0.8828125
train loss:  0.26681816577911377
train gradient:  0.0770546633997546
iteration : 48
train acc:  0.8671875
train loss:  0.3531720042228699
train gradient:  0.133002465418027
iteration : 49
train acc:  0.8671875
train loss:  0.2925409972667694
train gradient:  0.12092176628328925
iteration : 50
train acc:  0.875
train loss:  0.252407044172287
train gradient:  0.08215666613072077
iteration : 51
train acc:  0.8203125
train loss:  0.33266428112983704
train gradient:  0.26005865301586506
iteration : 52
train acc:  0.90625
train loss:  0.2083359807729721
train gradient:  0.08391916520324351
iteration : 53
train acc:  0.859375
train loss:  0.3215476870536804
train gradient:  0.10353780702408269
iteration : 54
train acc:  0.9296875
train loss:  0.22289691865444183
train gradient:  0.07641225897971352
iteration : 55
train acc:  0.8515625
train loss:  0.3113201856613159
train gradient:  0.12772922655994087
iteration : 56
train acc:  0.90625
train loss:  0.2617286443710327
train gradient:  0.10640998494386159
iteration : 57
train acc:  0.859375
train loss:  0.31865787506103516
train gradient:  0.10404836680334252
iteration : 58
train acc:  0.8359375
train loss:  0.30570188164711
train gradient:  0.16456417978694726
iteration : 59
train acc:  0.8515625
train loss:  0.33880677819252014
train gradient:  0.1948162177563793
iteration : 60
train acc:  0.890625
train loss:  0.26527807116508484
train gradient:  0.12955055815749417
iteration : 61
train acc:  0.875
train loss:  0.3558366596698761
train gradient:  0.12426687910647198
iteration : 62
train acc:  0.890625
train loss:  0.24632515013217926
train gradient:  0.11658567963858707
iteration : 63
train acc:  0.875
train loss:  0.3907468318939209
train gradient:  0.16478888318924362
iteration : 64
train acc:  0.7421875
train loss:  0.4745590388774872
train gradient:  0.24484539223708782
iteration : 65
train acc:  0.8203125
train loss:  0.379427433013916
train gradient:  0.19627280740699393
iteration : 66
train acc:  0.8125
train loss:  0.4172966182231903
train gradient:  0.1863428178883728
iteration : 67
train acc:  0.8125
train loss:  0.4264695644378662
train gradient:  0.21508834492381385
iteration : 68
train acc:  0.8359375
train loss:  0.364492267370224
train gradient:  0.2463188206285679
iteration : 69
train acc:  0.8046875
train loss:  0.41322842240333557
train gradient:  0.20142421986412487
iteration : 70
train acc:  0.890625
train loss:  0.3079181909561157
train gradient:  0.09743276753501277
iteration : 71
train acc:  0.8359375
train loss:  0.3966299891471863
train gradient:  0.17806477596692954
iteration : 72
train acc:  0.859375
train loss:  0.3613166809082031
train gradient:  0.24138060547565116
iteration : 73
train acc:  0.84375
train loss:  0.36833491921424866
train gradient:  0.29191231612731505
iteration : 74
train acc:  0.90625
train loss:  0.2686581015586853
train gradient:  0.11733278322244359
iteration : 75
train acc:  0.859375
train loss:  0.3079356551170349
train gradient:  0.1513653473990632
iteration : 76
train acc:  0.859375
train loss:  0.3030661344528198
train gradient:  0.14654070366875324
iteration : 77
train acc:  0.8359375
train loss:  0.3156660795211792
train gradient:  0.1674298288799359
iteration : 78
train acc:  0.890625
train loss:  0.2109626829624176
train gradient:  0.08201823288017664
iteration : 79
train acc:  0.8359375
train loss:  0.3392065167427063
train gradient:  0.1179477158132531
iteration : 80
train acc:  0.875
train loss:  0.2934606671333313
train gradient:  0.12523005318145958
iteration : 81
train acc:  0.875
train loss:  0.3054932951927185
train gradient:  0.20208468637372923
iteration : 82
train acc:  0.9140625
train loss:  0.23524664342403412
train gradient:  0.05846218072991825
iteration : 83
train acc:  0.84375
train loss:  0.2887077331542969
train gradient:  0.11112359442165921
iteration : 84
train acc:  0.8671875
train loss:  0.343250036239624
train gradient:  0.1448505442487652
iteration : 85
train acc:  0.859375
train loss:  0.27106621861457825
train gradient:  0.0956064838857208
iteration : 86
train acc:  0.8359375
train loss:  0.3546833395957947
train gradient:  0.19118205210891026
iteration : 87
train acc:  0.828125
train loss:  0.35881146788597107
train gradient:  0.12713167435832373
iteration : 88
train acc:  0.8828125
train loss:  0.2804272770881653
train gradient:  0.13740060750940883
iteration : 89
train acc:  0.859375
train loss:  0.33991312980651855
train gradient:  0.10911379709428255
iteration : 90
train acc:  0.859375
train loss:  0.3206697404384613
train gradient:  0.16323939820719024
iteration : 91
train acc:  0.8515625
train loss:  0.3381766378879547
train gradient:  0.13593167068587494
iteration : 92
train acc:  0.8671875
train loss:  0.3093618154525757
train gradient:  0.13553447263611304
iteration : 93
train acc:  0.8984375
train loss:  0.23071914911270142
train gradient:  0.09697127802628443
iteration : 94
train acc:  0.890625
train loss:  0.30200710892677307
train gradient:  0.1030634232195957
iteration : 95
train acc:  0.8828125
train loss:  0.26351919770240784
train gradient:  0.11307655236285089
iteration : 96
train acc:  0.8515625
train loss:  0.3586471676826477
train gradient:  0.17125276699896624
iteration : 97
train acc:  0.875
train loss:  0.28350648283958435
train gradient:  0.1149841879456322
iteration : 98
train acc:  0.875
train loss:  0.3502647280693054
train gradient:  0.15261396006518452
iteration : 99
train acc:  0.8671875
train loss:  0.24821023643016815
train gradient:  0.07396452117993028
iteration : 100
train acc:  0.828125
train loss:  0.307570219039917
train gradient:  0.09846391582593535
iteration : 101
train acc:  0.796875
train loss:  0.37414494156837463
train gradient:  0.17394763288358997
iteration : 102
train acc:  0.828125
train loss:  0.33456581830978394
train gradient:  0.11288441567858226
iteration : 103
train acc:  0.859375
train loss:  0.2651927173137665
train gradient:  0.08273989476047999
iteration : 104
train acc:  0.8125
train loss:  0.3303123712539673
train gradient:  0.1388358217127858
iteration : 105
train acc:  0.8203125
train loss:  0.42486000061035156
train gradient:  0.21515821399573654
iteration : 106
train acc:  0.890625
train loss:  0.2816246449947357
train gradient:  0.1308868291380175
iteration : 107
train acc:  0.9140625
train loss:  0.2597450613975525
train gradient:  0.09296810403139825
iteration : 108
train acc:  0.859375
train loss:  0.2913702130317688
train gradient:  0.09964351585351576
iteration : 109
train acc:  0.8671875
train loss:  0.31432604789733887
train gradient:  0.11531741129259436
iteration : 110
train acc:  0.890625
train loss:  0.3214186131954193
train gradient:  0.1540548468349952
iteration : 111
train acc:  0.90625
train loss:  0.2928375005722046
train gradient:  0.13324448422395574
iteration : 112
train acc:  0.9296875
train loss:  0.264779269695282
train gradient:  0.07835821090795297
iteration : 113
train acc:  0.8203125
train loss:  0.33369210362434387
train gradient:  0.12740101533380196
iteration : 114
train acc:  0.8828125
train loss:  0.2487536072731018
train gradient:  0.08719630369591838
iteration : 115
train acc:  0.84375
train loss:  0.35654765367507935
train gradient:  0.29008685956662195
iteration : 116
train acc:  0.9296875
train loss:  0.18882256746292114
train gradient:  0.0852923534227536
iteration : 117
train acc:  0.8828125
train loss:  0.2415708303451538
train gradient:  0.08234226289879669
iteration : 118
train acc:  0.8671875
train loss:  0.31278809905052185
train gradient:  0.13636933306208052
iteration : 119
train acc:  0.84375
train loss:  0.3155753016471863
train gradient:  0.1399635568858722
iteration : 120
train acc:  0.8046875
train loss:  0.3675542175769806
train gradient:  0.1534678127264311
iteration : 121
train acc:  0.8828125
train loss:  0.29019778966903687
train gradient:  0.11510465584517393
iteration : 122
train acc:  0.9140625
train loss:  0.2563852369785309
train gradient:  0.08477426213922679
iteration : 123
train acc:  0.859375
train loss:  0.28418833017349243
train gradient:  0.14558904079763915
iteration : 124
train acc:  0.875
train loss:  0.29315513372421265
train gradient:  0.12408067391367329
iteration : 125
train acc:  0.875
train loss:  0.2928064465522766
train gradient:  0.19097292516374365
iteration : 126
train acc:  0.8828125
train loss:  0.3098234534263611
train gradient:  0.11601212473063255
iteration : 127
train acc:  0.8828125
train loss:  0.2656211853027344
train gradient:  0.11005654554246642
iteration : 128
train acc:  0.890625
train loss:  0.28550708293914795
train gradient:  0.1413231322487291
iteration : 129
train acc:  0.8671875
train loss:  0.25609102845191956
train gradient:  0.09359828226990653
iteration : 130
train acc:  0.859375
train loss:  0.3568705916404724
train gradient:  0.13776554495343976
iteration : 131
train acc:  0.8515625
train loss:  0.3461267650127411
train gradient:  0.1998163997994018
iteration : 132
train acc:  0.921875
train loss:  0.24150151014328003
train gradient:  0.08232077517543633
iteration : 133
train acc:  0.8828125
train loss:  0.27671778202056885
train gradient:  0.1304691987365175
iteration : 134
train acc:  0.8515625
train loss:  0.35215526819229126
train gradient:  0.14643542027173945
iteration : 135
train acc:  0.859375
train loss:  0.27820825576782227
train gradient:  0.11655720375560105
iteration : 136
train acc:  0.8828125
train loss:  0.30165770649909973
train gradient:  0.1151415936602397
iteration : 137
train acc:  0.8515625
train loss:  0.3194679915904999
train gradient:  0.12514382862316786
iteration : 138
train acc:  0.8828125
train loss:  0.3048427700996399
train gradient:  0.10617946219929886
iteration : 139
train acc:  0.890625
train loss:  0.27696722745895386
train gradient:  0.16047863870437495
iteration : 140
train acc:  0.890625
train loss:  0.28826645016670227
train gradient:  0.1046205245322267
iteration : 141
train acc:  0.8671875
train loss:  0.2755209803581238
train gradient:  0.148728778627149
iteration : 142
train acc:  0.859375
train loss:  0.32243841886520386
train gradient:  0.1137699469289142
iteration : 143
train acc:  0.8984375
train loss:  0.28910499811172485
train gradient:  0.14274389372288065
iteration : 144
train acc:  0.8203125
train loss:  0.42667156457901
train gradient:  0.19337537687169157
iteration : 145
train acc:  0.8828125
train loss:  0.23887039721012115
train gradient:  0.09793346413824798
iteration : 146
train acc:  0.859375
train loss:  0.2841935455799103
train gradient:  0.1319201962323426
iteration : 147
train acc:  0.859375
train loss:  0.3095555305480957
train gradient:  0.1336857554552447
iteration : 148
train acc:  0.90625
train loss:  0.24866443872451782
train gradient:  0.08338580894062961
iteration : 149
train acc:  0.8671875
train loss:  0.29499301314353943
train gradient:  0.11065233678224078
iteration : 150
train acc:  0.84375
train loss:  0.3628160059452057
train gradient:  0.17691202008044224
iteration : 151
train acc:  0.8046875
train loss:  0.4238637089729309
train gradient:  0.24527078637424948
iteration : 152
train acc:  0.875
train loss:  0.2746235728263855
train gradient:  0.09834177187038577
iteration : 153
train acc:  0.8203125
train loss:  0.3756217062473297
train gradient:  0.17033922164358162
iteration : 154
train acc:  0.84375
train loss:  0.4059150218963623
train gradient:  0.1459378752037641
iteration : 155
train acc:  0.890625
train loss:  0.2366693913936615
train gradient:  0.10354977725205455
iteration : 156
train acc:  0.8515625
train loss:  0.29156017303466797
train gradient:  0.11396491698885726
iteration : 157
train acc:  0.890625
train loss:  0.29385995864868164
train gradient:  0.16871595028478448
iteration : 158
train acc:  0.8515625
train loss:  0.2770141065120697
train gradient:  0.11446169965129997
iteration : 159
train acc:  0.8359375
train loss:  0.308804988861084
train gradient:  0.19828820294740246
iteration : 160
train acc:  0.8515625
train loss:  0.4042704999446869
train gradient:  0.20571053815710905
iteration : 161
train acc:  0.8828125
train loss:  0.3171282410621643
train gradient:  0.10994454867386899
iteration : 162
train acc:  0.875
train loss:  0.2813551425933838
train gradient:  0.158716069575003
iteration : 163
train acc:  0.8671875
train loss:  0.35114502906799316
train gradient:  0.13114963544848685
iteration : 164
train acc:  0.828125
train loss:  0.3289971351623535
train gradient:  0.13704836511422977
iteration : 165
train acc:  0.8359375
train loss:  0.37553709745407104
train gradient:  0.18578856902753985
iteration : 166
train acc:  0.8828125
train loss:  0.2767042815685272
train gradient:  0.14979705782659747
iteration : 167
train acc:  0.8671875
train loss:  0.3004046678543091
train gradient:  0.13113680011079926
iteration : 168
train acc:  0.8515625
train loss:  0.3271874785423279
train gradient:  0.18873307095984992
iteration : 169
train acc:  0.8203125
train loss:  0.348998486995697
train gradient:  0.2172022301822737
iteration : 170
train acc:  0.8515625
train loss:  0.3697482645511627
train gradient:  0.24408664336816677
iteration : 171
train acc:  0.84375
train loss:  0.26396703720092773
train gradient:  0.12347850008846646
iteration : 172
train acc:  0.90625
train loss:  0.24277281761169434
train gradient:  0.09959766180816079
iteration : 173
train acc:  0.8359375
train loss:  0.33525505661964417
train gradient:  0.1688579555728293
iteration : 174
train acc:  0.84375
train loss:  0.38046830892562866
train gradient:  0.31495574893575945
iteration : 175
train acc:  0.8984375
train loss:  0.23554180562496185
train gradient:  0.07517452913978925
iteration : 176
train acc:  0.8828125
train loss:  0.26450762152671814
train gradient:  0.14829739757741905
iteration : 177
train acc:  0.8203125
train loss:  0.3928825855255127
train gradient:  0.20225275121538538
iteration : 178
train acc:  0.875
train loss:  0.2927051782608032
train gradient:  0.11620838829285585
iteration : 179
train acc:  0.8984375
train loss:  0.23268559575080872
train gradient:  0.08383048456656805
iteration : 180
train acc:  0.859375
train loss:  0.3079579174518585
train gradient:  0.13575527682871832
iteration : 181
train acc:  0.8671875
train loss:  0.3125753402709961
train gradient:  0.11260526420711434
iteration : 182
train acc:  0.875
train loss:  0.3358609080314636
train gradient:  0.1453627476507543
iteration : 183
train acc:  0.859375
train loss:  0.3707975745201111
train gradient:  0.158671780640452
iteration : 184
train acc:  0.90625
train loss:  0.2702428102493286
train gradient:  0.1376510855914856
iteration : 185
train acc:  0.8828125
train loss:  0.31385332345962524
train gradient:  0.15282849149755767
iteration : 186
train acc:  0.859375
train loss:  0.3524351119995117
train gradient:  0.14919058025054782
iteration : 187
train acc:  0.8515625
train loss:  0.34333372116088867
train gradient:  0.1858202098433246
iteration : 188
train acc:  0.8671875
train loss:  0.3638308644294739
train gradient:  0.1475105055874745
iteration : 189
train acc:  0.875
train loss:  0.3117856979370117
train gradient:  0.12966245055907866
iteration : 190
train acc:  0.8671875
train loss:  0.3212652802467346
train gradient:  0.15495524410819533
iteration : 191
train acc:  0.8359375
train loss:  0.41202935576438904
train gradient:  0.24853128391727192
iteration : 192
train acc:  0.8671875
train loss:  0.2576391100883484
train gradient:  0.11225068347030154
iteration : 193
train acc:  0.8984375
train loss:  0.25449007749557495
train gradient:  0.08236318239364802
iteration : 194
train acc:  0.828125
train loss:  0.4054871201515198
train gradient:  0.2057217399473028
iteration : 195
train acc:  0.84375
train loss:  0.3058156371116638
train gradient:  0.11523748720485964
iteration : 196
train acc:  0.890625
train loss:  0.3274300694465637
train gradient:  0.10445273689849273
iteration : 197
train acc:  0.875
train loss:  0.31473982334136963
train gradient:  0.19414483441328495
iteration : 198
train acc:  0.875
train loss:  0.31346043944358826
train gradient:  0.2909341685574879
iteration : 199
train acc:  0.828125
train loss:  0.32641661167144775
train gradient:  0.17711761229520473
iteration : 200
train acc:  0.90625
train loss:  0.22506728768348694
train gradient:  0.1528572235148163
iteration : 201
train acc:  0.8515625
train loss:  0.29620125889778137
train gradient:  0.0922590181888081
iteration : 202
train acc:  0.84375
train loss:  0.34248316287994385
train gradient:  0.18495232069390352
iteration : 203
train acc:  0.8671875
train loss:  0.3295819163322449
train gradient:  0.18143360652920387
iteration : 204
train acc:  0.8671875
train loss:  0.3697107434272766
train gradient:  0.1503031023499392
iteration : 205
train acc:  0.8671875
train loss:  0.3186655044555664
train gradient:  0.166078940951169
iteration : 206
train acc:  0.828125
train loss:  0.40265512466430664
train gradient:  0.1838647870311518
iteration : 207
train acc:  0.8828125
train loss:  0.33688515424728394
train gradient:  0.1744321079251524
iteration : 208
train acc:  0.8515625
train loss:  0.33976802229881287
train gradient:  0.13913687174880235
iteration : 209
train acc:  0.890625
train loss:  0.29775261878967285
train gradient:  0.14554260616345074
iteration : 210
train acc:  0.796875
train loss:  0.3963141441345215
train gradient:  0.13354722236122613
iteration : 211
train acc:  0.8984375
train loss:  0.2813248634338379
train gradient:  0.14479815902116772
iteration : 212
train acc:  0.8515625
train loss:  0.3427959084510803
train gradient:  0.11126360998891739
iteration : 213
train acc:  0.890625
train loss:  0.2755303978919983
train gradient:  0.11832884147833556
iteration : 214
train acc:  0.8515625
train loss:  0.3035981059074402
train gradient:  0.12301073285192238
iteration : 215
train acc:  0.859375
train loss:  0.33899250626564026
train gradient:  0.13206824762055003
iteration : 216
train acc:  0.8828125
train loss:  0.3030136227607727
train gradient:  0.12417312539342523
iteration : 217
train acc:  0.8125
train loss:  0.368044912815094
train gradient:  0.13722442867143192
iteration : 218
train acc:  0.859375
train loss:  0.30734652280807495
train gradient:  0.11783766353187923
iteration : 219
train acc:  0.8984375
train loss:  0.28009033203125
train gradient:  0.10534941254070013
iteration : 220
train acc:  0.8671875
train loss:  0.276842325925827
train gradient:  0.09795591792983795
iteration : 221
train acc:  0.9140625
train loss:  0.21624363958835602
train gradient:  0.1106800852041927
iteration : 222
train acc:  0.84375
train loss:  0.35372450947761536
train gradient:  0.14129018719122835
iteration : 223
train acc:  0.8671875
train loss:  0.315865159034729
train gradient:  0.12926613122101838
iteration : 224
train acc:  0.828125
train loss:  0.33337539434432983
train gradient:  0.10633773390913814
iteration : 225
train acc:  0.875
train loss:  0.31432610750198364
train gradient:  0.11511533142828607
iteration : 226
train acc:  0.8828125
train loss:  0.2681679129600525
train gradient:  0.10089871660894376
iteration : 227
train acc:  0.796875
train loss:  0.44063282012939453
train gradient:  0.21993727409979727
iteration : 228
train acc:  0.8046875
train loss:  0.41793879866600037
train gradient:  0.24042900784782834
iteration : 229
train acc:  0.890625
train loss:  0.2636529803276062
train gradient:  0.11073762616192814
iteration : 230
train acc:  0.8671875
train loss:  0.30023080110549927
train gradient:  0.12316342254071697
iteration : 231
train acc:  0.8515625
train loss:  0.34325382113456726
train gradient:  0.21763695195380153
iteration : 232
train acc:  0.859375
train loss:  0.3284600079059601
train gradient:  0.15665676649717156
iteration : 233
train acc:  0.859375
train loss:  0.287037193775177
train gradient:  0.11226378907217853
iteration : 234
train acc:  0.828125
train loss:  0.3559027314186096
train gradient:  0.11779357466806598
iteration : 235
train acc:  0.8828125
train loss:  0.27318882942199707
train gradient:  0.10457812397834587
iteration : 236
train acc:  0.84375
train loss:  0.3723808526992798
train gradient:  0.29024791059311567
iteration : 237
train acc:  0.84375
train loss:  0.35452374815940857
train gradient:  0.20369334555329482
iteration : 238
train acc:  0.875
train loss:  0.27464917302131653
train gradient:  0.09092333376662655
iteration : 239
train acc:  0.8828125
train loss:  0.30614960193634033
train gradient:  0.12812732906291205
iteration : 240
train acc:  0.890625
train loss:  0.25216448307037354
train gradient:  0.10406431894884896
iteration : 241
train acc:  0.8125
train loss:  0.36357977986335754
train gradient:  0.2053152310579065
iteration : 242
train acc:  0.9140625
train loss:  0.22014999389648438
train gradient:  0.05583789978159752
iteration : 243
train acc:  0.7734375
train loss:  0.4033178389072418
train gradient:  0.2384658213093943
iteration : 244
train acc:  0.9375
train loss:  0.19551727175712585
train gradient:  0.06357547856246473
iteration : 245
train acc:  0.859375
train loss:  0.288051962852478
train gradient:  0.10496268179937468
iteration : 246
train acc:  0.8984375
train loss:  0.297531396150589
train gradient:  0.10165770981965352
iteration : 247
train acc:  0.9296875
train loss:  0.248663067817688
train gradient:  0.11828374486930031
iteration : 248
train acc:  0.875
train loss:  0.3381219208240509
train gradient:  0.12099483573665624
iteration : 249
train acc:  0.8671875
train loss:  0.3287070393562317
train gradient:  0.20925862477766072
iteration : 250
train acc:  0.875
train loss:  0.3969235420227051
train gradient:  0.17221429679000907
iteration : 251
train acc:  0.859375
train loss:  0.34285083413124084
train gradient:  0.11639321850554189
iteration : 252
train acc:  0.8359375
train loss:  0.39547237753868103
train gradient:  0.21740755972830747
iteration : 253
train acc:  0.859375
train loss:  0.29672372341156006
train gradient:  0.08840298425510151
iteration : 254
train acc:  0.8828125
train loss:  0.3291163444519043
train gradient:  0.18316312456384848
iteration : 255
train acc:  0.8359375
train loss:  0.32007795572280884
train gradient:  0.1616889682950401
iteration : 256
train acc:  0.8671875
train loss:  0.36611413955688477
train gradient:  0.15773763133437504
iteration : 257
train acc:  0.859375
train loss:  0.3461613059043884
train gradient:  0.15090761445840428
iteration : 258
train acc:  0.8984375
train loss:  0.28143981099128723
train gradient:  0.1115250029013568
iteration : 259
train acc:  0.875
train loss:  0.3107224106788635
train gradient:  0.10291182045933521
iteration : 260
train acc:  0.84375
train loss:  0.39581477642059326
train gradient:  0.15864749762971073
iteration : 261
train acc:  0.875
train loss:  0.3003561198711395
train gradient:  0.11449366435411804
iteration : 262
train acc:  0.859375
train loss:  0.332017719745636
train gradient:  0.14876864806163173
iteration : 263
train acc:  0.921875
train loss:  0.24250108003616333
train gradient:  0.10252586958774117
iteration : 264
train acc:  0.8828125
train loss:  0.2989703416824341
train gradient:  0.09068876158423064
iteration : 265
train acc:  0.8515625
train loss:  0.2864499092102051
train gradient:  0.10865963170228972
iteration : 266
train acc:  0.890625
train loss:  0.2893316149711609
train gradient:  0.10751586045066401
iteration : 267
train acc:  0.8671875
train loss:  0.3233785927295685
train gradient:  0.1337393787396765
iteration : 268
train acc:  0.859375
train loss:  0.33544740080833435
train gradient:  0.12747137415200266
iteration : 269
train acc:  0.859375
train loss:  0.3300054669380188
train gradient:  0.13196670423234308
iteration : 270
train acc:  0.828125
train loss:  0.3698778450489044
train gradient:  0.19371021883311135
iteration : 271
train acc:  0.8984375
train loss:  0.22574667632579803
train gradient:  0.13204136547542478
iteration : 272
train acc:  0.9140625
train loss:  0.2749726176261902
train gradient:  0.12157070116064889
iteration : 273
train acc:  0.9140625
train loss:  0.2744908630847931
train gradient:  0.07538750872078515
iteration : 274
train acc:  0.8984375
train loss:  0.30497533082962036
train gradient:  0.13423218065670375
iteration : 275
train acc:  0.890625
train loss:  0.28261223435401917
train gradient:  0.08946543999944608
iteration : 276
train acc:  0.859375
train loss:  0.29716432094573975
train gradient:  0.1276483571010846
iteration : 277
train acc:  0.84375
train loss:  0.3014889359474182
train gradient:  0.10959585602987197
iteration : 278
train acc:  0.875
train loss:  0.28625279664993286
train gradient:  0.1156044816354503
iteration : 279
train acc:  0.8984375
train loss:  0.23862168192863464
train gradient:  0.09158203338988563
iteration : 280
train acc:  0.765625
train loss:  0.4856986701488495
train gradient:  0.26690526601238396
iteration : 281
train acc:  0.8203125
train loss:  0.35250112414360046
train gradient:  0.16037645772263245
iteration : 282
train acc:  0.8125
train loss:  0.3379741311073303
train gradient:  0.14141667580703185
iteration : 283
train acc:  0.90625
train loss:  0.2339002788066864
train gradient:  0.0994829823592858
iteration : 284
train acc:  0.875
train loss:  0.30063900351524353
train gradient:  0.1319970179019877
iteration : 285
train acc:  0.8984375
train loss:  0.307375967502594
train gradient:  0.14627238332970954
iteration : 286
train acc:  0.8828125
train loss:  0.2865106761455536
train gradient:  0.09168451055518011
iteration : 287
train acc:  0.84375
train loss:  0.30044132471084595
train gradient:  0.1165466829723457
iteration : 288
train acc:  0.796875
train loss:  0.4212379455566406
train gradient:  0.1724592762573655
iteration : 289
train acc:  0.8515625
train loss:  0.31306156516075134
train gradient:  0.2125012412456037
iteration : 290
train acc:  0.8671875
train loss:  0.32298892736434937
train gradient:  0.1652721747792916
iteration : 291
train acc:  0.8984375
train loss:  0.3052060604095459
train gradient:  0.15757254319821767
iteration : 292
train acc:  0.8359375
train loss:  0.2937901020050049
train gradient:  0.10378099753396242
iteration : 293
train acc:  0.8828125
train loss:  0.26686757802963257
train gradient:  0.11234808364048803
iteration : 294
train acc:  0.828125
train loss:  0.3427503705024719
train gradient:  0.12810689089393018
iteration : 295
train acc:  0.890625
train loss:  0.24905434250831604
train gradient:  0.08940888087418862
iteration : 296
train acc:  0.9375
train loss:  0.23774661123752594
train gradient:  0.08164430489650029
iteration : 297
train acc:  0.8515625
train loss:  0.3171233534812927
train gradient:  0.09228899435170837
iteration : 298
train acc:  0.828125
train loss:  0.2938891649246216
train gradient:  0.12853433683169369
iteration : 299
train acc:  0.859375
train loss:  0.2901679277420044
train gradient:  0.09667145897039209
iteration : 300
train acc:  0.875
train loss:  0.28981274366378784
train gradient:  0.0946505326993807
iteration : 301
train acc:  0.8984375
train loss:  0.2745298147201538
train gradient:  0.13691325545611188
iteration : 302
train acc:  0.84375
train loss:  0.33342552185058594
train gradient:  0.16810391123103255
iteration : 303
train acc:  0.875
train loss:  0.31451448798179626
train gradient:  0.11680877207248168
iteration : 304
train acc:  0.828125
train loss:  0.3551342785358429
train gradient:  0.15099013665271593
iteration : 305
train acc:  0.828125
train loss:  0.34134095907211304
train gradient:  0.13300841909910693
iteration : 306
train acc:  0.8671875
train loss:  0.3548632860183716
train gradient:  0.17517005052472429
iteration : 307
train acc:  0.875
train loss:  0.3416061997413635
train gradient:  0.14146812766916833
iteration : 308
train acc:  0.8046875
train loss:  0.3370436131954193
train gradient:  0.1617287059644688
iteration : 309
train acc:  0.8671875
train loss:  0.3256721496582031
train gradient:  0.14058069493557918
iteration : 310
train acc:  0.8125
train loss:  0.4130081534385681
train gradient:  0.2050123419136079
iteration : 311
train acc:  0.859375
train loss:  0.3118664026260376
train gradient:  0.1730481836259885
iteration : 312
train acc:  0.875
train loss:  0.3052828013896942
train gradient:  0.09255467936421984
iteration : 313
train acc:  0.84375
train loss:  0.40546005964279175
train gradient:  0.20990060740956668
iteration : 314
train acc:  0.8515625
train loss:  0.31345683336257935
train gradient:  0.1263789548739861
iteration : 315
train acc:  0.8828125
train loss:  0.26560989022254944
train gradient:  0.10485477438327806
iteration : 316
train acc:  0.8515625
train loss:  0.3278929889202118
train gradient:  0.12003768103719363
iteration : 317
train acc:  0.8828125
train loss:  0.3125741481781006
train gradient:  0.17743457237855856
iteration : 318
train acc:  0.8125
train loss:  0.3727106750011444
train gradient:  0.18752488051685995
iteration : 319
train acc:  0.8125
train loss:  0.34724634885787964
train gradient:  0.17490014583575553
iteration : 320
train acc:  0.8984375
train loss:  0.23917697370052338
train gradient:  0.0818874350516696
iteration : 321
train acc:  0.828125
train loss:  0.3201557397842407
train gradient:  0.13085760024590787
iteration : 322
train acc:  0.8515625
train loss:  0.3457205295562744
train gradient:  0.11998596255691359
iteration : 323
train acc:  0.84375
train loss:  0.373416930437088
train gradient:  0.19239202188487706
iteration : 324
train acc:  0.875
train loss:  0.3153840899467468
train gradient:  0.11967790263545589
iteration : 325
train acc:  0.859375
train loss:  0.366052508354187
train gradient:  0.19809659406950653
iteration : 326
train acc:  0.8515625
train loss:  0.3035723567008972
train gradient:  0.151943001785155
iteration : 327
train acc:  0.890625
train loss:  0.2558436691761017
train gradient:  0.10806980376742246
iteration : 328
train acc:  0.9296875
train loss:  0.20341309905052185
train gradient:  0.06371280436274013
iteration : 329
train acc:  0.9140625
train loss:  0.24487780034542084
train gradient:  0.122728128757149
iteration : 330
train acc:  0.8671875
train loss:  0.2878884971141815
train gradient:  0.1331894233020115
iteration : 331
train acc:  0.84375
train loss:  0.3613239526748657
train gradient:  0.1819260030165222
iteration : 332
train acc:  0.8203125
train loss:  0.32783088088035583
train gradient:  0.16830955437065312
iteration : 333
train acc:  0.890625
train loss:  0.27962762117385864
train gradient:  0.11041134298271921
iteration : 334
train acc:  0.796875
train loss:  0.4277111887931824
train gradient:  0.2359996710411996
iteration : 335
train acc:  0.8984375
train loss:  0.3218836188316345
train gradient:  0.11222223260871016
iteration : 336
train acc:  0.875
train loss:  0.32205730676651
train gradient:  0.12737391542152043
iteration : 337
train acc:  0.796875
train loss:  0.3951898217201233
train gradient:  0.2108541514578602
iteration : 338
train acc:  0.8671875
train loss:  0.3528699278831482
train gradient:  0.18008932667341288
iteration : 339
train acc:  0.8359375
train loss:  0.4018544554710388
train gradient:  0.21428063472726755
iteration : 340
train acc:  0.921875
train loss:  0.25004005432128906
train gradient:  0.09375022362592915
iteration : 341
train acc:  0.90625
train loss:  0.2478487640619278
train gradient:  0.06718497218793931
iteration : 342
train acc:  0.875
train loss:  0.2676581144332886
train gradient:  0.08096093307040893
iteration : 343
train acc:  0.8828125
train loss:  0.2684061527252197
train gradient:  0.09114151477380955
iteration : 344
train acc:  0.8203125
train loss:  0.38783422112464905
train gradient:  0.14733791197771762
iteration : 345
train acc:  0.8359375
train loss:  0.3494206666946411
train gradient:  0.1333104738082187
iteration : 346
train acc:  0.8828125
train loss:  0.3266638517379761
train gradient:  0.1452802902181067
iteration : 347
train acc:  0.8515625
train loss:  0.27326762676239014
train gradient:  0.16032360500855147
iteration : 348
train acc:  0.8046875
train loss:  0.36163365840911865
train gradient:  0.16153418516627926
iteration : 349
train acc:  0.828125
train loss:  0.39538130164146423
train gradient:  0.16584701667048665
iteration : 350
train acc:  0.859375
train loss:  0.2622816264629364
train gradient:  0.09295842239250297
iteration : 351
train acc:  0.84375
train loss:  0.34231677651405334
train gradient:  0.11690601573742815
iteration : 352
train acc:  0.921875
train loss:  0.2578442394733429
train gradient:  0.06953808553035097
iteration : 353
train acc:  0.828125
train loss:  0.3450919985771179
train gradient:  0.14825094342661954
iteration : 354
train acc:  0.8046875
train loss:  0.4460539221763611
train gradient:  0.20450687570868548
iteration : 355
train acc:  0.84375
train loss:  0.3750839829444885
train gradient:  0.16866600036845286
iteration : 356
train acc:  0.8515625
train loss:  0.3302193582057953
train gradient:  0.14465309855058817
iteration : 357
train acc:  0.8515625
train loss:  0.362270712852478
train gradient:  0.14287962865669496
iteration : 358
train acc:  0.875
train loss:  0.31602877378463745
train gradient:  0.18107818184875552
iteration : 359
train acc:  0.875
train loss:  0.3066985011100769
train gradient:  0.13190682137392132
iteration : 360
train acc:  0.8203125
train loss:  0.4010007977485657
train gradient:  0.12100476469037819
iteration : 361
train acc:  0.84375
train loss:  0.41830140352249146
train gradient:  0.18794359077353406
iteration : 362
train acc:  0.8828125
train loss:  0.291644424200058
train gradient:  0.07068525453459362
iteration : 363
train acc:  0.828125
train loss:  0.3568451404571533
train gradient:  0.14532651163149113
iteration : 364
train acc:  0.8828125
train loss:  0.3010586202144623
train gradient:  0.11111648927168218
iteration : 365
train acc:  0.8671875
train loss:  0.3162834048271179
train gradient:  0.15065542344288257
iteration : 366
train acc:  0.8515625
train loss:  0.3463948369026184
train gradient:  0.17150162540244612
iteration : 367
train acc:  0.8984375
train loss:  0.23208312690258026
train gradient:  0.07842284394190599
iteration : 368
train acc:  0.8515625
train loss:  0.30140170454978943
train gradient:  0.1455983884857624
iteration : 369
train acc:  0.890625
train loss:  0.2883591055870056
train gradient:  0.1906384486801574
iteration : 370
train acc:  0.859375
train loss:  0.3177019953727722
train gradient:  0.13180288466585544
iteration : 371
train acc:  0.875
train loss:  0.2920152544975281
train gradient:  0.09239121073659572
iteration : 372
train acc:  0.890625
train loss:  0.242208793759346
train gradient:  0.13652345042702213
iteration : 373
train acc:  0.8984375
train loss:  0.27192363142967224
train gradient:  0.08792864653269732
iteration : 374
train acc:  0.8515625
train loss:  0.32425493001937866
train gradient:  0.12943612574283409
iteration : 375
train acc:  0.8671875
train loss:  0.33345839381217957
train gradient:  0.20282148586995574
iteration : 376
train acc:  0.859375
train loss:  0.3130086064338684
train gradient:  0.09782892317589065
iteration : 377
train acc:  0.8125
train loss:  0.329083651304245
train gradient:  0.13639970097129867
iteration : 378
train acc:  0.828125
train loss:  0.40717613697052
train gradient:  0.18458697472211655
iteration : 379
train acc:  0.796875
train loss:  0.47390127182006836
train gradient:  0.28961765632482267
iteration : 380
train acc:  0.8515625
train loss:  0.32657960057258606
train gradient:  0.11013114023268403
iteration : 381
train acc:  0.921875
train loss:  0.25024616718292236
train gradient:  0.1000783291917506
iteration : 382
train acc:  0.796875
train loss:  0.3925154209136963
train gradient:  0.15533188142945661
iteration : 383
train acc:  0.875
train loss:  0.29071152210235596
train gradient:  0.0847976043591014
iteration : 384
train acc:  0.8203125
train loss:  0.3260062336921692
train gradient:  0.1286430421130607
iteration : 385
train acc:  0.84375
train loss:  0.3703806698322296
train gradient:  0.15637048760673347
iteration : 386
train acc:  0.8359375
train loss:  0.3090420961380005
train gradient:  0.1262614929234841
iteration : 387
train acc:  0.90625
train loss:  0.25332361459732056
train gradient:  0.06027342455502919
iteration : 388
train acc:  0.8515625
train loss:  0.3390049338340759
train gradient:  0.1292134457526931
iteration : 389
train acc:  0.828125
train loss:  0.4486023783683777
train gradient:  0.17548377955874084
iteration : 390
train acc:  0.9453125
train loss:  0.18738692998886108
train gradient:  0.060164733941659915
iteration : 391
train acc:  0.84375
train loss:  0.34530240297317505
train gradient:  0.1086659687931895
iteration : 392
train acc:  0.8515625
train loss:  0.33971309661865234
train gradient:  0.1179400370709253
iteration : 393
train acc:  0.8828125
train loss:  0.2985353469848633
train gradient:  0.16412454198171594
iteration : 394
train acc:  0.8828125
train loss:  0.2707446217536926
train gradient:  0.1165264965235213
iteration : 395
train acc:  0.828125
train loss:  0.3638368248939514
train gradient:  0.11332222876276699
iteration : 396
train acc:  0.859375
train loss:  0.37276971340179443
train gradient:  0.14336076679841303
iteration : 397
train acc:  0.9375
train loss:  0.20260609686374664
train gradient:  0.06744217539185035
iteration : 398
train acc:  0.890625
train loss:  0.2778050899505615
train gradient:  0.1071935596764181
iteration : 399
train acc:  0.8828125
train loss:  0.3379308581352234
train gradient:  0.18223486967197372
iteration : 400
train acc:  0.921875
train loss:  0.28218427300453186
train gradient:  0.07108795100609631
iteration : 401
train acc:  0.890625
train loss:  0.28266429901123047
train gradient:  0.12006691495148804
iteration : 402
train acc:  0.875
train loss:  0.29947036504745483
train gradient:  0.11074526952407143
iteration : 403
train acc:  0.859375
train loss:  0.3033144474029541
train gradient:  0.09933907205543975
iteration : 404
train acc:  0.8828125
train loss:  0.27341824769973755
train gradient:  0.09484211575402325
iteration : 405
train acc:  0.90625
train loss:  0.2760724723339081
train gradient:  0.10698235961070092
iteration : 406
train acc:  0.8515625
train loss:  0.30982765555381775
train gradient:  0.14491507412323112
iteration : 407
train acc:  0.8671875
train loss:  0.31981420516967773
train gradient:  0.14386531662658392
iteration : 408
train acc:  0.859375
train loss:  0.291454017162323
train gradient:  0.1052033436292325
iteration : 409
train acc:  0.8828125
train loss:  0.24794238805770874
train gradient:  0.07809212878352327
iteration : 410
train acc:  0.8984375
train loss:  0.30149325728416443
train gradient:  0.12088697586008393
iteration : 411
train acc:  0.828125
train loss:  0.4244811236858368
train gradient:  0.21569174921160642
iteration : 412
train acc:  0.796875
train loss:  0.47466641664505005
train gradient:  0.21519705636067485
iteration : 413
train acc:  0.890625
train loss:  0.27994781732559204
train gradient:  0.10970690388252319
iteration : 414
train acc:  0.8671875
train loss:  0.3111800253391266
train gradient:  0.10913113590780982
iteration : 415
train acc:  0.8828125
train loss:  0.30013135075569153
train gradient:  0.08412491669456797
iteration : 416
train acc:  0.8671875
train loss:  0.2986038029193878
train gradient:  0.11001521406991675
iteration : 417
train acc:  0.8203125
train loss:  0.40794867277145386
train gradient:  0.21063670814963606
iteration : 418
train acc:  0.859375
train loss:  0.27677464485168457
train gradient:  0.08234042583233724
iteration : 419
train acc:  0.8671875
train loss:  0.35109424591064453
train gradient:  0.19864963419350978
iteration : 420
train acc:  0.8515625
train loss:  0.30824342370033264
train gradient:  0.12817774907363383
iteration : 421
train acc:  0.828125
train loss:  0.3361070156097412
train gradient:  0.11543524851225972
iteration : 422
train acc:  0.859375
train loss:  0.3422211706638336
train gradient:  0.12785678413364623
iteration : 423
train acc:  0.8359375
train loss:  0.3214149475097656
train gradient:  0.11999064514802274
iteration : 424
train acc:  0.890625
train loss:  0.2497878223657608
train gradient:  0.11439596254130376
iteration : 425
train acc:  0.890625
train loss:  0.24217674136161804
train gradient:  0.1610408195414829
iteration : 426
train acc:  0.8828125
train loss:  0.27380192279815674
train gradient:  0.13154801382265086
iteration : 427
train acc:  0.8671875
train loss:  0.2662355899810791
train gradient:  0.10814286773714588
iteration : 428
train acc:  0.8515625
train loss:  0.3168729543685913
train gradient:  0.09890817477326001
iteration : 429
train acc:  0.8359375
train loss:  0.3481624126434326
train gradient:  0.11886949859775514
iteration : 430
train acc:  0.8359375
train loss:  0.37065431475639343
train gradient:  0.19122120543375057
iteration : 431
train acc:  0.890625
train loss:  0.27958667278289795
train gradient:  0.09255442474714434
iteration : 432
train acc:  0.8671875
train loss:  0.34021005034446716
train gradient:  0.1413732434602507
iteration : 433
train acc:  0.9140625
train loss:  0.26840633153915405
train gradient:  0.08057923358224417
iteration : 434
train acc:  0.8125
train loss:  0.3620367646217346
train gradient:  0.14684978431034862
iteration : 435
train acc:  0.8671875
train loss:  0.37866827845573425
train gradient:  0.18343250107356832
iteration : 436
train acc:  0.84375
train loss:  0.3688358962535858
train gradient:  0.16479411815295308
iteration : 437
train acc:  0.8984375
train loss:  0.2534577250480652
train gradient:  0.07566285074451098
iteration : 438
train acc:  0.8671875
train loss:  0.31823039054870605
train gradient:  0.1438067333508026
iteration : 439
train acc:  0.8671875
train loss:  0.2596712112426758
train gradient:  0.07058135654845775
iteration : 440
train acc:  0.921875
train loss:  0.2288237363100052
train gradient:  0.11958844283332484
iteration : 441
train acc:  0.828125
train loss:  0.363849937915802
train gradient:  0.17270810189721342
iteration : 442
train acc:  0.859375
train loss:  0.3101195693016052
train gradient:  0.12963773477525992
iteration : 443
train acc:  0.828125
train loss:  0.3355483412742615
train gradient:  0.11833263165357638
iteration : 444
train acc:  0.8984375
train loss:  0.24446190893650055
train gradient:  0.05667252912737034
iteration : 445
train acc:  0.8828125
train loss:  0.23872622847557068
train gradient:  0.0982464378894306
iteration : 446
train acc:  0.8359375
train loss:  0.3335459232330322
train gradient:  0.11600404352036033
iteration : 447
train acc:  0.875
train loss:  0.2655930817127228
train gradient:  0.08168279908536842
iteration : 448
train acc:  0.890625
train loss:  0.3071485161781311
train gradient:  0.10566936814206976
iteration : 449
train acc:  0.8828125
train loss:  0.2894444167613983
train gradient:  0.13535997811396
iteration : 450
train acc:  0.875
train loss:  0.33165961503982544
train gradient:  0.15039849433164904
iteration : 451
train acc:  0.828125
train loss:  0.3251284956932068
train gradient:  0.12513745389750514
iteration : 452
train acc:  0.8203125
train loss:  0.3269457221031189
train gradient:  0.12760259315532585
iteration : 453
train acc:  0.859375
train loss:  0.3478531241416931
train gradient:  0.14854056801461935
iteration : 454
train acc:  0.8671875
train loss:  0.35996025800704956
train gradient:  0.15023898002652525
iteration : 455
train acc:  0.84375
train loss:  0.2808990180492401
train gradient:  0.1378430540259469
iteration : 456
train acc:  0.921875
train loss:  0.2409520149230957
train gradient:  0.09752117185929002
iteration : 457
train acc:  0.8359375
train loss:  0.3450685739517212
train gradient:  0.13115124863917377
iteration : 458
train acc:  0.875
train loss:  0.2699018120765686
train gradient:  0.07290807790942923
iteration : 459
train acc:  0.875
train loss:  0.3039712607860565
train gradient:  0.11962519433374043
iteration : 460
train acc:  0.890625
train loss:  0.2903541326522827
train gradient:  0.09002649643253846
iteration : 461
train acc:  0.8671875
train loss:  0.34586101770401
train gradient:  0.1395449348271664
iteration : 462
train acc:  0.8203125
train loss:  0.42085814476013184
train gradient:  0.23201525566202102
iteration : 463
train acc:  0.8828125
train loss:  0.3008520007133484
train gradient:  0.21170156266407922
iteration : 464
train acc:  0.828125
train loss:  0.34720855951309204
train gradient:  0.14959991169713935
iteration : 465
train acc:  0.875
train loss:  0.3321055769920349
train gradient:  0.11807460262735349
iteration : 466
train acc:  0.8515625
train loss:  0.35896795988082886
train gradient:  0.1534240187912851
iteration : 467
train acc:  0.875
train loss:  0.32252830266952515
train gradient:  0.1392573816180499
iteration : 468
train acc:  0.875
train loss:  0.2391965538263321
train gradient:  0.09036526558778304
iteration : 469
train acc:  0.8203125
train loss:  0.36229103803634644
train gradient:  0.17502918565822112
iteration : 470
train acc:  0.90625
train loss:  0.2560739517211914
train gradient:  0.14616196784985258
iteration : 471
train acc:  0.8671875
train loss:  0.35056185722351074
train gradient:  0.1973361126570387
iteration : 472
train acc:  0.8828125
train loss:  0.32500410079956055
train gradient:  0.14389138547403119
iteration : 473
train acc:  0.8515625
train loss:  0.3036544919013977
train gradient:  0.11928273413461514
iteration : 474
train acc:  0.90625
train loss:  0.28872406482696533
train gradient:  0.12508966083042158
iteration : 475
train acc:  0.84375
train loss:  0.3825666904449463
train gradient:  0.12736248902341918
iteration : 476
train acc:  0.875
train loss:  0.2906670570373535
train gradient:  0.10306179779104674
iteration : 477
train acc:  0.859375
train loss:  0.33418089151382446
train gradient:  0.10720366352857987
iteration : 478
train acc:  0.8828125
train loss:  0.30768585205078125
train gradient:  0.10193161719735508
iteration : 479
train acc:  0.8828125
train loss:  0.3353504538536072
train gradient:  0.1493656894389907
iteration : 480
train acc:  0.84375
train loss:  0.2968997061252594
train gradient:  0.09180127057628956
iteration : 481
train acc:  0.828125
train loss:  0.3481568992137909
train gradient:  0.14847810405188316
iteration : 482
train acc:  0.8828125
train loss:  0.31197047233581543
train gradient:  0.10037251058630342
iteration : 483
train acc:  0.828125
train loss:  0.3476080298423767
train gradient:  0.1891946755094464
iteration : 484
train acc:  0.8046875
train loss:  0.41461724042892456
train gradient:  0.2715983525667067
iteration : 485
train acc:  0.8828125
train loss:  0.3200535774230957
train gradient:  0.11718943398843443
iteration : 486
train acc:  0.890625
train loss:  0.2617947459220886
train gradient:  0.10997314991425775
iteration : 487
train acc:  0.84375
train loss:  0.35452044010162354
train gradient:  0.17326068143245915
iteration : 488
train acc:  0.8984375
train loss:  0.254755437374115
train gradient:  0.10708686410841899
iteration : 489
train acc:  0.8203125
train loss:  0.4404621124267578
train gradient:  0.22718579434042038
iteration : 490
train acc:  0.84375
train loss:  0.326945424079895
train gradient:  0.12694322298562016
iteration : 491
train acc:  0.875
train loss:  0.29327118396759033
train gradient:  0.17589579979854975
iteration : 492
train acc:  0.8984375
train loss:  0.23488518595695496
train gradient:  0.07989139435919182
iteration : 493
train acc:  0.921875
train loss:  0.244479238986969
train gradient:  0.09906938608990763
iteration : 494
train acc:  0.8828125
train loss:  0.25453171133995056
train gradient:  0.11282677984988261
iteration : 495
train acc:  0.890625
train loss:  0.25361382961273193
train gradient:  0.1250066608667733
iteration : 496
train acc:  0.8671875
train loss:  0.33516114950180054
train gradient:  0.12713689468705575
iteration : 497
train acc:  0.859375
train loss:  0.31796935200691223
train gradient:  0.10359338654318083
iteration : 498
train acc:  0.84375
train loss:  0.2996656596660614
train gradient:  0.11478001935447062
iteration : 499
train acc:  0.8984375
train loss:  0.27196693420410156
train gradient:  0.10325611708355077
iteration : 500
train acc:  0.828125
train loss:  0.41714832186698914
train gradient:  0.18068995734214383
iteration : 501
train acc:  0.859375
train loss:  0.2666281461715698
train gradient:  0.10850961947948168
iteration : 502
train acc:  0.859375
train loss:  0.33173298835754395
train gradient:  0.18966590988134074
iteration : 503
train acc:  0.859375
train loss:  0.31835871934890747
train gradient:  0.11144997061782046
iteration : 504
train acc:  0.8515625
train loss:  0.3253170847892761
train gradient:  0.11494829861871121
iteration : 505
train acc:  0.84375
train loss:  0.33040016889572144
train gradient:  0.13356489311653885
iteration : 506
train acc:  0.8828125
train loss:  0.3298863172531128
train gradient:  0.18950525928065787
iteration : 507
train acc:  0.8046875
train loss:  0.4016387462615967
train gradient:  0.21146681947080878
iteration : 508
train acc:  0.890625
train loss:  0.2723190188407898
train gradient:  0.08022067691550201
iteration : 509
train acc:  0.8359375
train loss:  0.3166848421096802
train gradient:  0.14152832322818276
iteration : 510
train acc:  0.8828125
train loss:  0.3233422338962555
train gradient:  0.1408998597487416
iteration : 511
train acc:  0.8203125
train loss:  0.35104718804359436
train gradient:  0.180754519520361
iteration : 512
train acc:  0.8671875
train loss:  0.3514830768108368
train gradient:  0.16121454753978673
iteration : 513
train acc:  0.890625
train loss:  0.2371577024459839
train gradient:  0.09755577011610207
iteration : 514
train acc:  0.8515625
train loss:  0.33206871151924133
train gradient:  0.15507811617849615
iteration : 515
train acc:  0.8828125
train loss:  0.2418697476387024
train gradient:  0.09566111721115031
iteration : 516
train acc:  0.8984375
train loss:  0.23944172263145447
train gradient:  0.09290664197664979
iteration : 517
train acc:  0.8359375
train loss:  0.3615773320198059
train gradient:  0.16434318085884927
iteration : 518
train acc:  0.8046875
train loss:  0.375933438539505
train gradient:  0.19223833388424166
iteration : 519
train acc:  0.9453125
train loss:  0.20116212964057922
train gradient:  0.07299752311242604
iteration : 520
train acc:  0.8515625
train loss:  0.37451988458633423
train gradient:  0.1852741859091599
iteration : 521
train acc:  0.859375
train loss:  0.3137359619140625
train gradient:  0.12059976208803166
iteration : 522
train acc:  0.828125
train loss:  0.4097875654697418
train gradient:  0.1554399323972574
iteration : 523
train acc:  0.859375
train loss:  0.42365768551826477
train gradient:  0.1439632349230241
iteration : 524
train acc:  0.8671875
train loss:  0.3283880650997162
train gradient:  0.22445894370955718
iteration : 525
train acc:  0.84375
train loss:  0.3676601052284241
train gradient:  0.1782936675582692
iteration : 526
train acc:  0.9375
train loss:  0.2127271443605423
train gradient:  0.13241567032051027
iteration : 527
train acc:  0.921875
train loss:  0.192406564950943
train gradient:  0.11860333505748726
iteration : 528
train acc:  0.8203125
train loss:  0.30524125695228577
train gradient:  0.12031765582489051
iteration : 529
train acc:  0.84375
train loss:  0.3470686376094818
train gradient:  0.1833734371269785
iteration : 530
train acc:  0.8203125
train loss:  0.3966965079307556
train gradient:  0.23766731646343925
iteration : 531
train acc:  0.8828125
train loss:  0.31451499462127686
train gradient:  0.13880872681760212
iteration : 532
train acc:  0.890625
train loss:  0.2972155511379242
train gradient:  0.08568979386330261
iteration : 533
train acc:  0.890625
train loss:  0.26166102290153503
train gradient:  0.10206942046605948
iteration : 534
train acc:  0.890625
train loss:  0.2951752543449402
train gradient:  0.11473297754317884
iteration : 535
train acc:  0.8671875
train loss:  0.35529303550720215
train gradient:  0.14611777623512898
iteration : 536
train acc:  0.84375
train loss:  0.3419637680053711
train gradient:  0.1249096045838725
iteration : 537
train acc:  0.8515625
train loss:  0.33790165185928345
train gradient:  0.14295072150726396
iteration : 538
train acc:  0.8671875
train loss:  0.2654780447483063
train gradient:  0.07335889526236687
iteration : 539
train acc:  0.8671875
train loss:  0.377772718667984
train gradient:  0.20827777839403566
iteration : 540
train acc:  0.8671875
train loss:  0.3713648021221161
train gradient:  0.1554581166981029
iteration : 541
train acc:  0.859375
train loss:  0.28847092390060425
train gradient:  0.15252109450167123
iteration : 542
train acc:  0.875
train loss:  0.31350499391555786
train gradient:  0.1688289496745778
iteration : 543
train acc:  0.8046875
train loss:  0.4083455502986908
train gradient:  0.26754014584124175
iteration : 544
train acc:  0.859375
train loss:  0.3239707946777344
train gradient:  0.12939496680628343
iteration : 545
train acc:  0.8125
train loss:  0.38392359018325806
train gradient:  0.17934038467332675
iteration : 546
train acc:  0.890625
train loss:  0.24249839782714844
train gradient:  0.10565362158990643
iteration : 547
train acc:  0.8671875
train loss:  0.300425261259079
train gradient:  0.1950858575893342
iteration : 548
train acc:  0.8828125
train loss:  0.3093433976173401
train gradient:  0.13901764805746436
iteration : 549
train acc:  0.859375
train loss:  0.300117164850235
train gradient:  0.14005289574892588
iteration : 550
train acc:  0.859375
train loss:  0.3073154091835022
train gradient:  0.12548291499595943
iteration : 551
train acc:  0.890625
train loss:  0.278572142124176
train gradient:  0.09645671444865798
iteration : 552
train acc:  0.8359375
train loss:  0.39317768812179565
train gradient:  0.17992663053089228
iteration : 553
train acc:  0.90625
train loss:  0.26087674498558044
train gradient:  0.1076188621366665
iteration : 554
train acc:  0.875
train loss:  0.25773465633392334
train gradient:  0.08660195885764556
iteration : 555
train acc:  0.859375
train loss:  0.26525068283081055
train gradient:  0.0985802512294571
iteration : 556
train acc:  0.8984375
train loss:  0.24178293347358704
train gradient:  0.08732518319984345
iteration : 557
train acc:  0.828125
train loss:  0.3702748119831085
train gradient:  0.18027449303721765
iteration : 558
train acc:  0.8671875
train loss:  0.32765814661979675
train gradient:  0.14921578062501892
iteration : 559
train acc:  0.9140625
train loss:  0.27066570520401
train gradient:  0.10365549095377578
iteration : 560
train acc:  0.875
train loss:  0.29657939076423645
train gradient:  0.12327837289653937
iteration : 561
train acc:  0.8515625
train loss:  0.36028924584388733
train gradient:  0.1476532828071595
iteration : 562
train acc:  0.8984375
train loss:  0.2598554491996765
train gradient:  0.12247048948366152
iteration : 563
train acc:  0.890625
train loss:  0.30902403593063354
train gradient:  0.10858884637212969
iteration : 564
train acc:  0.8359375
train loss:  0.36317718029022217
train gradient:  0.26895982241102634
iteration : 565
train acc:  0.890625
train loss:  0.2808404564857483
train gradient:  0.1357183391682203
iteration : 566
train acc:  0.796875
train loss:  0.4261886477470398
train gradient:  0.2822486147325102
iteration : 567
train acc:  0.84375
train loss:  0.40179041028022766
train gradient:  0.2590940485517374
iteration : 568
train acc:  0.859375
train loss:  0.3100745677947998
train gradient:  0.10880605925486442
iteration : 569
train acc:  0.8515625
train loss:  0.3418532907962799
train gradient:  0.2457459862570534
iteration : 570
train acc:  0.828125
train loss:  0.374462366104126
train gradient:  0.18215578708661131
iteration : 571
train acc:  0.8203125
train loss:  0.3898637890815735
train gradient:  0.20326539894937642
iteration : 572
train acc:  0.8203125
train loss:  0.41972965002059937
train gradient:  0.2117087496194779
iteration : 573
train acc:  0.8515625
train loss:  0.3583666682243347
train gradient:  0.14122049627452024
iteration : 574
train acc:  0.8203125
train loss:  0.3552120625972748
train gradient:  0.19702543328602456
iteration : 575
train acc:  0.8828125
train loss:  0.25726646184921265
train gradient:  0.1388599063466628
iteration : 576
train acc:  0.828125
train loss:  0.3743914067745209
train gradient:  0.22487554794518422
iteration : 577
train acc:  0.828125
train loss:  0.42373067140579224
train gradient:  0.27598200792408295
iteration : 578
train acc:  0.828125
train loss:  0.36361274123191833
train gradient:  0.14552562237628924
iteration : 579
train acc:  0.8671875
train loss:  0.30387207865715027
train gradient:  0.22080153672406777
iteration : 580
train acc:  0.859375
train loss:  0.33550652861595154
train gradient:  0.10770255408849777
iteration : 581
train acc:  0.875
train loss:  0.29298198223114014
train gradient:  0.09599209782002929
iteration : 582
train acc:  0.859375
train loss:  0.28999191522598267
train gradient:  0.1200222629040973
iteration : 583
train acc:  0.7890625
train loss:  0.41267967224121094
train gradient:  0.16176588427691715
iteration : 584
train acc:  0.890625
train loss:  0.2707661986351013
train gradient:  0.07321316839059819
iteration : 585
train acc:  0.8671875
train loss:  0.26982539892196655
train gradient:  0.08658861565306591
iteration : 586
train acc:  0.875
train loss:  0.2958066165447235
train gradient:  0.11669355597971927
iteration : 587
train acc:  0.8671875
train loss:  0.3654933571815491
train gradient:  0.16400032690072908
iteration : 588
train acc:  0.8125
train loss:  0.4060461223125458
train gradient:  0.2503252596333542
iteration : 589
train acc:  0.8984375
train loss:  0.2622620761394501
train gradient:  0.10046755128668172
iteration : 590
train acc:  0.875
train loss:  0.3306160569190979
train gradient:  0.1343286669307448
iteration : 591
train acc:  0.859375
train loss:  0.3413301706314087
train gradient:  0.1960747879445135
iteration : 592
train acc:  0.8984375
train loss:  0.28621232509613037
train gradient:  0.09344592809642703
iteration : 593
train acc:  0.921875
train loss:  0.22792434692382812
train gradient:  0.1191696922387272
iteration : 594
train acc:  0.859375
train loss:  0.3595423996448517
train gradient:  0.18691722416691747
iteration : 595
train acc:  0.890625
train loss:  0.25873103737831116
train gradient:  0.08443221534599074
iteration : 596
train acc:  0.8671875
train loss:  0.2762582302093506
train gradient:  0.11145679687328379
iteration : 597
train acc:  0.8828125
train loss:  0.2887956202030182
train gradient:  0.11023603695400781
iteration : 598
train acc:  0.8671875
train loss:  0.27436563372612
train gradient:  0.09201330331241349
iteration : 599
train acc:  0.921875
train loss:  0.2206054925918579
train gradient:  0.07268041594071016
iteration : 600
train acc:  0.8828125
train loss:  0.2488880604505539
train gradient:  0.11426867569269862
iteration : 601
train acc:  0.859375
train loss:  0.34489014744758606
train gradient:  0.23447040701298488
iteration : 602
train acc:  0.8671875
train loss:  0.3452969789505005
train gradient:  0.10869955631266967
iteration : 603
train acc:  0.8671875
train loss:  0.3289288878440857
train gradient:  0.11191237539203444
iteration : 604
train acc:  0.875
train loss:  0.2405584752559662
train gradient:  0.07507106205744514
iteration : 605
train acc:  0.8828125
train loss:  0.2766479551792145
train gradient:  0.07859621453716312
iteration : 606
train acc:  0.9140625
train loss:  0.23349349200725555
train gradient:  0.1139696469876754
iteration : 607
train acc:  0.8984375
train loss:  0.2479362189769745
train gradient:  0.10700775547529358
iteration : 608
train acc:  0.84375
train loss:  0.32634639739990234
train gradient:  0.24931016256111907
iteration : 609
train acc:  0.8515625
train loss:  0.3090798258781433
train gradient:  0.11671123774314998
iteration : 610
train acc:  0.8671875
train loss:  0.30997636914253235
train gradient:  0.07977830760671552
iteration : 611
train acc:  0.859375
train loss:  0.3236534893512726
train gradient:  0.13355588018154962
iteration : 612
train acc:  0.875
train loss:  0.2872249484062195
train gradient:  0.11813790992546686
iteration : 613
train acc:  0.84375
train loss:  0.34295132756233215
train gradient:  0.2109027016021615
iteration : 614
train acc:  0.859375
train loss:  0.3502192497253418
train gradient:  0.11711192916532037
iteration : 615
train acc:  0.9140625
train loss:  0.24426504969596863
train gradient:  0.08461743811090122
iteration : 616
train acc:  0.8671875
train loss:  0.32186275720596313
train gradient:  0.12659509016057024
iteration : 617
train acc:  0.8359375
train loss:  0.36575525999069214
train gradient:  0.1903009767312065
iteration : 618
train acc:  0.8828125
train loss:  0.2887689471244812
train gradient:  0.08754847910887878
iteration : 619
train acc:  0.8359375
train loss:  0.3777589797973633
train gradient:  0.12878432929275258
iteration : 620
train acc:  0.8828125
train loss:  0.26758337020874023
train gradient:  0.09438496551175429
iteration : 621
train acc:  0.8984375
train loss:  0.275552362203598
train gradient:  0.11351703024812958
iteration : 622
train acc:  0.859375
train loss:  0.25839555263519287
train gradient:  0.09351259720522667
iteration : 623
train acc:  0.890625
train loss:  0.26496344804763794
train gradient:  0.08046483054767084
iteration : 624
train acc:  0.8203125
train loss:  0.45311883091926575
train gradient:  0.20305827416695418
iteration : 625
train acc:  0.8984375
train loss:  0.27621978521347046
train gradient:  0.09669139004005095
iteration : 626
train acc:  0.875
train loss:  0.2886093258857727
train gradient:  0.13067082352235646
iteration : 627
train acc:  0.859375
train loss:  0.36363527178764343
train gradient:  0.2183201722013848
iteration : 628
train acc:  0.8515625
train loss:  0.3720969557762146
train gradient:  0.19611033975213488
iteration : 629
train acc:  0.8515625
train loss:  0.30697450041770935
train gradient:  0.15799923359458592
iteration : 630
train acc:  0.8359375
train loss:  0.39997637271881104
train gradient:  0.29392195502423857
iteration : 631
train acc:  0.875
train loss:  0.31358087062835693
train gradient:  0.12120574075877409
iteration : 632
train acc:  0.859375
train loss:  0.2935194671154022
train gradient:  0.1359809361825033
iteration : 633
train acc:  0.84375
train loss:  0.36732396483421326
train gradient:  0.18465114664082355
iteration : 634
train acc:  0.859375
train loss:  0.2912200093269348
train gradient:  0.2822603876911789
iteration : 635
train acc:  0.890625
train loss:  0.2841596007347107
train gradient:  0.1408398737888571
iteration : 636
train acc:  0.8515625
train loss:  0.2996811866760254
train gradient:  0.15665666935491301
iteration : 637
train acc:  0.8515625
train loss:  0.27057021856307983
train gradient:  0.09624305761884717
iteration : 638
train acc:  0.8671875
train loss:  0.27763307094573975
train gradient:  0.10155381401355945
iteration : 639
train acc:  0.9140625
train loss:  0.25477612018585205
train gradient:  0.13185025550276247
iteration : 640
train acc:  0.8515625
train loss:  0.3318690061569214
train gradient:  0.1250577694804862
iteration : 641
train acc:  0.8203125
train loss:  0.38015639781951904
train gradient:  0.15765084187445041
iteration : 642
train acc:  0.828125
train loss:  0.38677266240119934
train gradient:  0.1371960878119162
iteration : 643
train acc:  0.8671875
train loss:  0.29329898953437805
train gradient:  0.13050145183787673
iteration : 644
train acc:  0.90625
train loss:  0.27022498846054077
train gradient:  0.11822901529374567
iteration : 645
train acc:  0.859375
train loss:  0.3075352907180786
train gradient:  0.12754108179016954
iteration : 646
train acc:  0.8671875
train loss:  0.3537219166755676
train gradient:  0.13964154882540453
iteration : 647
train acc:  0.859375
train loss:  0.3694884777069092
train gradient:  0.18648578391976756
iteration : 648
train acc:  0.90625
train loss:  0.2648966312408447
train gradient:  0.091793077139493
iteration : 649
train acc:  0.875
train loss:  0.297342985868454
train gradient:  0.18677160941490425
iteration : 650
train acc:  0.78125
train loss:  0.40138229727745056
train gradient:  0.16377360808015673
iteration : 651
train acc:  0.890625
train loss:  0.3456427752971649
train gradient:  0.16116347817802618
iteration : 652
train acc:  0.8515625
train loss:  0.4300558567047119
train gradient:  0.3036920458986607
iteration : 653
train acc:  0.875
train loss:  0.31173011660575867
train gradient:  0.11457950543396207
iteration : 654
train acc:  0.8671875
train loss:  0.34382832050323486
train gradient:  0.14786529086737
iteration : 655
train acc:  0.859375
train loss:  0.3673552870750427
train gradient:  0.22284617536301207
iteration : 656
train acc:  0.8984375
train loss:  0.24521243572235107
train gradient:  0.09921406298615162
iteration : 657
train acc:  0.8046875
train loss:  0.4192533493041992
train gradient:  0.16403267966977936
iteration : 658
train acc:  0.8515625
train loss:  0.2952185571193695
train gradient:  0.1448358284419291
iteration : 659
train acc:  0.8984375
train loss:  0.2751631736755371
train gradient:  0.08409748625035297
iteration : 660
train acc:  0.8671875
train loss:  0.2980895936489105
train gradient:  0.17642752599149378
iteration : 661
train acc:  0.859375
train loss:  0.33235251903533936
train gradient:  0.13769033658567348
iteration : 662
train acc:  0.8828125
train loss:  0.3019280433654785
train gradient:  0.1074526656737555
iteration : 663
train acc:  0.8515625
train loss:  0.3742750287055969
train gradient:  0.1642249266869487
iteration : 664
train acc:  0.8671875
train loss:  0.26703351736068726
train gradient:  0.08234149021321353
iteration : 665
train acc:  0.875
train loss:  0.34803420305252075
train gradient:  0.11915812569237036
iteration : 666
train acc:  0.8359375
train loss:  0.32706254720687866
train gradient:  0.14937960984653
iteration : 667
train acc:  0.875
train loss:  0.26752957701683044
train gradient:  0.10077494253384918
iteration : 668
train acc:  0.8671875
train loss:  0.33016330003738403
train gradient:  0.16281022237648787
iteration : 669
train acc:  0.84375
train loss:  0.2806374132633209
train gradient:  0.061311517302987374
iteration : 670
train acc:  0.859375
train loss:  0.2911427617073059
train gradient:  0.09864691620115179
iteration : 671
train acc:  0.8671875
train loss:  0.4045870900154114
train gradient:  0.18473713332480568
iteration : 672
train acc:  0.8203125
train loss:  0.32092416286468506
train gradient:  0.12241728337377653
iteration : 673
train acc:  0.875
train loss:  0.31598150730133057
train gradient:  0.1044388341525003
iteration : 674
train acc:  0.8515625
train loss:  0.28284990787506104
train gradient:  0.06930256462754462
iteration : 675
train acc:  0.875
train loss:  0.32340192794799805
train gradient:  0.08732440879565877
iteration : 676
train acc:  0.8515625
train loss:  0.32367339730262756
train gradient:  0.11671286761258085
iteration : 677
train acc:  0.84375
train loss:  0.3618052005767822
train gradient:  0.13400075516326537
iteration : 678
train acc:  0.859375
train loss:  0.2971178889274597
train gradient:  0.13918553262265376
iteration : 679
train acc:  0.8671875
train loss:  0.254599004983902
train gradient:  0.08640573559744935
iteration : 680
train acc:  0.8671875
train loss:  0.3523644208908081
train gradient:  0.15333680518616058
iteration : 681
train acc:  0.9140625
train loss:  0.24499022960662842
train gradient:  0.06760355729056552
iteration : 682
train acc:  0.828125
train loss:  0.3986760973930359
train gradient:  0.15775013317974954
iteration : 683
train acc:  0.8984375
train loss:  0.267276406288147
train gradient:  0.09352764020091787
iteration : 684
train acc:  0.875
train loss:  0.2950989902019501
train gradient:  0.1374732288731016
iteration : 685
train acc:  0.8515625
train loss:  0.2845495939254761
train gradient:  0.12522577673651517
iteration : 686
train acc:  0.875
train loss:  0.3305777907371521
train gradient:  0.13316702673997033
iteration : 687
train acc:  0.8828125
train loss:  0.33253058791160583
train gradient:  0.1223440077052104
iteration : 688
train acc:  0.84375
train loss:  0.3758354187011719
train gradient:  0.1503615977685341
iteration : 689
train acc:  0.8671875
train loss:  0.26734355092048645
train gradient:  0.13100382428831883
iteration : 690
train acc:  0.8046875
train loss:  0.36575910449028015
train gradient:  0.2070751242894462
iteration : 691
train acc:  0.890625
train loss:  0.27829355001449585
train gradient:  0.09843733017142178
iteration : 692
train acc:  0.875
train loss:  0.3125750422477722
train gradient:  0.0906977127083554
iteration : 693
train acc:  0.875
train loss:  0.32733750343322754
train gradient:  0.10937457394807909
iteration : 694
train acc:  0.890625
train loss:  0.3112076222896576
train gradient:  0.11623037853807562
iteration : 695
train acc:  0.8671875
train loss:  0.287741482257843
train gradient:  0.11873031446733147
iteration : 696
train acc:  0.84375
train loss:  0.3348190188407898
train gradient:  0.13582668839135517
iteration : 697
train acc:  0.9296875
train loss:  0.23058167099952698
train gradient:  0.08938743561469288
iteration : 698
train acc:  0.8671875
train loss:  0.35392624139785767
train gradient:  0.1495479197452512
iteration : 699
train acc:  0.8984375
train loss:  0.30347347259521484
train gradient:  0.0924316740172098
iteration : 700
train acc:  0.8515625
train loss:  0.31873366236686707
train gradient:  0.16129953868590613
iteration : 701
train acc:  0.8359375
train loss:  0.37307125329971313
train gradient:  0.16639429919304732
iteration : 702
train acc:  0.890625
train loss:  0.2678559720516205
train gradient:  0.0978902244032104
iteration : 703
train acc:  0.90625
train loss:  0.28042247891426086
train gradient:  0.11357332613849029
iteration : 704
train acc:  0.921875
train loss:  0.2140771448612213
train gradient:  0.07215506165010292
iteration : 705
train acc:  0.8984375
train loss:  0.275129109621048
train gradient:  0.09642229522532078
iteration : 706
train acc:  0.890625
train loss:  0.2758592367172241
train gradient:  0.12651088005550892
iteration : 707
train acc:  0.890625
train loss:  0.22147704660892487
train gradient:  0.08725441949243654
iteration : 708
train acc:  0.8515625
train loss:  0.31093621253967285
train gradient:  0.13137088221989213
iteration : 709
train acc:  0.875
train loss:  0.3281043767929077
train gradient:  0.14385687675280662
iteration : 710
train acc:  0.875
train loss:  0.2868001461029053
train gradient:  0.09874006670516201
iteration : 711
train acc:  0.8203125
train loss:  0.40019744634628296
train gradient:  0.19343742425263577
iteration : 712
train acc:  0.8515625
train loss:  0.33525633811950684
train gradient:  0.14638612684426666
iteration : 713
train acc:  0.921875
train loss:  0.22096140682697296
train gradient:  0.08303886651961201
iteration : 714
train acc:  0.921875
train loss:  0.25799277424812317
train gradient:  0.09477884950835046
iteration : 715
train acc:  0.828125
train loss:  0.40596699714660645
train gradient:  0.16074117531421178
iteration : 716
train acc:  0.8828125
train loss:  0.2154955118894577
train gradient:  0.09340358870054875
iteration : 717
train acc:  0.8515625
train loss:  0.324878990650177
train gradient:  0.14880285817341896
iteration : 718
train acc:  0.9140625
train loss:  0.23494860529899597
train gradient:  0.0936342552799641
iteration : 719
train acc:  0.859375
train loss:  0.2901967167854309
train gradient:  0.10889219296426801
iteration : 720
train acc:  0.8515625
train loss:  0.359916627407074
train gradient:  0.1408488053233498
iteration : 721
train acc:  0.8828125
train loss:  0.28885436058044434
train gradient:  0.10705072662177692
iteration : 722
train acc:  0.8828125
train loss:  0.25824669003486633
train gradient:  0.0911895929643068
iteration : 723
train acc:  0.8515625
train loss:  0.40847960114479065
train gradient:  0.19619719591589302
iteration : 724
train acc:  0.8984375
train loss:  0.27120134234428406
train gradient:  0.09177265564777563
iteration : 725
train acc:  0.8515625
train loss:  0.3781009018421173
train gradient:  0.15122083507501766
iteration : 726
train acc:  0.8671875
train loss:  0.31414544582366943
train gradient:  0.1231560284994728
iteration : 727
train acc:  0.8359375
train loss:  0.29738980531692505
train gradient:  0.1055667413079831
iteration : 728
train acc:  0.9140625
train loss:  0.26106423139572144
train gradient:  0.10519088357822143
iteration : 729
train acc:  0.828125
train loss:  0.36484068632125854
train gradient:  0.25514576030460057
iteration : 730
train acc:  0.8828125
train loss:  0.26681655645370483
train gradient:  0.07786448061258425
iteration : 731
train acc:  0.8671875
train loss:  0.27437910437583923
train gradient:  0.14074409276649436
iteration : 732
train acc:  0.875
train loss:  0.28543198108673096
train gradient:  0.0909859379793236
iteration : 733
train acc:  0.828125
train loss:  0.38074105978012085
train gradient:  0.17252664558012354
iteration : 734
train acc:  0.8515625
train loss:  0.3568391799926758
train gradient:  0.14226962234889162
iteration : 735
train acc:  0.8125
train loss:  0.3532995879650116
train gradient:  0.28341915602211204
iteration : 736
train acc:  0.8828125
train loss:  0.2761680483818054
train gradient:  0.10965555889051684
iteration : 737
train acc:  0.8515625
train loss:  0.2834373414516449
train gradient:  0.11867365868947981
iteration : 738
train acc:  0.859375
train loss:  0.37309911847114563
train gradient:  0.1468195771584074
iteration : 739
train acc:  0.8515625
train loss:  0.32506054639816284
train gradient:  0.19056772957653426
iteration : 740
train acc:  0.8984375
train loss:  0.2611830234527588
train gradient:  0.0939116565851943
iteration : 741
train acc:  0.8828125
train loss:  0.2701479196548462
train gradient:  0.09720983565433751
iteration : 742
train acc:  0.890625
train loss:  0.2863224148750305
train gradient:  0.11075086653262263
iteration : 743
train acc:  0.828125
train loss:  0.3427032232284546
train gradient:  0.12309162548705446
iteration : 744
train acc:  0.90625
train loss:  0.31567996740341187
train gradient:  0.23163131909405932
iteration : 745
train acc:  0.8828125
train loss:  0.30646175146102905
train gradient:  0.10952325360737282
iteration : 746
train acc:  0.84375
train loss:  0.3699703812599182
train gradient:  0.13843696477145684
iteration : 747
train acc:  0.828125
train loss:  0.36689579486846924
train gradient:  0.16386737813983165
iteration : 748
train acc:  0.859375
train loss:  0.3008246421813965
train gradient:  0.09127655203493593
iteration : 749
train acc:  0.90625
train loss:  0.2327268421649933
train gradient:  0.13474849100981917
iteration : 750
train acc:  0.890625
train loss:  0.2750064730644226
train gradient:  0.12392451024705967
iteration : 751
train acc:  0.8359375
train loss:  0.3342013359069824
train gradient:  0.17810251954754264
iteration : 752
train acc:  0.8515625
train loss:  0.30773109197616577
train gradient:  0.13197587274154876
iteration : 753
train acc:  0.84375
train loss:  0.3861338794231415
train gradient:  0.19910785401323305
iteration : 754
train acc:  0.8671875
train loss:  0.2728305160999298
train gradient:  0.11615555299514213
iteration : 755
train acc:  0.84375
train loss:  0.3519894480705261
train gradient:  0.18111583076062643
iteration : 756
train acc:  0.890625
train loss:  0.2940196990966797
train gradient:  0.0999753636269824
iteration : 757
train acc:  0.8671875
train loss:  0.30745255947113037
train gradient:  0.11982411379374286
iteration : 758
train acc:  0.859375
train loss:  0.35804954171180725
train gradient:  0.1936577193238158
iteration : 759
train acc:  0.828125
train loss:  0.33975934982299805
train gradient:  0.12311461448932426
iteration : 760
train acc:  0.8671875
train loss:  0.3282807469367981
train gradient:  0.17609998851832454
iteration : 761
train acc:  0.8828125
train loss:  0.25537973642349243
train gradient:  0.12726711020636605
iteration : 762
train acc:  0.8828125
train loss:  0.2658138871192932
train gradient:  0.08713537300211073
iteration : 763
train acc:  0.8125
train loss:  0.3725470304489136
train gradient:  0.14229099381066987
iteration : 764
train acc:  0.84375
train loss:  0.3632723093032837
train gradient:  0.15974820090039463
iteration : 765
train acc:  0.875
train loss:  0.2998548746109009
train gradient:  0.12552556078435131
iteration : 766
train acc:  0.921875
train loss:  0.22993576526641846
train gradient:  0.07459442785318288
iteration : 767
train acc:  0.875
train loss:  0.29532256722450256
train gradient:  0.1659635214416385
iteration : 768
train acc:  0.8671875
train loss:  0.3059893846511841
train gradient:  0.21992318413938017
iteration : 769
train acc:  0.8828125
train loss:  0.23955701291561127
train gradient:  0.10916458202678793
iteration : 770
train acc:  0.84375
train loss:  0.34703513979911804
train gradient:  0.2214209530082779
iteration : 771
train acc:  0.875
train loss:  0.3338702619075775
train gradient:  0.12064010719624516
iteration : 772
train acc:  0.8671875
train loss:  0.3270688056945801
train gradient:  0.18476584509515753
iteration : 773
train acc:  0.8984375
train loss:  0.2134648859500885
train gradient:  0.061535684755063685
iteration : 774
train acc:  0.90625
train loss:  0.2598838210105896
train gradient:  0.09735851326952917
iteration : 775
train acc:  0.875
train loss:  0.2918897569179535
train gradient:  0.13174242825229737
iteration : 776
train acc:  0.8203125
train loss:  0.33885622024536133
train gradient:  0.12553465217366838
iteration : 777
train acc:  0.890625
train loss:  0.23768121004104614
train gradient:  0.07942428954177867
iteration : 778
train acc:  0.859375
train loss:  0.305357962846756
train gradient:  0.17714710915829054
iteration : 779
train acc:  0.8359375
train loss:  0.3540058732032776
train gradient:  0.12330315424446456
iteration : 780
train acc:  0.8515625
train loss:  0.33691173791885376
train gradient:  0.15714630120092196
iteration : 781
train acc:  0.890625
train loss:  0.3095567524433136
train gradient:  1.3533768001773947
iteration : 782
train acc:  0.84375
train loss:  0.3104816973209381
train gradient:  0.1486851487332161
iteration : 783
train acc:  0.8671875
train loss:  0.31661468744277954
train gradient:  0.1153223539010989
iteration : 784
train acc:  0.8671875
train loss:  0.32252639532089233
train gradient:  0.17518367403205903
iteration : 785
train acc:  0.859375
train loss:  0.2842637300491333
train gradient:  0.09069328199774909
iteration : 786
train acc:  0.84375
train loss:  0.3004285991191864
train gradient:  0.13419878287502723
iteration : 787
train acc:  0.8828125
train loss:  0.3220687210559845
train gradient:  0.14343867342926397
iteration : 788
train acc:  0.8828125
train loss:  0.3012523055076599
train gradient:  0.12217487282638217
iteration : 789
train acc:  0.8515625
train loss:  0.3201368451118469
train gradient:  0.10278337463546564
iteration : 790
train acc:  0.859375
train loss:  0.2761407494544983
train gradient:  0.12077245231755966
iteration : 791
train acc:  0.8828125
train loss:  0.2706383466720581
train gradient:  0.11280348583681424
iteration : 792
train acc:  0.8828125
train loss:  0.2815914750099182
train gradient:  0.10004109484972719
iteration : 793
train acc:  0.8671875
train loss:  0.2824104428291321
train gradient:  0.13060307731341536
iteration : 794
train acc:  0.890625
train loss:  0.25286680459976196
train gradient:  0.0816763869474954
iteration : 795
train acc:  0.828125
train loss:  0.3228081464767456
train gradient:  0.1307160081733985
iteration : 796
train acc:  0.8203125
train loss:  0.35904181003570557
train gradient:  0.17848103761310752
iteration : 797
train acc:  0.8515625
train loss:  0.360622376203537
train gradient:  0.20235087386078143
iteration : 798
train acc:  0.84375
train loss:  0.3646981716156006
train gradient:  0.19527509814132543
iteration : 799
train acc:  0.8984375
train loss:  0.33223533630371094
train gradient:  0.10780451307052673
iteration : 800
train acc:  0.875
train loss:  0.34165507555007935
train gradient:  0.16969476969155262
iteration : 801
train acc:  0.8515625
train loss:  0.29830601811408997
train gradient:  0.14987688644103436
iteration : 802
train acc:  0.828125
train loss:  0.40190866589546204
train gradient:  0.18608099690306978
iteration : 803
train acc:  0.8828125
train loss:  0.2711604833602905
train gradient:  0.10745149846602846
iteration : 804
train acc:  0.8671875
train loss:  0.32398658990859985
train gradient:  0.14637734290451315
iteration : 805
train acc:  0.8828125
train loss:  0.2586500346660614
train gradient:  0.09692126299974337
iteration : 806
train acc:  0.953125
train loss:  0.1724044233560562
train gradient:  0.07974214419551982
iteration : 807
train acc:  0.84375
train loss:  0.3303622007369995
train gradient:  0.11438651832888916
iteration : 808
train acc:  0.8671875
train loss:  0.31890928745269775
train gradient:  0.10656933844462667
iteration : 809
train acc:  0.7890625
train loss:  0.44901078939437866
train gradient:  0.27438854780708655
iteration : 810
train acc:  0.8359375
train loss:  0.3411228060722351
train gradient:  0.13746789674330542
iteration : 811
train acc:  0.8125
train loss:  0.374009907245636
train gradient:  0.15904954745951377
iteration : 812
train acc:  0.8671875
train loss:  0.3444548547267914
train gradient:  0.1445129076636301
iteration : 813
train acc:  0.8828125
train loss:  0.3477823734283447
train gradient:  0.15347021250682086
iteration : 814
train acc:  0.9140625
train loss:  0.24170176684856415
train gradient:  0.09683132728916549
iteration : 815
train acc:  0.9140625
train loss:  0.23470205068588257
train gradient:  0.09427354539291971
iteration : 816
train acc:  0.875
train loss:  0.3749692738056183
train gradient:  0.12830865263882246
iteration : 817
train acc:  0.8515625
train loss:  0.3259866237640381
train gradient:  0.11925364137115735
iteration : 818
train acc:  0.859375
train loss:  0.3160350024700165
train gradient:  0.12068574875907193
iteration : 819
train acc:  0.8515625
train loss:  0.385791540145874
train gradient:  0.18251757985788408
iteration : 820
train acc:  0.8515625
train loss:  0.31941094994544983
train gradient:  0.16010334640267845
iteration : 821
train acc:  0.8515625
train loss:  0.3086710572242737
train gradient:  0.11310260923826486
iteration : 822
train acc:  0.8671875
train loss:  0.34110161662101746
train gradient:  0.14624081918092197
iteration : 823
train acc:  0.859375
train loss:  0.36063557863235474
train gradient:  0.13278690973291774
iteration : 824
train acc:  0.84375
train loss:  0.3167191743850708
train gradient:  0.156935270761908
iteration : 825
train acc:  0.875
train loss:  0.2979412376880646
train gradient:  0.14753511006467668
iteration : 826
train acc:  0.8359375
train loss:  0.33085623383522034
train gradient:  0.18138480743269683
iteration : 827
train acc:  0.8671875
train loss:  0.3263539671897888
train gradient:  0.15275051943091977
iteration : 828
train acc:  0.8515625
train loss:  0.3603617250919342
train gradient:  0.18110419223074226
iteration : 829
train acc:  0.828125
train loss:  0.32081303000450134
train gradient:  0.11475031368151532
iteration : 830
train acc:  0.84375
train loss:  0.3674861788749695
train gradient:  0.18993218864429137
iteration : 831
train acc:  0.890625
train loss:  0.24711942672729492
train gradient:  0.08858568599514946
iteration : 832
train acc:  0.890625
train loss:  0.347991406917572
train gradient:  0.19682924875388408
iteration : 833
train acc:  0.8359375
train loss:  0.32725459337234497
train gradient:  0.10555276789860087
iteration : 834
train acc:  0.8359375
train loss:  0.30630165338516235
train gradient:  0.12582688042518456
iteration : 835
train acc:  0.8671875
train loss:  0.2908002734184265
train gradient:  0.11207621344808547
iteration : 836
train acc:  0.8359375
train loss:  0.34528064727783203
train gradient:  0.12532589430345903
iteration : 837
train acc:  0.8515625
train loss:  0.31160295009613037
train gradient:  0.15671763572051312
iteration : 838
train acc:  0.8671875
train loss:  0.300812304019928
train gradient:  0.11380367634258419
iteration : 839
train acc:  0.8671875
train loss:  0.32594603300094604
train gradient:  0.2004268162393057
iteration : 840
train acc:  0.90625
train loss:  0.27837419509887695
train gradient:  0.12794520141693855
iteration : 841
train acc:  0.875
train loss:  0.2806411683559418
train gradient:  0.15296246609432684
iteration : 842
train acc:  0.8671875
train loss:  0.2926951050758362
train gradient:  0.12130238145414314
iteration : 843
train acc:  0.8203125
train loss:  0.3086150288581848
train gradient:  0.08083047224316421
iteration : 844
train acc:  0.8828125
train loss:  0.3268861174583435
train gradient:  0.13926458494425897
iteration : 845
train acc:  0.8515625
train loss:  0.3685262203216553
train gradient:  0.1749808386830356
iteration : 846
train acc:  0.8984375
train loss:  0.26970505714416504
train gradient:  0.08121674235311305
iteration : 847
train acc:  0.8984375
train loss:  0.2878531813621521
train gradient:  0.10699745709716257
iteration : 848
train acc:  0.828125
train loss:  0.4014625549316406
train gradient:  0.15520128074441095
iteration : 849
train acc:  0.8515625
train loss:  0.2818955183029175
train gradient:  0.08655164772337144
iteration : 850
train acc:  0.890625
train loss:  0.2520345449447632
train gradient:  0.09227699910964841
iteration : 851
train acc:  0.8671875
train loss:  0.28064805269241333
train gradient:  0.12467459081312306
iteration : 852
train acc:  0.8984375
train loss:  0.31416475772857666
train gradient:  0.08205475777913332
iteration : 853
train acc:  0.8828125
train loss:  0.2802295684814453
train gradient:  0.12832971067846743
iteration : 854
train acc:  0.8046875
train loss:  0.4385479688644409
train gradient:  0.19539872300724348
iteration : 855
train acc:  0.8828125
train loss:  0.32445940375328064
train gradient:  0.12001218974378682
iteration : 856
train acc:  0.828125
train loss:  0.35830312967300415
train gradient:  0.23714844467195384
iteration : 857
train acc:  0.8515625
train loss:  0.35520175099372864
train gradient:  0.17263677972279273
iteration : 858
train acc:  0.8359375
train loss:  0.38531747460365295
train gradient:  0.16364540171220227
iteration : 859
train acc:  0.8671875
train loss:  0.286970317363739
train gradient:  0.10758678559518595
iteration : 860
train acc:  0.859375
train loss:  0.3423987627029419
train gradient:  0.11655689868352531
iteration : 861
train acc:  0.859375
train loss:  0.2990815043449402
train gradient:  0.0861875360820413
iteration : 862
train acc:  0.8515625
train loss:  0.3206528425216675
train gradient:  0.13928158594797424
iteration : 863
train acc:  0.8203125
train loss:  0.3280773162841797
train gradient:  0.10379758745089046
iteration : 864
train acc:  0.8828125
train loss:  0.2770904302597046
train gradient:  0.10520452582047984
iteration : 865
train acc:  0.8828125
train loss:  0.2622227072715759
train gradient:  0.09288961050855134
iteration : 866
train acc:  0.84375
train loss:  0.29346850514411926
train gradient:  0.10146901876175543
iteration : 867
train acc:  0.8671875
train loss:  0.32845786213874817
train gradient:  0.12501374844128954
iteration : 868
train acc:  0.828125
train loss:  0.3491097688674927
train gradient:  0.15823968489813933
iteration : 869
train acc:  0.8671875
train loss:  0.3166183531284332
train gradient:  0.12306498246854801
iteration : 870
train acc:  0.84375
train loss:  0.31803470849990845
train gradient:  0.12659032965695877
iteration : 871
train acc:  0.9296875
train loss:  0.2179235965013504
train gradient:  0.09866230539771234
iteration : 872
train acc:  0.8515625
train loss:  0.3068395256996155
train gradient:  0.07493567934455057
iteration : 873
train acc:  0.875
train loss:  0.2577599883079529
train gradient:  0.10997713296245258
iteration : 874
train acc:  0.90625
train loss:  0.22686150670051575
train gradient:  0.05368756584995765
iteration : 875
train acc:  0.890625
train loss:  0.2703686058521271
train gradient:  0.11537865382193632
iteration : 876
train acc:  0.8671875
train loss:  0.3326594829559326
train gradient:  0.22506141982483807
iteration : 877
train acc:  0.875
train loss:  0.32803186774253845
train gradient:  0.11162310633639244
iteration : 878
train acc:  0.8359375
train loss:  0.3592500686645508
train gradient:  0.16384415212366582
iteration : 879
train acc:  0.8203125
train loss:  0.3196856379508972
train gradient:  0.144584146229832
iteration : 880
train acc:  0.8046875
train loss:  0.42795103788375854
train gradient:  0.24481391001066502
iteration : 881
train acc:  0.8515625
train loss:  0.32950857281684875
train gradient:  0.09805169183787026
iteration : 882
train acc:  0.875
train loss:  0.3013237416744232
train gradient:  0.11454213207931774
iteration : 883
train acc:  0.8828125
train loss:  0.2874504029750824
train gradient:  0.15762113041866943
iteration : 884
train acc:  0.8828125
train loss:  0.30467355251312256
train gradient:  0.15696115550646095
iteration : 885
train acc:  0.9296875
train loss:  0.2046176791191101
train gradient:  0.04101170752134714
iteration : 886
train acc:  0.8671875
train loss:  0.28306692838668823
train gradient:  0.09847900657493656
iteration : 887
train acc:  0.8046875
train loss:  0.3711283206939697
train gradient:  0.1553693770332109
iteration : 888
train acc:  0.84375
train loss:  0.3766990005970001
train gradient:  0.20769376184660832
iteration : 889
train acc:  0.8515625
train loss:  0.3002597689628601
train gradient:  0.15032228001210013
iteration : 890
train acc:  0.8515625
train loss:  0.37213215231895447
train gradient:  0.1571284927631592
iteration : 891
train acc:  0.90625
train loss:  0.23398882150650024
train gradient:  0.09189055294815027
iteration : 892
train acc:  0.90625
train loss:  0.26162293553352356
train gradient:  0.09864482686486095
iteration : 893
train acc:  0.859375
train loss:  0.3326696753501892
train gradient:  0.13341115352798016
iteration : 894
train acc:  0.8984375
train loss:  0.3102657198905945
train gradient:  0.09978697745594929
iteration : 895
train acc:  0.8828125
train loss:  0.32446545362472534
train gradient:  0.15077977463366812
iteration : 896
train acc:  0.8671875
train loss:  0.3271273672580719
train gradient:  0.12371402348832841
iteration : 897
train acc:  0.8359375
train loss:  0.41431254148483276
train gradient:  0.19996526764259906
iteration : 898
train acc:  0.875
train loss:  0.3114836513996124
train gradient:  0.11400180052041148
iteration : 899
train acc:  0.8828125
train loss:  0.3306306004524231
train gradient:  0.1188140146217206
iteration : 900
train acc:  0.875
train loss:  0.3303177058696747
train gradient:  0.13014452129752857
iteration : 901
train acc:  0.90625
train loss:  0.25596529245376587
train gradient:  0.10034779817782476
iteration : 902
train acc:  0.84375
train loss:  0.3503430187702179
train gradient:  0.15525771027117846
iteration : 903
train acc:  0.859375
train loss:  0.30008652806282043
train gradient:  0.13115797829441833
iteration : 904
train acc:  0.8828125
train loss:  0.2814520001411438
train gradient:  0.09937973102569005
iteration : 905
train acc:  0.890625
train loss:  0.2948806881904602
train gradient:  0.12662061119202792
iteration : 906
train acc:  0.8125
train loss:  0.39025235176086426
train gradient:  0.17904384544440843
iteration : 907
train acc:  0.953125
train loss:  0.20471084117889404
train gradient:  0.1105653536968559
iteration : 908
train acc:  0.875
train loss:  0.33434179425239563
train gradient:  0.17223365922704686
iteration : 909
train acc:  0.8984375
train loss:  0.30369046330451965
train gradient:  0.10167134537592364
iteration : 910
train acc:  0.8515625
train loss:  0.280860036611557
train gradient:  0.09800558639080313
iteration : 911
train acc:  0.859375
train loss:  0.3055814206600189
train gradient:  0.12134234957300473
iteration : 912
train acc:  0.890625
train loss:  0.30150026082992554
train gradient:  0.12612952085203527
iteration : 913
train acc:  0.8203125
train loss:  0.3364683985710144
train gradient:  0.1914203366600417
iteration : 914
train acc:  0.8359375
train loss:  0.36347225308418274
train gradient:  0.12667073924153022
iteration : 915
train acc:  0.859375
train loss:  0.3007122874259949
train gradient:  0.1207608491763973
iteration : 916
train acc:  0.84375
train loss:  0.3688061833381653
train gradient:  0.14303634904140794
iteration : 917
train acc:  0.796875
train loss:  0.42159557342529297
train gradient:  0.18035160392176847
iteration : 918
train acc:  0.8828125
train loss:  0.30018943548202515
train gradient:  0.09994040752802298
iteration : 919
train acc:  0.828125
train loss:  0.3520602285861969
train gradient:  0.14029870953307852
iteration : 920
train acc:  0.859375
train loss:  0.2908099889755249
train gradient:  0.13752829326493748
iteration : 921
train acc:  0.9140625
train loss:  0.28440964221954346
train gradient:  0.08925209015783139
iteration : 922
train acc:  0.875
train loss:  0.26369744539260864
train gradient:  0.08740743557750189
iteration : 923
train acc:  0.8984375
train loss:  0.27588385343551636
train gradient:  0.1042678843979469
iteration : 924
train acc:  0.9296875
train loss:  0.19539302587509155
train gradient:  0.06928585148719361
iteration : 925
train acc:  0.828125
train loss:  0.3252163827419281
train gradient:  0.13322032272755535
iteration : 926
train acc:  0.8359375
train loss:  0.32137367129325867
train gradient:  0.1205248731745403
iteration : 927
train acc:  0.875
train loss:  0.3005603849887848
train gradient:  0.12570843157491246
iteration : 928
train acc:  0.9140625
train loss:  0.21888364851474762
train gradient:  0.09275097683009242
iteration : 929
train acc:  0.875
train loss:  0.29789257049560547
train gradient:  0.1265163288572473
iteration : 930
train acc:  0.90625
train loss:  0.26391661167144775
train gradient:  0.07376617768187846
iteration : 931
train acc:  0.8984375
train loss:  0.25664159655570984
train gradient:  0.0815819434924562
iteration : 932
train acc:  0.921875
train loss:  0.2534569501876831
train gradient:  0.09506504165672901
iteration : 933
train acc:  0.84375
train loss:  0.3310137987136841
train gradient:  0.14802898056825303
iteration : 934
train acc:  0.8828125
train loss:  0.27384787797927856
train gradient:  0.09142071447657649
iteration : 935
train acc:  0.8828125
train loss:  0.2853178381919861
train gradient:  0.1175147799481875
iteration : 936
train acc:  0.8984375
train loss:  0.21018901467323303
train gradient:  0.0746706541859809
iteration : 937
train acc:  0.890625
train loss:  0.2799238860607147
train gradient:  0.11108156804519335
iteration : 938
train acc:  0.8671875
train loss:  0.339582622051239
train gradient:  0.16741152392107975
iteration : 939
train acc:  0.8515625
train loss:  0.37800902128219604
train gradient:  0.1338923285427627
iteration : 940
train acc:  0.859375
train loss:  0.33148112893104553
train gradient:  0.16485490038934833
iteration : 941
train acc:  0.859375
train loss:  0.27382051944732666
train gradient:  0.14969409682550866
iteration : 942
train acc:  0.875
train loss:  0.2871630787849426
train gradient:  0.10093708497578266
iteration : 943
train acc:  0.875
train loss:  0.29370883107185364
train gradient:  0.1555855346688752
iteration : 944
train acc:  0.859375
train loss:  0.2909347116947174
train gradient:  0.1047681989534905
iteration : 945
train acc:  0.8984375
train loss:  0.2886011302471161
train gradient:  0.10170412642133095
iteration : 946
train acc:  0.9296875
train loss:  0.22823935747146606
train gradient:  0.10037524689415891
iteration : 947
train acc:  0.8515625
train loss:  0.30054765939712524
train gradient:  0.09411167119868752
iteration : 948
train acc:  0.8671875
train loss:  0.264076828956604
train gradient:  0.11695913606971527
iteration : 949
train acc:  0.875
train loss:  0.32183024287223816
train gradient:  0.12781312171517037
iteration : 950
train acc:  0.8515625
train loss:  0.3125089406967163
train gradient:  0.19749272629160552
iteration : 951
train acc:  0.8046875
train loss:  0.3588363528251648
train gradient:  0.1681337881320765
iteration : 952
train acc:  0.8203125
train loss:  0.34123754501342773
train gradient:  0.23266968038292946
iteration : 953
train acc:  0.875
train loss:  0.28546375036239624
train gradient:  0.11366698445080678
iteration : 954
train acc:  0.875
train loss:  0.2776353657245636
train gradient:  0.12111424204584034
iteration : 955
train acc:  0.8359375
train loss:  0.3950577676296234
train gradient:  0.22610299579913376
iteration : 956
train acc:  0.828125
train loss:  0.3388093113899231
train gradient:  0.16108748923929198
iteration : 957
train acc:  0.8671875
train loss:  0.28440383076667786
train gradient:  0.1529881812569736
iteration : 958
train acc:  0.8515625
train loss:  0.2926246225833893
train gradient:  0.11380405451015367
iteration : 959
train acc:  0.8828125
train loss:  0.2974042296409607
train gradient:  0.12570959346859445
iteration : 960
train acc:  0.8984375
train loss:  0.2503122091293335
train gradient:  0.10233843694473031
iteration : 961
train acc:  0.8828125
train loss:  0.3333861827850342
train gradient:  0.11797920763866333
iteration : 962
train acc:  0.859375
train loss:  0.30223503708839417
train gradient:  0.14386335558112395
iteration : 963
train acc:  0.8515625
train loss:  0.3251999616622925
train gradient:  0.16931345523138971
iteration : 964
train acc:  0.859375
train loss:  0.29763317108154297
train gradient:  0.14178949683697678
iteration : 965
train acc:  0.8671875
train loss:  0.2840147614479065
train gradient:  0.1919869481035857
iteration : 966
train acc:  0.8828125
train loss:  0.3154504597187042
train gradient:  0.10636138619664719
iteration : 967
train acc:  0.8046875
train loss:  0.3441728353500366
train gradient:  0.15580218822717523
iteration : 968
train acc:  0.859375
train loss:  0.2800610661506653
train gradient:  0.10564164855200514
iteration : 969
train acc:  0.84375
train loss:  0.2755165100097656
train gradient:  0.10093811316285303
iteration : 970
train acc:  0.859375
train loss:  0.32995226979255676
train gradient:  0.1825509839700744
iteration : 971
train acc:  0.8203125
train loss:  0.34015995264053345
train gradient:  0.18209427315203205
iteration : 972
train acc:  0.890625
train loss:  0.2757643759250641
train gradient:  0.12377654526989347
iteration : 973
train acc:  0.8515625
train loss:  0.3913344740867615
train gradient:  0.3107854127241807
iteration : 974
train acc:  0.8515625
train loss:  0.32821065187454224
train gradient:  0.1636010049640864
iteration : 975
train acc:  0.859375
train loss:  0.36433497071266174
train gradient:  0.1666758049304647
iteration : 976
train acc:  0.890625
train loss:  0.269514262676239
train gradient:  0.09708370023359796
iteration : 977
train acc:  0.8125
train loss:  0.3474869728088379
train gradient:  0.13730566027588714
iteration : 978
train acc:  0.8671875
train loss:  0.35837632417678833
train gradient:  0.12930493937009863
iteration : 979
train acc:  0.8203125
train loss:  0.31853628158569336
train gradient:  0.19356120830003032
iteration : 980
train acc:  0.890625
train loss:  0.31581246852874756
train gradient:  0.11539821625316779
iteration : 981
train acc:  0.859375
train loss:  0.3563894033432007
train gradient:  0.14822733427827164
iteration : 982
train acc:  0.8828125
train loss:  0.3301987946033478
train gradient:  0.12760186961992348
iteration : 983
train acc:  0.8828125
train loss:  0.2850721478462219
train gradient:  0.10116439092047143
iteration : 984
train acc:  0.8515625
train loss:  0.2844531834125519
train gradient:  0.11869219483152961
iteration : 985
train acc:  0.875
train loss:  0.2698346972465515
train gradient:  0.14615762779592645
iteration : 986
train acc:  0.875
train loss:  0.27780699729919434
train gradient:  0.10226289117169189
iteration : 987
train acc:  0.859375
train loss:  0.33493489027023315
train gradient:  0.1928081024488128
iteration : 988
train acc:  0.8359375
train loss:  0.34060192108154297
train gradient:  0.12503668932732262
iteration : 989
train acc:  0.859375
train loss:  0.30353349447250366
train gradient:  0.14535120349942188
iteration : 990
train acc:  0.8515625
train loss:  0.38249099254608154
train gradient:  0.19934853209360298
iteration : 991
train acc:  0.828125
train loss:  0.3887662887573242
train gradient:  0.1665324937742494
iteration : 992
train acc:  0.8359375
train loss:  0.3779914677143097
train gradient:  0.1433935662403583
iteration : 993
train acc:  0.8671875
train loss:  0.3154563903808594
train gradient:  0.12518470959159542
iteration : 994
train acc:  0.8671875
train loss:  0.29619529843330383
train gradient:  0.1151793327872053
iteration : 995
train acc:  0.828125
train loss:  0.3372962474822998
train gradient:  0.15938691957967804
iteration : 996
train acc:  0.9140625
train loss:  0.25041627883911133
train gradient:  0.08322822471286177
iteration : 997
train acc:  0.890625
train loss:  0.24601617455482483
train gradient:  0.08001211382131375
iteration : 998
train acc:  0.84375
train loss:  0.3445483446121216
train gradient:  0.17195625574988957
iteration : 999
train acc:  0.8203125
train loss:  0.4124673902988434
train gradient:  0.18533019197632708
iteration : 1000
train acc:  0.859375
train loss:  0.2826295495033264
train gradient:  0.15982219879021242
iteration : 1001
train acc:  0.890625
train loss:  0.281434029340744
train gradient:  0.13261610472824908
iteration : 1002
train acc:  0.8671875
train loss:  0.27941569685935974
train gradient:  0.20073031379257147
iteration : 1003
train acc:  0.859375
train loss:  0.32030975818634033
train gradient:  0.11320684384513369
iteration : 1004
train acc:  0.84375
train loss:  0.29921919107437134
train gradient:  0.14735678133751365
iteration : 1005
train acc:  0.859375
train loss:  0.3642422556877136
train gradient:  0.1826532542240854
iteration : 1006
train acc:  0.84375
train loss:  0.2940952777862549
train gradient:  0.15978503064343008
iteration : 1007
train acc:  0.828125
train loss:  0.3398606777191162
train gradient:  0.10040684471631656
iteration : 1008
train acc:  0.84375
train loss:  0.3171283006668091
train gradient:  0.14152389566117685
iteration : 1009
train acc:  0.828125
train loss:  0.31001436710357666
train gradient:  0.08885119846407688
iteration : 1010
train acc:  0.859375
train loss:  0.32013940811157227
train gradient:  0.1217746613050668
iteration : 1011
train acc:  0.8828125
train loss:  0.26319053769111633
train gradient:  0.07547907921220043
iteration : 1012
train acc:  0.8359375
train loss:  0.3756967782974243
train gradient:  0.13849973621295752
iteration : 1013
train acc:  0.921875
train loss:  0.23640188574790955
train gradient:  0.0682669640272453
iteration : 1014
train acc:  0.8828125
train loss:  0.2933376133441925
train gradient:  0.10977736245443563
iteration : 1015
train acc:  0.890625
train loss:  0.3124462366104126
train gradient:  0.1709899412171867
iteration : 1016
train acc:  0.8828125
train loss:  0.25217223167419434
train gradient:  0.10327694082628586
iteration : 1017
train acc:  0.890625
train loss:  0.28405261039733887
train gradient:  0.1449007226539446
iteration : 1018
train acc:  0.8359375
train loss:  0.33921730518341064
train gradient:  0.14308139514201215
iteration : 1019
train acc:  0.84375
train loss:  0.32131385803222656
train gradient:  0.29945675051421516
iteration : 1020
train acc:  0.8125
train loss:  0.3684961795806885
train gradient:  0.163077225948967
iteration : 1021
train acc:  0.890625
train loss:  0.2805714011192322
train gradient:  0.09049946952804362
iteration : 1022
train acc:  0.859375
train loss:  0.24978506565093994
train gradient:  0.11273105988138504
iteration : 1023
train acc:  0.9140625
train loss:  0.2580193281173706
train gradient:  0.09028764921565544
iteration : 1024
train acc:  0.859375
train loss:  0.297526478767395
train gradient:  0.10184402614997913
iteration : 1025
train acc:  0.8359375
train loss:  0.35347476601600647
train gradient:  0.17112903029089166
iteration : 1026
train acc:  0.921875
train loss:  0.22942382097244263
train gradient:  0.09769181721497691
iteration : 1027
train acc:  0.9296875
train loss:  0.23108398914337158
train gradient:  0.06298904574956926
iteration : 1028
train acc:  0.859375
train loss:  0.3170735538005829
train gradient:  0.13058390563693037
iteration : 1029
train acc:  0.890625
train loss:  0.2503702640533447
train gradient:  0.10025875147772978
iteration : 1030
train acc:  0.859375
train loss:  0.32103389501571655
train gradient:  0.22197235962582146
iteration : 1031
train acc:  0.890625
train loss:  0.3253638446331024
train gradient:  0.1461140642019578
iteration : 1032
train acc:  0.8203125
train loss:  0.37326866388320923
train gradient:  0.154143355892674
iteration : 1033
train acc:  0.859375
train loss:  0.3008015751838684
train gradient:  0.12548135065643518
iteration : 1034
train acc:  0.8828125
train loss:  0.33472245931625366
train gradient:  0.11062168310782322
iteration : 1035
train acc:  0.8671875
train loss:  0.31915828585624695
train gradient:  0.1460226406296234
iteration : 1036
train acc:  0.8671875
train loss:  0.28695428371429443
train gradient:  0.14553268828124924
iteration : 1037
train acc:  0.890625
train loss:  0.241029292345047
train gradient:  0.08602016882539948
iteration : 1038
train acc:  0.84375
train loss:  0.3334342837333679
train gradient:  0.15634701067237866
iteration : 1039
train acc:  0.875
train loss:  0.35038238763809204
train gradient:  0.11436398285224385
iteration : 1040
train acc:  0.90625
train loss:  0.2535303235054016
train gradient:  0.08837936735112373
iteration : 1041
train acc:  0.8671875
train loss:  0.28275495767593384
train gradient:  0.10673520266137833
iteration : 1042
train acc:  0.859375
train loss:  0.2515302300453186
train gradient:  0.10402661828501249
iteration : 1043
train acc:  0.8828125
train loss:  0.31375813484191895
train gradient:  0.15914892735295133
iteration : 1044
train acc:  0.90625
train loss:  0.24415580928325653
train gradient:  0.10906674840889242
iteration : 1045
train acc:  0.84375
train loss:  0.37293264269828796
train gradient:  0.17956426229797864
iteration : 1046
train acc:  0.8671875
train loss:  0.2968416213989258
train gradient:  0.11132807910286913
iteration : 1047
train acc:  0.890625
train loss:  0.29603537917137146
train gradient:  0.10314282560610197
iteration : 1048
train acc:  0.8515625
train loss:  0.30504685640335083
train gradient:  0.09144328776799122
iteration : 1049
train acc:  0.8984375
train loss:  0.25559648871421814
train gradient:  0.09776853525610972
iteration : 1050
train acc:  0.8671875
train loss:  0.2935282289981842
train gradient:  0.12630134746781285
iteration : 1051
train acc:  0.90625
train loss:  0.22465629875659943
train gradient:  0.06326172369331756
iteration : 1052
train acc:  0.9296875
train loss:  0.18724758923053741
train gradient:  0.07339070787354526
iteration : 1053
train acc:  0.8671875
train loss:  0.31079959869384766
train gradient:  0.14224935892063978
iteration : 1054
train acc:  0.8984375
train loss:  0.32153815031051636
train gradient:  0.130559226174114
iteration : 1055
train acc:  0.8828125
train loss:  0.2552180588245392
train gradient:  0.07405760321285028
iteration : 1056
train acc:  0.828125
train loss:  0.3309594392776489
train gradient:  0.1196466521482978
iteration : 1057
train acc:  0.828125
train loss:  0.3816052973270416
train gradient:  0.21824514256677796
iteration : 1058
train acc:  0.875
train loss:  0.2502673864364624
train gradient:  0.10314756463127045
iteration : 1059
train acc:  0.8984375
train loss:  0.3237246870994568
train gradient:  0.1576859761410725
iteration : 1060
train acc:  0.875
train loss:  0.3065013885498047
train gradient:  0.11989953122134481
iteration : 1061
train acc:  0.90625
train loss:  0.24749934673309326
train gradient:  0.11695433197872643
iteration : 1062
train acc:  0.859375
train loss:  0.24552389979362488
train gradient:  0.13924685816499474
iteration : 1063
train acc:  0.90625
train loss:  0.25213468074798584
train gradient:  0.06942969917630376
iteration : 1064
train acc:  0.8671875
train loss:  0.33834606409072876
train gradient:  0.14967186639562455
iteration : 1065
train acc:  0.8203125
train loss:  0.3224646747112274
train gradient:  0.133652104326549
iteration : 1066
train acc:  0.8828125
train loss:  0.2717210650444031
train gradient:  0.09487523717275298
iteration : 1067
train acc:  0.875
train loss:  0.27534788846969604
train gradient:  0.13419725161869242
iteration : 1068
train acc:  0.859375
train loss:  0.3309950828552246
train gradient:  0.1333525204243085
iteration : 1069
train acc:  0.8671875
train loss:  0.33746862411499023
train gradient:  0.19684092243323242
iteration : 1070
train acc:  0.859375
train loss:  0.33627229928970337
train gradient:  0.21185595836443377
iteration : 1071
train acc:  0.8984375
train loss:  0.3258562684059143
train gradient:  0.1286052751468154
iteration : 1072
train acc:  0.8125
train loss:  0.439151406288147
train gradient:  0.2510983284466012
iteration : 1073
train acc:  0.90625
train loss:  0.35460200905799866
train gradient:  0.29821441170228996
iteration : 1074
train acc:  0.8203125
train loss:  0.38047730922698975
train gradient:  0.22096806321527318
iteration : 1075
train acc:  0.8359375
train loss:  0.43069741129875183
train gradient:  0.25055473971143316
iteration : 1076
train acc:  0.8984375
train loss:  0.24471428990364075
train gradient:  0.06656926592157832
iteration : 1077
train acc:  0.84375
train loss:  0.37231817841529846
train gradient:  0.18614344887420023
iteration : 1078
train acc:  0.8671875
train loss:  0.28442248702049255
train gradient:  0.23922817727541712
iteration : 1079
train acc:  0.8359375
train loss:  0.30734360218048096
train gradient:  0.137877268497335
iteration : 1080
train acc:  0.8984375
train loss:  0.2866024374961853
train gradient:  0.10460667133536453
iteration : 1081
train acc:  0.84375
train loss:  0.2885144352912903
train gradient:  0.18473630235617705
iteration : 1082
train acc:  0.875
train loss:  0.319850891828537
train gradient:  0.1279681061623833
iteration : 1083
train acc:  0.859375
train loss:  0.31441524624824524
train gradient:  0.1290277900689652
iteration : 1084
train acc:  0.921875
train loss:  0.26102545857429504
train gradient:  0.07409713221849781
iteration : 1085
train acc:  0.875
train loss:  0.2921629250049591
train gradient:  0.12736680287028787
iteration : 1086
train acc:  0.796875
train loss:  0.44631093740463257
train gradient:  0.29445847985437296
iteration : 1087
train acc:  0.875
train loss:  0.24422787129878998
train gradient:  0.07632007533852697
iteration : 1088
train acc:  0.890625
train loss:  0.3145385980606079
train gradient:  0.11529797726029643
iteration : 1089
train acc:  0.8125
train loss:  0.4170367419719696
train gradient:  0.21241632118335546
iteration : 1090
train acc:  0.9140625
train loss:  0.21677279472351074
train gradient:  0.08792239074888995
iteration : 1091
train acc:  0.8046875
train loss:  0.31851863861083984
train gradient:  0.19468380632946974
iteration : 1092
train acc:  0.921875
train loss:  0.2524932026863098
train gradient:  0.10092637278224964
iteration : 1093
train acc:  0.8515625
train loss:  0.35346299409866333
train gradient:  0.1282234980157707
iteration : 1094
train acc:  0.84375
train loss:  0.3325554430484772
train gradient:  0.1819743761723827
iteration : 1095
train acc:  0.875
train loss:  0.26638999581336975
train gradient:  0.12447370396629419
iteration : 1096
train acc:  0.8515625
train loss:  0.3210332691669464
train gradient:  0.1658094857969076
iteration : 1097
train acc:  0.8828125
train loss:  0.2941604256629944
train gradient:  0.12985233693683784
iteration : 1098
train acc:  0.8515625
train loss:  0.33665597438812256
train gradient:  0.1278185909425526
iteration : 1099
train acc:  0.875
train loss:  0.2626381516456604
train gradient:  0.1233894146978626
iteration : 1100
train acc:  0.8515625
train loss:  0.35595911741256714
train gradient:  0.14885930220466972
iteration : 1101
train acc:  0.875
train loss:  0.30380815267562866
train gradient:  0.09149905605119937
iteration : 1102
train acc:  0.8828125
train loss:  0.2749629616737366
train gradient:  0.09668765076885773
iteration : 1103
train acc:  0.875
train loss:  0.302329421043396
train gradient:  0.1114118313032459
iteration : 1104
train acc:  0.890625
train loss:  0.21914592385292053
train gradient:  0.09215421559407207
iteration : 1105
train acc:  0.8984375
train loss:  0.2845574617385864
train gradient:  0.1224552988962663
iteration : 1106
train acc:  0.8984375
train loss:  0.237822026014328
train gradient:  0.07519578712222098
iteration : 1107
train acc:  0.859375
train loss:  0.2653948962688446
train gradient:  0.08101489815022249
iteration : 1108
train acc:  0.875
train loss:  0.31660163402557373
train gradient:  0.12862135339210268
iteration : 1109
train acc:  0.859375
train loss:  0.3286855220794678
train gradient:  0.15034001563852645
iteration : 1110
train acc:  0.875
train loss:  0.27087804675102234
train gradient:  0.12151424969546964
iteration : 1111
train acc:  0.875
train loss:  0.30350929498672485
train gradient:  0.13855890923870268
iteration : 1112
train acc:  0.890625
train loss:  0.26067009568214417
train gradient:  0.10740787095699687
iteration : 1113
train acc:  0.875
train loss:  0.3154938519001007
train gradient:  0.10873607454184539
iteration : 1114
train acc:  0.8984375
train loss:  0.3077525496482849
train gradient:  0.15188547477981107
iteration : 1115
train acc:  0.8828125
train loss:  0.2934492826461792
train gradient:  0.08598006969823489
iteration : 1116
train acc:  0.8203125
train loss:  0.3620092272758484
train gradient:  0.16799521034511355
iteration : 1117
train acc:  0.890625
train loss:  0.24491329491138458
train gradient:  0.11893323324255285
iteration : 1118
train acc:  0.8984375
train loss:  0.2671319842338562
train gradient:  0.13411439101849154
iteration : 1119
train acc:  0.890625
train loss:  0.30727604031562805
train gradient:  0.13221197995074638
iteration : 1120
train acc:  0.890625
train loss:  0.25814202427864075
train gradient:  0.09892660359244644
iteration : 1121
train acc:  0.859375
train loss:  0.3314315676689148
train gradient:  0.14949825244588583
iteration : 1122
train acc:  0.8046875
train loss:  0.39926886558532715
train gradient:  0.23372502082427635
iteration : 1123
train acc:  0.8515625
train loss:  0.33795398473739624
train gradient:  0.1528801688298802
iteration : 1124
train acc:  0.859375
train loss:  0.28193625807762146
train gradient:  0.11818749088130547
iteration : 1125
train acc:  0.84375
train loss:  0.3251267969608307
train gradient:  0.14193754049319762
iteration : 1126
train acc:  0.8984375
train loss:  0.24939638376235962
train gradient:  0.0856043580269244
iteration : 1127
train acc:  0.90625
train loss:  0.2932012975215912
train gradient:  0.08540802669165773
iteration : 1128
train acc:  0.8203125
train loss:  0.3598935902118683
train gradient:  0.14909907762815633
iteration : 1129
train acc:  0.8359375
train loss:  0.3179035186767578
train gradient:  0.15641019792445512
iteration : 1130
train acc:  0.8515625
train loss:  0.3539394736289978
train gradient:  0.16936039559089272
iteration : 1131
train acc:  0.90625
train loss:  0.242252916097641
train gradient:  0.1646774155109749
iteration : 1132
train acc:  0.8671875
train loss:  0.2847498059272766
train gradient:  0.14989983143727187
iteration : 1133
train acc:  0.8828125
train loss:  0.2759479880332947
train gradient:  0.09998808842483374
iteration : 1134
train acc:  0.8828125
train loss:  0.24647653102874756
train gradient:  0.07890874229695433
iteration : 1135
train acc:  0.8671875
train loss:  0.3764972686767578
train gradient:  0.18065518912738676
iteration : 1136
train acc:  0.890625
train loss:  0.27070456743240356
train gradient:  0.17839872522594263
iteration : 1137
train acc:  0.8984375
train loss:  0.31068676710128784
train gradient:  0.11566643848382883
iteration : 1138
train acc:  0.8203125
train loss:  0.3858718276023865
train gradient:  0.12652725592196964
iteration : 1139
train acc:  0.9140625
train loss:  0.2957686483860016
train gradient:  0.16096958553069585
iteration : 1140
train acc:  0.875
train loss:  0.2838515341281891
train gradient:  0.10767932794844161
iteration : 1141
train acc:  0.8203125
train loss:  0.4566199779510498
train gradient:  0.49482352745139524
iteration : 1142
train acc:  0.8515625
train loss:  0.35372477769851685
train gradient:  0.1562450681778692
iteration : 1143
train acc:  0.890625
train loss:  0.2449342906475067
train gradient:  0.17362529637906865
iteration : 1144
train acc:  0.8671875
train loss:  0.2752887010574341
train gradient:  0.09102839409441744
iteration : 1145
train acc:  0.8984375
train loss:  0.24557068943977356
train gradient:  0.14475891524795226
iteration : 1146
train acc:  0.8671875
train loss:  0.29763317108154297
train gradient:  0.1329998944826853
iteration : 1147
train acc:  0.8671875
train loss:  0.2925468385219574
train gradient:  0.12863097393954853
iteration : 1148
train acc:  0.8515625
train loss:  0.33474022150039673
train gradient:  0.13477103091404363
iteration : 1149
train acc:  0.8828125
train loss:  0.29418325424194336
train gradient:  0.13422896953671395
iteration : 1150
train acc:  0.8359375
train loss:  0.40648216009140015
train gradient:  0.2659754421185308
iteration : 1151
train acc:  0.859375
train loss:  0.3319176733493805
train gradient:  0.12567760996082927
iteration : 1152
train acc:  0.890625
train loss:  0.2580968141555786
train gradient:  0.09383711636180572
iteration : 1153
train acc:  0.84375
train loss:  0.3122667074203491
train gradient:  0.1581799990002242
iteration : 1154
train acc:  0.8203125
train loss:  0.38805562257766724
train gradient:  0.20177844310076787
iteration : 1155
train acc:  0.90625
train loss:  0.2879052460193634
train gradient:  0.12686070890832582
iteration : 1156
train acc:  0.90625
train loss:  0.28527578711509705
train gradient:  0.11884042310560773
iteration : 1157
train acc:  0.875
train loss:  0.23311233520507812
train gradient:  0.08541748921929575
iteration : 1158
train acc:  0.875
train loss:  0.34187424182891846
train gradient:  0.14378957824458463
iteration : 1159
train acc:  0.8671875
train loss:  0.32551002502441406
train gradient:  0.1067704286538394
iteration : 1160
train acc:  0.84375
train loss:  0.3758857250213623
train gradient:  0.1268647119496053
iteration : 1161
train acc:  0.8125
train loss:  0.4430985450744629
train gradient:  0.22786595536833
iteration : 1162
train acc:  0.84375
train loss:  0.3061824440956116
train gradient:  0.10796476010726341
iteration : 1163
train acc:  0.890625
train loss:  0.26605069637298584
train gradient:  0.08837077965124458
iteration : 1164
train acc:  0.890625
train loss:  0.31146442890167236
train gradient:  0.16930169823238944
iteration : 1165
train acc:  0.8828125
train loss:  0.31961125135421753
train gradient:  0.15429799424782698
iteration : 1166
train acc:  0.8359375
train loss:  0.36748918890953064
train gradient:  0.20169331946984187
iteration : 1167
train acc:  0.9140625
train loss:  0.27432981133461
train gradient:  0.10930913214961308
iteration : 1168
train acc:  0.8828125
train loss:  0.27105703949928284
train gradient:  0.10270721484749709
iteration : 1169
train acc:  0.9140625
train loss:  0.23186051845550537
train gradient:  0.08488979764943005
iteration : 1170
train acc:  0.9140625
train loss:  0.2421848624944687
train gradient:  0.08402287929115487
iteration : 1171
train acc:  0.8515625
train loss:  0.3350536525249481
train gradient:  0.16256999489298418
iteration : 1172
train acc:  0.90625
train loss:  0.2571510076522827
train gradient:  0.07333238697747994
iteration : 1173
train acc:  0.84375
train loss:  0.37231311202049255
train gradient:  0.13164251135407684
iteration : 1174
train acc:  0.875
train loss:  0.3024646043777466
train gradient:  0.1506371150268886
iteration : 1175
train acc:  0.84375
train loss:  0.36588364839553833
train gradient:  0.18595569473505114
iteration : 1176
train acc:  0.875
train loss:  0.3215733468532562
train gradient:  0.11861570357966407
iteration : 1177
train acc:  0.875
train loss:  0.2772761583328247
train gradient:  0.10805730353504854
iteration : 1178
train acc:  0.8828125
train loss:  0.25969457626342773
train gradient:  0.17919498581121326
iteration : 1179
train acc:  0.8515625
train loss:  0.3012247681617737
train gradient:  0.09986076472128841
iteration : 1180
train acc:  0.90625
train loss:  0.22557136416435242
train gradient:  0.07821746127026194
iteration : 1181
train acc:  0.90625
train loss:  0.23939132690429688
train gradient:  0.0898904188049386
iteration : 1182
train acc:  0.8828125
train loss:  0.2882882356643677
train gradient:  0.12810622056012805
iteration : 1183
train acc:  0.859375
train loss:  0.26083892583847046
train gradient:  0.08022360788707551
iteration : 1184
train acc:  0.875
train loss:  0.29282546043395996
train gradient:  0.1331706636182669
iteration : 1185
train acc:  0.875
train loss:  0.2638230323791504
train gradient:  0.10583045166741784
iteration : 1186
train acc:  0.8828125
train loss:  0.2879036068916321
train gradient:  0.09892795214108212
iteration : 1187
train acc:  0.8984375
train loss:  0.25695884227752686
train gradient:  0.11344220545968958
iteration : 1188
train acc:  0.828125
train loss:  0.36770662665367126
train gradient:  0.2271899364711915
iteration : 1189
train acc:  0.859375
train loss:  0.2866770327091217
train gradient:  0.11813577057405626
iteration : 1190
train acc:  0.875
train loss:  0.3066094219684601
train gradient:  0.12048814184253759
iteration : 1191
train acc:  0.8828125
train loss:  0.32050928473472595
train gradient:  0.12988985191661984
iteration : 1192
train acc:  0.828125
train loss:  0.29343175888061523
train gradient:  0.1392272908994213
iteration : 1193
train acc:  0.8515625
train loss:  0.34473198652267456
train gradient:  0.1888391324795221
iteration : 1194
train acc:  0.8671875
train loss:  0.3302117884159088
train gradient:  0.1367936656459065
iteration : 1195
train acc:  0.7734375
train loss:  0.36580634117126465
train gradient:  0.13909451720172689
iteration : 1196
train acc:  0.8984375
train loss:  0.2693580389022827
train gradient:  0.0973034181952822
iteration : 1197
train acc:  0.8359375
train loss:  0.3534960448741913
train gradient:  0.12632924626109168
iteration : 1198
train acc:  0.8671875
train loss:  0.3130185306072235
train gradient:  0.13182927645105927
iteration : 1199
train acc:  0.84375
train loss:  0.33917054533958435
train gradient:  0.18187740829192306
iteration : 1200
train acc:  0.8828125
train loss:  0.2619081139564514
train gradient:  0.09746390904328231
iteration : 1201
train acc:  0.828125
train loss:  0.37036389112472534
train gradient:  0.1969422454795538
iteration : 1202
train acc:  0.8828125
train loss:  0.289886474609375
train gradient:  0.09554671469664835
iteration : 1203
train acc:  0.8515625
train loss:  0.3479292094707489
train gradient:  0.19918799762900447
iteration : 1204
train acc:  0.828125
train loss:  0.3864286541938782
train gradient:  0.2170279934181254
iteration : 1205
train acc:  0.84375
train loss:  0.33836206793785095
train gradient:  0.1624220269467559
iteration : 1206
train acc:  0.875
train loss:  0.27696722745895386
train gradient:  0.12443256981862136
iteration : 1207
train acc:  0.8671875
train loss:  0.26721838116645813
train gradient:  0.11988854000227311
iteration : 1208
train acc:  0.8671875
train loss:  0.34844404458999634
train gradient:  0.16276614065226241
iteration : 1209
train acc:  0.8828125
train loss:  0.26527196168899536
train gradient:  0.11102835829420635
iteration : 1210
train acc:  0.890625
train loss:  0.2926786541938782
train gradient:  0.11173651866493171
iteration : 1211
train acc:  0.84375
train loss:  0.3247895836830139
train gradient:  0.11028992884515688
iteration : 1212
train acc:  0.8828125
train loss:  0.30036741495132446
train gradient:  0.1314383871801784
iteration : 1213
train acc:  0.796875
train loss:  0.4079541862010956
train gradient:  0.2801918866126878
iteration : 1214
train acc:  0.859375
train loss:  0.3409772217273712
train gradient:  0.12464230829032646
iteration : 1215
train acc:  0.859375
train loss:  0.29450589418411255
train gradient:  0.14022159170144863
iteration : 1216
train acc:  0.8671875
train loss:  0.27096375823020935
train gradient:  0.11118912805060001
iteration : 1217
train acc:  0.8515625
train loss:  0.3170468211174011
train gradient:  0.13673910756079083
iteration : 1218
train acc:  0.8671875
train loss:  0.3280707001686096
train gradient:  0.11274945032947274
iteration : 1219
train acc:  0.8671875
train loss:  0.3096197247505188
train gradient:  0.11296324275036387
iteration : 1220
train acc:  0.8125
train loss:  0.4144626259803772
train gradient:  0.21189987799544413
iteration : 1221
train acc:  0.8671875
train loss:  0.26681989431381226
train gradient:  0.11288333623572301
iteration : 1222
train acc:  0.90625
train loss:  0.26734375953674316
train gradient:  0.08530079799710344
iteration : 1223
train acc:  0.875
train loss:  0.3284170627593994
train gradient:  0.1197150892866732
iteration : 1224
train acc:  0.859375
train loss:  0.29244059324264526
train gradient:  0.12409967160067886
iteration : 1225
train acc:  0.8515625
train loss:  0.37795591354370117
train gradient:  0.1847561494983671
iteration : 1226
train acc:  0.828125
train loss:  0.410641610622406
train gradient:  0.26242905574545705
iteration : 1227
train acc:  0.84375
train loss:  0.33143147826194763
train gradient:  0.12350816486557013
iteration : 1228
train acc:  0.78125
train loss:  0.40837758779525757
train gradient:  0.1900514855477517
iteration : 1229
train acc:  0.8671875
train loss:  0.2572077512741089
train gradient:  0.1286503080387248
iteration : 1230
train acc:  0.828125
train loss:  0.38266679644584656
train gradient:  0.18290316468413562
iteration : 1231
train acc:  0.8828125
train loss:  0.31148719787597656
train gradient:  0.11369132485969351
iteration : 1232
train acc:  0.8515625
train loss:  0.37163323163986206
train gradient:  0.2021305434263884
iteration : 1233
train acc:  0.921875
train loss:  0.2532365620136261
train gradient:  0.09703174759122343
iteration : 1234
train acc:  0.8203125
train loss:  0.3537517786026001
train gradient:  0.14898155352735776
iteration : 1235
train acc:  0.8515625
train loss:  0.3382461965084076
train gradient:  0.161374414174962
iteration : 1236
train acc:  0.890625
train loss:  0.2829877734184265
train gradient:  0.0977045578321537
iteration : 1237
train acc:  0.8046875
train loss:  0.36242711544036865
train gradient:  0.2456493494633338
iteration : 1238
train acc:  0.828125
train loss:  0.3564502000808716
train gradient:  0.1967404189781084
iteration : 1239
train acc:  0.859375
train loss:  0.27528244256973267
train gradient:  0.08563172412915716
iteration : 1240
train acc:  0.8828125
train loss:  0.3046919107437134
train gradient:  0.12430285331154939
iteration : 1241
train acc:  0.8984375
train loss:  0.2833061218261719
train gradient:  0.11089917423437926
iteration : 1242
train acc:  0.875
train loss:  0.2728976607322693
train gradient:  0.07269985171842636
iteration : 1243
train acc:  0.890625
train loss:  0.28980982303619385
train gradient:  0.0973846310186062
iteration : 1244
train acc:  0.8515625
train loss:  0.3344714045524597
train gradient:  0.1973685824188039
iteration : 1245
train acc:  0.859375
train loss:  0.2866347134113312
train gradient:  0.11624901051531562
iteration : 1246
train acc:  0.875
train loss:  0.2632359266281128
train gradient:  0.09642071712801148
iteration : 1247
train acc:  0.8359375
train loss:  0.3472557067871094
train gradient:  0.19852618439597589
iteration : 1248
train acc:  0.84375
train loss:  0.40511369705200195
train gradient:  0.246277485531334
iteration : 1249
train acc:  0.8671875
train loss:  0.286395788192749
train gradient:  0.17707660416522597
iteration : 1250
train acc:  0.8515625
train loss:  0.38157862424850464
train gradient:  0.19704991417592982
iteration : 1251
train acc:  0.828125
train loss:  0.36477765440940857
train gradient:  0.1483464950649262
iteration : 1252
train acc:  0.859375
train loss:  0.2847290635108948
train gradient:  0.0998460792140178
iteration : 1253
train acc:  0.859375
train loss:  0.3940737843513489
train gradient:  0.18130411930814636
iteration : 1254
train acc:  0.8359375
train loss:  0.30427029728889465
train gradient:  0.12130997963062719
iteration : 1255
train acc:  0.8828125
train loss:  0.2843944728374481
train gradient:  0.14627613914489151
iteration : 1256
train acc:  0.8671875
train loss:  0.2657889127731323
train gradient:  0.08217190911827468
iteration : 1257
train acc:  0.8203125
train loss:  0.41853925585746765
train gradient:  0.20181639680343938
iteration : 1258
train acc:  0.875
train loss:  0.302781879901886
train gradient:  0.11281033587319086
iteration : 1259
train acc:  0.8203125
train loss:  0.35162070393562317
train gradient:  0.16627326907717543
iteration : 1260
train acc:  0.8984375
train loss:  0.2629484534263611
train gradient:  0.18689755124859714
iteration : 1261
train acc:  0.8671875
train loss:  0.26258423924446106
train gradient:  0.09316882636966127
iteration : 1262
train acc:  0.8671875
train loss:  0.28000959753990173
train gradient:  0.14620973588889752
iteration : 1263
train acc:  0.875
train loss:  0.2554563283920288
train gradient:  0.09962907981442483
iteration : 1264
train acc:  0.8671875
train loss:  0.3010961413383484
train gradient:  0.11025986363625685
iteration : 1265
train acc:  0.828125
train loss:  0.45732906460762024
train gradient:  0.2533269282640228
iteration : 1266
train acc:  0.828125
train loss:  0.38190221786499023
train gradient:  0.2132551235973199
iteration : 1267
train acc:  0.90625
train loss:  0.20194999873638153
train gradient:  0.0594272590805342
iteration : 1268
train acc:  0.90625
train loss:  0.23773959279060364
train gradient:  0.0673733662339606
iteration : 1269
train acc:  0.8515625
train loss:  0.3929559588432312
train gradient:  0.18983781538866346
iteration : 1270
train acc:  0.875
train loss:  0.3263840973377228
train gradient:  0.11621387089574194
iteration : 1271
train acc:  0.859375
train loss:  0.32176363468170166
train gradient:  0.11097416016445379
iteration : 1272
train acc:  0.8828125
train loss:  0.325234055519104
train gradient:  0.12120663648858256
iteration : 1273
train acc:  0.8515625
train loss:  0.2760402262210846
train gradient:  0.09147096511731065
iteration : 1274
train acc:  0.8671875
train loss:  0.3136323094367981
train gradient:  0.11973977368044851
iteration : 1275
train acc:  0.9140625
train loss:  0.25235259532928467
train gradient:  0.09565788683277218
iteration : 1276
train acc:  0.921875
train loss:  0.21262040734291077
train gradient:  0.08902074399860722
iteration : 1277
train acc:  0.8671875
train loss:  0.30305200815200806
train gradient:  0.106319738199543
iteration : 1278
train acc:  0.8984375
train loss:  0.24871282279491425
train gradient:  0.09124279210576987
iteration : 1279
train acc:  0.8984375
train loss:  0.24631834030151367
train gradient:  0.09896495090129417
iteration : 1280
train acc:  0.8359375
train loss:  0.34792882204055786
train gradient:  0.17461234398539377
iteration : 1281
train acc:  0.84375
train loss:  0.3453797698020935
train gradient:  0.0957916771516072
iteration : 1282
train acc:  0.8203125
train loss:  0.4479714632034302
train gradient:  0.27512879637009174
iteration : 1283
train acc:  0.828125
train loss:  0.3418443202972412
train gradient:  0.14379433056724952
iteration : 1284
train acc:  0.8671875
train loss:  0.35999733209609985
train gradient:  0.1549451483937249
iteration : 1285
train acc:  0.8515625
train loss:  0.3317136764526367
train gradient:  0.1417345566389578
iteration : 1286
train acc:  0.8828125
train loss:  0.2587108910083771
train gradient:  0.0918798803594296
iteration : 1287
train acc:  0.8828125
train loss:  0.29110950231552124
train gradient:  0.09787917049434346
iteration : 1288
train acc:  0.84375
train loss:  0.3172236680984497
train gradient:  0.1278395462967526
iteration : 1289
train acc:  0.8359375
train loss:  0.3761005997657776
train gradient:  0.244592288449036
iteration : 1290
train acc:  0.8359375
train loss:  0.3741404712200165
train gradient:  0.2644721397407803
iteration : 1291
train acc:  0.8828125
train loss:  0.29393911361694336
train gradient:  0.08105397895091755
iteration : 1292
train acc:  0.8984375
train loss:  0.2926802635192871
train gradient:  0.09210153903983972
iteration : 1293
train acc:  0.828125
train loss:  0.39278489351272583
train gradient:  0.20335406338612452
iteration : 1294
train acc:  0.8828125
train loss:  0.3165111541748047
train gradient:  0.10364707841648302
iteration : 1295
train acc:  0.8203125
train loss:  0.3967578113079071
train gradient:  0.1928022424491965
iteration : 1296
train acc:  0.84375
train loss:  0.3084154427051544
train gradient:  0.09731702553331163
iteration : 1297
train acc:  0.828125
train loss:  0.40115660429000854
train gradient:  0.16548360174483595
iteration : 1298
train acc:  0.8828125
train loss:  0.3212701380252838
train gradient:  0.13625067915216138
iteration : 1299
train acc:  0.859375
train loss:  0.3389240503311157
train gradient:  0.11674336489919833
iteration : 1300
train acc:  0.921875
train loss:  0.255332350730896
train gradient:  0.09059808579831373
iteration : 1301
train acc:  0.9140625
train loss:  0.2517869472503662
train gradient:  0.11190234565753171
iteration : 1302
train acc:  0.8828125
train loss:  0.2898385226726532
train gradient:  0.1526558300392128
iteration : 1303
train acc:  0.8359375
train loss:  0.2917695939540863
train gradient:  0.09783519907374064
iteration : 1304
train acc:  0.8203125
train loss:  0.3588121831417084
train gradient:  0.20529104654919625
iteration : 1305
train acc:  0.90625
train loss:  0.24763444066047668
train gradient:  0.08300720277389009
iteration : 1306
train acc:  0.8671875
train loss:  0.3421778678894043
train gradient:  0.16706084672745442
iteration : 1307
train acc:  0.890625
train loss:  0.3214937448501587
train gradient:  0.1173162857899206
iteration : 1308
train acc:  0.84375
train loss:  0.3433776795864105
train gradient:  0.10326085771766615
iteration : 1309
train acc:  0.8671875
train loss:  0.3246088922023773
train gradient:  0.13833367696465448
iteration : 1310
train acc:  0.8671875
train loss:  0.29809319972991943
train gradient:  0.12064925474205201
iteration : 1311
train acc:  0.8203125
train loss:  0.36570480465888977
train gradient:  0.21851217206753604
iteration : 1312
train acc:  0.8828125
train loss:  0.34497737884521484
train gradient:  0.14213089620614466
iteration : 1313
train acc:  0.8359375
train loss:  0.3784354031085968
train gradient:  0.14001347997182267
iteration : 1314
train acc:  0.8671875
train loss:  0.3546385169029236
train gradient:  0.16702006343689094
iteration : 1315
train acc:  0.921875
train loss:  0.24828842282295227
train gradient:  0.11906377091001469
iteration : 1316
train acc:  0.9140625
train loss:  0.26717448234558105
train gradient:  0.12587930842989337
iteration : 1317
train acc:  0.859375
train loss:  0.29831135272979736
train gradient:  0.12609616772850768
iteration : 1318
train acc:  0.8203125
train loss:  0.36352020502090454
train gradient:  0.1018417601345736
iteration : 1319
train acc:  0.859375
train loss:  0.3423936665058136
train gradient:  0.09338882059831259
iteration : 1320
train acc:  0.84375
train loss:  0.34517091512680054
train gradient:  0.22052631272212614
iteration : 1321
train acc:  0.8671875
train loss:  0.3660051226615906
train gradient:  0.1600440116352002
iteration : 1322
train acc:  0.859375
train loss:  0.30798041820526123
train gradient:  0.19314122163107028
iteration : 1323
train acc:  0.828125
train loss:  0.42116469144821167
train gradient:  0.2244628060916954
iteration : 1324
train acc:  0.875
train loss:  0.29771992564201355
train gradient:  0.11319906224016155
iteration : 1325
train acc:  0.8515625
train loss:  0.31593039631843567
train gradient:  0.1141544207208701
iteration : 1326
train acc:  0.859375
train loss:  0.3700624704360962
train gradient:  0.18442009004249643
iteration : 1327
train acc:  0.9140625
train loss:  0.2941487431526184
train gradient:  0.1188923929706018
iteration : 1328
train acc:  0.8828125
train loss:  0.3673032224178314
train gradient:  0.1284132136734861
iteration : 1329
train acc:  0.8203125
train loss:  0.37510740756988525
train gradient:  0.14508225851698314
iteration : 1330
train acc:  0.890625
train loss:  0.2513565421104431
train gradient:  0.09395315879616616
iteration : 1331
train acc:  0.875
train loss:  0.2955257296562195
train gradient:  0.12111687175700825
iteration : 1332
train acc:  0.8515625
train loss:  0.31003278493881226
train gradient:  0.12132822536347214
iteration : 1333
train acc:  0.90625
train loss:  0.27047598361968994
train gradient:  0.1030771501113574
iteration : 1334
train acc:  0.90625
train loss:  0.24600857496261597
train gradient:  0.12788239574448426
iteration : 1335
train acc:  0.796875
train loss:  0.3725554645061493
train gradient:  0.14735767607791608
iteration : 1336
train acc:  0.8828125
train loss:  0.28299689292907715
train gradient:  0.11942062451699317
iteration : 1337
train acc:  0.8828125
train loss:  0.31301456689834595
train gradient:  0.13164826061636586
iteration : 1338
train acc:  0.8515625
train loss:  0.32732439041137695
train gradient:  0.10303807994071847
iteration : 1339
train acc:  0.890625
train loss:  0.2297474443912506
train gradient:  0.08695766747206503
iteration : 1340
train acc:  0.875
train loss:  0.3102952837944031
train gradient:  0.08416354066027955
iteration : 1341
train acc:  0.890625
train loss:  0.2679798901081085
train gradient:  0.08871347090601751
iteration : 1342
train acc:  0.8515625
train loss:  0.3133252263069153
train gradient:  0.12099059603052575
iteration : 1343
train acc:  0.8828125
train loss:  0.3042132258415222
train gradient:  0.11318235270785981
iteration : 1344
train acc:  0.8828125
train loss:  0.24479253590106964
train gradient:  0.11115259608209145
iteration : 1345
train acc:  0.8125
train loss:  0.3397894501686096
train gradient:  0.1269761745787183
iteration : 1346
train acc:  0.8828125
train loss:  0.3133169412612915
train gradient:  0.16129468567732072
iteration : 1347
train acc:  0.8984375
train loss:  0.27027320861816406
train gradient:  0.10209758162295597
iteration : 1348
train acc:  0.8671875
train loss:  0.3353504538536072
train gradient:  0.13135417944757496
iteration : 1349
train acc:  0.875
train loss:  0.37295079231262207
train gradient:  0.2037165366433465
iteration : 1350
train acc:  0.828125
train loss:  0.3596937656402588
train gradient:  0.1489550312184122
iteration : 1351
train acc:  0.90625
train loss:  0.2629133462905884
train gradient:  0.14186377131286143
iteration : 1352
train acc:  0.84375
train loss:  0.3191579282283783
train gradient:  0.13932039676321947
iteration : 1353
train acc:  0.8125
train loss:  0.4002484083175659
train gradient:  0.15745887765701438
iteration : 1354
train acc:  0.8828125
train loss:  0.271977961063385
train gradient:  0.14712986788831017
iteration : 1355
train acc:  0.875
train loss:  0.2709431052207947
train gradient:  0.12983514964404072
iteration : 1356
train acc:  0.875
train loss:  0.238416850566864
train gradient:  0.09673478121773328
iteration : 1357
train acc:  0.8671875
train loss:  0.3133341073989868
train gradient:  0.09758385979357212
iteration : 1358
train acc:  0.8203125
train loss:  0.36618271470069885
train gradient:  0.1961750335760959
iteration : 1359
train acc:  0.84375
train loss:  0.3623411953449249
train gradient:  0.16647206938856762
iteration : 1360
train acc:  0.8359375
train loss:  0.32663631439208984
train gradient:  0.13901144442664448
iteration : 1361
train acc:  0.8515625
train loss:  0.3044518828392029
train gradient:  0.10181985616368212
iteration : 1362
train acc:  0.90625
train loss:  0.2714554965496063
train gradient:  0.11479275701205734
iteration : 1363
train acc:  0.828125
train loss:  0.31789860129356384
train gradient:  0.170665569998892
iteration : 1364
train acc:  0.8828125
train loss:  0.33211493492126465
train gradient:  0.12297570230646657
iteration : 1365
train acc:  0.84375
train loss:  0.35183385014533997
train gradient:  0.15483875901769845
iteration : 1366
train acc:  0.859375
train loss:  0.3133435547351837
train gradient:  0.16765719935084125
iteration : 1367
train acc:  0.8671875
train loss:  0.3609752655029297
train gradient:  0.16193704132266026
iteration : 1368
train acc:  0.8671875
train loss:  0.3547688126564026
train gradient:  0.18619809400768494
iteration : 1369
train acc:  0.84375
train loss:  0.37424492835998535
train gradient:  0.1825470566031846
iteration : 1370
train acc:  0.875
train loss:  0.34894877672195435
train gradient:  0.12111179584773096
iteration : 1371
train acc:  0.9140625
train loss:  0.23079320788383484
train gradient:  0.11041521019257114
iteration : 1372
train acc:  0.859375
train loss:  0.35494518280029297
train gradient:  0.31868832023579946
iteration : 1373
train acc:  0.8203125
train loss:  0.40814444422721863
train gradient:  0.21395521392137484
iteration : 1374
train acc:  0.8359375
train loss:  0.3003300726413727
train gradient:  0.14355031771157525
iteration : 1375
train acc:  0.8359375
train loss:  0.32503798604011536
train gradient:  0.1286662720222241
iteration : 1376
train acc:  0.90625
train loss:  0.25292009115219116
train gradient:  0.07501784702878155
iteration : 1377
train acc:  0.84375
train loss:  0.3235321044921875
train gradient:  0.12728223935732086
iteration : 1378
train acc:  0.9375
train loss:  0.21983376145362854
train gradient:  0.06264563328455654
iteration : 1379
train acc:  0.8359375
train loss:  0.3162943124771118
train gradient:  0.16129282967675673
iteration : 1380
train acc:  0.859375
train loss:  0.30823928117752075
train gradient:  0.10842662617059716
iteration : 1381
train acc:  0.90625
train loss:  0.2418198585510254
train gradient:  0.06257849797018018
iteration : 1382
train acc:  0.84375
train loss:  0.31764456629753113
train gradient:  0.095384778213696
iteration : 1383
train acc:  0.8671875
train loss:  0.34247392416000366
train gradient:  0.14640439146707512
iteration : 1384
train acc:  0.828125
train loss:  0.3388447165489197
train gradient:  0.14500047230696728
iteration : 1385
train acc:  0.8984375
train loss:  0.32040613889694214
train gradient:  0.12065192548528614
iteration : 1386
train acc:  0.8828125
train loss:  0.29292961955070496
train gradient:  0.08853008264342883
iteration : 1387
train acc:  0.84375
train loss:  0.30947890877723694
train gradient:  0.1148487500858761
iteration : 1388
train acc:  0.90625
train loss:  0.2631533145904541
train gradient:  0.08913198110414869
iteration : 1389
train acc:  0.8515625
train loss:  0.26074424386024475
train gradient:  0.09646591391749038
iteration : 1390
train acc:  0.875
train loss:  0.26814574003219604
train gradient:  0.08495427329553926
iteration : 1391
train acc:  0.9296875
train loss:  0.234103724360466
train gradient:  0.08112940453026733
iteration : 1392
train acc:  0.8046875
train loss:  0.38944268226623535
train gradient:  0.14500851303014045
iteration : 1393
train acc:  0.859375
train loss:  0.3035106956958771
train gradient:  0.12476800446050612
iteration : 1394
train acc:  0.90625
train loss:  0.2628144919872284
train gradient:  0.08934801289946717
iteration : 1395
train acc:  0.8671875
train loss:  0.3093186616897583
train gradient:  0.1309337112093879
iteration : 1396
train acc:  0.8359375
train loss:  0.3595505356788635
train gradient:  0.14938377137966677
iteration : 1397
train acc:  0.890625
train loss:  0.24279287457466125
train gradient:  0.10481216828516929
iteration : 1398
train acc:  0.8515625
train loss:  0.39201951026916504
train gradient:  0.1645767927637824
iteration : 1399
train acc:  0.8515625
train loss:  0.2923264801502228
train gradient:  0.118088773325945
iteration : 1400
train acc:  0.8984375
train loss:  0.25985854864120483
train gradient:  0.08013670539313199
iteration : 1401
train acc:  0.8984375
train loss:  0.2578212022781372
train gradient:  0.13297150329884458
iteration : 1402
train acc:  0.8828125
train loss:  0.2852495312690735
train gradient:  0.1512269422992166
iteration : 1403
train acc:  0.8671875
train loss:  0.3158792555332184
train gradient:  0.12546297825982722
iteration : 1404
train acc:  0.8515625
train loss:  0.32427293062210083
train gradient:  0.1443024810276977
iteration : 1405
train acc:  0.875
train loss:  0.2519076466560364
train gradient:  0.09478088102781555
iteration : 1406
train acc:  0.8515625
train loss:  0.33853423595428467
train gradient:  0.16259195014979969
iteration : 1407
train acc:  0.8203125
train loss:  0.40732720494270325
train gradient:  0.16704418501666823
iteration : 1408
train acc:  0.859375
train loss:  0.2952539324760437
train gradient:  0.09966785426289519
iteration : 1409
train acc:  0.859375
train loss:  0.3247572183609009
train gradient:  0.10145244753388573
iteration : 1410
train acc:  0.8359375
train loss:  0.33868107199668884
train gradient:  0.16008879412043903
iteration : 1411
train acc:  0.921875
train loss:  0.2341080904006958
train gradient:  0.0649785167854903
iteration : 1412
train acc:  0.8359375
train loss:  0.35596054792404175
train gradient:  0.1748259002689992
iteration : 1413
train acc:  0.8671875
train loss:  0.29005616903305054
train gradient:  0.14247183217555703
iteration : 1414
train acc:  0.84375
train loss:  0.3782079815864563
train gradient:  0.21385698546268636
iteration : 1415
train acc:  0.875
train loss:  0.2927181124687195
train gradient:  0.1018808204246054
iteration : 1416
train acc:  0.84375
train loss:  0.3145330846309662
train gradient:  0.12674763249235196
iteration : 1417
train acc:  0.859375
train loss:  0.36494600772857666
train gradient:  0.15397729830675375
iteration : 1418
train acc:  0.8828125
train loss:  0.2775912582874298
train gradient:  0.11026096970998248
iteration : 1419
train acc:  0.8515625
train loss:  0.2851203382015228
train gradient:  0.1480141743788736
iteration : 1420
train acc:  0.9296875
train loss:  0.22103336453437805
train gradient:  0.06902151071616462
iteration : 1421
train acc:  0.8671875
train loss:  0.3642888069152832
train gradient:  0.1997243759463206
iteration : 1422
train acc:  0.90625
train loss:  0.34842628240585327
train gradient:  0.1650494180154021
iteration : 1423
train acc:  0.8671875
train loss:  0.32891756296157837
train gradient:  0.1146903924196255
iteration : 1424
train acc:  0.90625
train loss:  0.3057944178581238
train gradient:  0.10860264639061763
iteration : 1425
train acc:  0.859375
train loss:  0.3053450882434845
train gradient:  0.14525190402253896
iteration : 1426
train acc:  0.859375
train loss:  0.3300338089466095
train gradient:  0.17960521750818026
iteration : 1427
train acc:  0.8671875
train loss:  0.32401037216186523
train gradient:  0.19316487619828512
iteration : 1428
train acc:  0.765625
train loss:  0.38537168502807617
train gradient:  0.16735363876646495
iteration : 1429
train acc:  0.828125
train loss:  0.3309418857097626
train gradient:  0.12724372549957302
iteration : 1430
train acc:  0.8984375
train loss:  0.2780574560165405
train gradient:  0.10221186352183168
iteration : 1431
train acc:  0.890625
train loss:  0.2807479500770569
train gradient:  0.13306819204823178
iteration : 1432
train acc:  0.90625
train loss:  0.29466474056243896
train gradient:  0.14892299604036843
iteration : 1433
train acc:  0.859375
train loss:  0.3327379822731018
train gradient:  0.16627572240353766
iteration : 1434
train acc:  0.8203125
train loss:  0.45139575004577637
train gradient:  0.20186650828618719
iteration : 1435
train acc:  0.84375
train loss:  0.36225467920303345
train gradient:  0.1825502296567586
iteration : 1436
train acc:  0.796875
train loss:  0.4098515212535858
train gradient:  0.19474821517294233
iteration : 1437
train acc:  0.8984375
train loss:  0.2608315646648407
train gradient:  0.09463054415543032
iteration : 1438
train acc:  0.828125
train loss:  0.37643563747406006
train gradient:  0.1455989996954558
iteration : 1439
train acc:  0.8984375
train loss:  0.2364448457956314
train gradient:  0.09718511257970437
iteration : 1440
train acc:  0.8828125
train loss:  0.3068980276584625
train gradient:  0.11928509924830522
iteration : 1441
train acc:  0.8203125
train loss:  0.33010807633399963
train gradient:  0.13017148465968137
iteration : 1442
train acc:  0.8984375
train loss:  0.2207154929637909
train gradient:  0.11245182274619567
iteration : 1443
train acc:  0.875
train loss:  0.28186964988708496
train gradient:  0.09513794514737983
iteration : 1444
train acc:  0.8984375
train loss:  0.26006001234054565
train gradient:  0.09064516552543014
iteration : 1445
train acc:  0.8828125
train loss:  0.2778581380844116
train gradient:  0.19622811663228087
iteration : 1446
train acc:  0.8828125
train loss:  0.26873892545700073
train gradient:  0.1601269191579611
iteration : 1447
train acc:  0.8671875
train loss:  0.2932530641555786
train gradient:  0.19687107738745602
iteration : 1448
train acc:  0.859375
train loss:  0.3199595808982849
train gradient:  0.138743685839298
iteration : 1449
train acc:  0.875
train loss:  0.2527230381965637
train gradient:  0.10378265214246499
iteration : 1450
train acc:  0.8984375
train loss:  0.27649641036987305
train gradient:  0.0841825706362774
iteration : 1451
train acc:  0.8671875
train loss:  0.2837105393409729
train gradient:  0.12339295356838027
iteration : 1452
train acc:  0.8984375
train loss:  0.2414487600326538
train gradient:  0.11214674306157335
iteration : 1453
train acc:  0.875
train loss:  0.268943190574646
train gradient:  0.11723633507414973
iteration : 1454
train acc:  0.8515625
train loss:  0.3681730628013611
train gradient:  0.1675190904788044
iteration : 1455
train acc:  0.8671875
train loss:  0.32510441541671753
train gradient:  0.13691030454682512
iteration : 1456
train acc:  0.8984375
train loss:  0.2578606903553009
train gradient:  0.08790147195539659
iteration : 1457
train acc:  0.890625
train loss:  0.2865177094936371
train gradient:  0.08906599662317762
iteration : 1458
train acc:  0.828125
train loss:  0.36567673087120056
train gradient:  0.1626824578271507
iteration : 1459
train acc:  0.859375
train loss:  0.32247501611709595
train gradient:  0.11193894597571864
iteration : 1460
train acc:  0.8359375
train loss:  0.3484210968017578
train gradient:  0.1293500278002602
iteration : 1461
train acc:  0.890625
train loss:  0.27362197637557983
train gradient:  0.16365085238754448
iteration : 1462
train acc:  0.8671875
train loss:  0.2799537777900696
train gradient:  0.13503275805953752
iteration : 1463
train acc:  0.8828125
train loss:  0.27531135082244873
train gradient:  0.14037550695784695
iteration : 1464
train acc:  0.8984375
train loss:  0.21388770639896393
train gradient:  0.08921971930092273
iteration : 1465
train acc:  0.84375
train loss:  0.322997123003006
train gradient:  0.10663824974120456
iteration : 1466
train acc:  0.859375
train loss:  0.3606966733932495
train gradient:  0.14047105934533094
iteration : 1467
train acc:  0.84375
train loss:  0.4088321924209595
train gradient:  0.20250043343743623
iteration : 1468
train acc:  0.8828125
train loss:  0.23940354585647583
train gradient:  0.07816843189625346
iteration : 1469
train acc:  0.875
train loss:  0.2700253129005432
train gradient:  0.10243914412255548
iteration : 1470
train acc:  0.828125
train loss:  0.3212378919124603
train gradient:  0.1432502488545237
iteration : 1471
train acc:  0.8984375
train loss:  0.25714555382728577
train gradient:  0.08187901744950486
iteration : 1472
train acc:  0.859375
train loss:  0.36233359575271606
train gradient:  0.12819760990721918
iteration : 1473
train acc:  0.8984375
train loss:  0.22607821226119995
train gradient:  0.07446320632063866
iteration : 1474
train acc:  0.890625
train loss:  0.2529306709766388
train gradient:  0.10933482863644839
iteration : 1475
train acc:  0.859375
train loss:  0.389412522315979
train gradient:  0.19938647018415093
iteration : 1476
train acc:  0.8828125
train loss:  0.2644668519496918
train gradient:  0.09457175875113502
iteration : 1477
train acc:  0.859375
train loss:  0.3350379467010498
train gradient:  0.10118786562188993
iteration : 1478
train acc:  0.84375
train loss:  0.33635377883911133
train gradient:  0.11741970569803475
iteration : 1479
train acc:  0.8046875
train loss:  0.4562714397907257
train gradient:  0.24612440486916254
iteration : 1480
train acc:  0.828125
train loss:  0.38038119673728943
train gradient:  0.15458571870205645
iteration : 1481
train acc:  0.8203125
train loss:  0.3927609920501709
train gradient:  0.2399149774259462
iteration : 1482
train acc:  0.828125
train loss:  0.3332326114177704
train gradient:  0.16750650364574748
iteration : 1483
train acc:  0.90625
train loss:  0.2415076494216919
train gradient:  0.09021360003966995
iteration : 1484
train acc:  0.8828125
train loss:  0.3946817219257355
train gradient:  0.18407383378605996
iteration : 1485
train acc:  0.8828125
train loss:  0.3628627359867096
train gradient:  0.12519744482575765
iteration : 1486
train acc:  0.9140625
train loss:  0.20504748821258545
train gradient:  0.05849033237560306
iteration : 1487
train acc:  0.9140625
train loss:  0.246879443526268
train gradient:  0.11310434850755334
iteration : 1488
train acc:  0.859375
train loss:  0.29536983370780945
train gradient:  0.1076202712258449
iteration : 1489
train acc:  0.8984375
train loss:  0.25544214248657227
train gradient:  0.06972230336483531
iteration : 1490
train acc:  0.8515625
train loss:  0.3348073959350586
train gradient:  0.15787876985187266
iteration : 1491
train acc:  0.8984375
train loss:  0.2478446513414383
train gradient:  0.12432165304482633
iteration : 1492
train acc:  0.8359375
train loss:  0.27661648392677307
train gradient:  0.13396016183629106
iteration : 1493
train acc:  0.859375
train loss:  0.31644922494888306
train gradient:  0.10357268875453099
iteration : 1494
train acc:  0.8984375
train loss:  0.26305970549583435
train gradient:  0.14183027719166283
iteration : 1495
train acc:  0.8359375
train loss:  0.3118451237678528
train gradient:  0.14352458872374368
iteration : 1496
train acc:  0.890625
train loss:  0.24881714582443237
train gradient:  0.11576223761331603
iteration : 1497
train acc:  0.9296875
train loss:  0.24637001752853394
train gradient:  0.08675433927249833
iteration : 1498
train acc:  0.796875
train loss:  0.4035479426383972
train gradient:  0.18295336885934665
iteration : 1499
train acc:  0.828125
train loss:  0.34084123373031616
train gradient:  0.14785034825885135
iteration : 1500
train acc:  0.890625
train loss:  0.25098928809165955
train gradient:  0.07967366204824328
iteration : 1501
train acc:  0.8828125
train loss:  0.2803371846675873
train gradient:  0.1771642220797111
iteration : 1502
train acc:  0.8203125
train loss:  0.3884957730770111
train gradient:  0.14406609238078288
iteration : 1503
train acc:  0.9296875
train loss:  0.26181909441947937
train gradient:  0.10926450629169876
iteration : 1504
train acc:  0.890625
train loss:  0.28448227047920227
train gradient:  0.15804431905557226
iteration : 1505
train acc:  0.828125
train loss:  0.2979111671447754
train gradient:  0.1334538212374759
iteration : 1506
train acc:  0.8671875
train loss:  0.3440554141998291
train gradient:  0.11344926817072822
iteration : 1507
train acc:  0.8828125
train loss:  0.262312114238739
train gradient:  0.1334340530492497
iteration : 1508
train acc:  0.8671875
train loss:  0.3286551833152771
train gradient:  0.12699547024208763
iteration : 1509
train acc:  0.8984375
train loss:  0.2664737105369568
train gradient:  0.10555042462617067
iteration : 1510
train acc:  0.9296875
train loss:  0.2500261068344116
train gradient:  0.06622232996218601
iteration : 1511
train acc:  0.84375
train loss:  0.3231554329395294
train gradient:  0.1349134501832626
iteration : 1512
train acc:  0.8984375
train loss:  0.25605490803718567
train gradient:  0.12473949966860963
iteration : 1513
train acc:  0.9140625
train loss:  0.25249019265174866
train gradient:  0.10503263178196967
iteration : 1514
train acc:  0.8203125
train loss:  0.339738130569458
train gradient:  0.13339057374341795
iteration : 1515
train acc:  0.90625
train loss:  0.25154349207878113
train gradient:  0.07255411411282295
iteration : 1516
train acc:  0.8359375
train loss:  0.4150484800338745
train gradient:  0.1527825857081031
iteration : 1517
train acc:  0.8984375
train loss:  0.31388795375823975
train gradient:  0.12541709654131136
iteration : 1518
train acc:  0.8203125
train loss:  0.3121337294578552
train gradient:  0.1385651241958328
iteration : 1519
train acc:  0.8515625
train loss:  0.3495262861251831
train gradient:  0.15903945250891838
iteration : 1520
train acc:  0.8203125
train loss:  0.3460277020931244
train gradient:  0.19036525662628478
iteration : 1521
train acc:  0.8671875
train loss:  0.3133183717727661
train gradient:  0.12321643396573383
iteration : 1522
train acc:  0.8515625
train loss:  0.36421334743499756
train gradient:  0.18817768312400762
iteration : 1523
train acc:  0.8828125
train loss:  0.24023208022117615
train gradient:  0.11402483846199653
iteration : 1524
train acc:  0.921875
train loss:  0.2664202153682709
train gradient:  0.10549868387173712
iteration : 1525
train acc:  0.8671875
train loss:  0.33041125535964966
train gradient:  0.13103445476198383
iteration : 1526
train acc:  0.8828125
train loss:  0.2820358872413635
train gradient:  0.10019524515111856
iteration : 1527
train acc:  0.890625
train loss:  0.2880232632160187
train gradient:  0.09896371370337793
iteration : 1528
train acc:  0.8984375
train loss:  0.24095651507377625
train gradient:  0.08190717858556659
iteration : 1529
train acc:  0.875
train loss:  0.3389626741409302
train gradient:  0.25116889205475273
iteration : 1530
train acc:  0.8984375
train loss:  0.2385234832763672
train gradient:  0.08704041619308486
iteration : 1531
train acc:  0.8203125
train loss:  0.38759177923202515
train gradient:  0.18714134500363383
iteration : 1532
train acc:  0.875
train loss:  0.30782872438430786
train gradient:  0.09125129305130081
iteration : 1533
train acc:  0.8359375
train loss:  0.3422826826572418
train gradient:  0.145215738599595
iteration : 1534
train acc:  0.890625
train loss:  0.21397697925567627
train gradient:  0.10848926808472957
iteration : 1535
train acc:  0.8828125
train loss:  0.26105761528015137
train gradient:  0.09090574317002237
iteration : 1536
train acc:  0.8125
train loss:  0.3663908839225769
train gradient:  0.21001843184005975
iteration : 1537
train acc:  0.875
train loss:  0.31743526458740234
train gradient:  0.1635049362387217
iteration : 1538
train acc:  0.8125
train loss:  0.36311960220336914
train gradient:  0.1784419765960196
iteration : 1539
train acc:  0.859375
train loss:  0.3111083507537842
train gradient:  0.15428340440549576
iteration : 1540
train acc:  0.8671875
train loss:  0.3046528100967407
train gradient:  0.1335601501499747
iteration : 1541
train acc:  0.8671875
train loss:  0.27873897552490234
train gradient:  0.07957801259746608
iteration : 1542
train acc:  0.8828125
train loss:  0.3380504250526428
train gradient:  0.14434722209851525
iteration : 1543
train acc:  0.890625
train loss:  0.3354198634624481
train gradient:  0.17843238979789594
iteration : 1544
train acc:  0.875
train loss:  0.26612257957458496
train gradient:  0.12603707569985512
iteration : 1545
train acc:  0.875
train loss:  0.27395108342170715
train gradient:  0.11271908479073624
iteration : 1546
train acc:  0.8671875
train loss:  0.2756285071372986
train gradient:  0.12551780825852557
iteration : 1547
train acc:  0.8828125
train loss:  0.3162929117679596
train gradient:  0.17075200664000736
iteration : 1548
train acc:  0.84375
train loss:  0.3716694414615631
train gradient:  0.1976760570756418
iteration : 1549
train acc:  0.921875
train loss:  0.20509669184684753
train gradient:  0.09537776002947562
iteration : 1550
train acc:  0.8359375
train loss:  0.38057681918144226
train gradient:  0.20506597668200938
iteration : 1551
train acc:  0.8125
train loss:  0.4082820415496826
train gradient:  0.24751895284698952
iteration : 1552
train acc:  0.90625
train loss:  0.224714457988739
train gradient:  0.08294045203248515
iteration : 1553
train acc:  0.859375
train loss:  0.31221455335617065
train gradient:  0.10577345679701258
iteration : 1554
train acc:  0.8671875
train loss:  0.25098106265068054
train gradient:  0.08094068757400391
iteration : 1555
train acc:  0.875
train loss:  0.2839975655078888
train gradient:  0.11249515040822741
iteration : 1556
train acc:  0.90625
train loss:  0.23483601212501526
train gradient:  0.0954027288317037
iteration : 1557
train acc:  0.84375
train loss:  0.3568105101585388
train gradient:  0.17599633712430984
iteration : 1558
train acc:  0.8515625
train loss:  0.3208802342414856
train gradient:  0.19147739698154875
iteration : 1559
train acc:  0.890625
train loss:  0.26203885674476624
train gradient:  0.0961571941476749
iteration : 1560
train acc:  0.9296875
train loss:  0.24316394329071045
train gradient:  0.10229501942966646
iteration : 1561
train acc:  0.875
train loss:  0.2977394461631775
train gradient:  0.10992430705156217
iteration : 1562
train acc:  0.8515625
train loss:  0.31056684255599976
train gradient:  0.1345757008304552
iteration : 1563
train acc:  0.9140625
train loss:  0.23327505588531494
train gradient:  0.08751035325023303
iteration : 1564
train acc:  0.84375
train loss:  0.31774479150772095
train gradient:  0.14219721302890603
iteration : 1565
train acc:  0.875
train loss:  0.306851327419281
train gradient:  0.15110583012274864
iteration : 1566
train acc:  0.875
train loss:  0.32092899084091187
train gradient:  0.14368597412809206
iteration : 1567
train acc:  0.796875
train loss:  0.4088451862335205
train gradient:  0.24800807710225278
iteration : 1568
train acc:  0.8828125
train loss:  0.28123533725738525
train gradient:  0.09633073506873918
iteration : 1569
train acc:  0.875
train loss:  0.2846737205982208
train gradient:  0.08694432004103579
iteration : 1570
train acc:  0.875
train loss:  0.2704521417617798
train gradient:  0.09338479867858825
iteration : 1571
train acc:  0.8515625
train loss:  0.3155949115753174
train gradient:  0.13619077198708937
iteration : 1572
train acc:  0.8984375
train loss:  0.2383720427751541
train gradient:  0.10904289117506111
iteration : 1573
train acc:  0.875
train loss:  0.2512601912021637
train gradient:  0.11968716679250666
iteration : 1574
train acc:  0.8203125
train loss:  0.4252815544605255
train gradient:  0.1963196188649141
iteration : 1575
train acc:  0.8046875
train loss:  0.36646685004234314
train gradient:  0.14566936411080342
iteration : 1576
train acc:  0.9140625
train loss:  0.19470356404781342
train gradient:  0.13637602267939872
iteration : 1577
train acc:  0.8359375
train loss:  0.37787166237831116
train gradient:  0.1845022084501188
iteration : 1578
train acc:  0.8828125
train loss:  0.3199886083602905
train gradient:  0.1702094997969922
iteration : 1579
train acc:  0.8671875
train loss:  0.32159000635147095
train gradient:  0.1499050677765435
iteration : 1580
train acc:  0.84375
train loss:  0.3663825988769531
train gradient:  0.1330642369924171
iteration : 1581
train acc:  0.9140625
train loss:  0.2289908081293106
train gradient:  0.10000955725903965
iteration : 1582
train acc:  0.90625
train loss:  0.21589674055576324
train gradient:  0.09864024042252093
iteration : 1583
train acc:  0.890625
train loss:  0.2684597969055176
train gradient:  0.09022893876603884
iteration : 1584
train acc:  0.8046875
train loss:  0.41459986567497253
train gradient:  0.3950809971417341
iteration : 1585
train acc:  0.875
train loss:  0.3170948922634125
train gradient:  0.14887974575428647
iteration : 1586
train acc:  0.84375
train loss:  0.36850234866142273
train gradient:  0.20074218881702693
iteration : 1587
train acc:  0.84375
train loss:  0.30967020988464355
train gradient:  0.1672166411997886
iteration : 1588
train acc:  0.8515625
train loss:  0.3473896086215973
train gradient:  0.19245368959112502
iteration : 1589
train acc:  0.8203125
train loss:  0.39202672243118286
train gradient:  0.14074849395309277
iteration : 1590
train acc:  0.875
train loss:  0.3321678042411804
train gradient:  0.16731400299990362
iteration : 1591
train acc:  0.8359375
train loss:  0.3666755259037018
train gradient:  0.19989409076111575
iteration : 1592
train acc:  0.875
train loss:  0.3214675784111023
train gradient:  0.2905423193604461
iteration : 1593
train acc:  0.9375
train loss:  0.1861729621887207
train gradient:  0.08260810404392742
iteration : 1594
train acc:  0.875
train loss:  0.3601071834564209
train gradient:  0.146447595443141
iteration : 1595
train acc:  0.8984375
train loss:  0.26261773705482483
train gradient:  0.10263722016499503
iteration : 1596
train acc:  0.828125
train loss:  0.34416505694389343
train gradient:  0.1294308175354813
iteration : 1597
train acc:  0.8671875
train loss:  0.28294867277145386
train gradient:  0.12809686997701092
iteration : 1598
train acc:  0.90625
train loss:  0.29769185185432434
train gradient:  0.1459919968644976
iteration : 1599
train acc:  0.90625
train loss:  0.2397383451461792
train gradient:  0.09234297357650835
iteration : 1600
train acc:  0.859375
train loss:  0.300340473651886
train gradient:  0.10900390056292611
iteration : 1601
train acc:  0.84375
train loss:  0.3645743429660797
train gradient:  0.15414957734932658
iteration : 1602
train acc:  0.9375
train loss:  0.22966927289962769
train gradient:  0.07599595975295904
iteration : 1603
train acc:  0.8828125
train loss:  0.2723116874694824
train gradient:  0.10755915401960724
iteration : 1604
train acc:  0.8515625
train loss:  0.2888259291648865
train gradient:  0.10700543554647257
iteration : 1605
train acc:  0.84375
train loss:  0.3153107464313507
train gradient:  0.1110232943351986
iteration : 1606
train acc:  0.875
train loss:  0.2916634976863861
train gradient:  0.1168461540633802
iteration : 1607
train acc:  0.875
train loss:  0.2858351469039917
train gradient:  0.17093697597552393
iteration : 1608
train acc:  0.8671875
train loss:  0.2972225546836853
train gradient:  0.15632161854569976
iteration : 1609
train acc:  0.875
train loss:  0.32194802165031433
train gradient:  0.15554524249551727
iteration : 1610
train acc:  0.828125
train loss:  0.37152525782585144
train gradient:  0.22277175441077082
iteration : 1611
train acc:  0.9140625
train loss:  0.2512553632259369
train gradient:  0.11237375393116299
iteration : 1612
train acc:  0.859375
train loss:  0.3481566607952118
train gradient:  0.24842173812655077
iteration : 1613
train acc:  0.890625
train loss:  0.3193560242652893
train gradient:  0.11198762683764234
iteration : 1614
train acc:  0.8515625
train loss:  0.32647377252578735
train gradient:  0.1489946200386236
iteration : 1615
train acc:  0.84375
train loss:  0.37234121561050415
train gradient:  0.2872192195383535
iteration : 1616
train acc:  0.90625
train loss:  0.2548312842845917
train gradient:  0.09130789035861311
iteration : 1617
train acc:  0.875
train loss:  0.27749311923980713
train gradient:  0.11159280040882827
iteration : 1618
train acc:  0.8515625
train loss:  0.3435733914375305
train gradient:  0.14333054902322498
iteration : 1619
train acc:  0.8359375
train loss:  0.33606940507888794
train gradient:  0.14206240940413234
iteration : 1620
train acc:  0.875
train loss:  0.278836190700531
train gradient:  0.11010785740736805
iteration : 1621
train acc:  0.8984375
train loss:  0.30222558975219727
train gradient:  0.12339861852907834
iteration : 1622
train acc:  0.8671875
train loss:  0.31588560342788696
train gradient:  0.15552711018947543
iteration : 1623
train acc:  0.8515625
train loss:  0.3130435347557068
train gradient:  0.14085585574973303
iteration : 1624
train acc:  0.8828125
train loss:  0.32951968908309937
train gradient:  0.14321710738983512
iteration : 1625
train acc:  0.9296875
train loss:  0.22895018756389618
train gradient:  0.0709935099206683
iteration : 1626
train acc:  0.8828125
train loss:  0.2776748538017273
train gradient:  0.12125701999120406
iteration : 1627
train acc:  0.8671875
train loss:  0.33658891916275024
train gradient:  0.18295179667035716
iteration : 1628
train acc:  0.8671875
train loss:  0.3265063762664795
train gradient:  0.13418731082781832
iteration : 1629
train acc:  0.875
train loss:  0.2962793707847595
train gradient:  0.1658146961935304
iteration : 1630
train acc:  0.8984375
train loss:  0.25964340567588806
train gradient:  0.12233555700802796
iteration : 1631
train acc:  0.8828125
train loss:  0.3258158266544342
train gradient:  0.1304338237853478
iteration : 1632
train acc:  0.875
train loss:  0.3101365268230438
train gradient:  0.13033613537148525
iteration : 1633
train acc:  0.8203125
train loss:  0.42721664905548096
train gradient:  0.27692642689562286
iteration : 1634
train acc:  0.8359375
train loss:  0.3999576270580292
train gradient:  0.18914728821419807
iteration : 1635
train acc:  0.8515625
train loss:  0.34457653760910034
train gradient:  0.159805456473425
iteration : 1636
train acc:  0.859375
train loss:  0.36505377292633057
train gradient:  0.13656573817911216
iteration : 1637
train acc:  0.84375
train loss:  0.3622475266456604
train gradient:  0.1531833235433846
iteration : 1638
train acc:  0.828125
train loss:  0.34346839785575867
train gradient:  0.14328297672166
iteration : 1639
train acc:  0.921875
train loss:  0.30944913625717163
train gradient:  0.19865811735325511
iteration : 1640
train acc:  0.8515625
train loss:  0.3171229362487793
train gradient:  0.13408368338293145
iteration : 1641
train acc:  0.828125
train loss:  0.33231985569000244
train gradient:  0.19754843307161812
iteration : 1642
train acc:  0.890625
train loss:  0.2440752387046814
train gradient:  0.0703478141340246
iteration : 1643
train acc:  0.84375
train loss:  0.311580091714859
train gradient:  0.12302877905500348
iteration : 1644
train acc:  0.859375
train loss:  0.32764679193496704
train gradient:  0.1726000266479253
iteration : 1645
train acc:  0.8359375
train loss:  0.3283765912055969
train gradient:  0.12321486788346434
iteration : 1646
train acc:  0.875
train loss:  0.2835560441017151
train gradient:  0.1534062870972514
iteration : 1647
train acc:  0.8203125
train loss:  0.38825398683547974
train gradient:  0.2072494186667004
iteration : 1648
train acc:  0.8125
train loss:  0.36063218116760254
train gradient:  0.15694783830820236
iteration : 1649
train acc:  0.875
train loss:  0.31737765669822693
train gradient:  0.1201308817855977
iteration : 1650
train acc:  0.8671875
train loss:  0.3371407389640808
train gradient:  0.10156931902719975
iteration : 1651
train acc:  0.8984375
train loss:  0.25116533041000366
train gradient:  0.09334605818552748
iteration : 1652
train acc:  0.84375
train loss:  0.27154088020324707
train gradient:  0.08296285166402455
iteration : 1653
train acc:  0.828125
train loss:  0.3558114469051361
train gradient:  0.14484641524975017
iteration : 1654
train acc:  0.8671875
train loss:  0.3002249300479889
train gradient:  0.09658919722785715
iteration : 1655
train acc:  0.8125
train loss:  0.4005588889122009
train gradient:  0.12246867876665397
iteration : 1656
train acc:  0.9140625
train loss:  0.27076512575149536
train gradient:  0.0843084707653828
iteration : 1657
train acc:  0.8984375
train loss:  0.2680957317352295
train gradient:  0.10598873893906839
iteration : 1658
train acc:  0.859375
train loss:  0.31708431243896484
train gradient:  0.14871314998018867
iteration : 1659
train acc:  0.890625
train loss:  0.2952817678451538
train gradient:  0.11006168387752145
iteration : 1660
train acc:  0.8828125
train loss:  0.30626195669174194
train gradient:  0.1196144876914662
iteration : 1661
train acc:  0.859375
train loss:  0.3802878260612488
train gradient:  0.1836936444253735
iteration : 1662
train acc:  0.890625
train loss:  0.2943940758705139
train gradient:  0.11577128677054753
iteration : 1663
train acc:  0.8515625
train loss:  0.3248478174209595
train gradient:  0.14650612928558981
iteration : 1664
train acc:  0.84375
train loss:  0.28990352153778076
train gradient:  0.12858072411933394
iteration : 1665
train acc:  0.859375
train loss:  0.3116726577281952
train gradient:  0.18041432875381846
iteration : 1666
train acc:  0.8984375
train loss:  0.2585679292678833
train gradient:  0.08067555069865887
iteration : 1667
train acc:  0.875
train loss:  0.28489863872528076
train gradient:  0.12345009570163606
iteration : 1668
train acc:  0.8671875
train loss:  0.2906533479690552
train gradient:  0.11997307082278752
iteration : 1669
train acc:  0.859375
train loss:  0.2846616506576538
train gradient:  0.11537060309509706
iteration : 1670
train acc:  0.8359375
train loss:  0.3501443564891815
train gradient:  0.1466247120306156
iteration : 1671
train acc:  0.8828125
train loss:  0.2742353677749634
train gradient:  0.0972032458838287
iteration : 1672
train acc:  0.8515625
train loss:  0.2867943048477173
train gradient:  0.09630815465408603
iteration : 1673
train acc:  0.84375
train loss:  0.32313042879104614
train gradient:  0.17488009394486909
iteration : 1674
train acc:  0.859375
train loss:  0.31871306896209717
train gradient:  0.11653346547303131
iteration : 1675
train acc:  0.8984375
train loss:  0.23624461889266968
train gradient:  0.07922265130730854
iteration : 1676
train acc:  0.84375
train loss:  0.3924596607685089
train gradient:  0.1865684875020296
iteration : 1677
train acc:  0.8515625
train loss:  0.3202803134918213
train gradient:  0.148572080513093
iteration : 1678
train acc:  0.8984375
train loss:  0.20607376098632812
train gradient:  0.05453828634958782
iteration : 1679
train acc:  0.84375
train loss:  0.3077589273452759
train gradient:  0.13581474065837038
iteration : 1680
train acc:  0.8359375
train loss:  0.344791978597641
train gradient:  0.17964652892198255
iteration : 1681
train acc:  0.90625
train loss:  0.23730698227882385
train gradient:  0.10498531903718358
iteration : 1682
train acc:  0.828125
train loss:  0.38076576590538025
train gradient:  0.14294222512771643
iteration : 1683
train acc:  0.90625
train loss:  0.23268692195415497
train gradient:  0.09401398306739824
iteration : 1684
train acc:  0.8046875
train loss:  0.3915128707885742
train gradient:  0.19107954385873765
iteration : 1685
train acc:  0.8359375
train loss:  0.3033086657524109
train gradient:  0.09609713764490042
iteration : 1686
train acc:  0.8125
train loss:  0.37514638900756836
train gradient:  0.1979974726986967
iteration : 1687
train acc:  0.8515625
train loss:  0.4257795810699463
train gradient:  0.19429698187376993
iteration : 1688
train acc:  0.8828125
train loss:  0.31700700521469116
train gradient:  0.12008193842383326
iteration : 1689
train acc:  0.8984375
train loss:  0.3133428990840912
train gradient:  0.15954319836705383
iteration : 1690
train acc:  0.8828125
train loss:  0.28069084882736206
train gradient:  0.12289680524671914
iteration : 1691
train acc:  0.921875
train loss:  0.2303881049156189
train gradient:  0.11432910216039806
iteration : 1692
train acc:  0.859375
train loss:  0.3348391056060791
train gradient:  0.15402708720579217
iteration : 1693
train acc:  0.859375
train loss:  0.28670409321784973
train gradient:  0.13889144977310391
iteration : 1694
train acc:  0.8125
train loss:  0.4474666714668274
train gradient:  0.24448210290660827
iteration : 1695
train acc:  0.78125
train loss:  0.4453875720500946
train gradient:  0.1754122427918835
iteration : 1696
train acc:  0.921875
train loss:  0.24921080470085144
train gradient:  0.080368001471233
iteration : 1697
train acc:  0.84375
train loss:  0.3691209554672241
train gradient:  0.16046136035503725
iteration : 1698
train acc:  0.921875
train loss:  0.22855661809444427
train gradient:  0.07795393243654028
iteration : 1699
train acc:  0.8046875
train loss:  0.4110413193702698
train gradient:  0.17434110225709176
iteration : 1700
train acc:  0.828125
train loss:  0.32999399304389954
train gradient:  0.15623996801372003
iteration : 1701
train acc:  0.890625
train loss:  0.32813525199890137
train gradient:  0.0991116502198272
iteration : 1702
train acc:  0.90625
train loss:  0.21249419450759888
train gradient:  0.06438284639281847
iteration : 1703
train acc:  0.875
train loss:  0.3469739854335785
train gradient:  0.14306848311803716
iteration : 1704
train acc:  0.7734375
train loss:  0.3891937732696533
train gradient:  0.2257060744427714
iteration : 1705
train acc:  0.90625
train loss:  0.26702916622161865
train gradient:  0.14525937051752907
iteration : 1706
train acc:  0.859375
train loss:  0.2993399500846863
train gradient:  0.09831573578705233
iteration : 1707
train acc:  0.8203125
train loss:  0.36846238374710083
train gradient:  0.23889126320128712
iteration : 1708
train acc:  0.875
train loss:  0.2930063307285309
train gradient:  0.11184192075674775
iteration : 1709
train acc:  0.8671875
train loss:  0.27450501918792725
train gradient:  0.09991880450208888
iteration : 1710
train acc:  0.8359375
train loss:  0.41856902837753296
train gradient:  0.19766280039734474
iteration : 1711
train acc:  0.8984375
train loss:  0.26817238330841064
train gradient:  0.06919946370064366
iteration : 1712
train acc:  0.890625
train loss:  0.30237749218940735
train gradient:  0.11263186664728322
iteration : 1713
train acc:  0.828125
train loss:  0.3645766079425812
train gradient:  0.13107518841993887
iteration : 1714
train acc:  0.890625
train loss:  0.27304351329803467
train gradient:  0.09397548313785649
iteration : 1715
train acc:  0.828125
train loss:  0.3298131227493286
train gradient:  0.1029848355996702
iteration : 1716
train acc:  0.84375
train loss:  0.3016458749771118
train gradient:  0.115353793200076
iteration : 1717
train acc:  0.9140625
train loss:  0.27496665716171265
train gradient:  0.1273449429176454
iteration : 1718
train acc:  0.8203125
train loss:  0.4030512571334839
train gradient:  0.18949004088742138
iteration : 1719
train acc:  0.8203125
train loss:  0.3309054374694824
train gradient:  0.13256718707463433
iteration : 1720
train acc:  0.84375
train loss:  0.3038961887359619
train gradient:  0.09705815282547893
iteration : 1721
train acc:  0.875
train loss:  0.372927725315094
train gradient:  0.12428859287131461
iteration : 1722
train acc:  0.8046875
train loss:  0.38413071632385254
train gradient:  0.2318511297027345
iteration : 1723
train acc:  0.8515625
train loss:  0.3238779902458191
train gradient:  0.12603711895208908
iteration : 1724
train acc:  0.890625
train loss:  0.3001866042613983
train gradient:  0.09931922876525309
iteration : 1725
train acc:  0.796875
train loss:  0.34364914894104004
train gradient:  0.1558392792894197
iteration : 1726
train acc:  0.8828125
train loss:  0.29389214515686035
train gradient:  0.12511600420550156
iteration : 1727
train acc:  0.7890625
train loss:  0.3919219970703125
train gradient:  0.17702215817647735
iteration : 1728
train acc:  0.8671875
train loss:  0.31985095143318176
train gradient:  0.23200749842995677
iteration : 1729
train acc:  0.828125
train loss:  0.38060224056243896
train gradient:  0.1506272344967306
iteration : 1730
train acc:  0.890625
train loss:  0.26854562759399414
train gradient:  0.11887990437900707
iteration : 1731
train acc:  0.8828125
train loss:  0.2952668070793152
train gradient:  0.12424112810097944
iteration : 1732
train acc:  0.875
train loss:  0.28881698846817017
train gradient:  0.08784462581328324
iteration : 1733
train acc:  0.90625
train loss:  0.2446688562631607
train gradient:  0.08578368038583652
iteration : 1734
train acc:  0.90625
train loss:  0.22720745205879211
train gradient:  0.10456553093252136
iteration : 1735
train acc:  0.8203125
train loss:  0.3795678913593292
train gradient:  0.18459889692940237
iteration : 1736
train acc:  0.828125
train loss:  0.37588679790496826
train gradient:  0.15352674750889178
iteration : 1737
train acc:  0.8515625
train loss:  0.3120962381362915
train gradient:  0.08951582186659253
iteration : 1738
train acc:  0.8984375
train loss:  0.28014230728149414
train gradient:  0.12016530250437876
iteration : 1739
train acc:  0.8359375
train loss:  0.3354848325252533
train gradient:  0.1284528474456183
iteration : 1740
train acc:  0.8984375
train loss:  0.26421618461608887
train gradient:  0.11430504173423038
iteration : 1741
train acc:  0.796875
train loss:  0.39311274886131287
train gradient:  0.16235739165005159
iteration : 1742
train acc:  0.859375
train loss:  0.317407488822937
train gradient:  0.2347890929366514
iteration : 1743
train acc:  0.8671875
train loss:  0.30429935455322266
train gradient:  0.11151585665240422
iteration : 1744
train acc:  0.8828125
train loss:  0.29020825028419495
train gradient:  0.08420471453148663
iteration : 1745
train acc:  0.8671875
train loss:  0.3372538685798645
train gradient:  0.12653719611642456
iteration : 1746
train acc:  0.859375
train loss:  0.3383674621582031
train gradient:  0.13515563927821594
iteration : 1747
train acc:  0.859375
train loss:  0.3393484950065613
train gradient:  0.22008140964251582
iteration : 1748
train acc:  0.9140625
train loss:  0.21112921833992004
train gradient:  0.06926111768263375
iteration : 1749
train acc:  0.7890625
train loss:  0.41760072112083435
train gradient:  0.1924239191996746
iteration : 1750
train acc:  0.875
train loss:  0.2835172712802887
train gradient:  0.1443295064397998
iteration : 1751
train acc:  0.875
train loss:  0.2404274344444275
train gradient:  0.07508705666830642
iteration : 1752
train acc:  0.890625
train loss:  0.24235354363918304
train gradient:  0.09004678633580251
iteration : 1753
train acc:  0.8671875
train loss:  0.2787740230560303
train gradient:  0.10961124286358441
iteration : 1754
train acc:  0.8828125
train loss:  0.23753011226654053
train gradient:  0.08537328493869731
iteration : 1755
train acc:  0.8984375
train loss:  0.2193358838558197
train gradient:  0.09286121908471862
iteration : 1756
train acc:  0.8828125
train loss:  0.2959611117839813
train gradient:  0.09149832516282473
iteration : 1757
train acc:  0.8984375
train loss:  0.2510169744491577
train gradient:  0.07631608379025152
iteration : 1758
train acc:  0.84375
train loss:  0.3344760239124298
train gradient:  0.18816171448517327
iteration : 1759
train acc:  0.8203125
train loss:  0.2857068181037903
train gradient:  0.15063183940090286
iteration : 1760
train acc:  0.8359375
train loss:  0.3537757098674774
train gradient:  0.12816572131516918
iteration : 1761
train acc:  0.8046875
train loss:  0.46770769357681274
train gradient:  0.27167931576548887
iteration : 1762
train acc:  0.890625
train loss:  0.2784327268600464
train gradient:  0.13666526547089558
iteration : 1763
train acc:  0.84375
train loss:  0.30621659755706787
train gradient:  0.12213884969397605
iteration : 1764
train acc:  0.8203125
train loss:  0.3633444905281067
train gradient:  0.12662485806214602
iteration : 1765
train acc:  0.875
train loss:  0.30951979756355286
train gradient:  0.1669298932993407
iteration : 1766
train acc:  0.8671875
train loss:  0.2781643867492676
train gradient:  0.07192324371799508
iteration : 1767
train acc:  0.875
train loss:  0.2824503183364868
train gradient:  0.09523571851328391
iteration : 1768
train acc:  0.9140625
train loss:  0.2122100591659546
train gradient:  0.0802653088968397
iteration : 1769
train acc:  0.828125
train loss:  0.3480018973350525
train gradient:  0.17047854098124293
iteration : 1770
train acc:  0.8671875
train loss:  0.34216389060020447
train gradient:  0.20958455492747505
iteration : 1771
train acc:  0.765625
train loss:  0.4751587510108948
train gradient:  0.242013746972449
iteration : 1772
train acc:  0.8984375
train loss:  0.2987814247608185
train gradient:  0.10175752383235127
iteration : 1773
train acc:  0.875
train loss:  0.26318442821502686
train gradient:  0.09488727645832552
iteration : 1774
train acc:  0.84375
train loss:  0.31621253490448
train gradient:  0.15659134519929535
iteration : 1775
train acc:  0.828125
train loss:  0.3442075252532959
train gradient:  0.16138225346636703
iteration : 1776
train acc:  0.8828125
train loss:  0.28837141394615173
train gradient:  0.08504488193959271
iteration : 1777
train acc:  0.8671875
train loss:  0.25599750876426697
train gradient:  0.11138183753279103
iteration : 1778
train acc:  0.8515625
train loss:  0.29394519329071045
train gradient:  0.12326811484437253
iteration : 1779
train acc:  0.7734375
train loss:  0.450112521648407
train gradient:  0.24241770540477664
iteration : 1780
train acc:  0.8984375
train loss:  0.2539016306400299
train gradient:  0.0849738836751315
iteration : 1781
train acc:  0.8671875
train loss:  0.34343528747558594
train gradient:  0.16102112328290344
iteration : 1782
train acc:  0.859375
train loss:  0.3242003917694092
train gradient:  0.10581537774087764
iteration : 1783
train acc:  0.8515625
train loss:  0.40223753452301025
train gradient:  0.1347703087307552
iteration : 1784
train acc:  0.90625
train loss:  0.2199629843235016
train gradient:  0.08214704233324248
iteration : 1785
train acc:  0.890625
train loss:  0.3140765428543091
train gradient:  0.11426468344150625
iteration : 1786
train acc:  0.921875
train loss:  0.23538559675216675
train gradient:  0.09893870882732975
iteration : 1787
train acc:  0.9140625
train loss:  0.21850216388702393
train gradient:  0.0681830116381148
iteration : 1788
train acc:  0.8671875
train loss:  0.2761518955230713
train gradient:  0.1002960237461364
iteration : 1789
train acc:  0.8671875
train loss:  0.23625247180461884
train gradient:  0.07924525152975438
iteration : 1790
train acc:  0.859375
train loss:  0.29875317215919495
train gradient:  0.12030060462933703
iteration : 1791
train acc:  0.8984375
train loss:  0.34646910429000854
train gradient:  0.1530566347271494
iteration : 1792
train acc:  0.8125
train loss:  0.3803473711013794
train gradient:  0.1365546726076001
iteration : 1793
train acc:  0.859375
train loss:  0.24335503578186035
train gradient:  0.06880793892040005
iteration : 1794
train acc:  0.859375
train loss:  0.30800366401672363
train gradient:  0.09454555688513233
iteration : 1795
train acc:  0.8359375
train loss:  0.3654114007949829
train gradient:  0.18793323834306042
iteration : 1796
train acc:  0.875
train loss:  0.29695042967796326
train gradient:  0.08794393308957944
iteration : 1797
train acc:  0.8984375
train loss:  0.27377936244010925
train gradient:  0.110774914557503
iteration : 1798
train acc:  0.8203125
train loss:  0.37545979022979736
train gradient:  0.1577680315903981
iteration : 1799
train acc:  0.8359375
train loss:  0.325123131275177
train gradient:  0.11763807547881772
iteration : 1800
train acc:  0.890625
train loss:  0.260823130607605
train gradient:  0.09672214095916168
iteration : 1801
train acc:  0.8984375
train loss:  0.26900774240493774
train gradient:  0.11050792521593245
iteration : 1802
train acc:  0.8515625
train loss:  0.36631739139556885
train gradient:  0.20767029053360253
iteration : 1803
train acc:  0.890625
train loss:  0.270464152097702
train gradient:  0.09652323812532478
iteration : 1804
train acc:  0.8671875
train loss:  0.262187659740448
train gradient:  0.14048971540800642
iteration : 1805
train acc:  0.8515625
train loss:  0.2906617820262909
train gradient:  0.21670175140662457
iteration : 1806
train acc:  0.8359375
train loss:  0.36574655771255493
train gradient:  0.17799873132110022
iteration : 1807
train acc:  0.875
train loss:  0.30274295806884766
train gradient:  0.10142591585272609
iteration : 1808
train acc:  0.84375
train loss:  0.36288875341415405
train gradient:  0.1836532802461221
iteration : 1809
train acc:  0.875
train loss:  0.3232277035713196
train gradient:  0.12576982149509594
iteration : 1810
train acc:  0.828125
train loss:  0.3979248106479645
train gradient:  0.15014917699573918
iteration : 1811
train acc:  0.875
train loss:  0.25108349323272705
train gradient:  0.156345781325933
iteration : 1812
train acc:  0.8828125
train loss:  0.31670081615448
train gradient:  0.09924091049166614
iteration : 1813
train acc:  0.8515625
train loss:  0.32666701078414917
train gradient:  0.13013613655146009
iteration : 1814
train acc:  0.8515625
train loss:  0.2906869649887085
train gradient:  0.09437226585523123
iteration : 1815
train acc:  0.8046875
train loss:  0.427672803401947
train gradient:  0.1847353733700524
iteration : 1816
train acc:  0.875
train loss:  0.3843240737915039
train gradient:  0.2678260903008061
iteration : 1817
train acc:  0.859375
train loss:  0.3392654061317444
train gradient:  0.11747619709146709
iteration : 1818
train acc:  0.8359375
train loss:  0.41545790433883667
train gradient:  0.2847653431225948
iteration : 1819
train acc:  0.8359375
train loss:  0.34777259826660156
train gradient:  0.20819820415088805
iteration : 1820
train acc:  0.859375
train loss:  0.3865776062011719
train gradient:  0.20483675434505966
iteration : 1821
train acc:  0.875
train loss:  0.29070132970809937
train gradient:  0.13244072813270938
iteration : 1822
train acc:  0.890625
train loss:  0.2765086889266968
train gradient:  0.1289304288411406
iteration : 1823
train acc:  0.8828125
train loss:  0.2580662965774536
train gradient:  0.12268158477850118
iteration : 1824
train acc:  0.875
train loss:  0.3222319185733795
train gradient:  0.1071096339190571
iteration : 1825
train acc:  0.8203125
train loss:  0.2976709008216858
train gradient:  0.0985943499992793
iteration : 1826
train acc:  0.8671875
train loss:  0.2523653507232666
train gradient:  0.1178733097284193
iteration : 1827
train acc:  0.8671875
train loss:  0.3470718264579773
train gradient:  0.14595417394475113
iteration : 1828
train acc:  0.84375
train loss:  0.3529656231403351
train gradient:  0.14042425614579518
iteration : 1829
train acc:  0.890625
train loss:  0.23461881279945374
train gradient:  0.12854574282154213
iteration : 1830
train acc:  0.90625
train loss:  0.2882407307624817
train gradient:  0.17182983498543178
iteration : 1831
train acc:  0.8515625
train loss:  0.2806479036808014
train gradient:  0.162391835311974
iteration : 1832
train acc:  0.8515625
train loss:  0.32316476106643677
train gradient:  0.16098003330680266
iteration : 1833
train acc:  0.8203125
train loss:  0.3887858986854553
train gradient:  0.2857423732669434
iteration : 1834
train acc:  0.8359375
train loss:  0.3194652795791626
train gradient:  0.16849058812446766
iteration : 1835
train acc:  0.828125
train loss:  0.33053821325302124
train gradient:  0.1254207833343659
iteration : 1836
train acc:  0.8203125
train loss:  0.38590627908706665
train gradient:  0.20756439961735484
iteration : 1837
train acc:  0.8671875
train loss:  0.3155747354030609
train gradient:  0.1256736630196562
iteration : 1838
train acc:  0.8828125
train loss:  0.2563154697418213
train gradient:  0.09605542950860553
iteration : 1839
train acc:  0.890625
train loss:  0.2959650754928589
train gradient:  0.13740511600907412
iteration : 1840
train acc:  0.921875
train loss:  0.23725740611553192
train gradient:  0.07893688141691618
iteration : 1841
train acc:  0.8359375
train loss:  0.3588009178638458
train gradient:  0.1548055043641598
iteration : 1842
train acc:  0.8515625
train loss:  0.35071852803230286
train gradient:  0.1783905726901613
iteration : 1843
train acc:  0.890625
train loss:  0.30475547909736633
train gradient:  0.14743602257615196
iteration : 1844
train acc:  0.9140625
train loss:  0.22715207934379578
train gradient:  0.08608372339293695
iteration : 1845
train acc:  0.8046875
train loss:  0.4085625112056732
train gradient:  0.18293368415998654
iteration : 1846
train acc:  0.8125
train loss:  0.4475993514060974
train gradient:  0.22491905159603331
iteration : 1847
train acc:  0.8515625
train loss:  0.38501644134521484
train gradient:  0.14741581660840264
iteration : 1848
train acc:  0.8828125
train loss:  0.2856983542442322
train gradient:  0.10880369591962626
iteration : 1849
train acc:  0.8359375
train loss:  0.33437585830688477
train gradient:  0.12896070210816488
iteration : 1850
train acc:  0.8828125
train loss:  0.31282103061676025
train gradient:  0.13618335370122042
iteration : 1851
train acc:  0.875
train loss:  0.24967704713344574
train gradient:  0.10304490931250766
iteration : 1852
train acc:  0.8515625
train loss:  0.3605993092060089
train gradient:  0.16617883411481021
iteration : 1853
train acc:  0.84375
train loss:  0.40798521041870117
train gradient:  0.20581719504990387
iteration : 1854
train acc:  0.875
train loss:  0.27549412846565247
train gradient:  0.12955144775955746
iteration : 1855
train acc:  0.9296875
train loss:  0.2544659674167633
train gradient:  0.1850750046300232
iteration : 1856
train acc:  0.921875
train loss:  0.2345525026321411
train gradient:  0.18477701374228853
iteration : 1857
train acc:  0.90625
train loss:  0.22776609659194946
train gradient:  0.08802123184055312
iteration : 1858
train acc:  0.875
train loss:  0.3378806412220001
train gradient:  0.22918666356658296
iteration : 1859
train acc:  0.9140625
train loss:  0.25569337606430054
train gradient:  0.08481685607362369
iteration : 1860
train acc:  0.84375
train loss:  0.29283806681632996
train gradient:  0.09735625568898319
iteration : 1861
train acc:  0.8515625
train loss:  0.3044966459274292
train gradient:  0.10032629513028833
iteration : 1862
train acc:  0.828125
train loss:  0.3514864444732666
train gradient:  0.2713535813471768
iteration : 1863
train acc:  0.8984375
train loss:  0.260677307844162
train gradient:  0.08526725286864306
iteration : 1864
train acc:  0.8671875
train loss:  0.3311975598335266
train gradient:  0.1108697476698007
iteration : 1865
train acc:  0.8671875
train loss:  0.26371830701828003
train gradient:  0.09937436142586023
iteration : 1866
train acc:  0.8359375
train loss:  0.34212246537208557
train gradient:  0.1544688885325437
iteration : 1867
train acc:  0.9296875
train loss:  0.24455636739730835
train gradient:  0.10629829526374068
iteration : 1868
train acc:  0.875
train loss:  0.27584388852119446
train gradient:  0.09026737879562433
iteration : 1869
train acc:  0.8359375
train loss:  0.3091851770877838
train gradient:  0.10912926736278279
iteration : 1870
train acc:  0.8515625
train loss:  0.3437275290489197
train gradient:  0.14210158643634696
iteration : 1871
train acc:  0.8359375
train loss:  0.3873376250267029
train gradient:  0.13270131168416854
iteration : 1872
train acc:  0.875
train loss:  0.30158862471580505
train gradient:  0.07973350328367253
iteration : 1873
train acc:  0.875
train loss:  0.4055635333061218
train gradient:  0.18178297537673774
iteration : 1874
train acc:  0.8515625
train loss:  0.38326433300971985
train gradient:  0.20062045409394452
iteration : 1875
train acc:  0.8359375
train loss:  0.3466776907444
train gradient:  0.14226624800780668
iteration : 1876
train acc:  0.859375
train loss:  0.3461307883262634
train gradient:  0.16072709096119864
iteration : 1877
train acc:  0.8828125
train loss:  0.25863510370254517
train gradient:  0.09115987171723383
iteration : 1878
train acc:  0.90625
train loss:  0.21389901638031006
train gradient:  0.06530821085591848
iteration : 1879
train acc:  0.8359375
train loss:  0.33432507514953613
train gradient:  0.11947434523373959
iteration : 1880
train acc:  0.90625
train loss:  0.2648186683654785
train gradient:  0.15562317586358806
iteration : 1881
train acc:  0.859375
train loss:  0.35319602489471436
train gradient:  0.14407800430546058
iteration : 1882
train acc:  0.8359375
train loss:  0.3650822937488556
train gradient:  0.16879438242870032
iteration : 1883
train acc:  0.84375
train loss:  0.32274630665779114
train gradient:  0.10915095680005056
iteration : 1884
train acc:  0.890625
train loss:  0.29874566197395325
train gradient:  0.09687817873605946
iteration : 1885
train acc:  0.890625
train loss:  0.2684246301651001
train gradient:  0.14612133731754318
iteration : 1886
train acc:  0.8671875
train loss:  0.3331034183502197
train gradient:  0.1747653676359483
iteration : 1887
train acc:  0.890625
train loss:  0.2731221914291382
train gradient:  0.06708584479559192
iteration : 1888
train acc:  0.8515625
train loss:  0.3457992672920227
train gradient:  0.15468668319921558
iteration : 1889
train acc:  0.8359375
train loss:  0.44968876242637634
train gradient:  0.22366698137929347
iteration : 1890
train acc:  0.8125
train loss:  0.4042130708694458
train gradient:  0.19382922422455262
iteration : 1891
train acc:  0.8828125
train loss:  0.27805766463279724
train gradient:  0.12069727004596774
iteration : 1892
train acc:  0.859375
train loss:  0.3801700472831726
train gradient:  0.12991396685492101
iteration : 1893
train acc:  0.8125
train loss:  0.35154595971107483
train gradient:  0.20465385720699208
iteration : 1894
train acc:  0.8359375
train loss:  0.3632933795452118
train gradient:  0.14139296547461516
iteration : 1895
train acc:  0.859375
train loss:  0.30825692415237427
train gradient:  0.14817405912912635
iteration : 1896
train acc:  0.84375
train loss:  0.30921369791030884
train gradient:  0.09509839675619403
iteration : 1897
train acc:  0.890625
train loss:  0.307822048664093
train gradient:  0.10834606548064882
iteration : 1898
train acc:  0.890625
train loss:  0.2542495131492615
train gradient:  0.10171988814340645
iteration : 1899
train acc:  0.890625
train loss:  0.33325251936912537
train gradient:  0.0851423517388574
iteration : 1900
train acc:  0.90625
train loss:  0.2428220510482788
train gradient:  0.08940338654139238
iteration : 1901
train acc:  0.8671875
train loss:  0.27313679456710815
train gradient:  0.11810631874441226
iteration : 1902
train acc:  0.78125
train loss:  0.4172510504722595
train gradient:  0.17270488222167096
iteration : 1903
train acc:  0.84375
train loss:  0.3520117998123169
train gradient:  0.15263467245711448
iteration : 1904
train acc:  0.890625
train loss:  0.2686292827129364
train gradient:  0.09457662733586764
iteration : 1905
train acc:  0.8203125
train loss:  0.33974915742874146
train gradient:  0.13790872054217218
iteration : 1906
train acc:  0.9296875
train loss:  0.21781614422798157
train gradient:  0.09893195913749632
iteration : 1907
train acc:  0.8359375
train loss:  0.3509105443954468
train gradient:  0.14857087077189618
iteration : 1908
train acc:  0.859375
train loss:  0.3147711753845215
train gradient:  0.13273715522929352
iteration : 1909
train acc:  0.828125
train loss:  0.336729496717453
train gradient:  0.11679117987195754
iteration : 1910
train acc:  0.8203125
train loss:  0.3627898097038269
train gradient:  0.1179009223502185
iteration : 1911
train acc:  0.8125
train loss:  0.347276896238327
train gradient:  0.1198807746134742
iteration : 1912
train acc:  0.8828125
train loss:  0.2636123597621918
train gradient:  0.07216985901939009
iteration : 1913
train acc:  0.8671875
train loss:  0.3093540668487549
train gradient:  0.13721058702268868
iteration : 1914
train acc:  0.875
train loss:  0.2914181649684906
train gradient:  0.0994821385863276
iteration : 1915
train acc:  0.8671875
train loss:  0.3082164525985718
train gradient:  0.11891107335052893
iteration : 1916
train acc:  0.8046875
train loss:  0.36879438161849976
train gradient:  0.11294806388253024
iteration : 1917
train acc:  0.8984375
train loss:  0.2955223321914673
train gradient:  0.0958862664385373
iteration : 1918
train acc:  0.875
train loss:  0.3051651120185852
train gradient:  0.12784591811107152
iteration : 1919
train acc:  0.828125
train loss:  0.37540286779403687
train gradient:  0.18927556550170738
iteration : 1920
train acc:  0.8671875
train loss:  0.282844603061676
train gradient:  0.09306880772998063
iteration : 1921
train acc:  0.9140625
train loss:  0.2746196687221527
train gradient:  0.13652094203431525
iteration : 1922
train acc:  0.8359375
train loss:  0.39269864559173584
train gradient:  0.12637647966841847
iteration : 1923
train acc:  0.8046875
train loss:  0.34949740767478943
train gradient:  0.11170596269966702
iteration : 1924
train acc:  0.8984375
train loss:  0.2942047715187073
train gradient:  0.1330907475242647
iteration : 1925
train acc:  0.8671875
train loss:  0.33765870332717896
train gradient:  0.13216235495100923
iteration : 1926
train acc:  0.875
train loss:  0.31712910532951355
train gradient:  0.11559404322207584
iteration : 1927
train acc:  0.8671875
train loss:  0.2977132201194763
train gradient:  0.11831455280433165
iteration : 1928
train acc:  0.84375
train loss:  0.3119155168533325
train gradient:  0.1233779349226866
iteration : 1929
train acc:  0.8203125
train loss:  0.4014922082424164
train gradient:  0.1658912058533995
iteration : 1930
train acc:  0.8671875
train loss:  0.2864833474159241
train gradient:  0.09953117790409483
iteration : 1931
train acc:  0.8671875
train loss:  0.3592132031917572
train gradient:  0.15690421231565452
iteration : 1932
train acc:  0.8984375
train loss:  0.2439340353012085
train gradient:  0.07427524636999552
iteration : 1933
train acc:  0.8828125
train loss:  0.26133137941360474
train gradient:  0.08375451930312197
iteration : 1934
train acc:  0.9140625
train loss:  0.22619007527828217
train gradient:  0.0765485017653945
