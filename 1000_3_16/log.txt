program start:
num_rounds= 3
node_emb_dim= 16

----------------------------------------new_epoch--------------------------------------

epoch:  0
iteration : 0
train acc:  0.4296875
train loss:  0.7320830225944519
train gradient:  4.220587639817309
iteration : 1
train acc:  0.53125
train loss:  0.6957780122756958
train gradient:  2.488061160649883
iteration : 2
train acc:  0.578125
train loss:  0.698117733001709
train gradient:  4.5242167424776465
iteration : 3
train acc:  0.53125
train loss:  0.7088662385940552
train gradient:  3.5555471976792594
iteration : 4
train acc:  0.5390625
train loss:  0.6878124475479126
train gradient:  1.4186434591314847
iteration : 5
train acc:  0.5078125
train loss:  0.6894611120223999
train gradient:  1.3644461956753486
iteration : 6
train acc:  0.5859375
train loss:  0.6681524515151978
train gradient:  0.7669726607673331
iteration : 7
train acc:  0.609375
train loss:  0.7089792490005493
train gradient:  3.0447119470715736
iteration : 8
train acc:  0.4765625
train loss:  0.7066971063613892
train gradient:  0.7298685352836801
iteration : 9
train acc:  0.5
train loss:  0.7119298577308655
train gradient:  1.0146747153376043
iteration : 10
train acc:  0.625
train loss:  0.6454658508300781
train gradient:  0.739022949067389
iteration : 11
train acc:  0.515625
train loss:  0.6818969249725342
train gradient:  0.7677172911567521
iteration : 12
train acc:  0.4609375
train loss:  0.6986626386642456
train gradient:  0.5851325494683257
iteration : 13
train acc:  0.515625
train loss:  0.6898208856582642
train gradient:  0.884496023730724
iteration : 14
train acc:  0.5703125
train loss:  0.6568822860717773
train gradient:  0.46409280943471765
iteration : 15
train acc:  0.6171875
train loss:  0.7069268226623535
train gradient:  2.5291102317740686
iteration : 16
train acc:  0.546875
train loss:  0.666023850440979
train gradient:  0.30102371530869987
iteration : 17
train acc:  0.5078125
train loss:  0.6971277594566345
train gradient:  0.37298136084507344
iteration : 18
train acc:  0.5390625
train loss:  0.7205445766448975
train gradient:  0.9574820748136472
iteration : 19
train acc:  0.515625
train loss:  0.7125680446624756
train gradient:  1.2601813108509414
iteration : 20
train acc:  0.6015625
train loss:  0.6575959920883179
train gradient:  0.5882377102897288
iteration : 21
train acc:  0.5546875
train loss:  0.6743770837783813
train gradient:  1.0716896848625461
iteration : 22
train acc:  0.5859375
train loss:  0.7142133712768555
train gradient:  1.111402907900229
iteration : 23
train acc:  0.5703125
train loss:  0.6647741198539734
train gradient:  0.5842006747923573
iteration : 24
train acc:  0.5703125
train loss:  0.6838489770889282
train gradient:  0.6334750520515399
iteration : 25
train acc:  0.6328125
train loss:  0.6530609130859375
train gradient:  0.35806425557124666
iteration : 26
train acc:  0.578125
train loss:  0.6501187086105347
train gradient:  0.35613939345819473
iteration : 27
train acc:  0.5859375
train loss:  0.685816764831543
train gradient:  0.4717612204454011
iteration : 28
train acc:  0.53125
train loss:  0.6755492091178894
train gradient:  0.29432070670984734
iteration : 29
train acc:  0.578125
train loss:  0.6490238904953003
train gradient:  0.2994980988269082
iteration : 30
train acc:  0.640625
train loss:  0.6463271379470825
train gradient:  0.2874043123157575
iteration : 31
train acc:  0.6015625
train loss:  0.6857022047042847
train gradient:  0.6340058920782825
iteration : 32
train acc:  0.515625
train loss:  0.6915409564971924
train gradient:  0.7067815538318594
iteration : 33
train acc:  0.5625
train loss:  0.6695069670677185
train gradient:  0.7162416288167955
iteration : 34
train acc:  0.453125
train loss:  0.7130765318870544
train gradient:  0.9710901674656844
iteration : 35
train acc:  0.59375
train loss:  0.6909065842628479
train gradient:  0.44075869398379036
iteration : 36
train acc:  0.5703125
train loss:  0.6967153549194336
train gradient:  0.48268379708912745
iteration : 37
train acc:  0.578125
train loss:  0.6699998378753662
train gradient:  0.685899705649498
iteration : 38
train acc:  0.546875
train loss:  0.706893801689148
train gradient:  0.7301748306888125
iteration : 39
train acc:  0.6171875
train loss:  0.69501131772995
train gradient:  0.785342856894877
iteration : 40
train acc:  0.578125
train loss:  0.6598374843597412
train gradient:  0.3254831334054703
iteration : 41
train acc:  0.625
train loss:  0.6635068655014038
train gradient:  0.9777282347497449
iteration : 42
train acc:  0.5234375
train loss:  0.6947663426399231
train gradient:  1.0544034601545291
iteration : 43
train acc:  0.609375
train loss:  0.6736935973167419
train gradient:  0.6135229995780132
iteration : 44
train acc:  0.625
train loss:  0.6740096807479858
train gradient:  0.4997213234178325
iteration : 45
train acc:  0.5546875
train loss:  0.6625702381134033
train gradient:  0.24948672734325394
iteration : 46
train acc:  0.546875
train loss:  0.65535569190979
train gradient:  0.32369287403764935
iteration : 47
train acc:  0.625
train loss:  0.6974973678588867
train gradient:  0.5970015910385225
iteration : 48
train acc:  0.578125
train loss:  0.6660540103912354
train gradient:  0.39864014104617485
iteration : 49
train acc:  0.59375
train loss:  0.6832166910171509
train gradient:  0.5298208798639943
iteration : 50
train acc:  0.5546875
train loss:  0.6865759491920471
train gradient:  0.7821942626590701
iteration : 51
train acc:  0.546875
train loss:  0.7283897399902344
train gradient:  0.8066089198505566
iteration : 52
train acc:  0.609375
train loss:  0.6531813144683838
train gradient:  0.6804070499408862
iteration : 53
train acc:  0.59375
train loss:  0.6859296560287476
train gradient:  0.5186058039170831
iteration : 54
train acc:  0.640625
train loss:  0.6314238905906677
train gradient:  0.3244284933320399
iteration : 55
train acc:  0.6484375
train loss:  0.6578103303909302
train gradient:  0.5811643318956875
iteration : 56
train acc:  0.640625
train loss:  0.6327268481254578
train gradient:  0.2672253449166867
iteration : 57
train acc:  0.5625
train loss:  0.6677255630493164
train gradient:  0.6947407448153778
iteration : 58
train acc:  0.6328125
train loss:  0.6772041320800781
train gradient:  0.3627874596738038
iteration : 59
train acc:  0.625
train loss:  0.6507573127746582
train gradient:  0.4768511905586388
iteration : 60
train acc:  0.5546875
train loss:  0.6856753826141357
train gradient:  0.3632941123125699
iteration : 61
train acc:  0.6640625
train loss:  0.6250791549682617
train gradient:  0.37982486690527295
iteration : 62
train acc:  0.6640625
train loss:  0.6555087566375732
train gradient:  0.8500954620510208
iteration : 63
train acc:  0.671875
train loss:  0.6483721733093262
train gradient:  0.48675049512833934
iteration : 64
train acc:  0.640625
train loss:  0.6406497955322266
train gradient:  0.22614095035883416
iteration : 65
train acc:  0.6484375
train loss:  0.6332110166549683
train gradient:  0.646183145437258
iteration : 66
train acc:  0.625
train loss:  0.6471066474914551
train gradient:  0.508462319862395
iteration : 67
train acc:  0.578125
train loss:  0.6580298542976379
train gradient:  0.28574271509031984
iteration : 68
train acc:  0.5625
train loss:  0.6738590002059937
train gradient:  0.4791994250189364
iteration : 69
train acc:  0.6484375
train loss:  0.6278690695762634
train gradient:  0.2157278488357232
iteration : 70
train acc:  0.6953125
train loss:  0.6079093217849731
train gradient:  0.4801708561313738
iteration : 71
train acc:  0.625
train loss:  0.6401216983795166
train gradient:  0.4750301249212267
iteration : 72
train acc:  0.6171875
train loss:  0.6410875916481018
train gradient:  0.3907101534337787
iteration : 73
train acc:  0.6015625
train loss:  0.6336480379104614
train gradient:  0.4725409516943798
iteration : 74
train acc:  0.625
train loss:  0.6284093856811523
train gradient:  0.29360991502778794
iteration : 75
train acc:  0.6484375
train loss:  0.6368457078933716
train gradient:  0.25704093181962934
iteration : 76
train acc:  0.6796875
train loss:  0.6720373034477234
train gradient:  1.1280092838233473
iteration : 77
train acc:  0.5703125
train loss:  0.6731447577476501
train gradient:  0.4587243080782045
iteration : 78
train acc:  0.546875
train loss:  0.7278026342391968
train gradient:  0.9734042906623307
iteration : 79
train acc:  0.5703125
train loss:  0.6737642288208008
train gradient:  0.32784965618652806
iteration : 80
train acc:  0.6328125
train loss:  0.6320651769638062
train gradient:  0.23082890999439803
iteration : 81
train acc:  0.59375
train loss:  0.6506979465484619
train gradient:  0.4844532345762929
iteration : 82
train acc:  0.625
train loss:  0.6200288534164429
train gradient:  0.48494678078726716
iteration : 83
train acc:  0.6875
train loss:  0.5969588756561279
train gradient:  0.3179866262658149
iteration : 84
train acc:  0.5859375
train loss:  0.6347001791000366
train gradient:  0.40856977873543887
iteration : 85
train acc:  0.6640625
train loss:  0.6208187341690063
train gradient:  0.4872143530964481
iteration : 86
train acc:  0.6171875
train loss:  0.6307730674743652
train gradient:  0.40104372427688156
iteration : 87
train acc:  0.5859375
train loss:  0.664313793182373
train gradient:  0.5117238147049661
iteration : 88
train acc:  0.65625
train loss:  0.6580440998077393
train gradient:  1.236991929528839
iteration : 89
train acc:  0.59375
train loss:  0.6435511112213135
train gradient:  0.48546389896649333
iteration : 90
train acc:  0.609375
train loss:  0.6303039789199829
train gradient:  0.212467144285028
iteration : 91
train acc:  0.6171875
train loss:  0.6273988485336304
train gradient:  0.2193537677825464
iteration : 92
train acc:  0.625
train loss:  0.633965790271759
train gradient:  0.29229691229470073
iteration : 93
train acc:  0.6328125
train loss:  0.6394411325454712
train gradient:  0.27942836451254954
iteration : 94
train acc:  0.625
train loss:  0.6445557475090027
train gradient:  0.36138409818169626
iteration : 95
train acc:  0.5546875
train loss:  0.7096719741821289
train gradient:  0.7304600974806095
iteration : 96
train acc:  0.671875
train loss:  0.5911444425582886
train gradient:  0.2789455191233655
iteration : 97
train acc:  0.609375
train loss:  0.6221252083778381
train gradient:  0.6948927632428741
iteration : 98
train acc:  0.609375
train loss:  0.6451377868652344
train gradient:  0.7889580050413078
iteration : 99
train acc:  0.6875
train loss:  0.5962888598442078
train gradient:  0.4727001413121511
iteration : 100
train acc:  0.6484375
train loss:  0.6212784647941589
train gradient:  0.38995845177450905
iteration : 101
train acc:  0.5859375
train loss:  0.6454991698265076
train gradient:  0.41385758813047263
iteration : 102
train acc:  0.640625
train loss:  0.6161784529685974
train gradient:  0.3597236483748891
iteration : 103
train acc:  0.609375
train loss:  0.6603118777275085
train gradient:  0.7022344857028017
iteration : 104
train acc:  0.6953125
train loss:  0.5885612368583679
train gradient:  0.5236332497224402
iteration : 105
train acc:  0.6640625
train loss:  0.5946618914604187
train gradient:  0.21312788730076443
iteration : 106
train acc:  0.6171875
train loss:  0.6284797191619873
train gradient:  0.3500705318801374
iteration : 107
train acc:  0.65625
train loss:  0.6443399786949158
train gradient:  4.308502197558256
iteration : 108
train acc:  0.6328125
train loss:  0.672762930393219
train gradient:  0.8472508811809167
iteration : 109
train acc:  0.640625
train loss:  0.6353403329849243
train gradient:  0.3317529898711343
iteration : 110
train acc:  0.65625
train loss:  0.5908125638961792
train gradient:  0.36553072622181365
iteration : 111
train acc:  0.6484375
train loss:  0.6019852161407471
train gradient:  0.48348617117352893
iteration : 112
train acc:  0.6171875
train loss:  0.6526217460632324
train gradient:  0.6228329247615125
iteration : 113
train acc:  0.6171875
train loss:  0.6274371147155762
train gradient:  0.46541294848541104
iteration : 114
train acc:  0.625
train loss:  0.6686463356018066
train gradient:  0.703011737773894
iteration : 115
train acc:  0.609375
train loss:  0.6549026966094971
train gradient:  0.4675845279816749
iteration : 116
train acc:  0.6015625
train loss:  0.663402259349823
train gradient:  0.6124901235945854
iteration : 117
train acc:  0.6875
train loss:  0.600521445274353
train gradient:  0.7011160822259632
iteration : 118
train acc:  0.6953125
train loss:  0.5994168519973755
train gradient:  0.5518129715316293
iteration : 119
train acc:  0.6484375
train loss:  0.6087040901184082
train gradient:  0.3705219387571736
iteration : 120
train acc:  0.59375
train loss:  0.6680761575698853
train gradient:  0.6210657111261334
iteration : 121
train acc:  0.6796875
train loss:  0.6080192923545837
train gradient:  0.3139273114875036
iteration : 122
train acc:  0.6171875
train loss:  0.5961502194404602
train gradient:  0.4937912684610559
iteration : 123
train acc:  0.609375
train loss:  0.6185576915740967
train gradient:  0.4371089941496199
iteration : 124
train acc:  0.6171875
train loss:  0.6612709164619446
train gradient:  0.419726856349127
iteration : 125
train acc:  0.65625
train loss:  0.5965330600738525
train gradient:  0.28219894221996983
iteration : 126
train acc:  0.6015625
train loss:  0.630692720413208
train gradient:  0.33115381471842986
iteration : 127
train acc:  0.5546875
train loss:  0.7279969453811646
train gradient:  1.744103485033803
iteration : 128
train acc:  0.578125
train loss:  0.6634760499000549
train gradient:  0.5331693906422896
iteration : 129
train acc:  0.6484375
train loss:  0.606126070022583
train gradient:  0.3539150877926884
iteration : 130
train acc:  0.578125
train loss:  0.6995607018470764
train gradient:  0.6094903325387584
iteration : 131
train acc:  0.6640625
train loss:  0.6107528209686279
train gradient:  0.6748693531397267
iteration : 132
train acc:  0.609375
train loss:  0.6521640419960022
train gradient:  0.30265602734810865
iteration : 133
train acc:  0.5859375
train loss:  0.6387193202972412
train gradient:  0.5053791928812144
iteration : 134
train acc:  0.6328125
train loss:  0.6244444847106934
train gradient:  0.4057913773020073
iteration : 135
train acc:  0.5859375
train loss:  0.6182054281234741
train gradient:  0.38833030919230616
iteration : 136
train acc:  0.65625
train loss:  0.6326192617416382
train gradient:  0.29724440356740056
iteration : 137
train acc:  0.609375
train loss:  0.6650241613388062
train gradient:  0.9307275367553223
iteration : 138
train acc:  0.6953125
train loss:  0.5817873477935791
train gradient:  0.3942626826854639
iteration : 139
train acc:  0.609375
train loss:  0.6468153595924377
train gradient:  0.45818652829193546
iteration : 140
train acc:  0.6796875
train loss:  0.6014015674591064
train gradient:  0.5331926933550792
iteration : 141
train acc:  0.6875
train loss:  0.610878586769104
train gradient:  0.6564995250889478
iteration : 142
train acc:  0.71875
train loss:  0.5723373889923096
train gradient:  0.29350686850940155
iteration : 143
train acc:  0.671875
train loss:  0.6018197536468506
train gradient:  0.4610636694170155
iteration : 144
train acc:  0.6640625
train loss:  0.6221539974212646
train gradient:  0.25569585766891184
iteration : 145
train acc:  0.6640625
train loss:  0.6582707166671753
train gradient:  0.679172650270685
iteration : 146
train acc:  0.65625
train loss:  0.5870532393455505
train gradient:  0.41604159189134465
iteration : 147
train acc:  0.6875
train loss:  0.6112608313560486
train gradient:  0.6820643594350114
iteration : 148
train acc:  0.703125
train loss:  0.5836557745933533
train gradient:  0.48749577017730444
iteration : 149
train acc:  0.671875
train loss:  0.5866471529006958
train gradient:  0.3037252176865821
iteration : 150
train acc:  0.65625
train loss:  0.5913420915603638
train gradient:  0.5073803613544918
iteration : 151
train acc:  0.6796875
train loss:  0.6268081665039062
train gradient:  0.8131788891436461
iteration : 152
train acc:  0.65625
train loss:  0.6007332801818848
train gradient:  0.45828642171819056
iteration : 153
train acc:  0.5546875
train loss:  0.6551064252853394
train gradient:  0.4828716577168222
iteration : 154
train acc:  0.65625
train loss:  0.6104545593261719
train gradient:  0.48629529432095037
iteration : 155
train acc:  0.71875
train loss:  0.5530155897140503
train gradient:  0.24881530763472967
iteration : 156
train acc:  0.625
train loss:  0.6200472712516785
train gradient:  0.3379092387530988
iteration : 157
train acc:  0.6171875
train loss:  0.6184924244880676
train gradient:  0.26983921136881767
iteration : 158
train acc:  0.5859375
train loss:  0.6776797771453857
train gradient:  0.4648200128846012
iteration : 159
train acc:  0.6484375
train loss:  0.5979540348052979
train gradient:  0.36515074796954833
iteration : 160
train acc:  0.6484375
train loss:  0.6424014568328857
train gradient:  0.7414871238136118
iteration : 161
train acc:  0.6796875
train loss:  0.6178690195083618
train gradient:  0.7470197394329403
iteration : 162
train acc:  0.65625
train loss:  0.6146553158760071
train gradient:  0.3800041875804982
iteration : 163
train acc:  0.6640625
train loss:  0.5818474292755127
train gradient:  0.3902105370373148
iteration : 164
train acc:  0.625
train loss:  0.6564942598342896
train gradient:  0.44064122721493637
iteration : 165
train acc:  0.65625
train loss:  0.6269711852073669
train gradient:  0.5723328350852209
iteration : 166
train acc:  0.671875
train loss:  0.6072190999984741
train gradient:  0.381682693844592
iteration : 167
train acc:  0.703125
train loss:  0.599929690361023
train gradient:  0.6058625449900704
iteration : 168
train acc:  0.6484375
train loss:  0.6255061030387878
train gradient:  0.5047111603195852
iteration : 169
train acc:  0.6484375
train loss:  0.6076277494430542
train gradient:  0.47252632042780496
iteration : 170
train acc:  0.609375
train loss:  0.6423740386962891
train gradient:  0.4314936056650317
iteration : 171
train acc:  0.6796875
train loss:  0.6111981868743896
train gradient:  0.3848427184989876
iteration : 172
train acc:  0.6953125
train loss:  0.574923038482666
train gradient:  0.31413148481468994
iteration : 173
train acc:  0.640625
train loss:  0.5964522957801819
train gradient:  0.6440736380789629
iteration : 174
train acc:  0.625
train loss:  0.6116495132446289
train gradient:  0.6576620814112977
iteration : 175
train acc:  0.6875
train loss:  0.5889984369277954
train gradient:  0.22465414558238692
iteration : 176
train acc:  0.6953125
train loss:  0.6173171997070312
train gradient:  0.3138026288486233
iteration : 177
train acc:  0.6484375
train loss:  0.6260533332824707
train gradient:  0.5594352996126779
iteration : 178
train acc:  0.7109375
train loss:  0.5812050104141235
train gradient:  0.4299443642873802
iteration : 179
train acc:  0.6171875
train loss:  0.6226437091827393
train gradient:  0.2810149755197393
iteration : 180
train acc:  0.6328125
train loss:  0.6449025869369507
train gradient:  0.4879693485055195
iteration : 181
train acc:  0.625
train loss:  0.61236572265625
train gradient:  0.48882477232833876
iteration : 182
train acc:  0.734375
train loss:  0.5765645503997803
train gradient:  0.6980076227924061
iteration : 183
train acc:  0.625
train loss:  0.6225346326828003
train gradient:  0.4781204049550076
iteration : 184
train acc:  0.671875
train loss:  0.5771799683570862
train gradient:  0.5530077493443482
iteration : 185
train acc:  0.6484375
train loss:  0.6422408819198608
train gradient:  0.6779537602309369
iteration : 186
train acc:  0.5703125
train loss:  0.6772136688232422
train gradient:  1.5295409873747308
iteration : 187
train acc:  0.5703125
train loss:  0.6688244342803955
train gradient:  0.881260071885894
iteration : 188
train acc:  0.671875
train loss:  0.5864439606666565
train gradient:  0.29451042834555163
iteration : 189
train acc:  0.625
train loss:  0.6057288646697998
train gradient:  0.5436969992475353
iteration : 190
train acc:  0.703125
train loss:  0.5761972665786743
train gradient:  0.41490701780832595
iteration : 191
train acc:  0.65625
train loss:  0.6394758820533752
train gradient:  0.6199371758154384
iteration : 192
train acc:  0.6484375
train loss:  0.5819718837738037
train gradient:  0.3918000209998053
iteration : 193
train acc:  0.6484375
train loss:  0.6289257407188416
train gradient:  0.4142268623312294
iteration : 194
train acc:  0.6328125
train loss:  0.6236869692802429
train gradient:  0.44664291094114345
iteration : 195
train acc:  0.765625
train loss:  0.5289896726608276
train gradient:  0.3582348285203991
iteration : 196
train acc:  0.640625
train loss:  0.6342887878417969
train gradient:  0.4050718312610894
iteration : 197
train acc:  0.6328125
train loss:  0.5865335464477539
train gradient:  0.29267592175335
iteration : 198
train acc:  0.6796875
train loss:  0.5985689163208008
train gradient:  0.4698993442758208
iteration : 199
train acc:  0.7109375
train loss:  0.5869626402854919
train gradient:  0.4917781065347237
iteration : 200
train acc:  0.7265625
train loss:  0.5321479439735413
train gradient:  0.3427365990226972
iteration : 201
train acc:  0.625
train loss:  0.5769960880279541
train gradient:  0.41173560606154386
iteration : 202
train acc:  0.671875
train loss:  0.6329923868179321
train gradient:  0.5976326448037704
iteration : 203
train acc:  0.625
train loss:  0.6115789413452148
train gradient:  0.30329768816362884
iteration : 204
train acc:  0.7109375
train loss:  0.580019474029541
train gradient:  0.3601189569212856
iteration : 205
train acc:  0.6328125
train loss:  0.6236425042152405
train gradient:  0.931301600054006
iteration : 206
train acc:  0.625
train loss:  0.655977189540863
train gradient:  0.8615418693662014
iteration : 207
train acc:  0.65625
train loss:  0.5626122951507568
train gradient:  0.4554226578016266
iteration : 208
train acc:  0.6875
train loss:  0.5947625041007996
train gradient:  0.3351809005688749
iteration : 209
train acc:  0.6640625
train loss:  0.5935734510421753
train gradient:  0.3609042986093207
iteration : 210
train acc:  0.6328125
train loss:  0.6259270906448364
train gradient:  0.7105604808126239
iteration : 211
train acc:  0.7890625
train loss:  0.5033820867538452
train gradient:  0.362677013049945
iteration : 212
train acc:  0.6953125
train loss:  0.6119769811630249
train gradient:  0.717978992733286
iteration : 213
train acc:  0.609375
train loss:  0.6641511917114258
train gradient:  0.5455785383981555
iteration : 214
train acc:  0.671875
train loss:  0.5866851210594177
train gradient:  0.49337602757761045
iteration : 215
train acc:  0.703125
train loss:  0.581853985786438
train gradient:  0.4168964931526933
iteration : 216
train acc:  0.7109375
train loss:  0.5680699348449707
train gradient:  0.2792811057072376
iteration : 217
train acc:  0.6640625
train loss:  0.5881092548370361
train gradient:  0.32872851016046023
iteration : 218
train acc:  0.6875
train loss:  0.5938469171524048
train gradient:  0.5451209967973067
iteration : 219
train acc:  0.671875
train loss:  0.5900899767875671
train gradient:  0.47379608016583863
iteration : 220
train acc:  0.6484375
train loss:  0.6420644521713257
train gradient:  0.44665067918728896
iteration : 221
train acc:  0.6640625
train loss:  0.6033648252487183
train gradient:  0.4720648192935311
iteration : 222
train acc:  0.765625
train loss:  0.5575223565101624
train gradient:  0.39667961767961124
iteration : 223
train acc:  0.6640625
train loss:  0.620797336101532
train gradient:  0.6206023044932724
iteration : 224
train acc:  0.75
train loss:  0.5616384744644165
train gradient:  1.5879340806135704
iteration : 225
train acc:  0.640625
train loss:  0.6259344816207886
train gradient:  0.5506465840231184
iteration : 226
train acc:  0.703125
train loss:  0.5954947471618652
train gradient:  0.5606909992387826
iteration : 227
train acc:  0.640625
train loss:  0.6065022945404053
train gradient:  0.4524178029843482
iteration : 228
train acc:  0.6640625
train loss:  0.5658310055732727
train gradient:  0.23986796796993412
iteration : 229
train acc:  0.765625
train loss:  0.5513606071472168
train gradient:  0.6870307443344484
iteration : 230
train acc:  0.609375
train loss:  0.6387261748313904
train gradient:  0.6752129242267075
iteration : 231
train acc:  0.7109375
train loss:  0.5717493295669556
train gradient:  0.41798852139634374
iteration : 232
train acc:  0.703125
train loss:  0.6222385764122009
train gradient:  0.6024417092255939
iteration : 233
train acc:  0.671875
train loss:  0.5889703035354614
train gradient:  0.4552216116012653
iteration : 234
train acc:  0.6875
train loss:  0.5776807069778442
train gradient:  0.3180652308592531
iteration : 235
train acc:  0.6328125
train loss:  0.5844862461090088
train gradient:  0.5909660553077589
iteration : 236
train acc:  0.671875
train loss:  0.5885789394378662
train gradient:  0.49429533043723173
iteration : 237
train acc:  0.6953125
train loss:  0.5498736500740051
train gradient:  0.3850958310878316
iteration : 238
train acc:  0.6328125
train loss:  0.6329171657562256
train gradient:  0.4579891058479502
iteration : 239
train acc:  0.6953125
train loss:  0.6132664680480957
train gradient:  0.37139706732513456
iteration : 240
train acc:  0.703125
train loss:  0.5793831944465637
train gradient:  0.5548497130426966
iteration : 241
train acc:  0.6328125
train loss:  0.6302013397216797
train gradient:  0.6565378566661437
iteration : 242
train acc:  0.75
train loss:  0.5309855937957764
train gradient:  0.4929686445460503
iteration : 243
train acc:  0.703125
train loss:  0.5780988931655884
train gradient:  0.4378501231223026
iteration : 244
train acc:  0.6796875
train loss:  0.6057287454605103
train gradient:  0.5298018865140565
iteration : 245
train acc:  0.7421875
train loss:  0.5516902804374695
train gradient:  0.4777722134353603
iteration : 246
train acc:  0.6484375
train loss:  0.6108577251434326
train gradient:  0.5473511273350835
iteration : 247
train acc:  0.6640625
train loss:  0.6072661876678467
train gradient:  0.6417663494422725
iteration : 248
train acc:  0.6953125
train loss:  0.592354416847229
train gradient:  0.6457389339830177
iteration : 249
train acc:  0.75
train loss:  0.5285519957542419
train gradient:  0.42551356368252985
iteration : 250
train acc:  0.6640625
train loss:  0.5728383660316467
train gradient:  0.344381774604485
iteration : 251
train acc:  0.6796875
train loss:  0.575065016746521
train gradient:  0.579190329960557
iteration : 252
train acc:  0.7421875
train loss:  0.5514469146728516
train gradient:  0.6008108938959564
iteration : 253
train acc:  0.7265625
train loss:  0.5724503397941589
train gradient:  0.5730475072936443
iteration : 254
train acc:  0.7265625
train loss:  0.5429417490959167
train gradient:  0.6263178590500785
iteration : 255
train acc:  0.6171875
train loss:  0.6439300775527954
train gradient:  0.8410386966642274
iteration : 256
train acc:  0.7265625
train loss:  0.5747162699699402
train gradient:  0.5768352460739159
iteration : 257
train acc:  0.7109375
train loss:  0.5395870208740234
train gradient:  0.5028423342500081
iteration : 258
train acc:  0.6640625
train loss:  0.5544751882553101
train gradient:  0.521759259488731
iteration : 259
train acc:  0.7109375
train loss:  0.5323527455329895
train gradient:  0.4079629952560091
iteration : 260
train acc:  0.6328125
train loss:  0.6179214715957642
train gradient:  0.6532982136829067
iteration : 261
train acc:  0.671875
train loss:  0.6270910501480103
train gradient:  0.6058290201128054
iteration : 262
train acc:  0.6484375
train loss:  0.6550962924957275
train gradient:  0.8878156958660067
iteration : 263
train acc:  0.6796875
train loss:  0.5486370325088501
train gradient:  0.4735227288662725
iteration : 264
train acc:  0.671875
train loss:  0.570793628692627
train gradient:  0.5271235825910386
iteration : 265
train acc:  0.6875
train loss:  0.5945641994476318
train gradient:  0.5196315877399817
iteration : 266
train acc:  0.671875
train loss:  0.5831433534622192
train gradient:  0.47320493126596197
iteration : 267
train acc:  0.7265625
train loss:  0.5591983199119568
train gradient:  0.5604842251503067
iteration : 268
train acc:  0.6796875
train loss:  0.5651471614837646
train gradient:  0.6123363760755897
iteration : 269
train acc:  0.703125
train loss:  0.5685662031173706
train gradient:  0.7392466804446661
iteration : 270
train acc:  0.7109375
train loss:  0.5550594925880432
train gradient:  0.6463471031492096
iteration : 271
train acc:  0.6953125
train loss:  0.5513186454772949
train gradient:  0.5306268358135644
iteration : 272
train acc:  0.71875
train loss:  0.5576023459434509
train gradient:  0.5152340479356203
iteration : 273
train acc:  0.7421875
train loss:  0.5642925500869751
train gradient:  0.7458416426190531
iteration : 274
train acc:  0.6953125
train loss:  0.61469966173172
train gradient:  0.7194944624341184
iteration : 275
train acc:  0.7421875
train loss:  0.5301469564437866
train gradient:  0.5631212906200112
iteration : 276
train acc:  0.703125
train loss:  0.5682262778282166
train gradient:  0.6292706890446769
iteration : 277
train acc:  0.6171875
train loss:  0.6853470206260681
train gradient:  1.065396076116824
iteration : 278
train acc:  0.7265625
train loss:  0.5687078237533569
train gradient:  0.5346127821553632
iteration : 279
train acc:  0.65625
train loss:  0.589231014251709
train gradient:  1.0064337514504504
iteration : 280
train acc:  0.640625
train loss:  0.5915690660476685
train gradient:  0.766117147383256
iteration : 281
train acc:  0.7421875
train loss:  0.5219782590866089
train gradient:  0.40451096364184674
iteration : 282
train acc:  0.71875
train loss:  0.5621947050094604
train gradient:  0.7316569101543209
iteration : 283
train acc:  0.7421875
train loss:  0.5410691499710083
train gradient:  0.36465378084880656
iteration : 284
train acc:  0.59375
train loss:  0.6340175867080688
train gradient:  0.4507035793890992
iteration : 285
train acc:  0.7265625
train loss:  0.5390616059303284
train gradient:  0.29657585210878623
iteration : 286
train acc:  0.609375
train loss:  0.5844389200210571
train gradient:  0.5288993568125646
iteration : 287
train acc:  0.6796875
train loss:  0.5721385478973389
train gradient:  0.49852667987596005
iteration : 288
train acc:  0.7421875
train loss:  0.5759311318397522
train gradient:  0.5807386420192319
iteration : 289
train acc:  0.7109375
train loss:  0.558391809463501
train gradient:  0.4602297361604018
iteration : 290
train acc:  0.6484375
train loss:  0.619210958480835
train gradient:  0.5230795440137386
iteration : 291
train acc:  0.703125
train loss:  0.5075473785400391
train gradient:  0.4784221104817507
iteration : 292
train acc:  0.734375
train loss:  0.5010985732078552
train gradient:  0.3577670158117381
iteration : 293
train acc:  0.671875
train loss:  0.5875319242477417
train gradient:  1.4157387131418766
iteration : 294
train acc:  0.6484375
train loss:  0.5993804931640625
train gradient:  0.6486587505438458
iteration : 295
train acc:  0.7578125
train loss:  0.5004522800445557
train gradient:  0.26213485841235257
iteration : 296
train acc:  0.75
train loss:  0.5997331142425537
train gradient:  1.0353428671061213
iteration : 297
train acc:  0.6953125
train loss:  0.5842592716217041
train gradient:  0.4424631071479085
iteration : 298
train acc:  0.765625
train loss:  0.4707561433315277
train gradient:  0.3973804970574427
iteration : 299
train acc:  0.6953125
train loss:  0.6256988048553467
train gradient:  1.1621145451600334
iteration : 300
train acc:  0.7109375
train loss:  0.5610696077346802
train gradient:  0.7027634237225209
iteration : 301
train acc:  0.734375
train loss:  0.5281341075897217
train gradient:  0.5314624260413396
iteration : 302
train acc:  0.703125
train loss:  0.6402068138122559
train gradient:  0.6721716541607206
iteration : 303
train acc:  0.671875
train loss:  0.5930923819541931
train gradient:  0.5443513478355194
iteration : 304
train acc:  0.71875
train loss:  0.5323134660720825
train gradient:  0.5736934026583683
iteration : 305
train acc:  0.7421875
train loss:  0.5664721727371216
train gradient:  0.3987274457763198
iteration : 306
train acc:  0.671875
train loss:  0.6380907893180847
train gradient:  0.9692335813325668
iteration : 307
train acc:  0.7265625
train loss:  0.5250533819198608
train gradient:  0.5160687138426923
iteration : 308
train acc:  0.71875
train loss:  0.5551565885543823
train gradient:  0.4234728101645963
iteration : 309
train acc:  0.734375
train loss:  0.527880847454071
train gradient:  0.3286404007217576
iteration : 310
train acc:  0.8203125
train loss:  0.45676112174987793
train gradient:  0.3605958026767018
iteration : 311
train acc:  0.6875
train loss:  0.6058839559555054
train gradient:  0.397681057970759
iteration : 312
train acc:  0.765625
train loss:  0.5025953054428101
train gradient:  0.3218871520004496
iteration : 313
train acc:  0.796875
train loss:  0.47589004039764404
train gradient:  0.40194089518744447
iteration : 314
train acc:  0.6328125
train loss:  0.5913141965866089
train gradient:  0.3878988682857821
iteration : 315
train acc:  0.6328125
train loss:  0.6159701347351074
train gradient:  0.5775254992743666
iteration : 316
train acc:  0.6875
train loss:  0.5618348121643066
train gradient:  0.42197585544816196
iteration : 317
train acc:  0.671875
train loss:  0.5890888571739197
train gradient:  0.40190413519913204
iteration : 318
train acc:  0.7265625
train loss:  0.5402746200561523
train gradient:  0.30452282693375543
iteration : 319
train acc:  0.7421875
train loss:  0.5476701259613037
train gradient:  0.4218254680651139
iteration : 320
train acc:  0.734375
train loss:  0.5209113359451294
train gradient:  0.3552356513745906
iteration : 321
train acc:  0.7265625
train loss:  0.5481451153755188
train gradient:  0.41179791144436256
iteration : 322
train acc:  0.703125
train loss:  0.5885601043701172
train gradient:  0.5002837785164521
iteration : 323
train acc:  0.7578125
train loss:  0.5604349970817566
train gradient:  0.4928162224687195
iteration : 324
train acc:  0.7265625
train loss:  0.5441733002662659
train gradient:  0.40922888465461027
iteration : 325
train acc:  0.7734375
train loss:  0.545203447341919
train gradient:  0.28391835008420535
iteration : 326
train acc:  0.6796875
train loss:  0.637322187423706
train gradient:  0.6516058494311374
iteration : 327
train acc:  0.65625
train loss:  0.5831843614578247
train gradient:  0.40843122207535865
iteration : 328
train acc:  0.59375
train loss:  0.6179357767105103
train gradient:  0.7380118890121109
iteration : 329
train acc:  0.7421875
train loss:  0.5302249193191528
train gradient:  0.3939660153766169
iteration : 330
train acc:  0.6953125
train loss:  0.5552222728729248
train gradient:  0.41931996625482454
iteration : 331
train acc:  0.734375
train loss:  0.5549845695495605
train gradient:  0.31627548123678567
iteration : 332
train acc:  0.6875
train loss:  0.5600656270980835
train gradient:  0.5408241642913305
iteration : 333
train acc:  0.7109375
train loss:  0.5692602396011353
train gradient:  0.44869249174836023
iteration : 334
train acc:  0.7109375
train loss:  0.5603917837142944
train gradient:  0.4801210431988949
iteration : 335
train acc:  0.7265625
train loss:  0.5588847994804382
train gradient:  0.2677766474033729
iteration : 336
train acc:  0.734375
train loss:  0.5245746374130249
train gradient:  0.3432634320667432
iteration : 337
train acc:  0.6015625
train loss:  0.623863160610199
train gradient:  0.5984396272933895
iteration : 338
train acc:  0.6640625
train loss:  0.5880402326583862
train gradient:  0.6133795893876827
iteration : 339
train acc:  0.6640625
train loss:  0.5920940637588501
train gradient:  0.3137237570372382
iteration : 340
train acc:  0.6640625
train loss:  0.5676068663597107
train gradient:  0.6156654472892309
iteration : 341
train acc:  0.6875
train loss:  0.606838047504425
train gradient:  1.122225983472449
iteration : 342
train acc:  0.6640625
train loss:  0.5643840432167053
train gradient:  0.44068205952355555
iteration : 343
train acc:  0.7578125
train loss:  0.5344228744506836
train gradient:  0.3406522920591435
iteration : 344
train acc:  0.75
train loss:  0.5075274705886841
train gradient:  0.3256749258495898
iteration : 345
train acc:  0.7109375
train loss:  0.5775696039199829
train gradient:  0.6786520867551554
iteration : 346
train acc:  0.6328125
train loss:  0.5906040668487549
train gradient:  0.5032855244316237
iteration : 347
train acc:  0.6875
train loss:  0.5195404291152954
train gradient:  0.40266475806193996
iteration : 348
train acc:  0.65625
train loss:  0.5712980031967163
train gradient:  0.4808393238959216
iteration : 349
train acc:  0.7109375
train loss:  0.546962320804596
train gradient:  0.6754217703807901
iteration : 350
train acc:  0.7421875
train loss:  0.5336605310440063
train gradient:  0.6253556890289236
iteration : 351
train acc:  0.6875
train loss:  0.5481563806533813
train gradient:  0.44594983279310196
iteration : 352
train acc:  0.734375
train loss:  0.5303976535797119
train gradient:  0.41065202636140524
iteration : 353
train acc:  0.75
train loss:  0.5253586769104004
train gradient:  0.34340404199585367
iteration : 354
train acc:  0.7109375
train loss:  0.5613515973091125
train gradient:  0.4512283177906388
iteration : 355
train acc:  0.734375
train loss:  0.5585194826126099
train gradient:  0.4689749196235641
iteration : 356
train acc:  0.7421875
train loss:  0.5134310722351074
train gradient:  0.5863295270006953
iteration : 357
train acc:  0.7421875
train loss:  0.5165578126907349
train gradient:  0.591457850085613
iteration : 358
train acc:  0.734375
train loss:  0.5250915884971619
train gradient:  0.4483407363830185
iteration : 359
train acc:  0.7421875
train loss:  0.5177675485610962
train gradient:  0.5217175105138111
iteration : 360
train acc:  0.7265625
train loss:  0.5448242425918579
train gradient:  0.45308459890945024
iteration : 361
train acc:  0.71875
train loss:  0.5398870706558228
train gradient:  0.4548592125689138
iteration : 362
train acc:  0.734375
train loss:  0.5307356119155884
train gradient:  0.6065374086125948
iteration : 363
train acc:  0.703125
train loss:  0.5515422821044922
train gradient:  0.4909871118812072
iteration : 364
train acc:  0.6875
train loss:  0.585326611995697
train gradient:  0.5561505947065155
iteration : 365
train acc:  0.71875
train loss:  0.5818798542022705
train gradient:  0.8728490940233453
iteration : 366
train acc:  0.6640625
train loss:  0.594692051410675
train gradient:  0.43611796935730546
iteration : 367
train acc:  0.7578125
train loss:  0.5316869020462036
train gradient:  0.42668165394595287
iteration : 368
train acc:  0.7109375
train loss:  0.5329979062080383
train gradient:  0.6081520622648936
iteration : 369
train acc:  0.6875
train loss:  0.5811205506324768
train gradient:  0.5134777414188454
iteration : 370
train acc:  0.7890625
train loss:  0.4759620428085327
train gradient:  0.5566669534324025
iteration : 371
train acc:  0.6875
train loss:  0.5744908452033997
train gradient:  0.5917663229758701
iteration : 372
train acc:  0.65625
train loss:  0.5954549908638
train gradient:  0.6166551668855322
iteration : 373
train acc:  0.7265625
train loss:  0.5293415784835815
train gradient:  0.4838467700250652
iteration : 374
train acc:  0.7109375
train loss:  0.5653008222579956
train gradient:  0.6437148799039863
iteration : 375
train acc:  0.6796875
train loss:  0.5677266120910645
train gradient:  0.749679126904125
iteration : 376
train acc:  0.71875
train loss:  0.5470989346504211
train gradient:  0.6793763352617274
iteration : 377
train acc:  0.7265625
train loss:  0.584735095500946
train gradient:  0.6350800770583674
iteration : 378
train acc:  0.65625
train loss:  0.5771994590759277
train gradient:  0.6542198853819545
iteration : 379
train acc:  0.6796875
train loss:  0.5786429643630981
train gradient:  0.627999752733855
iteration : 380
train acc:  0.7109375
train loss:  0.5057188272476196
train gradient:  0.34132627293820417
iteration : 381
train acc:  0.671875
train loss:  0.5594894289970398
train gradient:  0.7690730957537146
iteration : 382
train acc:  0.796875
train loss:  0.525097131729126
train gradient:  0.5605990146638383
iteration : 383
train acc:  0.6484375
train loss:  0.600411057472229
train gradient:  0.6774218466150117
iteration : 384
train acc:  0.6953125
train loss:  0.5773345232009888
train gradient:  0.37141048963030615
iteration : 385
train acc:  0.7265625
train loss:  0.5035451650619507
train gradient:  0.3997806059042088
iteration : 386
train acc:  0.7265625
train loss:  0.5335396528244019
train gradient:  0.5170121947299753
iteration : 387
train acc:  0.6484375
train loss:  0.609278678894043
train gradient:  0.740745000959963
iteration : 388
train acc:  0.7109375
train loss:  0.5524308681488037
train gradient:  0.6159474538795081
iteration : 389
train acc:  0.75
train loss:  0.5236013531684875
train gradient:  0.4412558075924742
iteration : 390
train acc:  0.6484375
train loss:  0.663230299949646
train gradient:  0.8696801250102941
iteration : 391
train acc:  0.7265625
train loss:  0.5138145685195923
train gradient:  0.4390334122420439
iteration : 392
train acc:  0.6796875
train loss:  0.5503333806991577
train gradient:  3.3158298673483344
iteration : 393
train acc:  0.7578125
train loss:  0.4938686490058899
train gradient:  0.38745618544689564
iteration : 394
train acc:  0.6640625
train loss:  0.5693284869194031
train gradient:  0.6120532470138242
iteration : 395
train acc:  0.734375
train loss:  0.5585048794746399
train gradient:  0.4324242765044174
iteration : 396
train acc:  0.6484375
train loss:  0.6162359118461609
train gradient:  0.6351003958965815
iteration : 397
train acc:  0.734375
train loss:  0.5667070150375366
train gradient:  0.550912382864518
iteration : 398
train acc:  0.75
train loss:  0.5853708386421204
train gradient:  0.7418287189951343
iteration : 399
train acc:  0.71875
train loss:  0.5353740453720093
train gradient:  0.5110463541422865
iteration : 400
train acc:  0.7109375
train loss:  0.5362690687179565
train gradient:  0.6290093968232591
iteration : 401
train acc:  0.6484375
train loss:  0.6439251899719238
train gradient:  0.7426178150307191
iteration : 402
train acc:  0.7421875
train loss:  0.5506671071052551
train gradient:  0.47382920962206754
iteration : 403
train acc:  0.7890625
train loss:  0.5009344816207886
train gradient:  0.615285285235351
iteration : 404
train acc:  0.71875
train loss:  0.6042119264602661
train gradient:  0.6900373655995558
iteration : 405
train acc:  0.6953125
train loss:  0.5649778246879578
train gradient:  0.7392046787848251
iteration : 406
train acc:  0.6796875
train loss:  0.5351238250732422
train gradient:  0.5023519270324086
iteration : 407
train acc:  0.7109375
train loss:  0.5208614468574524
train gradient:  0.429732874195491
iteration : 408
train acc:  0.7265625
train loss:  0.5332393646240234
train gradient:  0.4497272988664507
iteration : 409
train acc:  0.75
train loss:  0.5198472738265991
train gradient:  0.4813625190389836
iteration : 410
train acc:  0.6953125
train loss:  0.5340793132781982
train gradient:  0.43560567023648245
iteration : 411
train acc:  0.71875
train loss:  0.5387027263641357
train gradient:  0.4566360744799545
iteration : 412
train acc:  0.6953125
train loss:  0.5369845628738403
train gradient:  0.5024891632857313
iteration : 413
train acc:  0.6796875
train loss:  0.5564938187599182
train gradient:  0.5253685725211494
iteration : 414
train acc:  0.7421875
train loss:  0.5713571310043335
train gradient:  0.6593927412831327
iteration : 415
train acc:  0.75
train loss:  0.5029441118240356
train gradient:  0.360119421411068
iteration : 416
train acc:  0.6875
train loss:  0.5300208926200867
train gradient:  0.5651442619405536
iteration : 417
train acc:  0.7265625
train loss:  0.5391915440559387
train gradient:  0.5795829203034515
iteration : 418
train acc:  0.7578125
train loss:  0.5054235458374023
train gradient:  0.3928685530389315
iteration : 419
train acc:  0.7421875
train loss:  0.5245518684387207
train gradient:  0.3666466365885835
iteration : 420
train acc:  0.671875
train loss:  0.6169135570526123
train gradient:  0.7999253263221978
iteration : 421
train acc:  0.7109375
train loss:  0.5768192410469055
train gradient:  0.5789239082418265
iteration : 422
train acc:  0.6953125
train loss:  0.5370243787765503
train gradient:  0.6773538170068389
iteration : 423
train acc:  0.6484375
train loss:  0.6331281065940857
train gradient:  0.6962564238254281
iteration : 424
train acc:  0.7421875
train loss:  0.49924203753471375
train gradient:  0.6180631834314796
iteration : 425
train acc:  0.71875
train loss:  0.5380343198776245
train gradient:  0.5035024076510268
iteration : 426
train acc:  0.734375
train loss:  0.525631844997406
train gradient:  0.5386750843003875
iteration : 427
train acc:  0.7421875
train loss:  0.5172172784805298
train gradient:  0.4983301004803183
iteration : 428
train acc:  0.65625
train loss:  0.651201069355011
train gradient:  0.9503812662215516
iteration : 429
train acc:  0.765625
train loss:  0.5272529125213623
train gradient:  0.5457096105463866
iteration : 430
train acc:  0.7578125
train loss:  0.5129948854446411
train gradient:  0.5771784429633807
iteration : 431
train acc:  0.7265625
train loss:  0.5527111887931824
train gradient:  0.5682742844427234
iteration : 432
train acc:  0.734375
train loss:  0.5428137183189392
train gradient:  0.4444698338952973
iteration : 433
train acc:  0.734375
train loss:  0.5251575112342834
train gradient:  0.4515079268993168
iteration : 434
train acc:  0.7109375
train loss:  0.5209825038909912
train gradient:  0.4120619891680321
iteration : 435
train acc:  0.7421875
train loss:  0.5641976594924927
train gradient:  0.9370544870400429
iteration : 436
train acc:  0.78125
train loss:  0.4633488655090332
train gradient:  0.40349300270502747
iteration : 437
train acc:  0.78125
train loss:  0.48180291056632996
train gradient:  0.40374639184142697
iteration : 438
train acc:  0.7109375
train loss:  0.5493613481521606
train gradient:  0.5840829795260596
iteration : 439
train acc:  0.7265625
train loss:  0.5554240942001343
train gradient:  0.5428414503186899
iteration : 440
train acc:  0.703125
train loss:  0.5151983499526978
train gradient:  0.48563936259840035
iteration : 441
train acc:  0.7109375
train loss:  0.5551387071609497
train gradient:  0.5465379969861889
iteration : 442
train acc:  0.75
train loss:  0.5357255339622498
train gradient:  0.7193439181387424
iteration : 443
train acc:  0.734375
train loss:  0.5601104497909546
train gradient:  0.5743513209200761
iteration : 444
train acc:  0.75
train loss:  0.5252031087875366
train gradient:  0.4852070174347803
iteration : 445
train acc:  0.71875
train loss:  0.5481967329978943
train gradient:  0.53809815422587
iteration : 446
train acc:  0.671875
train loss:  0.6319791674613953
train gradient:  0.7096080118288215
iteration : 447
train acc:  0.75
train loss:  0.5384413003921509
train gradient:  0.5754238059255637
iteration : 448
train acc:  0.6796875
train loss:  0.5558983087539673
train gradient:  0.4932131062926308
iteration : 449
train acc:  0.7734375
train loss:  0.49284273386001587
train gradient:  0.7282602923232131
iteration : 450
train acc:  0.6640625
train loss:  0.640849232673645
train gradient:  0.8367052035616112
iteration : 451
train acc:  0.7421875
train loss:  0.5262798070907593
train gradient:  0.6953871803894713
iteration : 452
train acc:  0.734375
train loss:  0.5389323234558105
train gradient:  0.6094489645859988
iteration : 453
train acc:  0.6875
train loss:  0.5827584266662598
train gradient:  1.1251128636484187
iteration : 454
train acc:  0.6796875
train loss:  0.5868752002716064
train gradient:  0.9687311185134881
iteration : 455
train acc:  0.7265625
train loss:  0.5689477920532227
train gradient:  0.656081481709532
iteration : 456
train acc:  0.7109375
train loss:  0.5592834949493408
train gradient:  0.4978274228841879
iteration : 457
train acc:  0.765625
train loss:  0.48420560359954834
train gradient:  0.46177327975209886
iteration : 458
train acc:  0.7109375
train loss:  0.5730327367782593
train gradient:  0.5897994808187517
iteration : 459
train acc:  0.71875
train loss:  0.5358853340148926
train gradient:  0.4570413955346463
iteration : 460
train acc:  0.7109375
train loss:  0.557327389717102
train gradient:  0.5486761117099949
iteration : 461
train acc:  0.7265625
train loss:  0.5546914339065552
train gradient:  0.5468854308906979
iteration : 462
train acc:  0.75
train loss:  0.5359814167022705
train gradient:  0.5857453100732866
iteration : 463
train acc:  0.65625
train loss:  0.6169247031211853
train gradient:  0.8639457062469733
iteration : 464
train acc:  0.6796875
train loss:  0.5898952484130859
train gradient:  0.5897061173702639
iteration : 465
train acc:  0.75
train loss:  0.520941436290741
train gradient:  0.5501425326748959
iteration : 466
train acc:  0.7109375
train loss:  0.5980421304702759
train gradient:  0.6913337522530515
iteration : 467
train acc:  0.7421875
train loss:  0.4813317656517029
train gradient:  0.4068892954835534
iteration : 468
train acc:  0.6640625
train loss:  0.5915931463241577
train gradient:  0.562716532614413
iteration : 469
train acc:  0.703125
train loss:  0.5697981119155884
train gradient:  0.6761140623576437
iteration : 470
train acc:  0.75
train loss:  0.514218807220459
train gradient:  0.519092324940288
iteration : 471
train acc:  0.734375
train loss:  0.5151627063751221
train gradient:  0.5241311795670149
iteration : 472
train acc:  0.71875
train loss:  0.5578434467315674
train gradient:  0.5554638720238884
iteration : 473
train acc:  0.84375
train loss:  0.43936046957969666
train gradient:  0.29649106151052335
iteration : 474
train acc:  0.7421875
train loss:  0.5008061528205872
train gradient:  0.4814838622209525
iteration : 475
train acc:  0.7421875
train loss:  0.5079621076583862
train gradient:  0.3512981314626669
iteration : 476
train acc:  0.6953125
train loss:  0.6398171782493591
train gradient:  0.5447567317863224
iteration : 477
train acc:  0.71875
train loss:  0.5200082063674927
train gradient:  0.39936040409291335
iteration : 478
train acc:  0.703125
train loss:  0.5548527836799622
train gradient:  0.44487642828289997
iteration : 479
train acc:  0.75
train loss:  0.5430917739868164
train gradient:  0.4202263917749049
iteration : 480
train acc:  0.6953125
train loss:  0.5399072170257568
train gradient:  0.4922162764416642
iteration : 481
train acc:  0.640625
train loss:  0.5908071994781494
train gradient:  0.5028545468004293
iteration : 482
train acc:  0.7421875
train loss:  0.5368276834487915
train gradient:  0.43121790993771947
iteration : 483
train acc:  0.78125
train loss:  0.46950697898864746
train gradient:  0.3731988712389588
iteration : 484
train acc:  0.703125
train loss:  0.5099716782569885
train gradient:  0.5469352590537119
iteration : 485
train acc:  0.734375
train loss:  0.4919871985912323
train gradient:  0.43683081678181046
iteration : 486
train acc:  0.71875
train loss:  0.5579955577850342
train gradient:  0.4200484841031243
iteration : 487
train acc:  0.7734375
train loss:  0.4652172327041626
train gradient:  0.4380254719635044
iteration : 488
train acc:  0.7734375
train loss:  0.4956470727920532
train gradient:  0.46475309926378733
iteration : 489
train acc:  0.7421875
train loss:  0.5443772673606873
train gradient:  0.5617137259311553
iteration : 490
train acc:  0.78125
train loss:  0.47369101643562317
train gradient:  0.3911465689137117
iteration : 491
train acc:  0.75
train loss:  0.49918174743652344
train gradient:  0.4534414256444029
iteration : 492
train acc:  0.765625
train loss:  0.47352302074432373
train gradient:  0.39984219822969236
iteration : 493
train acc:  0.7265625
train loss:  0.5032546520233154
train gradient:  0.4649579683808379
iteration : 494
train acc:  0.6953125
train loss:  0.5578893423080444
train gradient:  0.676132205310351
iteration : 495
train acc:  0.7890625
train loss:  0.474607914686203
train gradient:  0.3129870649329472
iteration : 496
train acc:  0.7890625
train loss:  0.4974591135978699
train gradient:  0.4930521189551655
iteration : 497
train acc:  0.765625
train loss:  0.47031188011169434
train gradient:  0.3641663089721665
iteration : 498
train acc:  0.7265625
train loss:  0.5143946409225464
train gradient:  0.5243737213795855
iteration : 499
train acc:  0.6640625
train loss:  0.6168020963668823
train gradient:  0.6177143088762623
iteration : 500
train acc:  0.7734375
train loss:  0.47037744522094727
train gradient:  0.3671276499117959
iteration : 501
train acc:  0.671875
train loss:  0.5934337377548218
train gradient:  0.751723653418484
iteration : 502
train acc:  0.71875
train loss:  0.5461039543151855
train gradient:  0.7116255229869227
iteration : 503
train acc:  0.8046875
train loss:  0.48042166233062744
train gradient:  0.4933915437324653
iteration : 504
train acc:  0.6484375
train loss:  0.569487452507019
train gradient:  0.7475800638243615
iteration : 505
train acc:  0.6875
train loss:  0.5658732652664185
train gradient:  1.1896230749285923
iteration : 506
train acc:  0.6953125
train loss:  0.5999120473861694
train gradient:  0.7186515694467082
iteration : 507
train acc:  0.796875
train loss:  0.5072205066680908
train gradient:  0.5685192092576192
iteration : 508
train acc:  0.7890625
train loss:  0.4601910710334778
train gradient:  0.532004612854434
iteration : 509
train acc:  0.734375
train loss:  0.5266709327697754
train gradient:  0.570668026120732
iteration : 510
train acc:  0.796875
train loss:  0.45622873306274414
train gradient:  0.5438001184973912
iteration : 511
train acc:  0.8046875
train loss:  0.4595346748828888
train gradient:  0.4316142276083833
iteration : 512
train acc:  0.78125
train loss:  0.49574732780456543
train gradient:  0.6219013556720254
iteration : 513
train acc:  0.796875
train loss:  0.498058557510376
train gradient:  0.7742723733664199
iteration : 514
train acc:  0.7578125
train loss:  0.4799215495586395
train gradient:  0.5162587548537869
iteration : 515
train acc:  0.75
train loss:  0.50728440284729
train gradient:  0.5965201359794929
iteration : 516
train acc:  0.734375
train loss:  0.5550214052200317
train gradient:  0.674494321473287
iteration : 517
train acc:  0.7734375
train loss:  0.5035169720649719
train gradient:  0.7749508832675149
iteration : 518
train acc:  0.703125
train loss:  0.5371066927909851
train gradient:  0.5119970023403813
iteration : 519
train acc:  0.7265625
train loss:  0.5680009126663208
train gradient:  0.5984711501002034
iteration : 520
train acc:  0.6875
train loss:  0.5205428004264832
train gradient:  0.5841088630179045
iteration : 521
train acc:  0.8125
train loss:  0.4406771659851074
train gradient:  0.4077208713384654
iteration : 522
train acc:  0.75
train loss:  0.49180009961128235
train gradient:  0.5469739634200567
iteration : 523
train acc:  0.7265625
train loss:  0.4739795923233032
train gradient:  0.370681200288139
iteration : 524
train acc:  0.7421875
train loss:  0.5066364407539368
train gradient:  0.46274158714690267
iteration : 525
train acc:  0.7421875
train loss:  0.5529352426528931
train gradient:  0.4441679668081695
iteration : 526
train acc:  0.7578125
train loss:  0.4763564169406891
train gradient:  0.4004818521479119
iteration : 527
train acc:  0.796875
train loss:  0.4791129529476166
train gradient:  0.5812438194435605
iteration : 528
train acc:  0.7734375
train loss:  0.46312013268470764
train gradient:  0.3173411566451629
iteration : 529
train acc:  0.703125
train loss:  0.5342849493026733
train gradient:  0.7007807262634871
iteration : 530
train acc:  0.7734375
train loss:  0.5209978222846985
train gradient:  0.6142204939605497
iteration : 531
train acc:  0.75
train loss:  0.5213302373886108
train gradient:  0.3921583395842063
iteration : 532
train acc:  0.7421875
train loss:  0.5184334516525269
train gradient:  0.5023571854238013
iteration : 533
train acc:  0.7265625
train loss:  0.5242763757705688
train gradient:  0.4002655720681549
iteration : 534
train acc:  0.7578125
train loss:  0.4911681115627289
train gradient:  0.5581537489685906
iteration : 535
train acc:  0.8125
train loss:  0.42171216011047363
train gradient:  0.368015603190071
iteration : 536
train acc:  0.78125
train loss:  0.44999822974205017
train gradient:  0.5166150155340075
iteration : 537
train acc:  0.703125
train loss:  0.5269035696983337
train gradient:  0.39403749328505155
iteration : 538
train acc:  0.71875
train loss:  0.5561589002609253
train gradient:  0.6283994911740332
iteration : 539
train acc:  0.7890625
train loss:  0.4920808970928192
train gradient:  0.4740866198917027
iteration : 540
train acc:  0.7734375
train loss:  0.5006318092346191
train gradient:  0.4861254477399609
iteration : 541
train acc:  0.7578125
train loss:  0.49742987751960754
train gradient:  0.5661669722027506
iteration : 542
train acc:  0.7578125
train loss:  0.48305651545524597
train gradient:  0.45362976559075036
iteration : 543
train acc:  0.7265625
train loss:  0.45420336723327637
train gradient:  0.4790608145007625
iteration : 544
train acc:  0.734375
train loss:  0.4956939220428467
train gradient:  0.5800423432601111
iteration : 545
train acc:  0.734375
train loss:  0.51444411277771
train gradient:  0.7133521072529159
iteration : 546
train acc:  0.8046875
train loss:  0.45326465368270874
train gradient:  0.5321009276812196
iteration : 547
train acc:  0.75
train loss:  0.514707088470459
train gradient:  0.4835949427122204
iteration : 548
train acc:  0.7578125
train loss:  0.48957353830337524
train gradient:  0.5167070760416594
iteration : 549
train acc:  0.765625
train loss:  0.45394426584243774
train gradient:  0.5111195164142781
iteration : 550
train acc:  0.734375
train loss:  0.5077444314956665
train gradient:  0.6047103501452111
iteration : 551
train acc:  0.796875
train loss:  0.4689669609069824
train gradient:  0.4173057220975551
iteration : 552
train acc:  0.765625
train loss:  0.4843733310699463
train gradient:  0.6835629374309528
iteration : 553
train acc:  0.734375
train loss:  0.517306387424469
train gradient:  1.1883398329662573
iteration : 554
train acc:  0.765625
train loss:  0.47191423177719116
train gradient:  0.7595933231133204
iteration : 555
train acc:  0.7578125
train loss:  0.48969244956970215
train gradient:  0.4067622762453034
iteration : 556
train acc:  0.71875
train loss:  0.5705468058586121
train gradient:  0.5532055930307991
iteration : 557
train acc:  0.78125
train loss:  0.4633471369743347
train gradient:  0.5050552835046032
iteration : 558
train acc:  0.7265625
train loss:  0.5107357501983643
train gradient:  0.6487863507874466
iteration : 559
train acc:  0.7109375
train loss:  0.5202693939208984
train gradient:  0.5594964580118849
iteration : 560
train acc:  0.7890625
train loss:  0.48120808601379395
train gradient:  0.5301909038212144
iteration : 561
train acc:  0.75
train loss:  0.5102279186248779
train gradient:  0.5609648279357088
iteration : 562
train acc:  0.7265625
train loss:  0.5360467433929443
train gradient:  0.5232850550990324
iteration : 563
train acc:  0.75
train loss:  0.5078767538070679
train gradient:  0.6098627796275484
iteration : 564
train acc:  0.7578125
train loss:  0.4874061942100525
train gradient:  0.599480935084529
iteration : 565
train acc:  0.796875
train loss:  0.5060479044914246
train gradient:  0.6599398885551406
iteration : 566
train acc:  0.7734375
train loss:  0.4890735149383545
train gradient:  0.6042012183341051
iteration : 567
train acc:  0.6484375
train loss:  0.5782376527786255
train gradient:  0.8798655883527349
iteration : 568
train acc:  0.7421875
train loss:  0.5155516862869263
train gradient:  0.5778974559296957
iteration : 569
train acc:  0.7421875
train loss:  0.5235913991928101
train gradient:  0.8837233033800126
iteration : 570
train acc:  0.7265625
train loss:  0.5690940022468567
train gradient:  0.7780197512539593
iteration : 571
train acc:  0.75
train loss:  0.46654319763183594
train gradient:  0.5474855549708502
iteration : 572
train acc:  0.71875
train loss:  0.530884861946106
train gradient:  0.6095243650197053
iteration : 573
train acc:  0.6953125
train loss:  0.5831761360168457
train gradient:  0.595631308254272
iteration : 574
train acc:  0.7421875
train loss:  0.5516284704208374
train gradient:  1.02019524260553
iteration : 575
train acc:  0.6875
train loss:  0.5291353464126587
train gradient:  0.5672760475396262
iteration : 576
train acc:  0.8125
train loss:  0.4862358570098877
train gradient:  0.8854682384958913
iteration : 577
train acc:  0.6953125
train loss:  0.5048757195472717
train gradient:  0.42331330315144144
iteration : 578
train acc:  0.7109375
train loss:  0.5388907194137573
train gradient:  0.4498197014673195
iteration : 579
train acc:  0.7578125
train loss:  0.5083651542663574
train gradient:  0.5140631511413211
iteration : 580
train acc:  0.765625
train loss:  0.47112423181533813
train gradient:  0.6198211485317571
iteration : 581
train acc:  0.796875
train loss:  0.42072224617004395
train gradient:  0.4173258619799171
iteration : 582
train acc:  0.78125
train loss:  0.47028934955596924
train gradient:  0.7939477980697278
iteration : 583
train acc:  0.7734375
train loss:  0.4777238368988037
train gradient:  0.511741117108315
iteration : 584
train acc:  0.7578125
train loss:  0.5331666469573975
train gradient:  0.35089593722205675
iteration : 585
train acc:  0.765625
train loss:  0.5051571130752563
train gradient:  0.45073646498802966
iteration : 586
train acc:  0.7265625
train loss:  0.506805419921875
train gradient:  0.5020584018019045
iteration : 587
train acc:  0.75
train loss:  0.4779083728790283
train gradient:  0.3830284247514024
iteration : 588
train acc:  0.734375
train loss:  0.5589256882667542
train gradient:  0.6157514830872322
iteration : 589
train acc:  0.75
train loss:  0.4513303339481354
train gradient:  0.453167893462538
iteration : 590
train acc:  0.7890625
train loss:  0.46115994453430176
train gradient:  0.4227042213129705
iteration : 591
train acc:  0.7734375
train loss:  0.5109847784042358
train gradient:  0.5577441485134238
iteration : 592
train acc:  0.765625
train loss:  0.48224273324012756
train gradient:  0.6043826553060259
iteration : 593
train acc:  0.765625
train loss:  0.4935768246650696
train gradient:  0.5127651097292578
iteration : 594
train acc:  0.796875
train loss:  0.4795030355453491
train gradient:  0.5769947712726577
iteration : 595
train acc:  0.7890625
train loss:  0.4975922703742981
train gradient:  0.5575678353213391
iteration : 596
train acc:  0.75
train loss:  0.5167481303215027
train gradient:  0.7171327488418545
iteration : 597
train acc:  0.6953125
train loss:  0.5564115047454834
train gradient:  0.7338563853784882
iteration : 598
train acc:  0.7578125
train loss:  0.49869638681411743
train gradient:  0.5989959110274279
iteration : 599
train acc:  0.7265625
train loss:  0.47143974900245667
train gradient:  0.44592962832887795
iteration : 600
train acc:  0.7265625
train loss:  0.5014597177505493
train gradient:  0.6847282748601679
iteration : 601
train acc:  0.734375
train loss:  0.5566617846488953
train gradient:  0.7094043405065993
iteration : 602
train acc:  0.7734375
train loss:  0.4776534140110016
train gradient:  0.5318488062530784
iteration : 603
train acc:  0.703125
train loss:  0.6086874604225159
train gradient:  0.6934767602124603
iteration : 604
train acc:  0.6953125
train loss:  0.5516194701194763
train gradient:  0.69365622875379
iteration : 605
train acc:  0.640625
train loss:  0.7098223567008972
train gradient:  0.9960312475123095
iteration : 606
train acc:  0.7421875
train loss:  0.4890223443508148
train gradient:  0.6230340444862972
iteration : 607
train acc:  0.6953125
train loss:  0.5625441670417786
train gradient:  0.5280197509336079
iteration : 608
train acc:  0.6875
train loss:  0.5309745669364929
train gradient:  0.7170185307738015
iteration : 609
train acc:  0.765625
train loss:  0.5209345817565918
train gradient:  0.484809596791708
iteration : 610
train acc:  0.734375
train loss:  0.5251352190971375
train gradient:  0.5364760129704664
iteration : 611
train acc:  0.71875
train loss:  0.5325350761413574
train gradient:  0.5153280211187214
iteration : 612
train acc:  0.765625
train loss:  0.5209853649139404
train gradient:  0.48104018873959975
iteration : 613
train acc:  0.71875
train loss:  0.5500764846801758
train gradient:  0.4816039386211777
iteration : 614
train acc:  0.71875
train loss:  0.5000578165054321
train gradient:  0.551411531994386
iteration : 615
train acc:  0.75
train loss:  0.5182664394378662
train gradient:  0.6307779483051701
iteration : 616
train acc:  0.75
train loss:  0.49810755252838135
train gradient:  0.4073292127738885
iteration : 617
train acc:  0.7578125
train loss:  0.5226656198501587
train gradient:  0.6817320816546171
iteration : 618
train acc:  0.8046875
train loss:  0.44583407044410706
train gradient:  0.42970848998071015
iteration : 619
train acc:  0.734375
train loss:  0.5200625061988831
train gradient:  0.43141715813338055
iteration : 620
train acc:  0.75
train loss:  0.443554162979126
train gradient:  0.3623692747765416
iteration : 621
train acc:  0.796875
train loss:  0.48445478081703186
train gradient:  0.4530262190790328
iteration : 622
train acc:  0.734375
train loss:  0.5111016631126404
train gradient:  0.5496597223822823
iteration : 623
train acc:  0.71875
train loss:  0.4804382622241974
train gradient:  0.42187594696114916
iteration : 624
train acc:  0.8203125
train loss:  0.4361216425895691
train gradient:  0.37844866740430266
iteration : 625
train acc:  0.7109375
train loss:  0.5494691729545593
train gradient:  0.5105708863567187
iteration : 626
train acc:  0.7265625
train loss:  0.5627851486206055
train gradient:  0.6394209448579524
iteration : 627
train acc:  0.734375
train loss:  0.5554651021957397
train gradient:  0.5097665133929443
iteration : 628
train acc:  0.7421875
train loss:  0.47333770990371704
train gradient:  0.4278843139915403
iteration : 629
train acc:  0.7109375
train loss:  0.5408435463905334
train gradient:  0.5851860958291806
iteration : 630
train acc:  0.8046875
train loss:  0.48167502880096436
train gradient:  0.44931679386294865
iteration : 631
train acc:  0.703125
train loss:  0.5539302229881287
train gradient:  0.6857324209792414
iteration : 632
train acc:  0.7109375
train loss:  0.5528764128684998
train gradient:  0.49326029985282627
iteration : 633
train acc:  0.7890625
train loss:  0.4488258957862854
train gradient:  0.3701141840186747
iteration : 634
train acc:  0.703125
train loss:  0.5081671476364136
train gradient:  0.4567909684012418
iteration : 635
train acc:  0.765625
train loss:  0.48245111107826233
train gradient:  0.4289896474512217
iteration : 636
train acc:  0.734375
train loss:  0.4559999704360962
train gradient:  0.38930311268269546
iteration : 637
train acc:  0.84375
train loss:  0.41508370637893677
train gradient:  0.2668780483295971
iteration : 638
train acc:  0.7421875
train loss:  0.516962468624115
train gradient:  0.5246635748382686
iteration : 639
train acc:  0.7265625
train loss:  0.5231485366821289
train gradient:  0.4309086446203391
iteration : 640
train acc:  0.8203125
train loss:  0.4344269037246704
train gradient:  0.4980340559193018
iteration : 641
train acc:  0.6796875
train loss:  0.55812668800354
train gradient:  0.6571369912544882
iteration : 642
train acc:  0.734375
train loss:  0.4778038263320923
train gradient:  0.6693204626422711
iteration : 643
train acc:  0.7578125
train loss:  0.46158573031425476
train gradient:  0.5027346134539624
iteration : 644
train acc:  0.7578125
train loss:  0.4522407054901123
train gradient:  0.4013535963826741
iteration : 645
train acc:  0.7734375
train loss:  0.5065751075744629
train gradient:  0.591508131879867
iteration : 646
train acc:  0.7109375
train loss:  0.5661725997924805
train gradient:  1.1721420610709563
iteration : 647
train acc:  0.765625
train loss:  0.4901283383369446
train gradient:  0.5180002497932942
iteration : 648
train acc:  0.7578125
train loss:  0.48800933361053467
train gradient:  0.5267182645833779
iteration : 649
train acc:  0.84375
train loss:  0.44813263416290283
train gradient:  0.35978052024605023
iteration : 650
train acc:  0.78125
train loss:  0.4255257546901703
train gradient:  0.4451856641111454
iteration : 651
train acc:  0.71875
train loss:  0.5519465804100037
train gradient:  0.6642854971213117
iteration : 652
train acc:  0.734375
train loss:  0.4847186505794525
train gradient:  0.5117309671881705
iteration : 653
train acc:  0.71875
train loss:  0.5157451033592224
train gradient:  0.763238834647423
iteration : 654
train acc:  0.8046875
train loss:  0.438400000333786
train gradient:  0.4420807853197135
iteration : 655
train acc:  0.7734375
train loss:  0.4590507745742798
train gradient:  0.4886789926042631
iteration : 656
train acc:  0.7734375
train loss:  0.5348696708679199
train gradient:  0.6198072496672127
iteration : 657
train acc:  0.75
train loss:  0.46403181552886963
train gradient:  0.4365995230302368
iteration : 658
train acc:  0.8046875
train loss:  0.4295288920402527
train gradient:  0.34910502335476096
iteration : 659
train acc:  0.7734375
train loss:  0.4540300965309143
train gradient:  0.4171315450548675
iteration : 660
train acc:  0.7421875
train loss:  0.5561565160751343
train gradient:  0.663710139934653
iteration : 661
train acc:  0.7890625
train loss:  0.47358208894729614
train gradient:  0.6045371924666634
iteration : 662
train acc:  0.8046875
train loss:  0.4559635519981384
train gradient:  0.47406886797109304
iteration : 663
train acc:  0.7109375
train loss:  0.5292820930480957
train gradient:  0.583406457221446
iteration : 664
train acc:  0.7421875
train loss:  0.5101833343505859
train gradient:  0.6759247175243779
iteration : 665
train acc:  0.75
train loss:  0.46246522665023804
train gradient:  0.49829111745405336
iteration : 666
train acc:  0.765625
train loss:  0.48559585213661194
train gradient:  0.4402715175137705
iteration : 667
train acc:  0.8203125
train loss:  0.41228342056274414
train gradient:  0.5206119080595882
iteration : 668
train acc:  0.8046875
train loss:  0.45122215151786804
train gradient:  0.4761024201197925
iteration : 669
train acc:  0.7734375
train loss:  0.46896082162857056
train gradient:  0.6088538567898437
iteration : 670
train acc:  0.765625
train loss:  0.5129855275154114
train gradient:  0.5249727536909241
iteration : 671
train acc:  0.734375
train loss:  0.49451926350593567
train gradient:  0.6284709377573678
iteration : 672
train acc:  0.71875
train loss:  0.5433254241943359
train gradient:  0.8043239489229003
iteration : 673
train acc:  0.71875
train loss:  0.48004966974258423
train gradient:  0.4673571004180676
iteration : 674
train acc:  0.796875
train loss:  0.4393431544303894
train gradient:  0.569224704018113
iteration : 675
train acc:  0.7421875
train loss:  0.4973229765892029
train gradient:  0.6749230038675236
iteration : 676
train acc:  0.71875
train loss:  0.5064551830291748
train gradient:  0.5544037047187835
iteration : 677
train acc:  0.828125
train loss:  0.4277805685997009
train gradient:  0.8336926423062547
iteration : 678
train acc:  0.7109375
train loss:  0.5318460464477539
train gradient:  0.5711670182390158
iteration : 679
train acc:  0.7734375
train loss:  0.446172297000885
train gradient:  0.5233435956422476
iteration : 680
train acc:  0.6953125
train loss:  0.5372572541236877
train gradient:  0.5791230146987487
iteration : 681
train acc:  0.7109375
train loss:  0.5594437122344971
train gradient:  1.0514690579189072
iteration : 682
train acc:  0.7421875
train loss:  0.5024962425231934
train gradient:  0.6061782727143539
iteration : 683
train acc:  0.7109375
train loss:  0.5968905091285706
train gradient:  0.8248936607153807
iteration : 684
train acc:  0.8203125
train loss:  0.48250168561935425
train gradient:  0.5561465998421906
iteration : 685
train acc:  0.7109375
train loss:  0.5288136005401611
train gradient:  0.624572415645309
iteration : 686
train acc:  0.7421875
train loss:  0.5190170407295227
train gradient:  0.8458249301836505
iteration : 687
train acc:  0.78125
train loss:  0.4704725742340088
train gradient:  0.6384193748136966
iteration : 688
train acc:  0.8359375
train loss:  0.43611231446266174
train gradient:  0.46915616009265243
iteration : 689
train acc:  0.7890625
train loss:  0.4548622965812683
train gradient:  0.5139180482717824
iteration : 690
train acc:  0.8046875
train loss:  0.4399690330028534
train gradient:  0.5050713770732707
iteration : 691
train acc:  0.8046875
train loss:  0.43066051602363586
train gradient:  0.4253182269332438
iteration : 692
train acc:  0.7421875
train loss:  0.5048971772193909
train gradient:  0.7009144934647816
iteration : 693
train acc:  0.7890625
train loss:  0.4595678150653839
train gradient:  0.5317751182890078
iteration : 694
train acc:  0.7578125
train loss:  0.4927741587162018
train gradient:  0.6806425056200722
iteration : 695
train acc:  0.859375
train loss:  0.432894766330719
train gradient:  0.3086256982332162
iteration : 696
train acc:  0.78125
train loss:  0.45167621970176697
train gradient:  0.40271947169523076
iteration : 697
train acc:  0.765625
train loss:  0.46637144684791565
train gradient:  0.5438978029880923
iteration : 698
train acc:  0.75
train loss:  0.5351375937461853
train gradient:  1.0250035029641098
iteration : 699
train acc:  0.78125
train loss:  0.4668143391609192
train gradient:  0.6638417284135281
iteration : 700
train acc:  0.7890625
train loss:  0.44748014211654663
train gradient:  0.4112690513508325
iteration : 701
train acc:  0.71875
train loss:  0.49531733989715576
train gradient:  0.542647087815672
iteration : 702
train acc:  0.7890625
train loss:  0.48319801688194275
train gradient:  0.6278158896425496
iteration : 703
train acc:  0.7109375
train loss:  0.569495439529419
train gradient:  0.7590538146378695
iteration : 704
train acc:  0.7421875
train loss:  0.5107328295707703
train gradient:  0.5880067541034257
iteration : 705
train acc:  0.765625
train loss:  0.5325694680213928
train gradient:  0.6649980378708423
iteration : 706
train acc:  0.7734375
train loss:  0.48783940076828003
train gradient:  0.5151182747463003
iteration : 707
train acc:  0.734375
train loss:  0.5624253749847412
train gradient:  0.7594391327697173
iteration : 708
train acc:  0.8125
train loss:  0.45337188243865967
train gradient:  0.47097895286304825
iteration : 709
train acc:  0.796875
train loss:  0.399150013923645
train gradient:  0.42192056184461374
iteration : 710
train acc:  0.7890625
train loss:  0.456390917301178
train gradient:  0.3945888664413063
iteration : 711
train acc:  0.75
train loss:  0.5367447733879089
train gradient:  0.5323734613900062
iteration : 712
train acc:  0.765625
train loss:  0.45279985666275024
train gradient:  0.456901985632565
iteration : 713
train acc:  0.7265625
train loss:  0.5819209814071655
train gradient:  0.7223317762919801
iteration : 714
train acc:  0.7265625
train loss:  0.5316038131713867
train gradient:  0.507367780661246
iteration : 715
train acc:  0.765625
train loss:  0.5326420068740845
train gradient:  0.8878020074502702
iteration : 716
train acc:  0.7734375
train loss:  0.48768407106399536
train gradient:  0.41668871239271965
iteration : 717
train acc:  0.8203125
train loss:  0.4101119041442871
train gradient:  0.38160584913267925
iteration : 718
train acc:  0.8046875
train loss:  0.4601234197616577
train gradient:  0.5152508676937027
iteration : 719
train acc:  0.828125
train loss:  0.403772234916687
train gradient:  0.28007622266319393
iteration : 720
train acc:  0.7421875
train loss:  0.5198148488998413
train gradient:  0.4545806368962147
iteration : 721
train acc:  0.78125
train loss:  0.44600266218185425
train gradient:  0.37215122641814136
iteration : 722
train acc:  0.7421875
train loss:  0.5059369802474976
train gradient:  0.5473237900274797
iteration : 723
train acc:  0.78125
train loss:  0.5353481769561768
train gradient:  0.41183614647419486
iteration : 724
train acc:  0.7265625
train loss:  0.5302524566650391
train gradient:  0.49775296415300946
iteration : 725
train acc:  0.7734375
train loss:  0.4669051170349121
train gradient:  0.34809444277708546
iteration : 726
train acc:  0.7578125
train loss:  0.4886830449104309
train gradient:  0.5551601450187814
iteration : 727
train acc:  0.6875
train loss:  0.579937219619751
train gradient:  0.729955627900656
iteration : 728
train acc:  0.71875
train loss:  0.5267491340637207
train gradient:  0.7009172568614812
iteration : 729
train acc:  0.8125
train loss:  0.45237696170806885
train gradient:  0.5651101907277176
iteration : 730
train acc:  0.7734375
train loss:  0.4692859649658203
train gradient:  0.5306695633036431
iteration : 731
train acc:  0.765625
train loss:  0.5009413957595825
train gradient:  0.5585870233751231
iteration : 732
train acc:  0.75
train loss:  0.5500292778015137
train gradient:  0.5923213792477303
iteration : 733
train acc:  0.7421875
train loss:  0.49398577213287354
train gradient:  0.5088827081949558
iteration : 734
train acc:  0.796875
train loss:  0.4247466027736664
train gradient:  0.5227787631052299
iteration : 735
train acc:  0.8359375
train loss:  0.44337907433509827
train gradient:  0.3910127771569678
iteration : 736
train acc:  0.796875
train loss:  0.4493374824523926
train gradient:  0.5010813967960388
iteration : 737
train acc:  0.703125
train loss:  0.5628151893615723
train gradient:  0.8713492414412087
iteration : 738
train acc:  0.8359375
train loss:  0.4195577800273895
train gradient:  0.45384735837522516
iteration : 739
train acc:  0.796875
train loss:  0.4680110812187195
train gradient:  0.6193509537372657
iteration : 740
train acc:  0.7578125
train loss:  0.4938701391220093
train gradient:  0.5212462243336442
iteration : 741
train acc:  0.7578125
train loss:  0.5257781744003296
train gradient:  0.6987206301189997
iteration : 742
train acc:  0.7890625
train loss:  0.4773367643356323
train gradient:  0.7589098594920989
iteration : 743
train acc:  0.7734375
train loss:  0.5040358304977417
train gradient:  0.5602589755365979
iteration : 744
train acc:  0.796875
train loss:  0.4456571638584137
train gradient:  0.4868548019642957
iteration : 745
train acc:  0.7890625
train loss:  0.5269765257835388
train gradient:  0.5989894046574786
iteration : 746
train acc:  0.796875
train loss:  0.47404229640960693
train gradient:  0.5608331180887104
iteration : 747
train acc:  0.78125
train loss:  0.4928470849990845
train gradient:  0.4428636488192977
iteration : 748
train acc:  0.78125
train loss:  0.41297754645347595
train gradient:  0.38440642105947537
iteration : 749
train acc:  0.734375
train loss:  0.5536148548126221
train gradient:  0.576132380502484
iteration : 750
train acc:  0.7421875
train loss:  0.4762369990348816
train gradient:  0.539940788450001
iteration : 751
train acc:  0.7578125
train loss:  0.48157352209091187
train gradient:  0.38859275630618345
iteration : 752
train acc:  0.8125
train loss:  0.4285016655921936
train gradient:  0.43265662862814275
iteration : 753
train acc:  0.8046875
train loss:  0.4301754832267761
train gradient:  0.4289449888979985
iteration : 754
train acc:  0.78125
train loss:  0.43555814027786255
train gradient:  0.39955991986331896
iteration : 755
train acc:  0.78125
train loss:  0.42072948813438416
train gradient:  0.48021466116518136
iteration : 756
train acc:  0.7265625
train loss:  0.5213631987571716
train gradient:  0.49515085796606023
iteration : 757
train acc:  0.7734375
train loss:  0.43724530935287476
train gradient:  0.5366370445562809
iteration : 758
train acc:  0.7265625
train loss:  0.534622073173523
train gradient:  0.6563995859219575
iteration : 759
train acc:  0.8125
train loss:  0.4495212435722351
train gradient:  0.4289587914726181
iteration : 760
train acc:  0.7265625
train loss:  0.48908358812332153
train gradient:  0.46559386595847857
iteration : 761
train acc:  0.75
train loss:  0.5123544931411743
train gradient:  0.5431809664684047
iteration : 762
train acc:  0.7890625
train loss:  0.5155191421508789
train gradient:  0.5363781972028034
iteration : 763
train acc:  0.75
train loss:  0.4817957580089569
train gradient:  0.6707426669847052
iteration : 764
train acc:  0.765625
train loss:  0.48954036831855774
train gradient:  0.7195423234862719
iteration : 765
train acc:  0.7421875
train loss:  0.46887728571891785
train gradient:  0.5427069914799993
iteration : 766
train acc:  0.8359375
train loss:  0.3965599238872528
train gradient:  0.6191469861550284
iteration : 767
train acc:  0.8359375
train loss:  0.4050227999687195
train gradient:  0.39136313192515
iteration : 768
train acc:  0.7890625
train loss:  0.4325045645236969
train gradient:  0.4996406443964749
iteration : 769
train acc:  0.765625
train loss:  0.47139301896095276
train gradient:  0.6494326988811193
iteration : 770
train acc:  0.7109375
train loss:  0.48752081394195557
train gradient:  0.5046389456519221
iteration : 771
train acc:  0.8125
train loss:  0.44518762826919556
train gradient:  0.4847220930285716
iteration : 772
train acc:  0.8046875
train loss:  0.40800535678863525
train gradient:  0.5120951789998236
iteration : 773
train acc:  0.7109375
train loss:  0.5988885164260864
train gradient:  0.8872045122314454
iteration : 774
train acc:  0.75
train loss:  0.5411427021026611
train gradient:  0.6364352564214453
iteration : 775
train acc:  0.828125
train loss:  0.41027575731277466
train gradient:  0.4180601193943583
iteration : 776
train acc:  0.78125
train loss:  0.4397522509098053
train gradient:  0.4543645799042494
iteration : 777
train acc:  0.765625
train loss:  0.48293042182922363
train gradient:  0.4771814186255377
iteration : 778
train acc:  0.7578125
train loss:  0.5172609090805054
train gradient:  0.7023052002239212
iteration : 779
train acc:  0.734375
train loss:  0.5115854740142822
train gradient:  0.7156617162304876
iteration : 780
train acc:  0.8203125
train loss:  0.40777069330215454
train gradient:  0.4508340946129205
iteration : 781
train acc:  0.7578125
train loss:  0.4684816002845764
train gradient:  0.5855149811945488
iteration : 782
train acc:  0.765625
train loss:  0.49568039178848267
train gradient:  0.6343263621130844
iteration : 783
train acc:  0.78125
train loss:  0.44380778074264526
train gradient:  0.5264458841215824
iteration : 784
train acc:  0.8125
train loss:  0.4810676574707031
train gradient:  0.7554343471316038
iteration : 785
train acc:  0.8359375
train loss:  0.3894069790840149
train gradient:  0.4553951626620111
iteration : 786
train acc:  0.8125
train loss:  0.4001331329345703
train gradient:  0.38143947015073987
iteration : 787
train acc:  0.75
train loss:  0.5415022373199463
train gradient:  0.8464570566425872
iteration : 788
train acc:  0.796875
train loss:  0.4402666687965393
train gradient:  0.4508780907366084
iteration : 789
train acc:  0.71875
train loss:  0.5522627234458923
train gradient:  0.7111548330380311
iteration : 790
train acc:  0.71875
train loss:  0.5119215250015259
train gradient:  0.6667641800012719
iteration : 791
train acc:  0.7578125
train loss:  0.4852791428565979
train gradient:  0.7266226670993655
iteration : 792
train acc:  0.7890625
train loss:  0.4491221308708191
train gradient:  0.5592918710398381
iteration : 793
train acc:  0.7734375
train loss:  0.4749128520488739
train gradient:  0.6036007765120941
iteration : 794
train acc:  0.78125
train loss:  0.42350783944129944
train gradient:  0.4872354774869729
iteration : 795
train acc:  0.7890625
train loss:  0.43300336599349976
train gradient:  0.4346587687663574
iteration : 796
train acc:  0.796875
train loss:  0.4545913338661194
train gradient:  0.6913905783313974
iteration : 797
train acc:  0.75
train loss:  0.494693398475647
train gradient:  0.5656245437743415
iteration : 798
train acc:  0.7734375
train loss:  0.4903930723667145
train gradient:  0.4918803916027229
iteration : 799
train acc:  0.78125
train loss:  0.4244823455810547
train gradient:  0.5414707537858008
iteration : 800
train acc:  0.7578125
train loss:  0.4817984998226166
train gradient:  0.5838811947356836
iteration : 801
train acc:  0.75
train loss:  0.5312036275863647
train gradient:  0.5951909103536993
iteration : 802
train acc:  0.7734375
train loss:  0.47791558504104614
train gradient:  0.40432395946042804
iteration : 803
train acc:  0.828125
train loss:  0.41477838158607483
train gradient:  0.3702175968435168
iteration : 804
train acc:  0.8046875
train loss:  0.4310649037361145
train gradient:  0.41429915268316914
iteration : 805
train acc:  0.859375
train loss:  0.3399404287338257
train gradient:  0.37239376319538414
iteration : 806
train acc:  0.8125
train loss:  0.43665027618408203
train gradient:  0.44314048492903413
iteration : 807
train acc:  0.7109375
train loss:  0.5442276000976562
train gradient:  0.5095036257597538
iteration : 808
train acc:  0.7421875
train loss:  0.5412130355834961
train gradient:  0.7018909648011753
iteration : 809
train acc:  0.765625
train loss:  0.49140653014183044
train gradient:  0.5554786949361823
iteration : 810
train acc:  0.7890625
train loss:  0.4223552346229553
train gradient:  0.5716185025091224
iteration : 811
train acc:  0.734375
train loss:  0.49501726031303406
train gradient:  0.3837631427518012
iteration : 812
train acc:  0.734375
train loss:  0.49816396832466125
train gradient:  0.5143596553463329
iteration : 813
train acc:  0.859375
train loss:  0.42254573106765747
train gradient:  0.3735208367001383
iteration : 814
train acc:  0.7734375
train loss:  0.4831003248691559
train gradient:  0.41340042378308783
iteration : 815
train acc:  0.8359375
train loss:  0.4152587950229645
train gradient:  0.4155931726577005
iteration : 816
train acc:  0.765625
train loss:  0.43275055289268494
train gradient:  0.832145350246416
iteration : 817
train acc:  0.78125
train loss:  0.4280873239040375
train gradient:  0.40751386368861625
iteration : 818
train acc:  0.7890625
train loss:  0.4513241648674011
train gradient:  0.376229177334419
iteration : 819
train acc:  0.8125
train loss:  0.43861812353134155
train gradient:  0.34597305410359697
iteration : 820
train acc:  0.78125
train loss:  0.4930260479450226
train gradient:  0.6082986371731145
iteration : 821
train acc:  0.796875
train loss:  0.454209566116333
train gradient:  0.4786962640104221
iteration : 822
train acc:  0.765625
train loss:  0.5485250949859619
train gradient:  1.2967762483354994
iteration : 823
train acc:  0.8359375
train loss:  0.4800070822238922
train gradient:  0.6236342687408182
iteration : 824
train acc:  0.7421875
train loss:  0.4753649830818176
train gradient:  0.5515152173667826
iteration : 825
train acc:  0.765625
train loss:  0.4645562767982483
train gradient:  0.37332314356010726
iteration : 826
train acc:  0.703125
train loss:  0.5882248878479004
train gradient:  0.7594233607790157
iteration : 827
train acc:  0.8125
train loss:  0.4496012330055237
train gradient:  0.3733629841982879
iteration : 828
train acc:  0.765625
train loss:  0.4708195924758911
train gradient:  0.45720507438229563
iteration : 829
train acc:  0.7109375
train loss:  0.5632529258728027
train gradient:  0.7095767130811871
iteration : 830
train acc:  0.78125
train loss:  0.48709583282470703
train gradient:  0.5553011361933499
iteration : 831
train acc:  0.8125
train loss:  0.4265117645263672
train gradient:  0.37356778753382597
iteration : 832
train acc:  0.796875
train loss:  0.4555838704109192
train gradient:  0.4345324964431795
iteration : 833
train acc:  0.78125
train loss:  0.4329163432121277
train gradient:  0.9569210671758419
iteration : 834
train acc:  0.8046875
train loss:  0.4867819547653198
train gradient:  0.7434540801850293
iteration : 835
train acc:  0.7421875
train loss:  0.5167055726051331
train gradient:  0.5972886321222591
iteration : 836
train acc:  0.828125
train loss:  0.4288763999938965
train gradient:  0.43646553989231696
iteration : 837
train acc:  0.703125
train loss:  0.5245046615600586
train gradient:  0.7826321113646748
iteration : 838
train acc:  0.75
train loss:  0.5200643539428711
train gradient:  0.5768200624430183
iteration : 839
train acc:  0.7421875
train loss:  0.5355097055435181
train gradient:  0.7547931969312272
iteration : 840
train acc:  0.7890625
train loss:  0.42717617750167847
train gradient:  0.47715850585138014
iteration : 841
train acc:  0.75
train loss:  0.5135716795921326
train gradient:  0.7506016792145394
iteration : 842
train acc:  0.78125
train loss:  0.4783450663089752
train gradient:  0.7702769086854033
iteration : 843
train acc:  0.765625
train loss:  0.5287643671035767
train gradient:  0.5887776500391155
iteration : 844
train acc:  0.8125
train loss:  0.45549365878105164
train gradient:  0.626315165724818
iteration : 845
train acc:  0.796875
train loss:  0.42955514788627625
train gradient:  0.6244554819310455
iteration : 846
train acc:  0.7578125
train loss:  0.5105184316635132
train gradient:  0.5876834364670118
iteration : 847
train acc:  0.7890625
train loss:  0.40836870670318604
train gradient:  0.41940681115872014
iteration : 848
train acc:  0.8046875
train loss:  0.4243350327014923
train gradient:  0.3525582831732266
iteration : 849
train acc:  0.78125
train loss:  0.42772993445396423
train gradient:  0.5336538999367739
iteration : 850
train acc:  0.796875
train loss:  0.43888139724731445
train gradient:  0.33214026951493625
iteration : 851
train acc:  0.765625
train loss:  0.4455592930316925
train gradient:  0.430402511379169
iteration : 852
train acc:  0.7265625
train loss:  0.4813534915447235
train gradient:  0.48968282351593384
iteration : 853
train acc:  0.75
train loss:  0.482728511095047
train gradient:  0.6391071624552809
iteration : 854
train acc:  0.7578125
train loss:  0.48808497190475464
train gradient:  0.7049692600456583
iteration : 855
train acc:  0.7421875
train loss:  0.5158783197402954
train gradient:  0.6489237649153148
iteration : 856
train acc:  0.7734375
train loss:  0.49587732553482056
train gradient:  0.6377986994052367
iteration : 857
train acc:  0.8046875
train loss:  0.4656652510166168
train gradient:  0.5888532178549872
iteration : 858
train acc:  0.828125
train loss:  0.3913410007953644
train gradient:  0.42370754012913103
iteration : 859
train acc:  0.734375
train loss:  0.5359492301940918
train gradient:  0.6600208273164717
iteration : 860
train acc:  0.796875
train loss:  0.4712967872619629
train gradient:  0.3712298687342142
iteration : 861
train acc:  0.75
train loss:  0.4745948016643524
train gradient:  0.4537781016502969
iteration : 862
train acc:  0.8125
train loss:  0.449738472700119
train gradient:  0.46232335590375584
iteration : 863
train acc:  0.78125
train loss:  0.47122612595558167
train gradient:  0.43772467246248414
iteration : 864
train acc:  0.75
train loss:  0.48521220684051514
train gradient:  0.730015095807168
iteration : 865
train acc:  0.796875
train loss:  0.43352508544921875
train gradient:  0.3186554790954601
iteration : 866
train acc:  0.78125
train loss:  0.41934633255004883
train gradient:  0.41365099529553667
iteration : 867
train acc:  0.7890625
train loss:  0.4630665183067322
train gradient:  0.42934979878602075
iteration : 868
train acc:  0.71875
train loss:  0.5209033489227295
train gradient:  0.4095379430258642
iteration : 869
train acc:  0.7265625
train loss:  0.5401301383972168
train gradient:  0.5947985749729643
iteration : 870
train acc:  0.7578125
train loss:  0.4634872078895569
train gradient:  0.4806716103446193
iteration : 871
train acc:  0.8359375
train loss:  0.3840504288673401
train gradient:  0.4365250874055169
iteration : 872
train acc:  0.7265625
train loss:  0.4908328652381897
train gradient:  0.635810178284507
iteration : 873
train acc:  0.78125
train loss:  0.493588387966156
train gradient:  0.5409746266339495
iteration : 874
train acc:  0.7734375
train loss:  0.500187337398529
train gradient:  0.5962287407015052
iteration : 875
train acc:  0.7109375
train loss:  0.6041855812072754
train gradient:  0.8911582740571893
iteration : 876
train acc:  0.78125
train loss:  0.484572172164917
train gradient:  0.7481506657500165
iteration : 877
train acc:  0.796875
train loss:  0.43687495589256287
train gradient:  0.7696321057606748
iteration : 878
train acc:  0.8359375
train loss:  0.4021247327327728
train gradient:  0.37814692530329047
iteration : 879
train acc:  0.796875
train loss:  0.4399432837963104
train gradient:  0.44277544970435756
iteration : 880
train acc:  0.75
train loss:  0.5019575953483582
train gradient:  0.5398494563752938
iteration : 881
train acc:  0.765625
train loss:  0.49617505073547363
train gradient:  0.46055093067809094
iteration : 882
train acc:  0.78125
train loss:  0.45903798937797546
train gradient:  0.41526214797300537
iteration : 883
train acc:  0.796875
train loss:  0.43257009983062744
train gradient:  0.4107553205682793
iteration : 884
train acc:  0.796875
train loss:  0.4364229440689087
train gradient:  0.7187911581724638
iteration : 885
train acc:  0.7734375
train loss:  0.421522319316864
train gradient:  0.3591061666829194
iteration : 886
train acc:  0.7578125
train loss:  0.4741029143333435
train gradient:  0.5634062158790832
iteration : 887
train acc:  0.796875
train loss:  0.4408372640609741
train gradient:  0.7525381975447635
iteration : 888
train acc:  0.8046875
train loss:  0.41939085721969604
train gradient:  0.36373895847949755
iteration : 889
train acc:  0.75
train loss:  0.48131394386291504
train gradient:  0.562721523448735
iteration : 890
train acc:  0.78125
train loss:  0.47512882947921753
train gradient:  0.4794776254155159
iteration : 891
train acc:  0.796875
train loss:  0.48217320442199707
train gradient:  0.7126289425207456
iteration : 892
train acc:  0.796875
train loss:  0.415446400642395
train gradient:  0.36195720024469713
iteration : 893
train acc:  0.8046875
train loss:  0.40739357471466064
train gradient:  0.47117172633452686
iteration : 894
train acc:  0.8125
train loss:  0.42562729120254517
train gradient:  0.4245798113840995
iteration : 895
train acc:  0.765625
train loss:  0.46890464425086975
train gradient:  0.5404269718419208
iteration : 896
train acc:  0.734375
train loss:  0.5834840536117554
train gradient:  0.6477936212168681
iteration : 897
train acc:  0.7109375
train loss:  0.5376816391944885
train gradient:  1.113143960576336
iteration : 898
train acc:  0.8828125
train loss:  0.3527199923992157
train gradient:  0.3722283528514091
iteration : 899
train acc:  0.7421875
train loss:  0.49022814631462097
train gradient:  0.5691223782203276
iteration : 900
train acc:  0.84375
train loss:  0.3667796552181244
train gradient:  0.3016718097321711
iteration : 901
train acc:  0.796875
train loss:  0.4331807494163513
train gradient:  0.4489332387069832
iteration : 902
train acc:  0.7734375
train loss:  0.5065975189208984
train gradient:  0.5363628184803632
iteration : 903
train acc:  0.765625
train loss:  0.48908132314682007
train gradient:  0.4876284548812534
iteration : 904
train acc:  0.8203125
train loss:  0.40940040349960327
train gradient:  0.4805349130705965
iteration : 905
train acc:  0.78125
train loss:  0.48074889183044434
train gradient:  0.6389725227875142
iteration : 906
train acc:  0.796875
train loss:  0.4985484480857849
train gradient:  0.41516229818633016
iteration : 907
train acc:  0.7890625
train loss:  0.4103192389011383
train gradient:  0.6897990177280031
iteration : 908
train acc:  0.8125
train loss:  0.4538837969303131
train gradient:  0.43376345116294535
iteration : 909
train acc:  0.8046875
train loss:  0.45661166310310364
train gradient:  0.47203622419652735
iteration : 910
train acc:  0.7578125
train loss:  0.4948473572731018
train gradient:  0.34818123842386106
iteration : 911
train acc:  0.78125
train loss:  0.5105939507484436
train gradient:  0.4848169185481508
iteration : 912
train acc:  0.765625
train loss:  0.4777551293373108
train gradient:  0.5057746239754546
iteration : 913
train acc:  0.7734375
train loss:  0.4715924859046936
train gradient:  0.6925025670060168
iteration : 914
train acc:  0.7890625
train loss:  0.467112272977829
train gradient:  0.41709004747831907
iteration : 915
train acc:  0.8125
train loss:  0.43159306049346924
train gradient:  0.42193585369963815
iteration : 916
train acc:  0.703125
train loss:  0.5803640484809875
train gradient:  0.6591224657473533
iteration : 917
train acc:  0.765625
train loss:  0.47426149249076843
train gradient:  0.4965331368283527
iteration : 918
train acc:  0.7265625
train loss:  0.5722241997718811
train gradient:  0.6409571918270972
iteration : 919
train acc:  0.7734375
train loss:  0.44613075256347656
train gradient:  0.4258665215926151
iteration : 920
train acc:  0.7578125
train loss:  0.5346375703811646
train gradient:  0.5147438898592901
iteration : 921
train acc:  0.7734375
train loss:  0.4489554464817047
train gradient:  0.43994476882948674
iteration : 922
train acc:  0.7578125
train loss:  0.5220905542373657
train gradient:  0.5411430092164715
iteration : 923
train acc:  0.75
train loss:  0.5080438852310181
train gradient:  0.454489475542459
iteration : 924
train acc:  0.7578125
train loss:  0.48702073097229004
train gradient:  0.4167966066348095
iteration : 925
train acc:  0.703125
train loss:  0.5549256801605225
train gradient:  0.6405623475795645
iteration : 926
train acc:  0.828125
train loss:  0.4245452880859375
train gradient:  0.41532048831614654
iteration : 927
train acc:  0.8125
train loss:  0.4033481478691101
train gradient:  0.2767033963932234
iteration : 928
train acc:  0.859375
train loss:  0.3919409513473511
train gradient:  0.33761114576153267
iteration : 929
train acc:  0.8046875
train loss:  0.44160696864128113
train gradient:  0.382369523941136
iteration : 930
train acc:  0.75
train loss:  0.46341055631637573
train gradient:  0.42442134635972656
iteration : 931
train acc:  0.8046875
train loss:  0.4641494154930115
train gradient:  0.5763708816085575
iteration : 932
train acc:  0.6796875
train loss:  0.5995698571205139
train gradient:  0.7020046254236385
iteration : 933
train acc:  0.765625
train loss:  0.43503743410110474
train gradient:  0.4980902832798796
iteration : 934
train acc:  0.7578125
train loss:  0.4830854535102844
train gradient:  0.5043344938060931
iteration : 935
train acc:  0.8359375
train loss:  0.41599589586257935
train gradient:  0.3447403072863166
iteration : 936
train acc:  0.7890625
train loss:  0.49565356969833374
train gradient:  0.4959874031060602
iteration : 937
train acc:  0.84375
train loss:  0.42788803577423096
train gradient:  0.568853967109417
iteration : 938
train acc:  0.7734375
train loss:  0.4908734858036041
train gradient:  0.4749292515254043
iteration : 939
train acc:  0.8046875
train loss:  0.42021602392196655
train gradient:  0.49278878329111975
iteration : 940
train acc:  0.828125
train loss:  0.41324755549430847
train gradient:  0.4189350184725286
iteration : 941
train acc:  0.8125
train loss:  0.44401633739471436
train gradient:  0.48230985987732006
iteration : 942
train acc:  0.8125
train loss:  0.4008137583732605
train gradient:  0.3173363845905574
iteration : 943
train acc:  0.78125
train loss:  0.4167579412460327
train gradient:  0.41391569121511357
iteration : 944
train acc:  0.7890625
train loss:  0.4612058699131012
train gradient:  0.6600051876449882
iteration : 945
train acc:  0.7578125
train loss:  0.4972478747367859
train gradient:  0.5916658936344852
iteration : 946
train acc:  0.8125
train loss:  0.4126117527484894
train gradient:  0.33499052770103804
iteration : 947
train acc:  0.78125
train loss:  0.41780349612236023
train gradient:  0.4485105955679565
iteration : 948
train acc:  0.75
train loss:  0.4828947186470032
train gradient:  0.7863230807225035
iteration : 949
train acc:  0.7890625
train loss:  0.45098885893821716
train gradient:  0.5964754723650596
iteration : 950
train acc:  0.8046875
train loss:  0.43373748660087585
train gradient:  0.382628243111121
iteration : 951
train acc:  0.7890625
train loss:  0.4483955502510071
train gradient:  0.5193583650493434
iteration : 952
train acc:  0.7890625
train loss:  0.4912512004375458
train gradient:  0.4179341219222906
iteration : 953
train acc:  0.796875
train loss:  0.4283601641654968
train gradient:  0.3891221811522261
iteration : 954
train acc:  0.75
train loss:  0.4746187925338745
train gradient:  0.33830170990817265
iteration : 955
train acc:  0.796875
train loss:  0.5100628137588501
train gradient:  0.5420847082109971
iteration : 956
train acc:  0.796875
train loss:  0.41002631187438965
train gradient:  0.38247867059528146
iteration : 957
train acc:  0.703125
train loss:  0.5420152544975281
train gradient:  0.6634221208789943
iteration : 958
train acc:  0.8125
train loss:  0.42859840393066406
train gradient:  0.5227298080044923
iteration : 959
train acc:  0.78125
train loss:  0.47428983449935913
train gradient:  0.2510151282109538
iteration : 960
train acc:  0.8515625
train loss:  0.3768966794013977
train gradient:  0.46465241598954565
iteration : 961
train acc:  0.765625
train loss:  0.4365825355052948
train gradient:  0.49490358937260004
iteration : 962
train acc:  0.78125
train loss:  0.44876328110694885
train gradient:  0.3391650744128887
iteration : 963
train acc:  0.7734375
train loss:  0.4658971130847931
train gradient:  0.5862231585685721
iteration : 964
train acc:  0.7421875
train loss:  0.492072194814682
train gradient:  0.47849957991932185
iteration : 965
train acc:  0.796875
train loss:  0.4648160934448242
train gradient:  0.4406152894026525
iteration : 966
train acc:  0.8125
train loss:  0.38719213008880615
train gradient:  0.3269894650731239
iteration : 967
train acc:  0.75
train loss:  0.5058318972587585
train gradient:  0.6763967695647656
iteration : 968
train acc:  0.7265625
train loss:  0.5557136535644531
train gradient:  0.5397731524175529
iteration : 969
train acc:  0.78125
train loss:  0.4740186929702759
train gradient:  0.5006418786610506
iteration : 970
train acc:  0.78125
train loss:  0.4358549416065216
train gradient:  0.5244351045463402
iteration : 971
train acc:  0.8203125
train loss:  0.4182894229888916
train gradient:  0.6172577115443929
iteration : 972
train acc:  0.7890625
train loss:  0.440738320350647
train gradient:  0.3736977786409541
iteration : 973
train acc:  0.8671875
train loss:  0.3541632890701294
train gradient:  0.35107320551233656
iteration : 974
train acc:  0.796875
train loss:  0.41182631254196167
train gradient:  0.37980139462553403
iteration : 975
train acc:  0.828125
train loss:  0.4002806544303894
train gradient:  0.39909320486766053
iteration : 976
train acc:  0.765625
train loss:  0.46614277362823486
train gradient:  0.6811187999933399
iteration : 977
train acc:  0.8046875
train loss:  0.4516461491584778
train gradient:  0.48899205005172586
iteration : 978
train acc:  0.765625
train loss:  0.46356213092803955
train gradient:  0.38478029054849483
iteration : 979
train acc:  0.734375
train loss:  0.4775218367576599
train gradient:  0.4009951460088489
iteration : 980
train acc:  0.828125
train loss:  0.4432455897331238
train gradient:  0.5236919567066861
iteration : 981
train acc:  0.8125
train loss:  0.4658992886543274
train gradient:  0.4820746012530272
iteration : 982
train acc:  0.8046875
train loss:  0.44382989406585693
train gradient:  0.5023657186702182
iteration : 983
train acc:  0.7890625
train loss:  0.440187007188797
train gradient:  0.37077518673431786
iteration : 984
train acc:  0.7578125
train loss:  0.5511635541915894
train gradient:  0.8731302445132084
iteration : 985
train acc:  0.8046875
train loss:  0.480457603931427
train gradient:  0.6972846568603518
iteration : 986
train acc:  0.7265625
train loss:  0.5798618793487549
train gradient:  0.828877097883888
iteration : 987
train acc:  0.7578125
train loss:  0.48170462250709534
train gradient:  0.8714457011679696
iteration : 988
train acc:  0.8125
train loss:  0.4027983844280243
train gradient:  0.5780832731701184
iteration : 989
train acc:  0.8125
train loss:  0.4621444344520569
train gradient:  0.5099058772248132
iteration : 990
train acc:  0.7734375
train loss:  0.48641881346702576
train gradient:  0.7162653142602904
iteration : 991
train acc:  0.765625
train loss:  0.4942743182182312
train gradient:  0.6157708798464244
iteration : 992
train acc:  0.796875
train loss:  0.43200862407684326
train gradient:  0.5311430169405778
iteration : 993
train acc:  0.7578125
train loss:  0.44779568910598755
train gradient:  0.5492522406052276
iteration : 994
train acc:  0.8125
train loss:  0.4426368772983551
train gradient:  0.5465904750539509
iteration : 995
train acc:  0.8046875
train loss:  0.42613887786865234
train gradient:  0.3659477064428336
iteration : 996
train acc:  0.859375
train loss:  0.35666579008102417
train gradient:  0.3688975818713884
iteration : 997
train acc:  0.8359375
train loss:  0.3814849257469177
train gradient:  0.45067690968706414
iteration : 998
train acc:  0.7578125
train loss:  0.4794991612434387
train gradient:  0.5413369313628866
iteration : 999
train acc:  0.8359375
train loss:  0.4213019907474518
train gradient:  0.6583525661478165
iteration : 1000
train acc:  0.8359375
train loss:  0.42878463864326477
train gradient:  0.48365512557264406
iteration : 1001
train acc:  0.7890625
train loss:  0.4347861111164093
train gradient:  0.42614911645403614
iteration : 1002
train acc:  0.7890625
train loss:  0.48060786724090576
train gradient:  0.8640346775195729
iteration : 1003
train acc:  0.765625
train loss:  0.5049504041671753
train gradient:  0.5481738876229814
iteration : 1004
train acc:  0.8046875
train loss:  0.4504256546497345
train gradient:  0.49550596078497955
iteration : 1005
train acc:  0.7265625
train loss:  0.4886992871761322
train gradient:  0.58759829795592
iteration : 1006
train acc:  0.7734375
train loss:  0.4666016101837158
train gradient:  0.5679770745255588
iteration : 1007
train acc:  0.8359375
train loss:  0.35397106409072876
train gradient:  0.3663406011972064
iteration : 1008
train acc:  0.8515625
train loss:  0.441521018743515
train gradient:  0.48556002155877787
iteration : 1009
train acc:  0.8046875
train loss:  0.4716764986515045
train gradient:  0.5660818862190901
iteration : 1010
train acc:  0.78125
train loss:  0.49244630336761475
train gradient:  0.5555413532269244
iteration : 1011
train acc:  0.7578125
train loss:  0.46635714173316956
train gradient:  0.5161889220077627
iteration : 1012
train acc:  0.7890625
train loss:  0.42077672481536865
train gradient:  0.694293042399578
iteration : 1013
train acc:  0.796875
train loss:  0.4395115077495575
train gradient:  0.5970652097706955
iteration : 1014
train acc:  0.78125
train loss:  0.48864126205444336
train gradient:  0.5886654017660853
iteration : 1015
train acc:  0.828125
train loss:  0.4103342890739441
train gradient:  0.3708293728342338
iteration : 1016
train acc:  0.828125
train loss:  0.3849317133426666
train gradient:  0.4024503356191534
iteration : 1017
train acc:  0.8203125
train loss:  0.44766274094581604
train gradient:  0.7235410281749423
iteration : 1018
train acc:  0.7734375
train loss:  0.4140242040157318
train gradient:  0.457407103472751
iteration : 1019
train acc:  0.75
train loss:  0.4893642067909241
train gradient:  0.5912577666349224
iteration : 1020
train acc:  0.7734375
train loss:  0.43328869342803955
train gradient:  0.5439551602391284
iteration : 1021
train acc:  0.78125
train loss:  0.4926416575908661
train gradient:  0.5166143994390819
iteration : 1022
train acc:  0.796875
train loss:  0.4283936023712158
train gradient:  0.5974955197640384
iteration : 1023
train acc:  0.828125
train loss:  0.4214455485343933
train gradient:  0.45949611434132964
iteration : 1024
train acc:  0.7421875
train loss:  0.5204334259033203
train gradient:  0.6507272622476277
iteration : 1025
train acc:  0.8203125
train loss:  0.39141544699668884
train gradient:  0.3926264232437493
iteration : 1026
train acc:  0.828125
train loss:  0.41823074221611023
train gradient:  0.36912802629358676
iteration : 1027
train acc:  0.828125
train loss:  0.4028860628604889
train gradient:  0.40878036006868707
iteration : 1028
train acc:  0.8046875
train loss:  0.4294559359550476
train gradient:  0.6586567953658539
iteration : 1029
train acc:  0.7421875
train loss:  0.49244189262390137
train gradient:  0.6502016510060455
iteration : 1030
train acc:  0.8125
train loss:  0.3876790404319763
train gradient:  0.4982094470257098
iteration : 1031
train acc:  0.828125
train loss:  0.423525333404541
train gradient:  0.6767902368424075
iteration : 1032
train acc:  0.75
train loss:  0.4503200054168701
train gradient:  0.5976510259508185
iteration : 1033
train acc:  0.8046875
train loss:  0.42085516452789307
train gradient:  0.4975456072018285
iteration : 1034
train acc:  0.875
train loss:  0.36469167470932007
train gradient:  0.3773758904036384
iteration : 1035
train acc:  0.765625
train loss:  0.434856116771698
train gradient:  0.47685028274834085
iteration : 1036
train acc:  0.75
train loss:  0.5615909695625305
train gradient:  0.6826954239184031
iteration : 1037
train acc:  0.8046875
train loss:  0.44139865040779114
train gradient:  0.46242291849252937
iteration : 1038
train acc:  0.796875
train loss:  0.46153390407562256
train gradient:  0.5623377538472114
iteration : 1039
train acc:  0.7265625
train loss:  0.45377060770988464
train gradient:  0.7077147582896486
iteration : 1040
train acc:  0.7734375
train loss:  0.4528634250164032
train gradient:  0.48860736144749667
iteration : 1041
train acc:  0.8515625
train loss:  0.3673880696296692
train gradient:  0.39244368039630656
iteration : 1042
train acc:  0.7890625
train loss:  0.44709300994873047
train gradient:  0.4069200016802849
iteration : 1043
train acc:  0.796875
train loss:  0.4487040936946869
train gradient:  0.528507286484787
iteration : 1044
train acc:  0.7421875
train loss:  0.5101046562194824
train gradient:  0.600830343780655
iteration : 1045
train acc:  0.828125
train loss:  0.42006567120552063
train gradient:  0.5286497300201933
iteration : 1046
train acc:  0.78125
train loss:  0.40982529520988464
train gradient:  0.42506923924001067
iteration : 1047
train acc:  0.796875
train loss:  0.41008812189102173
train gradient:  0.5098571687552531
iteration : 1048
train acc:  0.765625
train loss:  0.4605266749858856
train gradient:  0.49704666761511573
iteration : 1049
train acc:  0.8203125
train loss:  0.34583544731140137
train gradient:  0.4049837605147846
iteration : 1050
train acc:  0.765625
train loss:  0.5439964532852173
train gradient:  0.6975958491163428
iteration : 1051
train acc:  0.8203125
train loss:  0.4210900366306305
train gradient:  0.5187354009328976
iteration : 1052
train acc:  0.78125
train loss:  0.455681174993515
train gradient:  0.47742596071510923
iteration : 1053
train acc:  0.7578125
train loss:  0.5009469985961914
train gradient:  0.6044086708265669
iteration : 1054
train acc:  0.828125
train loss:  0.44502365589141846
train gradient:  0.5588989464272232
iteration : 1055
train acc:  0.8046875
train loss:  0.43562397360801697
train gradient:  0.47221392854367356
iteration : 1056
train acc:  0.78125
train loss:  0.4417468011379242
train gradient:  0.47325333827854715
iteration : 1057
train acc:  0.8046875
train loss:  0.42670515179634094
train gradient:  0.4101523273027461
iteration : 1058
train acc:  0.765625
train loss:  0.4624468684196472
train gradient:  0.5017273202927335
iteration : 1059
train acc:  0.828125
train loss:  0.41408008337020874
train gradient:  0.44100692244201417
iteration : 1060
train acc:  0.7734375
train loss:  0.4826865792274475
train gradient:  0.5093186393338576
iteration : 1061
train acc:  0.765625
train loss:  0.48644188046455383
train gradient:  0.6020524112073233
iteration : 1062
train acc:  0.78125
train loss:  0.4577893018722534
train gradient:  0.6802537068493916
iteration : 1063
train acc:  0.7578125
train loss:  0.5047615766525269
train gradient:  0.5529117114536835
iteration : 1064
train acc:  0.7890625
train loss:  0.47075194120407104
train gradient:  0.5139159939178599
iteration : 1065
train acc:  0.7578125
train loss:  0.43956607580184937
train gradient:  0.6150268006910189
iteration : 1066
train acc:  0.828125
train loss:  0.44095224142074585
train gradient:  0.5689375756958541
iteration : 1067
train acc:  0.84375
train loss:  0.38951027393341064
train gradient:  0.5380776187882277
iteration : 1068
train acc:  0.7890625
train loss:  0.5006991624832153
train gradient:  0.5340470418395509
iteration : 1069
train acc:  0.765625
train loss:  0.4305846095085144
train gradient:  0.35695114342400047
iteration : 1070
train acc:  0.796875
train loss:  0.43663060665130615
train gradient:  0.5244106892184592
iteration : 1071
train acc:  0.8203125
train loss:  0.4323562979698181
train gradient:  0.41835145663912543
iteration : 1072
train acc:  0.8203125
train loss:  0.40553632378578186
train gradient:  0.3998306932599834
iteration : 1073
train acc:  0.8515625
train loss:  0.41134119033813477
train gradient:  0.6096189414221693
iteration : 1074
train acc:  0.8359375
train loss:  0.3989430069923401
train gradient:  0.4496503709671011
iteration : 1075
train acc:  0.8515625
train loss:  0.3800491690635681
train gradient:  0.48529076211108857
iteration : 1076
train acc:  0.7109375
train loss:  0.547315776348114
train gradient:  0.6161922164607143
iteration : 1077
train acc:  0.796875
train loss:  0.4689929485321045
train gradient:  0.738248419834925
iteration : 1078
train acc:  0.78125
train loss:  0.42670369148254395
train gradient:  0.5715801527365976
iteration : 1079
train acc:  0.78125
train loss:  0.43893009424209595
train gradient:  0.5487949912002127
iteration : 1080
train acc:  0.8125
train loss:  0.38536399602890015
train gradient:  0.44284490549753186
iteration : 1081
train acc:  0.75
train loss:  0.4728850722312927
train gradient:  0.5568952262584735
iteration : 1082
train acc:  0.8671875
train loss:  0.3608057498931885
train gradient:  0.40599894078595344
iteration : 1083
train acc:  0.8359375
train loss:  0.410641074180603
train gradient:  0.36895144115806694
iteration : 1084
train acc:  0.8046875
train loss:  0.4461137354373932
train gradient:  0.3930142138017264
iteration : 1085
train acc:  0.828125
train loss:  0.43150895833969116
train gradient:  0.3777812447076842
iteration : 1086
train acc:  0.7890625
train loss:  0.39950260519981384
train gradient:  0.3179710226321467
iteration : 1087
train acc:  0.8203125
train loss:  0.4143688976764679
train gradient:  0.46064578107334025
iteration : 1088
train acc:  0.828125
train loss:  0.4189377427101135
train gradient:  0.4370847154479768
iteration : 1089
train acc:  0.7578125
train loss:  0.41565507650375366
train gradient:  0.5383593925297363
iteration : 1090
train acc:  0.796875
train loss:  0.43174368143081665
train gradient:  0.5058034223582433
iteration : 1091
train acc:  0.7578125
train loss:  0.5394132137298584
train gradient:  0.7062458817749715
iteration : 1092
train acc:  0.78125
train loss:  0.4908864498138428
train gradient:  0.5962808630436754
iteration : 1093
train acc:  0.8046875
train loss:  0.45734602212905884
train gradient:  0.534873948371223
iteration : 1094
train acc:  0.765625
train loss:  0.499515563249588
train gradient:  0.5358566453112705
iteration : 1095
train acc:  0.7890625
train loss:  0.4032973349094391
train gradient:  0.3507750187449394
iteration : 1096
train acc:  0.796875
train loss:  0.416107714176178
train gradient:  0.34702461938791357
iteration : 1097
train acc:  0.7734375
train loss:  0.4624510109424591
train gradient:  0.4988962190173583
iteration : 1098
train acc:  0.84375
train loss:  0.37557023763656616
train gradient:  0.316845511523328
iteration : 1099
train acc:  0.765625
train loss:  0.47563329339027405
train gradient:  0.5536429798846182
iteration : 1100
train acc:  0.78125
train loss:  0.3959599733352661
train gradient:  0.33618530909194777
iteration : 1101
train acc:  0.7421875
train loss:  0.5368683338165283
train gradient:  0.6566814754753152
iteration : 1102
train acc:  0.7734375
train loss:  0.48887115716934204
train gradient:  0.5824303107360245
iteration : 1103
train acc:  0.78125
train loss:  0.5072172284126282
train gradient:  0.6095508234145487
iteration : 1104
train acc:  0.796875
train loss:  0.45854437351226807
train gradient:  0.6119768422033939
iteration : 1105
train acc:  0.8046875
train loss:  0.39182165265083313
train gradient:  0.37101119268207156
iteration : 1106
train acc:  0.7578125
train loss:  0.49842971563339233
train gradient:  0.5538317709070174
iteration : 1107
train acc:  0.796875
train loss:  0.4822012186050415
train gradient:  0.46385599427003027
iteration : 1108
train acc:  0.8125
train loss:  0.37493056058883667
train gradient:  0.3467967759260312
iteration : 1109
train acc:  0.7265625
train loss:  0.4736047089099884
train gradient:  0.504617425917328
iteration : 1110
train acc:  0.8671875
train loss:  0.36188024282455444
train gradient:  0.38806674229572813
iteration : 1111
train acc:  0.828125
train loss:  0.42631518840789795
train gradient:  0.4000187666667488
iteration : 1112
train acc:  0.734375
train loss:  0.5434170961380005
train gradient:  0.9116864231964069
iteration : 1113
train acc:  0.8046875
train loss:  0.41673043370246887
train gradient:  0.47228156618220907
iteration : 1114
train acc:  0.78125
train loss:  0.4877638816833496
train gradient:  0.6450921458786318
iteration : 1115
train acc:  0.7734375
train loss:  0.42564934492111206
train gradient:  0.5508304963244273
iteration : 1116
train acc:  0.8359375
train loss:  0.38554346561431885
train gradient:  0.2929093736489655
iteration : 1117
train acc:  0.8125
train loss:  0.44869619607925415
train gradient:  0.5131573430691547
iteration : 1118
train acc:  0.765625
train loss:  0.5168107748031616
train gradient:  0.618449810228045
iteration : 1119
train acc:  0.8125
train loss:  0.4518408477306366
train gradient:  0.5957020390176067
iteration : 1120
train acc:  0.7734375
train loss:  0.47808024287223816
train gradient:  0.5043122203304284
iteration : 1121
train acc:  0.78125
train loss:  0.47143763303756714
train gradient:  0.5524752996097171
iteration : 1122
train acc:  0.8046875
train loss:  0.4020874500274658
train gradient:  0.3467494319926647
iteration : 1123
train acc:  0.828125
train loss:  0.41973796486854553
train gradient:  0.5811732469933841
iteration : 1124
train acc:  0.8046875
train loss:  0.44073083996772766
train gradient:  0.4869380434007612
iteration : 1125
train acc:  0.75
train loss:  0.45646220445632935
train gradient:  0.5044021068720861
iteration : 1126
train acc:  0.8125
train loss:  0.4358784854412079
train gradient:  0.47759306525774536
iteration : 1127
train acc:  0.8203125
train loss:  0.43462684750556946
train gradient:  0.44649958852109056
iteration : 1128
train acc:  0.78125
train loss:  0.4049844443798065
train gradient:  0.39354893800273855
iteration : 1129
train acc:  0.8046875
train loss:  0.44218429923057556
train gradient:  0.6406397292913779
iteration : 1130
train acc:  0.828125
train loss:  0.40568313002586365
train gradient:  0.6530288603885486
iteration : 1131
train acc:  0.734375
train loss:  0.5313049554824829
train gradient:  0.9235125818893655
iteration : 1132
train acc:  0.78125
train loss:  0.48061028122901917
train gradient:  0.5900879735830914
iteration : 1133
train acc:  0.8671875
train loss:  0.3220595121383667
train gradient:  0.30962958297002696
iteration : 1134
train acc:  0.875
train loss:  0.323042631149292
train gradient:  0.3182288624916409
iteration : 1135
train acc:  0.7734375
train loss:  0.43961966037750244
train gradient:  0.4558086901705302
iteration : 1136
train acc:  0.8046875
train loss:  0.4086368680000305
train gradient:  0.4615290437975376
iteration : 1137
train acc:  0.8125
train loss:  0.4180670380592346
train gradient:  0.43838400735344807
iteration : 1138
train acc:  0.765625
train loss:  0.45438867807388306
train gradient:  0.48786572220184965
iteration : 1139
train acc:  0.796875
train loss:  0.42982250452041626
train gradient:  0.5130210235537664
iteration : 1140
train acc:  0.78125
train loss:  0.43328845500946045
train gradient:  0.4209157930463518
iteration : 1141
train acc:  0.796875
train loss:  0.41523316502571106
train gradient:  0.46035175544785845
iteration : 1142
train acc:  0.8046875
train loss:  0.43103834986686707
train gradient:  0.4544982706513248
iteration : 1143
train acc:  0.828125
train loss:  0.43739503622055054
train gradient:  0.5418540393836793
iteration : 1144
train acc:  0.84375
train loss:  0.39690011739730835
train gradient:  0.4225860246312528
iteration : 1145
train acc:  0.8046875
train loss:  0.4338468611240387
train gradient:  0.4189339320017468
iteration : 1146
train acc:  0.828125
train loss:  0.37612682580947876
train gradient:  0.43462764417702543
iteration : 1147
train acc:  0.7734375
train loss:  0.4186849594116211
train gradient:  0.4832049160815558
iteration : 1148
train acc:  0.7734375
train loss:  0.40842169523239136
train gradient:  0.4972494926090893
iteration : 1149
train acc:  0.765625
train loss:  0.510148286819458
train gradient:  0.5443967287863698
iteration : 1150
train acc:  0.7734375
train loss:  0.4490908980369568
train gradient:  0.4662808916208477
iteration : 1151
train acc:  0.875
train loss:  0.32097625732421875
train gradient:  0.3425000764869714
iteration : 1152
train acc:  0.8046875
train loss:  0.44280707836151123
train gradient:  0.4317809247436882
iteration : 1153
train acc:  0.7734375
train loss:  0.4495137929916382
train gradient:  0.4994653018282517
iteration : 1154
train acc:  0.7890625
train loss:  0.4390854239463806
train gradient:  0.4800338633932693
iteration : 1155
train acc:  0.875
train loss:  0.34233540296554565
train gradient:  0.3541705299140666
iteration : 1156
train acc:  0.7421875
train loss:  0.5205460786819458
train gradient:  0.8510653820993983
iteration : 1157
train acc:  0.7578125
train loss:  0.4815181493759155
train gradient:  0.6135864244439414
iteration : 1158
train acc:  0.7890625
train loss:  0.46616125106811523
train gradient:  0.5024561033762287
iteration : 1159
train acc:  0.78125
train loss:  0.48368000984191895
train gradient:  0.8299836920330226
iteration : 1160
train acc:  0.8359375
train loss:  0.3747544288635254
train gradient:  0.3523940900379361
iteration : 1161
train acc:  0.8515625
train loss:  0.3866082429885864
train gradient:  0.3293419405577875
iteration : 1162
train acc:  0.8125
train loss:  0.4072303771972656
train gradient:  0.39980295387913517
iteration : 1163
train acc:  0.734375
train loss:  0.5040587186813354
train gradient:  0.5913388113246694
iteration : 1164
train acc:  0.8203125
train loss:  0.45769649744033813
train gradient:  0.6485955573677153
iteration : 1165
train acc:  0.84375
train loss:  0.3973231613636017
train gradient:  0.40597015966411154
iteration : 1166
train acc:  0.7734375
train loss:  0.5333433151245117
train gradient:  0.6756570066499694
iteration : 1167
train acc:  0.8203125
train loss:  0.4298095703125
train gradient:  0.4858517465074461
iteration : 1168
train acc:  0.765625
train loss:  0.523202657699585
train gradient:  0.4828806804983333
iteration : 1169
train acc:  0.796875
train loss:  0.42919379472732544
train gradient:  0.4293344427739918
iteration : 1170
train acc:  0.8125
train loss:  0.4121038317680359
train gradient:  0.3170438502548876
iteration : 1171
train acc:  0.8046875
train loss:  0.4570797383785248
train gradient:  0.5532125644731225
iteration : 1172
train acc:  0.8515625
train loss:  0.33489492535591125
train gradient:  0.33178098949585205
iteration : 1173
train acc:  0.75
train loss:  0.4885326623916626
train gradient:  0.5809745640204694
iteration : 1174
train acc:  0.828125
train loss:  0.4493492841720581
train gradient:  0.6922175351670619
iteration : 1175
train acc:  0.859375
train loss:  0.3559999465942383
train gradient:  0.3249086536400778
iteration : 1176
train acc:  0.8046875
train loss:  0.41265490651130676
train gradient:  0.556721118542588
iteration : 1177
train acc:  0.796875
train loss:  0.4211397171020508
train gradient:  0.5206501242159574
iteration : 1178
train acc:  0.765625
train loss:  0.5262846946716309
train gradient:  0.5768284809444637
iteration : 1179
train acc:  0.78125
train loss:  0.49118471145629883
train gradient:  0.5982104674548401
iteration : 1180
train acc:  0.8125
train loss:  0.38744568824768066
train gradient:  0.3439466697435382
iteration : 1181
train acc:  0.7734375
train loss:  0.43039795756340027
train gradient:  0.35865557925918207
iteration : 1182
train acc:  0.7578125
train loss:  0.4544551372528076
train gradient:  0.607445607415221
iteration : 1183
train acc:  0.8125
train loss:  0.43668076395988464
train gradient:  0.4879157088034959
iteration : 1184
train acc:  0.765625
train loss:  0.41512030363082886
train gradient:  0.5142226381305091
iteration : 1185
train acc:  0.8125
train loss:  0.41622087359428406
train gradient:  0.4212008382299631
iteration : 1186
train acc:  0.828125
train loss:  0.3907914161682129
train gradient:  0.4182356139727536
iteration : 1187
train acc:  0.8125
train loss:  0.43404021859169006
train gradient:  0.438909878612148
iteration : 1188
train acc:  0.765625
train loss:  0.514649510383606
train gradient:  0.7503174943696586
iteration : 1189
train acc:  0.8203125
train loss:  0.38570883870124817
train gradient:  0.49096646640774405
iteration : 1190
train acc:  0.75
train loss:  0.48387205600738525
train gradient:  0.5112171709759484
iteration : 1191
train acc:  0.8203125
train loss:  0.40709298849105835
train gradient:  0.4086815630585428
iteration : 1192
train acc:  0.7578125
train loss:  0.4320278763771057
train gradient:  0.4771189062005261
iteration : 1193
train acc:  0.765625
train loss:  0.5075901746749878
train gradient:  0.5549827519253434
iteration : 1194
train acc:  0.7890625
train loss:  0.45370200276374817
train gradient:  0.43965641889596546
iteration : 1195
train acc:  0.8203125
train loss:  0.3952978849411011
train gradient:  0.5083238349856162
iteration : 1196
train acc:  0.8203125
train loss:  0.40131688117980957
train gradient:  0.40979161786445784
iteration : 1197
train acc:  0.8125
train loss:  0.40349453687667847
train gradient:  0.4565687038758841
iteration : 1198
train acc:  0.8125
train loss:  0.4073038101196289
train gradient:  0.4929447758362369
iteration : 1199
train acc:  0.828125
train loss:  0.42552122473716736
train gradient:  0.5827423619534852
iteration : 1200
train acc:  0.859375
train loss:  0.3292884826660156
train gradient:  0.3849469262006214
iteration : 1201
train acc:  0.78125
train loss:  0.46512243151664734
train gradient:  0.5606854769279929
iteration : 1202
train acc:  0.8515625
train loss:  0.3412686288356781
train gradient:  0.3754550319616091
iteration : 1203
train acc:  0.78125
train loss:  0.4637199640274048
train gradient:  0.5448251795692954
iteration : 1204
train acc:  0.8203125
train loss:  0.39328664541244507
train gradient:  0.4663111594248302
iteration : 1205
train acc:  0.75
train loss:  0.5180985331535339
train gradient:  0.8096302088349008
iteration : 1206
train acc:  0.7890625
train loss:  0.4243362843990326
train gradient:  0.6631244213620082
iteration : 1207
train acc:  0.8203125
train loss:  0.4483349323272705
train gradient:  0.4462891088673217
iteration : 1208
train acc:  0.6953125
train loss:  0.5997375249862671
train gradient:  0.7654456636657598
iteration : 1209
train acc:  0.8515625
train loss:  0.3851766288280487
train gradient:  0.3546869331565371
iteration : 1210
train acc:  0.765625
train loss:  0.41826438903808594
train gradient:  0.5307995054716386
iteration : 1211
train acc:  0.8359375
train loss:  0.35779744386672974
train gradient:  0.31536842883766547
iteration : 1212
train acc:  0.75
train loss:  0.4730076193809509
train gradient:  0.6968326637062171
iteration : 1213
train acc:  0.796875
train loss:  0.42948779463768005
train gradient:  0.476348787026179
iteration : 1214
train acc:  0.8203125
train loss:  0.42523425817489624
train gradient:  0.32226104023927277
iteration : 1215
train acc:  0.859375
train loss:  0.3435767590999603
train gradient:  0.4702922271408579
iteration : 1216
train acc:  0.78125
train loss:  0.4035983681678772
train gradient:  0.4127018390938081
iteration : 1217
train acc:  0.765625
train loss:  0.49520301818847656
train gradient:  0.7523506450390877
iteration : 1218
train acc:  0.7578125
train loss:  0.5148802399635315
train gradient:  0.7964081600397651
iteration : 1219
train acc:  0.7734375
train loss:  0.45381060242652893
train gradient:  0.644669010053085
iteration : 1220
train acc:  0.8515625
train loss:  0.37179678678512573
train gradient:  0.44680732029006176
iteration : 1221
train acc:  0.765625
train loss:  0.49864715337753296
train gradient:  0.76096114121406
iteration : 1222
train acc:  0.765625
train loss:  0.5198627710342407
train gradient:  0.526079778251084
iteration : 1223
train acc:  0.7734375
train loss:  0.43305540084838867
train gradient:  0.38284003868887057
iteration : 1224
train acc:  0.828125
train loss:  0.39909982681274414
train gradient:  0.6493508933199672
iteration : 1225
train acc:  0.7265625
train loss:  0.4699268937110901
train gradient:  0.5888600603188674
iteration : 1226
train acc:  0.78125
train loss:  0.45703375339508057
train gradient:  0.586159627764477
iteration : 1227
train acc:  0.828125
train loss:  0.33885836601257324
train gradient:  0.5049129529204714
iteration : 1228
train acc:  0.8203125
train loss:  0.41996341943740845
train gradient:  0.5227710684938982
iteration : 1229
train acc:  0.8203125
train loss:  0.43126624822616577
train gradient:  0.42219483412802183
iteration : 1230
train acc:  0.875
train loss:  0.4124412536621094
train gradient:  0.7896723690912573
iteration : 1231
train acc:  0.7421875
train loss:  0.5070815086364746
train gradient:  0.7142370588995786
iteration : 1232
train acc:  0.8125
train loss:  0.3715190291404724
train gradient:  0.5126571968907973
iteration : 1233
train acc:  0.8125
train loss:  0.39824923872947693
train gradient:  0.499374946243678
iteration : 1234
train acc:  0.8125
train loss:  0.39652472734451294
train gradient:  0.4753712378204428
iteration : 1235
train acc:  0.7734375
train loss:  0.47141966223716736
train gradient:  0.6935716750190403
iteration : 1236
train acc:  0.7890625
train loss:  0.4397081136703491
train gradient:  0.5146857596846356
iteration : 1237
train acc:  0.84375
train loss:  0.41318774223327637
train gradient:  0.5368179325528823
iteration : 1238
train acc:  0.8671875
train loss:  0.348302960395813
train gradient:  0.3488666479086649
iteration : 1239
train acc:  0.8046875
train loss:  0.46943575143814087
train gradient:  0.6386900750611506
iteration : 1240
train acc:  0.7890625
train loss:  0.4763564169406891
train gradient:  0.6719269559401547
iteration : 1241
train acc:  0.8203125
train loss:  0.39949095249176025
train gradient:  0.488541327034318
iteration : 1242
train acc:  0.84375
train loss:  0.38656145334243774
train gradient:  0.39862016505984743
iteration : 1243
train acc:  0.8125
train loss:  0.41918396949768066
train gradient:  0.548672387498516
iteration : 1244
train acc:  0.8046875
train loss:  0.4160277545452118
train gradient:  0.48568533100016825
iteration : 1245
train acc:  0.7421875
train loss:  0.5018599033355713
train gradient:  0.6250060516536828
iteration : 1246
train acc:  0.734375
train loss:  0.4612368941307068
train gradient:  0.5365790140134733
iteration : 1247
train acc:  0.796875
train loss:  0.41115182638168335
train gradient:  0.49248254295390204
iteration : 1248
train acc:  0.7265625
train loss:  0.5239776372909546
train gradient:  0.5085090794871958
iteration : 1249
train acc:  0.7734375
train loss:  0.46902257204055786
train gradient:  0.5228159240426519
iteration : 1250
train acc:  0.78125
train loss:  0.4069945812225342
train gradient:  0.41851694128461603
iteration : 1251
train acc:  0.796875
train loss:  0.38251882791519165
train gradient:  0.33127864489899844
iteration : 1252
train acc:  0.734375
train loss:  0.4516061842441559
train gradient:  0.40559371927009996
iteration : 1253
train acc:  0.796875
train loss:  0.4659518301486969
train gradient:  0.3528110652895954
iteration : 1254
train acc:  0.859375
train loss:  0.3606303632259369
train gradient:  0.26054495001225286
iteration : 1255
train acc:  0.828125
train loss:  0.31741806864738464
train gradient:  0.2597941077741677
iteration : 1256
train acc:  0.828125
train loss:  0.4510050415992737
train gradient:  0.5268299627414845
iteration : 1257
train acc:  0.78125
train loss:  0.44843775033950806
train gradient:  0.4529039892893351
iteration : 1258
train acc:  0.8203125
train loss:  0.39523687958717346
train gradient:  0.33882492576679296
iteration : 1259
train acc:  0.796875
train loss:  0.46019992232322693
train gradient:  0.5245975594882331
iteration : 1260
train acc:  0.8203125
train loss:  0.44519901275634766
train gradient:  0.5479794824805917
iteration : 1261
train acc:  0.8046875
train loss:  0.40104925632476807
train gradient:  0.377748517257959
iteration : 1262
train acc:  0.75
train loss:  0.49039867520332336
train gradient:  0.7132279514679112
iteration : 1263
train acc:  0.8359375
train loss:  0.36488592624664307
train gradient:  0.3088754253016555
iteration : 1264
train acc:  0.7734375
train loss:  0.46334174275398254
train gradient:  0.5568633818561125
iteration : 1265
train acc:  0.78125
train loss:  0.4336138069629669
train gradient:  0.537229563981241
iteration : 1266
train acc:  0.8515625
train loss:  0.31628626585006714
train gradient:  0.24610193182063173
iteration : 1267
train acc:  0.8125
train loss:  0.350393682718277
train gradient:  0.3737447635064413
iteration : 1268
train acc:  0.8125
train loss:  0.42962831258773804
train gradient:  0.5065442649334976
iteration : 1269
train acc:  0.7734375
train loss:  0.47834181785583496
train gradient:  0.5699821553626951
iteration : 1270
train acc:  0.875
train loss:  0.33481565117836
train gradient:  0.3140268152747742
iteration : 1271
train acc:  0.78125
train loss:  0.4263797104358673
train gradient:  0.4066431153842233
iteration : 1272
train acc:  0.8203125
train loss:  0.3819795548915863
train gradient:  0.4258000609964932
iteration : 1273
train acc:  0.78125
train loss:  0.39754635095596313
train gradient:  0.33438053049802396
iteration : 1274
train acc:  0.796875
train loss:  0.4454859793186188
train gradient:  0.5894009363372519
iteration : 1275
train acc:  0.7890625
train loss:  0.4489835202693939
train gradient:  0.504778009797493
iteration : 1276
train acc:  0.8125
train loss:  0.4197744131088257
train gradient:  0.5413999060239318
iteration : 1277
train acc:  0.7734375
train loss:  0.43135860562324524
train gradient:  0.4613051425781586
iteration : 1278
train acc:  0.8203125
train loss:  0.3992927074432373
train gradient:  0.4300764457063216
iteration : 1279
train acc:  0.8125
train loss:  0.4367261826992035
train gradient:  0.5238733883759807
iteration : 1280
train acc:  0.78125
train loss:  0.4562768340110779
train gradient:  0.44207998434683476
iteration : 1281
train acc:  0.8046875
train loss:  0.41052892804145813
train gradient:  0.4340107300308591
iteration : 1282
train acc:  0.8046875
train loss:  0.37542879581451416
train gradient:  0.4512330458406167
iteration : 1283
train acc:  0.8125
train loss:  0.44573354721069336
train gradient:  0.47810133986555503
iteration : 1284
train acc:  0.8046875
train loss:  0.43428805470466614
train gradient:  0.5180335022905153
iteration : 1285
train acc:  0.8046875
train loss:  0.400969922542572
train gradient:  0.5466110607923395
iteration : 1286
train acc:  0.7890625
train loss:  0.4573667049407959
train gradient:  0.710142805080666
iteration : 1287
train acc:  0.7890625
train loss:  0.3891761600971222
train gradient:  0.5333561200755712
iteration : 1288
train acc:  0.7734375
train loss:  0.49362578988075256
train gradient:  0.5270996185361011
iteration : 1289
train acc:  0.765625
train loss:  0.466208279132843
train gradient:  0.7910809486017009
iteration : 1290
train acc:  0.8125
train loss:  0.4580497741699219
train gradient:  0.44135366206587606
iteration : 1291
train acc:  0.828125
train loss:  0.4069603681564331
train gradient:  0.40828907305427903
iteration : 1292
train acc:  0.8046875
train loss:  0.4191223382949829
train gradient:  0.37575220408804916
iteration : 1293
train acc:  0.7890625
train loss:  0.4290657043457031
train gradient:  0.5516729015309089
iteration : 1294
train acc:  0.8046875
train loss:  0.4289833605289459
train gradient:  0.5050430731824955
iteration : 1295
train acc:  0.75
train loss:  0.49457794427871704
train gradient:  0.5826400390291189
iteration : 1296
train acc:  0.8359375
train loss:  0.3525295853614807
train gradient:  0.25550468124730813
iteration : 1297
train acc:  0.78125
train loss:  0.47264164686203003
train gradient:  0.5405028411600452
iteration : 1298
train acc:  0.875
train loss:  0.38491007685661316
train gradient:  0.3943258348342543
iteration : 1299
train acc:  0.78125
train loss:  0.4247920513153076
train gradient:  0.4013099022437008
iteration : 1300
train acc:  0.7734375
train loss:  0.41797804832458496
train gradient:  0.5248744817158524
iteration : 1301
train acc:  0.7890625
train loss:  0.44201332330703735
train gradient:  0.39826396665485103
iteration : 1302
train acc:  0.78125
train loss:  0.39978691935539246
train gradient:  0.37151108826765955
iteration : 1303
train acc:  0.78125
train loss:  0.4210478961467743
train gradient:  0.45488574187762393
iteration : 1304
train acc:  0.7890625
train loss:  0.41222119331359863
train gradient:  0.33624157690952394
iteration : 1305
train acc:  0.7890625
train loss:  0.47381046414375305
train gradient:  0.6845080453476474
iteration : 1306
train acc:  0.8046875
train loss:  0.42676252126693726
train gradient:  0.5002407843200964
iteration : 1307
train acc:  0.7890625
train loss:  0.4100567102432251
train gradient:  0.26464992694194817
iteration : 1308
train acc:  0.875
train loss:  0.34716272354125977
train gradient:  0.4010089556251963
iteration : 1309
train acc:  0.78125
train loss:  0.49348002672195435
train gradient:  0.6553374879420487
iteration : 1310
train acc:  0.8203125
train loss:  0.36899659037590027
train gradient:  0.31542964008154106
iteration : 1311
train acc:  0.7890625
train loss:  0.4466807544231415
train gradient:  0.505757957618721
iteration : 1312
train acc:  0.78125
train loss:  0.5070037841796875
train gradient:  0.7486334296479721
iteration : 1313
train acc:  0.828125
train loss:  0.3944256901741028
train gradient:  0.45029814318359246
iteration : 1314
train acc:  0.8203125
train loss:  0.4045141041278839
train gradient:  0.5844059446823231
iteration : 1315
train acc:  0.828125
train loss:  0.3621505796909332
train gradient:  0.47354295171093536
iteration : 1316
train acc:  0.8125
train loss:  0.4095652401447296
train gradient:  0.48232696824143795
iteration : 1317
train acc:  0.8125
train loss:  0.363017737865448
train gradient:  0.37287925737919825
iteration : 1318
train acc:  0.8125
train loss:  0.430786669254303
train gradient:  0.47956102711026294
iteration : 1319
train acc:  0.796875
train loss:  0.42372602224349976
train gradient:  0.4854356881710443
iteration : 1320
train acc:  0.8046875
train loss:  0.4349033832550049
train gradient:  0.4790501068390388
iteration : 1321
train acc:  0.7421875
train loss:  0.4913572669029236
train gradient:  0.6405589131937103
iteration : 1322
train acc:  0.78125
train loss:  0.4461233615875244
train gradient:  0.6364451068051122
iteration : 1323
train acc:  0.8125
train loss:  0.46003457903862
train gradient:  0.5909824537491151
iteration : 1324
train acc:  0.8125
train loss:  0.46865513920783997
train gradient:  0.5234569367280542
iteration : 1325
train acc:  0.8203125
train loss:  0.3560277223587036
train gradient:  0.3137552477696113
iteration : 1326
train acc:  0.8125
train loss:  0.4101405441761017
train gradient:  0.5214013494040343
iteration : 1327
train acc:  0.8515625
train loss:  0.37818992137908936
train gradient:  0.438627961115185
iteration : 1328
train acc:  0.875
train loss:  0.36650195717811584
train gradient:  0.2878854533705539
iteration : 1329
train acc:  0.78125
train loss:  0.4675626754760742
train gradient:  0.6979771630549448
iteration : 1330
train acc:  0.765625
train loss:  0.5081892609596252
train gradient:  0.7561459881896122
iteration : 1331
train acc:  0.7890625
train loss:  0.42528223991394043
train gradient:  0.5906030614847657
iteration : 1332
train acc:  0.75
train loss:  0.5098199844360352
train gradient:  0.6641929497582773
iteration : 1333
train acc:  0.765625
train loss:  0.44354766607284546
train gradient:  0.4711487317893564
iteration : 1334
train acc:  0.7890625
train loss:  0.40891021490097046
train gradient:  0.5167268594226566
iteration : 1335
train acc:  0.8046875
train loss:  0.44092661142349243
train gradient:  0.5534717926387919
iteration : 1336
train acc:  0.796875
train loss:  0.4692172408103943
train gradient:  0.5662131346043185
iteration : 1337
train acc:  0.7890625
train loss:  0.4592367112636566
train gradient:  0.4958177442185665
iteration : 1338
train acc:  0.8359375
train loss:  0.35677212476730347
train gradient:  0.38043639463935724
iteration : 1339
train acc:  0.8046875
train loss:  0.4334588646888733
train gradient:  0.41863349278945394
iteration : 1340
train acc:  0.7734375
train loss:  0.47837018966674805
train gradient:  0.5681179454104244
iteration : 1341
train acc:  0.71875
train loss:  0.549462080001831
train gradient:  1.1683342672991361
iteration : 1342
train acc:  0.75
train loss:  0.4649111330509186
train gradient:  0.5343423630231461
iteration : 1343
train acc:  0.8203125
train loss:  0.3987373411655426
train gradient:  0.43400476890116474
iteration : 1344
train acc:  0.796875
train loss:  0.47216367721557617
train gradient:  0.538023795583024
iteration : 1345
train acc:  0.765625
train loss:  0.4966523051261902
train gradient:  0.5020987811621973
iteration : 1346
train acc:  0.8046875
train loss:  0.3818586468696594
train gradient:  0.44792659516793243
iteration : 1347
train acc:  0.7890625
train loss:  0.4517108201980591
train gradient:  0.47362831695790336
iteration : 1348
train acc:  0.8359375
train loss:  0.422873854637146
train gradient:  0.3727121938373596
iteration : 1349
train acc:  0.8359375
train loss:  0.397245854139328
train gradient:  0.3608872353998944
iteration : 1350
train acc:  0.78125
train loss:  0.5114295482635498
train gradient:  0.5390574393813239
iteration : 1351
train acc:  0.84375
train loss:  0.35860317945480347
train gradient:  0.3395692426322858
iteration : 1352
train acc:  0.78125
train loss:  0.47156575322151184
train gradient:  0.5199890005435852
iteration : 1353
train acc:  0.7734375
train loss:  0.4222179651260376
train gradient:  0.472707717311962
iteration : 1354
train acc:  0.7734375
train loss:  0.4287080764770508
train gradient:  0.32275376256503724
iteration : 1355
train acc:  0.8125
train loss:  0.4434877038002014
train gradient:  0.5056356310041819
iteration : 1356
train acc:  0.8359375
train loss:  0.4059025049209595
train gradient:  0.3634528821593128
iteration : 1357
train acc:  0.8046875
train loss:  0.44245409965515137
train gradient:  0.3993239568607127
iteration : 1358
train acc:  0.78125
train loss:  0.4333154559135437
train gradient:  0.4618822353405615
iteration : 1359
train acc:  0.84375
train loss:  0.3349434733390808
train gradient:  0.340643855853444
iteration : 1360
train acc:  0.8046875
train loss:  0.40515273809432983
train gradient:  0.36554215977375
iteration : 1361
train acc:  0.8046875
train loss:  0.39648640155792236
train gradient:  0.39401334567375007
iteration : 1362
train acc:  0.8125
train loss:  0.4046172499656677
train gradient:  0.4976626141111147
iteration : 1363
train acc:  0.8359375
train loss:  0.3816750645637512
train gradient:  0.3775731168864223
iteration : 1364
train acc:  0.78125
train loss:  0.45645394921302795
train gradient:  0.4992552628182794
iteration : 1365
train acc:  0.7890625
train loss:  0.4420034885406494
train gradient:  0.49484057963024747
iteration : 1366
train acc:  0.765625
train loss:  0.48346152901649475
train gradient:  0.4525487029624263
iteration : 1367
train acc:  0.8359375
train loss:  0.4193877875804901
train gradient:  0.4270863593461522
iteration : 1368
train acc:  0.8359375
train loss:  0.36199572682380676
train gradient:  0.4079331306423549
iteration : 1369
train acc:  0.84375
train loss:  0.40869665145874023
train gradient:  0.3734414951268913
iteration : 1370
train acc:  0.8125
train loss:  0.4415895342826843
train gradient:  0.5537350470275475
iteration : 1371
train acc:  0.8515625
train loss:  0.34861308336257935
train gradient:  0.4167507135839176
iteration : 1372
train acc:  0.7890625
train loss:  0.4634030759334564
train gradient:  0.6201217992697522
iteration : 1373
train acc:  0.765625
train loss:  0.45686057209968567
train gradient:  0.6527790297475684
iteration : 1374
train acc:  0.78125
train loss:  0.39288994669914246
train gradient:  0.4390513314316079
iteration : 1375
train acc:  0.71875
train loss:  0.5493004322052002
train gradient:  0.6171364925646986
iteration : 1376
train acc:  0.7578125
train loss:  0.5061146020889282
train gradient:  0.5545936374854348
iteration : 1377
train acc:  0.8125
train loss:  0.3620215058326721
train gradient:  0.3239898204276004
iteration : 1378
train acc:  0.7734375
train loss:  0.4835270047187805
train gradient:  0.4224082574955942
iteration : 1379
train acc:  0.75
train loss:  0.45864632725715637
train gradient:  0.44983517374302445
iteration : 1380
train acc:  0.765625
train loss:  0.4660947918891907
train gradient:  0.5048832365447516
iteration : 1381
train acc:  0.7890625
train loss:  0.4355127215385437
train gradient:  0.5966260477585823
iteration : 1382
train acc:  0.8359375
train loss:  0.41960030794143677
train gradient:  0.5283986076892699
iteration : 1383
train acc:  0.796875
train loss:  0.45075029134750366
train gradient:  0.5823498452086417
iteration : 1384
train acc:  0.7890625
train loss:  0.45663803815841675
train gradient:  0.5066301457327647
iteration : 1385
train acc:  0.7421875
train loss:  0.44360828399658203
train gradient:  0.453611997350603
iteration : 1386
train acc:  0.8046875
train loss:  0.45198482275009155
train gradient:  0.36494698183675667
iteration : 1387
train acc:  0.765625
train loss:  0.44017788767814636
train gradient:  0.35677201687494947
iteration : 1388
train acc:  0.8359375
train loss:  0.4049552083015442
train gradient:  0.3776099800629821
iteration : 1389
train acc:  0.8046875
train loss:  0.41226935386657715
train gradient:  0.3674094146132475
iteration : 1390
train acc:  0.7734375
train loss:  0.4302552044391632
train gradient:  0.41259445743362805
iteration : 1391
train acc:  0.7734375
train loss:  0.5122696161270142
train gradient:  0.6108128056707401
iteration : 1392
train acc:  0.7890625
train loss:  0.43839067220687866
train gradient:  0.6468094062868672
iteration : 1393
train acc:  0.8125
train loss:  0.4462484121322632
train gradient:  0.3734986817455955
iteration : 1394
train acc:  0.796875
train loss:  0.41556692123413086
train gradient:  0.5253720495562235
iteration : 1395
train acc:  0.8046875
train loss:  0.40081897377967834
train gradient:  0.47263583772031836
iteration : 1396
train acc:  0.8125
train loss:  0.46619459986686707
train gradient:  0.5517718421828106
iteration : 1397
train acc:  0.8125
train loss:  0.3560793697834015
train gradient:  0.27572862909461515
iteration : 1398
train acc:  0.796875
train loss:  0.3907474875450134
train gradient:  0.3964120283996722
iteration : 1399
train acc:  0.7421875
train loss:  0.4875873625278473
train gradient:  0.4472699767589382
iteration : 1400
train acc:  0.7734375
train loss:  0.4600987434387207
train gradient:  0.6538861662310884
iteration : 1401
train acc:  0.7734375
train loss:  0.43370702862739563
train gradient:  0.5278754490675607
iteration : 1402
train acc:  0.7890625
train loss:  0.3886752724647522
train gradient:  0.37044537737276123
iteration : 1403
train acc:  0.796875
train loss:  0.4549795985221863
train gradient:  0.4639727186386743
iteration : 1404
train acc:  0.8203125
train loss:  0.40736645460128784
train gradient:  0.464875099285165
iteration : 1405
train acc:  0.8984375
train loss:  0.3550572991371155
train gradient:  0.32428242499827825
iteration : 1406
train acc:  0.8203125
train loss:  0.39242303371429443
train gradient:  0.45636190410091504
iteration : 1407
train acc:  0.7890625
train loss:  0.4223976731300354
train gradient:  0.49866053218000356
iteration : 1408
train acc:  0.796875
train loss:  0.47255265712738037
train gradient:  0.5255510150462603
iteration : 1409
train acc:  0.8203125
train loss:  0.39030084013938904
train gradient:  0.40984453007549054
iteration : 1410
train acc:  0.7890625
train loss:  0.44509434700012207
train gradient:  0.6281761953362482
iteration : 1411
train acc:  0.8203125
train loss:  0.4017636775970459
train gradient:  0.40229012287837174
iteration : 1412
train acc:  0.7890625
train loss:  0.4505577087402344
train gradient:  0.5060691397877044
iteration : 1413
train acc:  0.828125
train loss:  0.4456568956375122
train gradient:  0.5839925571359524
iteration : 1414
train acc:  0.8125
train loss:  0.41380876302719116
train gradient:  0.39128093852922086
iteration : 1415
train acc:  0.7734375
train loss:  0.4903761148452759
train gradient:  0.5975295698926206
iteration : 1416
train acc:  0.78125
train loss:  0.444669634103775
train gradient:  0.4839038450437162
iteration : 1417
train acc:  0.8671875
train loss:  0.3818773627281189
train gradient:  0.39021798933461566
iteration : 1418
train acc:  0.8203125
train loss:  0.3924603760242462
train gradient:  0.5210307049088507
iteration : 1419
train acc:  0.8203125
train loss:  0.41459017992019653
train gradient:  0.43446078150194195
iteration : 1420
train acc:  0.8671875
train loss:  0.33529847860336304
train gradient:  0.33657495719447106
iteration : 1421
train acc:  0.78125
train loss:  0.41743940114974976
train gradient:  0.36718659898689865
iteration : 1422
train acc:  0.796875
train loss:  0.4592921733856201
train gradient:  0.5336275425944441
iteration : 1423
train acc:  0.859375
train loss:  0.348010390996933
train gradient:  0.2929746538170502
iteration : 1424
train acc:  0.78125
train loss:  0.5159937143325806
train gradient:  0.825866171868991
iteration : 1425
train acc:  0.875
train loss:  0.3487505614757538
train gradient:  0.3552905746297775
iteration : 1426
train acc:  0.8515625
train loss:  0.3667142391204834
train gradient:  0.420117941280679
iteration : 1427
train acc:  0.78125
train loss:  0.41162002086639404
train gradient:  0.37372184783390794
iteration : 1428
train acc:  0.8203125
train loss:  0.400016188621521
train gradient:  0.4028873192180981
iteration : 1429
train acc:  0.7890625
train loss:  0.4244024157524109
train gradient:  0.5258299721192783
iteration : 1430
train acc:  0.8046875
train loss:  0.3984656035900116
train gradient:  0.4504279315105914
iteration : 1431
train acc:  0.7890625
train loss:  0.442739874124527
train gradient:  0.4838246645723262
iteration : 1432
train acc:  0.765625
train loss:  0.42106714844703674
train gradient:  0.45143199646676613
iteration : 1433
train acc:  0.8359375
train loss:  0.3650766611099243
train gradient:  0.363162533797454
iteration : 1434
train acc:  0.7890625
train loss:  0.44590169191360474
train gradient:  0.546426861746762
iteration : 1435
train acc:  0.765625
train loss:  0.4733675420284271
train gradient:  0.4882573866795941
iteration : 1436
train acc:  0.78125
train loss:  0.4571644961833954
train gradient:  0.5111669699874406
iteration : 1437
train acc:  0.796875
train loss:  0.4379151463508606
train gradient:  0.44389572904692237
iteration : 1438
train acc:  0.84375
train loss:  0.3801230788230896
train gradient:  0.38954189353587493
iteration : 1439
train acc:  0.8359375
train loss:  0.37388503551483154
train gradient:  0.32766914615322656
iteration : 1440
train acc:  0.796875
train loss:  0.43482041358947754
train gradient:  0.42514881743394384
iteration : 1441
train acc:  0.8203125
train loss:  0.41895511746406555
train gradient:  0.3436917907250403
iteration : 1442
train acc:  0.8046875
train loss:  0.4300100803375244
train gradient:  0.37833932996758896
iteration : 1443
train acc:  0.796875
train loss:  0.46958011388778687
train gradient:  0.6037636645477152
iteration : 1444
train acc:  0.8203125
train loss:  0.3648534119129181
train gradient:  0.3661922232911272
iteration : 1445
train acc:  0.8125
train loss:  0.4000650644302368
train gradient:  0.3499578521352994
iteration : 1446
train acc:  0.828125
train loss:  0.411700576543808
train gradient:  0.4314788064052665
iteration : 1447
train acc:  0.8125
train loss:  0.43186283111572266
train gradient:  0.5336402385636345
iteration : 1448
train acc:  0.8203125
train loss:  0.39517390727996826
train gradient:  0.39134130580564136
iteration : 1449
train acc:  0.7890625
train loss:  0.44917798042297363
train gradient:  0.4157732402264187
iteration : 1450
train acc:  0.7734375
train loss:  0.4571859836578369
train gradient:  0.650239890989921
iteration : 1451
train acc:  0.8125
train loss:  0.40429165959358215
train gradient:  0.5979877723821012
iteration : 1452
train acc:  0.78125
train loss:  0.4648190140724182
train gradient:  0.5762846320460477
iteration : 1453
train acc:  0.8125
train loss:  0.37400758266448975
train gradient:  0.3294187089844667
iteration : 1454
train acc:  0.8359375
train loss:  0.41789311170578003
train gradient:  0.43761388236022203
iteration : 1455
train acc:  0.78125
train loss:  0.4426785409450531
train gradient:  0.41388592841460825
iteration : 1456
train acc:  0.828125
train loss:  0.47052446007728577
train gradient:  0.5700540351104619
iteration : 1457
train acc:  0.84375
train loss:  0.36915311217308044
train gradient:  0.38267030211572245
iteration : 1458
train acc:  0.7734375
train loss:  0.4912366569042206
train gradient:  0.6099058246944689
iteration : 1459
train acc:  0.734375
train loss:  0.5629364252090454
train gradient:  0.8211269400667119
iteration : 1460
train acc:  0.8359375
train loss:  0.41450804471969604
train gradient:  0.5029069210190205
iteration : 1461
train acc:  0.734375
train loss:  0.46631330251693726
train gradient:  0.6807949428142687
iteration : 1462
train acc:  0.8125
train loss:  0.34738194942474365
train gradient:  0.3924803161343397
iteration : 1463
train acc:  0.734375
train loss:  0.5321812629699707
train gradient:  0.6901574172833976
iteration : 1464
train acc:  0.7890625
train loss:  0.4257093667984009
train gradient:  0.5369756783282549
iteration : 1465
train acc:  0.765625
train loss:  0.479769766330719
train gradient:  0.5530577101813625
iteration : 1466
train acc:  0.8125
train loss:  0.4291630983352661
train gradient:  0.4445884668634107
iteration : 1467
train acc:  0.7421875
train loss:  0.4271690845489502
train gradient:  0.45207089259102123
iteration : 1468
train acc:  0.8515625
train loss:  0.3544576168060303
train gradient:  0.32276697286572176
iteration : 1469
train acc:  0.7890625
train loss:  0.40045976638793945
train gradient:  0.27796231557144185
iteration : 1470
train acc:  0.8046875
train loss:  0.4372073709964752
train gradient:  0.4120359294952016
iteration : 1471
train acc:  0.75
train loss:  0.43026044964790344
train gradient:  0.5452435296628511
iteration : 1472
train acc:  0.8203125
train loss:  0.405158668756485
train gradient:  0.4914568024917233
iteration : 1473
train acc:  0.8515625
train loss:  0.3418876528739929
train gradient:  0.3250723694825576
iteration : 1474
train acc:  0.8046875
train loss:  0.41882383823394775
train gradient:  0.3883653050115718
iteration : 1475
train acc:  0.78125
train loss:  0.4591315984725952
train gradient:  0.4595293231317227
iteration : 1476
train acc:  0.8203125
train loss:  0.3894062638282776
train gradient:  0.37265882794703975
iteration : 1477
train acc:  0.75
train loss:  0.4756540060043335
train gradient:  0.6313428443275434
iteration : 1478
train acc:  0.796875
train loss:  0.431215763092041
train gradient:  0.5747206681985778
iteration : 1479
train acc:  0.796875
train loss:  0.4518490433692932
train gradient:  0.438408623958264
iteration : 1480
train acc:  0.78125
train loss:  0.41409146785736084
train gradient:  0.3148873662024009
iteration : 1481
train acc:  0.8203125
train loss:  0.4059675931930542
train gradient:  0.32258244508886097
iteration : 1482
train acc:  0.7421875
train loss:  0.4855409562587738
train gradient:  0.5834037960998708
iteration : 1483
train acc:  0.8125
train loss:  0.4148060083389282
train gradient:  0.55247051539926
iteration : 1484
train acc:  0.8359375
train loss:  0.3794824182987213
train gradient:  0.43727868315332835
iteration : 1485
train acc:  0.8203125
train loss:  0.3924854099750519
train gradient:  0.3092597797265288
iteration : 1486
train acc:  0.828125
train loss:  0.3611161410808563
train gradient:  0.3459800225215249
iteration : 1487
train acc:  0.765625
train loss:  0.5315565466880798
train gradient:  0.6803739971777454
iteration : 1488
train acc:  0.828125
train loss:  0.410063773393631
train gradient:  0.4615863107414606
iteration : 1489
train acc:  0.765625
train loss:  0.4976724088191986
train gradient:  0.5479598984283878
iteration : 1490
train acc:  0.7890625
train loss:  0.4167688190937042
train gradient:  0.3771909228280507
iteration : 1491
train acc:  0.7890625
train loss:  0.4345771074295044
train gradient:  0.3718874203585829
iteration : 1492
train acc:  0.78125
train loss:  0.4298946261405945
train gradient:  0.5876341701871982
iteration : 1493
train acc:  0.765625
train loss:  0.42657965421676636
train gradient:  0.5265859522706955
iteration : 1494
train acc:  0.84375
train loss:  0.38299456238746643
train gradient:  0.40615518700169306
iteration : 1495
train acc:  0.734375
train loss:  0.4602839946746826
train gradient:  0.36462229733837404
iteration : 1496
train acc:  0.78125
train loss:  0.48073646426200867
train gradient:  0.47973586133239593
iteration : 1497
train acc:  0.8046875
train loss:  0.4448211193084717
train gradient:  0.49326786164658776
iteration : 1498
train acc:  0.796875
train loss:  0.4457207918167114
train gradient:  0.5021367100103337
iteration : 1499
train acc:  0.7265625
train loss:  0.49785488843917847
train gradient:  0.5414953990030482
iteration : 1500
train acc:  0.796875
train loss:  0.4249095916748047
train gradient:  0.5568907367919651
iteration : 1501
train acc:  0.8046875
train loss:  0.4407968521118164
train gradient:  0.29257774709250944
iteration : 1502
train acc:  0.8828125
train loss:  0.3375324010848999
train gradient:  0.39934239005112043
iteration : 1503
train acc:  0.8359375
train loss:  0.39181816577911377
train gradient:  0.3401727658668484
iteration : 1504
train acc:  0.8203125
train loss:  0.41176921129226685
train gradient:  0.3256535437054694
iteration : 1505
train acc:  0.7578125
train loss:  0.4747866690158844
train gradient:  0.6121370755029898
iteration : 1506
train acc:  0.8125
train loss:  0.4320273995399475
train gradient:  0.4832791502969494
iteration : 1507
train acc:  0.8125
train loss:  0.41515886783599854
train gradient:  0.3603350422863235
iteration : 1508
train acc:  0.8203125
train loss:  0.3911283016204834
train gradient:  0.3265322641839111
iteration : 1509
train acc:  0.8359375
train loss:  0.4271707236766815
train gradient:  0.5748485475445836
iteration : 1510
train acc:  0.8203125
train loss:  0.3800046443939209
train gradient:  0.511087814016626
iteration : 1511
train acc:  0.875
train loss:  0.36914992332458496
train gradient:  0.3690867911072253
iteration : 1512
train acc:  0.78125
train loss:  0.4363296329975128
train gradient:  0.4003762547228065
iteration : 1513
train acc:  0.7890625
train loss:  0.4560866951942444
train gradient:  0.567419997242939
iteration : 1514
train acc:  0.8359375
train loss:  0.4023982584476471
train gradient:  0.3618290133179197
iteration : 1515
train acc:  0.796875
train loss:  0.3949868679046631
train gradient:  0.4269298260979933
iteration : 1516
train acc:  0.8046875
train loss:  0.3916419446468353
train gradient:  0.3744169932637003
iteration : 1517
train acc:  0.8046875
train loss:  0.39831453561782837
train gradient:  0.3791780431694323
iteration : 1518
train acc:  0.8203125
train loss:  0.38032129406929016
train gradient:  0.3348309991419871
iteration : 1519
train acc:  0.7578125
train loss:  0.49004819989204407
train gradient:  0.8433448994534491
iteration : 1520
train acc:  0.8046875
train loss:  0.4538620114326477
train gradient:  0.498253764082951
iteration : 1521
train acc:  0.8203125
train loss:  0.4061945378780365
train gradient:  0.519295731182714
iteration : 1522
train acc:  0.75
train loss:  0.4795472025871277
train gradient:  0.609108640217866
iteration : 1523
train acc:  0.765625
train loss:  0.478648841381073
train gradient:  0.718415535932652
iteration : 1524
train acc:  0.78125
train loss:  0.44350582361221313
train gradient:  0.4235449156213021
iteration : 1525
train acc:  0.8203125
train loss:  0.42816394567489624
train gradient:  0.3475593757168597
iteration : 1526
train acc:  0.796875
train loss:  0.4564352035522461
train gradient:  0.5099495325277446
iteration : 1527
train acc:  0.8359375
train loss:  0.3430279493331909
train gradient:  0.41712645207464855
iteration : 1528
train acc:  0.78125
train loss:  0.43462419509887695
train gradient:  0.40161240303276735
iteration : 1529
train acc:  0.859375
train loss:  0.33640432357788086
train gradient:  0.3616693478669624
iteration : 1530
train acc:  0.7890625
train loss:  0.44526511430740356
train gradient:  0.6176532371146249
iteration : 1531
train acc:  0.75
train loss:  0.4362638294696808
train gradient:  0.5073924307763086
iteration : 1532
train acc:  0.8203125
train loss:  0.412509560585022
train gradient:  0.47745519410212933
iteration : 1533
train acc:  0.7578125
train loss:  0.4948215186595917
train gradient:  0.6674546372984409
iteration : 1534
train acc:  0.75
train loss:  0.5066912770271301
train gradient:  0.698313720607796
iteration : 1535
train acc:  0.8359375
train loss:  0.39291706681251526
train gradient:  0.5202529345017547
iteration : 1536
train acc:  0.828125
train loss:  0.40305328369140625
train gradient:  0.4226461518968567
iteration : 1537
train acc:  0.7890625
train loss:  0.45617014169692993
train gradient:  0.43193035854318274
iteration : 1538
train acc:  0.828125
train loss:  0.4203805923461914
train gradient:  0.4503387238931506
iteration : 1539
train acc:  0.875
train loss:  0.3816068768501282
train gradient:  0.366056670545656
iteration : 1540
train acc:  0.8359375
train loss:  0.37558838725090027
train gradient:  0.3060100769070125
iteration : 1541
train acc:  0.78125
train loss:  0.46990033984184265
train gradient:  0.48395012225919587
iteration : 1542
train acc:  0.8125
train loss:  0.41147375106811523
train gradient:  0.41908732606145616
iteration : 1543
train acc:  0.75
train loss:  0.4599727988243103
train gradient:  0.5399476222314274
iteration : 1544
train acc:  0.8046875
train loss:  0.4131450057029724
train gradient:  0.44830519877211616
iteration : 1545
train acc:  0.8203125
train loss:  0.37233230471611023
train gradient:  0.38682870584593987
iteration : 1546
train acc:  0.8203125
train loss:  0.3531842827796936
train gradient:  0.4485242049657353
iteration : 1547
train acc:  0.796875
train loss:  0.43861106038093567
train gradient:  0.704342704646006
iteration : 1548
train acc:  0.7265625
train loss:  0.5241358280181885
train gradient:  0.6884021101011459
iteration : 1549
train acc:  0.8125
train loss:  0.4370158910751343
train gradient:  0.5136393357249287
iteration : 1550
train acc:  0.8828125
train loss:  0.35666877031326294
train gradient:  0.28489131094229087
iteration : 1551
train acc:  0.84375
train loss:  0.3473357558250427
train gradient:  0.31873109180144615
iteration : 1552
train acc:  0.828125
train loss:  0.36473917961120605
train gradient:  0.40793429058046926
iteration : 1553
train acc:  0.7890625
train loss:  0.4454568326473236
train gradient:  0.46659704823875126
iteration : 1554
train acc:  0.8125
train loss:  0.45719391107559204
train gradient:  0.7305563312011267
iteration : 1555
train acc:  0.71875
train loss:  0.5146403312683105
train gradient:  0.6738265388140159
iteration : 1556
train acc:  0.8203125
train loss:  0.44037625193595886
train gradient:  0.421237295402135
iteration : 1557
train acc:  0.875
train loss:  0.3924441635608673
train gradient:  0.3785763723636557
iteration : 1558
train acc:  0.859375
train loss:  0.34540659189224243
train gradient:  0.371790436186236
iteration : 1559
train acc:  0.7421875
train loss:  0.4652242362499237
train gradient:  0.4393661622406918
iteration : 1560
train acc:  0.859375
train loss:  0.3617578148841858
train gradient:  0.527857949298352
iteration : 1561
train acc:  0.796875
train loss:  0.47951871156692505
train gradient:  0.47834652632665203
iteration : 1562
train acc:  0.7734375
train loss:  0.5027972459793091
train gradient:  0.8678697366238329
iteration : 1563
train acc:  0.8125
train loss:  0.35384485125541687
train gradient:  0.5305265417694872
iteration : 1564
train acc:  0.7890625
train loss:  0.44153743982315063
train gradient:  0.5254532269749739
iteration : 1565
train acc:  0.7890625
train loss:  0.4142974615097046
train gradient:  0.5794527429032312
iteration : 1566
train acc:  0.859375
train loss:  0.36532220244407654
train gradient:  0.3980748282916709
iteration : 1567
train acc:  0.7890625
train loss:  0.4132938086986542
train gradient:  0.40078335122296976
iteration : 1568
train acc:  0.7734375
train loss:  0.4120265543460846
train gradient:  0.4189285767765701
iteration : 1569
train acc:  0.859375
train loss:  0.3464372158050537
train gradient:  0.43422991635748825
iteration : 1570
train acc:  0.828125
train loss:  0.4146357774734497
train gradient:  0.5213804504881581
iteration : 1571
train acc:  0.8203125
train loss:  0.4283515512943268
train gradient:  0.41933649060780537
iteration : 1572
train acc:  0.828125
train loss:  0.3867707848548889
train gradient:  0.413325659308193
iteration : 1573
train acc:  0.859375
train loss:  0.3290790915489197
train gradient:  0.39862878777478994
iteration : 1574
train acc:  0.8515625
train loss:  0.3260333240032196
train gradient:  0.2723035411550849
iteration : 1575
train acc:  0.796875
train loss:  0.42724889516830444
train gradient:  0.43978527405850726
iteration : 1576
train acc:  0.8125
train loss:  0.4030492901802063
train gradient:  0.4687347499941454
iteration : 1577
train acc:  0.8203125
train loss:  0.39325109124183655
train gradient:  0.31845793057109034
iteration : 1578
train acc:  0.8203125
train loss:  0.41293323040008545
train gradient:  0.3883293450305231
iteration : 1579
train acc:  0.84375
train loss:  0.37618622183799744
train gradient:  0.34177041190117996
iteration : 1580
train acc:  0.8203125
train loss:  0.3809575140476227
train gradient:  0.4892442013058868
iteration : 1581
train acc:  0.8984375
train loss:  0.3053417503833771
train gradient:  0.4168487658921202
iteration : 1582
train acc:  0.828125
train loss:  0.35504892468452454
train gradient:  0.34407994717539364
iteration : 1583
train acc:  0.796875
train loss:  0.41871702671051025
train gradient:  0.4010685453176638
iteration : 1584
train acc:  0.7421875
train loss:  0.5036259889602661
train gradient:  0.49237265383322176
iteration : 1585
train acc:  0.7578125
train loss:  0.49642571806907654
train gradient:  0.614704159917297
iteration : 1586
train acc:  0.828125
train loss:  0.41273725032806396
train gradient:  0.573192827658877
iteration : 1587
train acc:  0.8671875
train loss:  0.31003981828689575
train gradient:  0.2339257156292578
iteration : 1588
train acc:  0.84375
train loss:  0.34186455607414246
train gradient:  0.3891642048947663
iteration : 1589
train acc:  0.78125
train loss:  0.3839186429977417
train gradient:  0.4549170260786126
iteration : 1590
train acc:  0.7890625
train loss:  0.41882339119911194
train gradient:  0.621180625929519
iteration : 1591
train acc:  0.796875
train loss:  0.43398451805114746
train gradient:  0.41972542395450485
iteration : 1592
train acc:  0.8984375
train loss:  0.3462604880332947
train gradient:  0.3154615197269195
iteration : 1593
train acc:  0.8203125
train loss:  0.3926295340061188
train gradient:  0.5266670078018845
iteration : 1594
train acc:  0.765625
train loss:  0.40558189153671265
train gradient:  0.5097527250381272
iteration : 1595
train acc:  0.7734375
train loss:  0.4253307580947876
train gradient:  0.5434116059844266
iteration : 1596
train acc:  0.8515625
train loss:  0.347781240940094
train gradient:  0.3333533285169448
iteration : 1597
train acc:  0.7734375
train loss:  0.4220190942287445
train gradient:  0.5640198382573113
iteration : 1598
train acc:  0.7265625
train loss:  0.501831591129303
train gradient:  0.5614883443098703
iteration : 1599
train acc:  0.8359375
train loss:  0.33766409754753113
train gradient:  0.45552980309959284
iteration : 1600
train acc:  0.8046875
train loss:  0.3904421925544739
train gradient:  0.4528859827515841
iteration : 1601
train acc:  0.828125
train loss:  0.38701289892196655
train gradient:  0.35970609041005613
iteration : 1602
train acc:  0.796875
train loss:  0.4277040660381317
train gradient:  0.529931692147553
iteration : 1603
train acc:  0.8515625
train loss:  0.359125018119812
train gradient:  0.28971858010829477
iteration : 1604
train acc:  0.84375
train loss:  0.3443683981895447
train gradient:  0.3212994963164047
iteration : 1605
train acc:  0.796875
train loss:  0.40195825695991516
train gradient:  0.3781531631387001
iteration : 1606
train acc:  0.890625
train loss:  0.32231274247169495
train gradient:  0.35858448800738657
iteration : 1607
train acc:  0.8203125
train loss:  0.4284091591835022
train gradient:  0.4326472979496774
iteration : 1608
train acc:  0.84375
train loss:  0.3969281315803528
train gradient:  0.4287866743506608
iteration : 1609
train acc:  0.7578125
train loss:  0.4747752845287323
train gradient:  0.6384433795724029
iteration : 1610
train acc:  0.8671875
train loss:  0.3319806456565857
train gradient:  0.2899332560874414
iteration : 1611
train acc:  0.8203125
train loss:  0.405166357755661
train gradient:  0.4295131714207767
iteration : 1612
train acc:  0.796875
train loss:  0.4178605377674103
train gradient:  0.41710851066277865
iteration : 1613
train acc:  0.75
train loss:  0.4935533106327057
train gradient:  0.7851826493294298
iteration : 1614
train acc:  0.8203125
train loss:  0.4232741892337799
train gradient:  0.4416146409207419
iteration : 1615
train acc:  0.859375
train loss:  0.3522343039512634
train gradient:  0.46563061712985276
iteration : 1616
train acc:  0.7734375
train loss:  0.49905720353126526
train gradient:  0.5328265895192217
iteration : 1617
train acc:  0.7734375
train loss:  0.5058385133743286
train gradient:  0.7249635594025219
iteration : 1618
train acc:  0.8359375
train loss:  0.3527188301086426
train gradient:  0.45599159137869155
iteration : 1619
train acc:  0.8125
train loss:  0.4746321439743042
train gradient:  0.5593253198032629
iteration : 1620
train acc:  0.78125
train loss:  0.48688721656799316
train gradient:  0.59141268975835
iteration : 1621
train acc:  0.8359375
train loss:  0.4195462465286255
train gradient:  0.4859056915771656
iteration : 1622
train acc:  0.8203125
train loss:  0.40150734782218933
train gradient:  0.3955175985373988
iteration : 1623
train acc:  0.8046875
train loss:  0.4162962734699249
train gradient:  0.44995104185006485
iteration : 1624
train acc:  0.8671875
train loss:  0.29908448457717896
train gradient:  0.2928957060298231
iteration : 1625
train acc:  0.828125
train loss:  0.38355571031570435
train gradient:  0.39276869044728246
iteration : 1626
train acc:  0.828125
train loss:  0.3769696354866028
train gradient:  0.5179500035607616
iteration : 1627
train acc:  0.8125
train loss:  0.4132174849510193
train gradient:  0.47635328709320957
iteration : 1628
train acc:  0.78125
train loss:  0.4493567943572998
train gradient:  0.6244040182546345
iteration : 1629
train acc:  0.796875
train loss:  0.42706531286239624
train gradient:  0.4047743273816521
iteration : 1630
train acc:  0.8359375
train loss:  0.39646124839782715
train gradient:  0.3967235127064465
iteration : 1631
train acc:  0.78125
train loss:  0.40640729665756226
train gradient:  0.3543375789843261
iteration : 1632
train acc:  0.8515625
train loss:  0.35613369941711426
train gradient:  0.33486392704418577
iteration : 1633
train acc:  0.8046875
train loss:  0.40693214535713196
train gradient:  0.453277045054462
iteration : 1634
train acc:  0.78125
train loss:  0.4811387360095978
train gradient:  0.5104640854344312
iteration : 1635
train acc:  0.84375
train loss:  0.36413949728012085
train gradient:  0.43931940794725505
iteration : 1636
train acc:  0.8203125
train loss:  0.42302197217941284
train gradient:  0.42377755005898843
iteration : 1637
train acc:  0.84375
train loss:  0.3533489406108856
train gradient:  0.5282621517910968
iteration : 1638
train acc:  0.8515625
train loss:  0.35578280687332153
train gradient:  0.46930199330454575
iteration : 1639
train acc:  0.7421875
train loss:  0.43573063611984253
train gradient:  0.5510882055779855
iteration : 1640
train acc:  0.796875
train loss:  0.4407837390899658
train gradient:  0.6399949911895766
iteration : 1641
train acc:  0.75
train loss:  0.4848549962043762
train gradient:  0.8037058817019458
iteration : 1642
train acc:  0.8046875
train loss:  0.446429967880249
train gradient:  0.5025776404060833
iteration : 1643
train acc:  0.8359375
train loss:  0.3552514314651489
train gradient:  0.38030403517478634
iteration : 1644
train acc:  0.828125
train loss:  0.36230945587158203
train gradient:  0.3657060534965678
iteration : 1645
train acc:  0.78125
train loss:  0.4842255711555481
train gradient:  0.6538194389115873
iteration : 1646
train acc:  0.78125
train loss:  0.4280625283718109
train gradient:  0.49673301512217266
iteration : 1647
train acc:  0.84375
train loss:  0.40520209074020386
train gradient:  0.4350781413464318
iteration : 1648
train acc:  0.8125
train loss:  0.38980525732040405
train gradient:  0.40800198824082956
iteration : 1649
train acc:  0.8515625
train loss:  0.4433460831642151
train gradient:  0.5168211660193869
iteration : 1650
train acc:  0.8125
train loss:  0.4233422577381134
train gradient:  0.4758238510977211
iteration : 1651
train acc:  0.7578125
train loss:  0.4462522864341736
train gradient:  0.47047401387105336
iteration : 1652
train acc:  0.828125
train loss:  0.39666321873664856
train gradient:  0.5008132578892581
iteration : 1653
train acc:  0.8671875
train loss:  0.3178391456604004
train gradient:  0.2385868900881541
iteration : 1654
train acc:  0.7734375
train loss:  0.4784134030342102
train gradient:  0.49957014899347163
iteration : 1655
train acc:  0.828125
train loss:  0.41043367981910706
train gradient:  0.4542431400670622
iteration : 1656
train acc:  0.8515625
train loss:  0.3923162519931793
train gradient:  0.34314527118659566
iteration : 1657
train acc:  0.8046875
train loss:  0.3727032542228699
train gradient:  0.3735587872565633
iteration : 1658
train acc:  0.8671875
train loss:  0.3302380442619324
train gradient:  0.38601236244888637
iteration : 1659
train acc:  0.8125
train loss:  0.44683635234832764
train gradient:  0.4797333132122859
iteration : 1660
train acc:  0.734375
train loss:  0.5290173888206482
train gradient:  0.838159900460929
iteration : 1661
train acc:  0.8046875
train loss:  0.45707178115844727
train gradient:  0.5703640584565708
iteration : 1662
train acc:  0.78125
train loss:  0.43078914284706116
train gradient:  0.4966179791749439
iteration : 1663
train acc:  0.859375
train loss:  0.34391388297080994
train gradient:  0.29646757417633285
iteration : 1664
train acc:  0.84375
train loss:  0.38323453068733215
train gradient:  0.467306570490115
iteration : 1665
train acc:  0.8125
train loss:  0.41761767864227295
train gradient:  0.4250306454979653
iteration : 1666
train acc:  0.828125
train loss:  0.37443023920059204
train gradient:  0.37386936119634645
iteration : 1667
train acc:  0.8671875
train loss:  0.3212583661079407
train gradient:  0.43299113638952574
iteration : 1668
train acc:  0.75
train loss:  0.5080439448356628
train gradient:  1.1612625534891021
iteration : 1669
train acc:  0.796875
train loss:  0.433394193649292
train gradient:  0.5387861051635976
iteration : 1670
train acc:  0.7578125
train loss:  0.4585154354572296
train gradient:  0.450144571188865
iteration : 1671
train acc:  0.84375
train loss:  0.3428339958190918
train gradient:  0.3689012915186039
iteration : 1672
train acc:  0.8203125
train loss:  0.3921794295310974
train gradient:  0.44900024468454897
iteration : 1673
train acc:  0.875
train loss:  0.3447616696357727
train gradient:  0.45392294903268654
iteration : 1674
train acc:  0.8046875
train loss:  0.45591068267822266
train gradient:  0.6275046763163188
iteration : 1675
train acc:  0.8203125
train loss:  0.41489845514297485
train gradient:  0.5948844904747252
iteration : 1676
train acc:  0.7421875
train loss:  0.5236291885375977
train gradient:  0.7490153368626219
iteration : 1677
train acc:  0.828125
train loss:  0.4234394431114197
train gradient:  0.44889515551363873
iteration : 1678
train acc:  0.828125
train loss:  0.37245965003967285
train gradient:  0.43302009602556857
iteration : 1679
train acc:  0.78125
train loss:  0.43648043274879456
train gradient:  0.4716390799868217
iteration : 1680
train acc:  0.828125
train loss:  0.37581753730773926
train gradient:  0.5542847517450441
iteration : 1681
train acc:  0.8203125
train loss:  0.42811304330825806
train gradient:  0.487396847119273
iteration : 1682
train acc:  0.7890625
train loss:  0.42808079719543457
train gradient:  0.46298043601553013
iteration : 1683
train acc:  0.828125
train loss:  0.41506272554397583
train gradient:  0.38728995295767604
iteration : 1684
train acc:  0.8515625
train loss:  0.41604188084602356
train gradient:  0.39108945974846365
iteration : 1685
train acc:  0.8046875
train loss:  0.4818970561027527
train gradient:  0.6375353405031161
iteration : 1686
train acc:  0.828125
train loss:  0.35657113790512085
train gradient:  0.3522384373390809
iteration : 1687
train acc:  0.7578125
train loss:  0.4521659016609192
train gradient:  0.50451006009515
iteration : 1688
train acc:  0.7734375
train loss:  0.43251270055770874
train gradient:  0.34765151619610535
iteration : 1689
train acc:  0.8046875
train loss:  0.3942795395851135
train gradient:  0.36021532499610665
iteration : 1690
train acc:  0.8359375
train loss:  0.43926557898521423
train gradient:  0.5036111035635877
iteration : 1691
train acc:  0.8359375
train loss:  0.38664352893829346
train gradient:  0.3825343524266812
iteration : 1692
train acc:  0.8515625
train loss:  0.38222184777259827
train gradient:  0.5220097847277616
iteration : 1693
train acc:  0.84375
train loss:  0.42306506633758545
train gradient:  0.47960348816820453
iteration : 1694
train acc:  0.8359375
train loss:  0.36886847019195557
train gradient:  0.5076018138032522
iteration : 1695
train acc:  0.796875
train loss:  0.399360716342926
train gradient:  0.46316923187581305
iteration : 1696
train acc:  0.78125
train loss:  0.4350733160972595
train gradient:  0.4516740364530929
iteration : 1697
train acc:  0.8359375
train loss:  0.38724586367607117
train gradient:  0.4467118251477529
iteration : 1698
train acc:  0.8203125
train loss:  0.36896857619285583
train gradient:  0.3116305705288992
iteration : 1699
train acc:  0.796875
train loss:  0.38602203130722046
train gradient:  0.29799426800483875
iteration : 1700
train acc:  0.859375
train loss:  0.37506845593452454
train gradient:  0.32033851181587236
iteration : 1701
train acc:  0.8203125
train loss:  0.3985711336135864
train gradient:  0.3653086281223963
iteration : 1702
train acc:  0.7734375
train loss:  0.35500019788742065
train gradient:  0.4026425574517357
iteration : 1703
train acc:  0.7578125
train loss:  0.4798241853713989
train gradient:  0.6408703979945121
iteration : 1704
train acc:  0.8203125
train loss:  0.3851851224899292
train gradient:  0.4403505699195175
iteration : 1705
train acc:  0.8125
train loss:  0.36773329973220825
train gradient:  0.3188433329357406
iteration : 1706
train acc:  0.8125
train loss:  0.4186531603336334
train gradient:  0.3917545457323439
iteration : 1707
train acc:  0.8046875
train loss:  0.40134283900260925
train gradient:  0.5887853487654753
iteration : 1708
train acc:  0.8671875
train loss:  0.31139999628067017
train gradient:  0.2789932218111548
iteration : 1709
train acc:  0.828125
train loss:  0.4342833161354065
train gradient:  0.6343515668310735
iteration : 1710
train acc:  0.84375
train loss:  0.3907721936702728
train gradient:  0.46090964248580046
iteration : 1711
train acc:  0.8046875
train loss:  0.4566470980644226
train gradient:  0.6734347754709438
iteration : 1712
train acc:  0.765625
train loss:  0.504588782787323
train gradient:  0.7064658408609897
iteration : 1713
train acc:  0.875
train loss:  0.341221421957016
train gradient:  0.4544988045056955
iteration : 1714
train acc:  0.796875
train loss:  0.4222643971443176
train gradient:  0.4489754531986501
iteration : 1715
train acc:  0.8359375
train loss:  0.42622435092926025
train gradient:  0.5160834171454576
iteration : 1716
train acc:  0.8125
train loss:  0.3930709958076477
train gradient:  0.4964820961900933
iteration : 1717
train acc:  0.8203125
train loss:  0.40281248092651367
train gradient:  0.39140763573121146
iteration : 1718
train acc:  0.78125
train loss:  0.48632627725601196
train gradient:  0.544561805658277
iteration : 1719
train acc:  0.8046875
train loss:  0.4562453329563141
train gradient:  0.5329099531199479
iteration : 1720
train acc:  0.828125
train loss:  0.3887336254119873
train gradient:  0.46602727662374566
iteration : 1721
train acc:  0.796875
train loss:  0.4264238476753235
train gradient:  0.4117437570059589
iteration : 1722
train acc:  0.8125
train loss:  0.42948827147483826
train gradient:  0.5657137872047454
iteration : 1723
train acc:  0.796875
train loss:  0.4565577507019043
train gradient:  0.5218296394812478
iteration : 1724
train acc:  0.8828125
train loss:  0.30247849225997925
train gradient:  0.3116399535162964
iteration : 1725
train acc:  0.796875
train loss:  0.4804323613643646
train gradient:  0.6898573049055297
iteration : 1726
train acc:  0.7890625
train loss:  0.42937272787094116
train gradient:  0.5041277556369647
iteration : 1727
train acc:  0.7890625
train loss:  0.4045197665691376
train gradient:  0.480086338660222
iteration : 1728
train acc:  0.7890625
train loss:  0.4373779296875
train gradient:  0.5343436015535923
iteration : 1729
train acc:  0.7421875
train loss:  0.5356659293174744
train gradient:  0.695086139484898
iteration : 1730
train acc:  0.78125
train loss:  0.4406582713127136
train gradient:  0.4895275928088745
iteration : 1731
train acc:  0.765625
train loss:  0.4821648597717285
train gradient:  0.778492901929958
iteration : 1732
train acc:  0.859375
train loss:  0.4100293815135956
train gradient:  0.3656130711970527
iteration : 1733
train acc:  0.78125
train loss:  0.557125449180603
train gradient:  0.9902523420563316
iteration : 1734
train acc:  0.84375
train loss:  0.36718666553497314
train gradient:  0.43693360486567046
iteration : 1735
train acc:  0.8125
train loss:  0.43339478969573975
train gradient:  0.5852539993039063
iteration : 1736
train acc:  0.7421875
train loss:  0.5204388499259949
train gradient:  0.5726836713982141
iteration : 1737
train acc:  0.8046875
train loss:  0.4036250114440918
train gradient:  0.37822798176955313
iteration : 1738
train acc:  0.8359375
train loss:  0.3567160367965698
train gradient:  0.2807118499351342
iteration : 1739
train acc:  0.8046875
train loss:  0.4236448407173157
train gradient:  0.4088807150743782
iteration : 1740
train acc:  0.8359375
train loss:  0.37820762395858765
train gradient:  0.40664569447151755
iteration : 1741
train acc:  0.8359375
train loss:  0.3536503314971924
train gradient:  0.44300371573205444
iteration : 1742
train acc:  0.8359375
train loss:  0.3457578718662262
train gradient:  0.31177668778786294
iteration : 1743
train acc:  0.8125
train loss:  0.35905981063842773
train gradient:  0.33444595012894685
iteration : 1744
train acc:  0.8125
train loss:  0.3740236759185791
train gradient:  0.2521377166559324
iteration : 1745
train acc:  0.84375
train loss:  0.34245479106903076
train gradient:  0.31831958739239247
iteration : 1746
train acc:  0.8359375
train loss:  0.3541155457496643
train gradient:  0.32508516045658015
iteration : 1747
train acc:  0.7890625
train loss:  0.42261767387390137
train gradient:  0.36552618027447886
iteration : 1748
train acc:  0.859375
train loss:  0.3815511167049408
train gradient:  0.3105629923220275
iteration : 1749
train acc:  0.8203125
train loss:  0.4105588495731354
train gradient:  0.40310006763843703
iteration : 1750
train acc:  0.7734375
train loss:  0.5102393627166748
train gradient:  0.6072393856945522
iteration : 1751
train acc:  0.8046875
train loss:  0.4263128936290741
train gradient:  0.37353220942723186
iteration : 1752
train acc:  0.8359375
train loss:  0.3675558865070343
train gradient:  0.30210407960086527
iteration : 1753
train acc:  0.7890625
train loss:  0.35851001739501953
train gradient:  0.4105249864215833
iteration : 1754
train acc:  0.8203125
train loss:  0.38940781354904175
train gradient:  0.2688869603894018
iteration : 1755
train acc:  0.8046875
train loss:  0.4323675036430359
train gradient:  0.5448329747172576
iteration : 1756
train acc:  0.8671875
train loss:  0.3722151517868042
train gradient:  0.38744248558941197
iteration : 1757
train acc:  0.8046875
train loss:  0.37267619371414185
train gradient:  0.3177294130903463
iteration : 1758
train acc:  0.765625
train loss:  0.44482600688934326
train gradient:  0.46690473714237507
iteration : 1759
train acc:  0.8203125
train loss:  0.4799513816833496
train gradient:  0.4410756745447773
iteration : 1760
train acc:  0.8203125
train loss:  0.40067172050476074
train gradient:  0.3399084954310507
iteration : 1761
train acc:  0.8125
train loss:  0.39379099011421204
train gradient:  0.4487805561684387
iteration : 1762
train acc:  0.828125
train loss:  0.40618228912353516
train gradient:  0.3869381972045317
iteration : 1763
train acc:  0.84375
train loss:  0.3293476104736328
train gradient:  0.31629099890562284
iteration : 1764
train acc:  0.8125
train loss:  0.46829819679260254
train gradient:  0.3474625272692086
iteration : 1765
train acc:  0.8046875
train loss:  0.41152477264404297
train gradient:  0.46471764041882807
iteration : 1766
train acc:  0.8203125
train loss:  0.39577168226242065
train gradient:  0.27846527496063544
iteration : 1767
train acc:  0.84375
train loss:  0.3452179729938507
train gradient:  0.31336994295815473
iteration : 1768
train acc:  0.796875
train loss:  0.46904897689819336
train gradient:  0.5707068788215522
iteration : 1769
train acc:  0.8203125
train loss:  0.3827192783355713
train gradient:  0.273401437423061
iteration : 1770
train acc:  0.796875
train loss:  0.41327571868896484
train gradient:  0.4922258721221373
iteration : 1771
train acc:  0.78125
train loss:  0.47060152888298035
train gradient:  0.5000435920379754
iteration : 1772
train acc:  0.8359375
train loss:  0.39679864048957825
train gradient:  0.37489254371746905
iteration : 1773
train acc:  0.8359375
train loss:  0.3845837116241455
train gradient:  0.48776181783082856
iteration : 1774
train acc:  0.8203125
train loss:  0.41010722517967224
train gradient:  0.36937969402871945
iteration : 1775
train acc:  0.8359375
train loss:  0.3962200880050659
train gradient:  0.34701965918191685
iteration : 1776
train acc:  0.828125
train loss:  0.39273601770401
train gradient:  0.4994628456787112
iteration : 1777
train acc:  0.828125
train loss:  0.40054619312286377
train gradient:  0.38206156709612116
iteration : 1778
train acc:  0.7734375
train loss:  0.47361472249031067
train gradient:  0.5722640934052307
iteration : 1779
train acc:  0.84375
train loss:  0.418737530708313
train gradient:  0.49805644496684565
iteration : 1780
train acc:  0.828125
train loss:  0.3484875559806824
train gradient:  0.27079814875633357
iteration : 1781
train acc:  0.8125
train loss:  0.4358322322368622
train gradient:  0.48413484316196265
iteration : 1782
train acc:  0.8046875
train loss:  0.3996521234512329
train gradient:  0.41881644130921114
iteration : 1783
train acc:  0.78125
train loss:  0.44493281841278076
train gradient:  0.7119115309825773
iteration : 1784
train acc:  0.765625
train loss:  0.4916684329509735
train gradient:  0.6834519100138655
iteration : 1785
train acc:  0.875
train loss:  0.35908809304237366
train gradient:  0.2944733673242599
iteration : 1786
train acc:  0.8125
train loss:  0.391185998916626
train gradient:  0.3102175510686189
iteration : 1787
train acc:  0.84375
train loss:  0.37375175952911377
train gradient:  0.4529684117563765
iteration : 1788
train acc:  0.8125
train loss:  0.4286532998085022
train gradient:  0.4129332436206377
iteration : 1789
train acc:  0.796875
train loss:  0.3732951581478119
train gradient:  0.3577310839213431
iteration : 1790
train acc:  0.796875
train loss:  0.4173162579536438
train gradient:  0.4324161816516564
iteration : 1791
train acc:  0.7734375
train loss:  0.4194013774394989
train gradient:  0.32052013027025217
iteration : 1792
train acc:  0.7734375
train loss:  0.45602983236312866
train gradient:  0.5109576687230258
iteration : 1793
train acc:  0.859375
train loss:  0.396428644657135
train gradient:  0.2981638884961881
iteration : 1794
train acc:  0.8125
train loss:  0.3892713189125061
train gradient:  0.3619729241933332
iteration : 1795
train acc:  0.8125
train loss:  0.4055824279785156
train gradient:  0.344001419715007
iteration : 1796
train acc:  0.7890625
train loss:  0.44893497228622437
train gradient:  0.42403517982863914
iteration : 1797
train acc:  0.8046875
train loss:  0.40046292543411255
train gradient:  0.332349820724867
iteration : 1798
train acc:  0.7890625
train loss:  0.4471525251865387
train gradient:  0.44882755239897537
iteration : 1799
train acc:  0.78125
train loss:  0.43887752294540405
train gradient:  0.37570707306711143
iteration : 1800
train acc:  0.8125
train loss:  0.3843935430049896
train gradient:  0.29377483607583693
iteration : 1801
train acc:  0.7890625
train loss:  0.4557120203971863
train gradient:  0.45720378364456793
iteration : 1802
train acc:  0.796875
train loss:  0.422149121761322
train gradient:  0.37191063902700355
iteration : 1803
train acc:  0.796875
train loss:  0.4214543402194977
train gradient:  0.3417398660293039
iteration : 1804
train acc:  0.84375
train loss:  0.4145122766494751
train gradient:  0.40483667640738125
iteration : 1805
train acc:  0.796875
train loss:  0.39720359444618225
train gradient:  0.3356348486193665
iteration : 1806
train acc:  0.75
train loss:  0.5302935838699341
train gradient:  0.5634357754890988
iteration : 1807
train acc:  0.8125
train loss:  0.42951512336730957
train gradient:  0.38069851604124616
iteration : 1808
train acc:  0.859375
train loss:  0.3433394432067871
train gradient:  0.281831674819819
iteration : 1809
train acc:  0.765625
train loss:  0.5257158875465393
train gradient:  0.5295854336651569
iteration : 1810
train acc:  0.8359375
train loss:  0.3732595443725586
train gradient:  0.35168017392446393
iteration : 1811
train acc:  0.78125
train loss:  0.43760940432548523
train gradient:  0.40858423256842086
iteration : 1812
train acc:  0.796875
train loss:  0.450676292181015
train gradient:  0.482809087096902
iteration : 1813
train acc:  0.78125
train loss:  0.41761717200279236
train gradient:  0.3857072162704738
iteration : 1814
train acc:  0.8515625
train loss:  0.40674495697021484
train gradient:  0.4133661783792647
iteration : 1815
train acc:  0.7890625
train loss:  0.43458956480026245
train gradient:  0.39398231232825204
iteration : 1816
train acc:  0.828125
train loss:  0.40218693017959595
train gradient:  0.38187180565966017
iteration : 1817
train acc:  0.828125
train loss:  0.397866427898407
train gradient:  0.27729262310960345
iteration : 1818
train acc:  0.8046875
train loss:  0.3818104863166809
train gradient:  0.3044864371921388
iteration : 1819
train acc:  0.828125
train loss:  0.3403379023075104
train gradient:  0.3225835652458678
iteration : 1820
train acc:  0.859375
train loss:  0.35159051418304443
train gradient:  0.25443454376593205
iteration : 1821
train acc:  0.765625
train loss:  0.4560301899909973
train gradient:  0.4057123028508347
iteration : 1822
train acc:  0.8828125
train loss:  0.3145671784877777
train gradient:  0.35338146450459046
iteration : 1823
train acc:  0.890625
train loss:  0.29563021659851074
train gradient:  0.15202532879919672
iteration : 1824
train acc:  0.765625
train loss:  0.4269849359989166
train gradient:  0.3056114490685508
iteration : 1825
train acc:  0.8046875
train loss:  0.3960438370704651
train gradient:  0.3772283329921502
iteration : 1826
train acc:  0.8046875
train loss:  0.41640836000442505
train gradient:  0.40367083976779466
iteration : 1827
train acc:  0.78125
train loss:  0.4417434334754944
train gradient:  0.35608702132071934
iteration : 1828
train acc:  0.796875
train loss:  0.38689547777175903
train gradient:  0.32683675935931406
iteration : 1829
train acc:  0.84375
train loss:  0.38685381412506104
train gradient:  0.28988264310772294
iteration : 1830
train acc:  0.8515625
train loss:  0.3691563606262207
train gradient:  0.3114803792568638
iteration : 1831
train acc:  0.828125
train loss:  0.4710192084312439
train gradient:  0.490956726064125
iteration : 1832
train acc:  0.8515625
train loss:  0.3455224633216858
train gradient:  0.34845338641249707
iteration : 1833
train acc:  0.890625
train loss:  0.33661675453186035
train gradient:  0.24505807432173626
iteration : 1834
train acc:  0.828125
train loss:  0.3801841735839844
train gradient:  0.38594649975055106
iteration : 1835
train acc:  0.8203125
train loss:  0.4075038433074951
train gradient:  0.4554998385172569
iteration : 1836
train acc:  0.796875
train loss:  0.42349401116371155
train gradient:  0.36962317876894846
iteration : 1837
train acc:  0.84375
train loss:  0.3325231075286865
train gradient:  0.2487330288245587
iteration : 1838
train acc:  0.8359375
train loss:  0.4062809348106384
train gradient:  0.34291842214808943
iteration : 1839
train acc:  0.7421875
train loss:  0.49085646867752075
train gradient:  0.4424313290605631
iteration : 1840
train acc:  0.8203125
train loss:  0.43426400423049927
train gradient:  0.4790178016872413
iteration : 1841
train acc:  0.8359375
train loss:  0.4228563904762268
train gradient:  0.38028788795377133
iteration : 1842
train acc:  0.796875
train loss:  0.3977850079536438
train gradient:  0.3569526742482365
iteration : 1843
train acc:  0.8515625
train loss:  0.340794175863266
train gradient:  0.2661971966144359
iteration : 1844
train acc:  0.8203125
train loss:  0.34978604316711426
train gradient:  0.29255861534568645
iteration : 1845
train acc:  0.8203125
train loss:  0.4410457909107208
train gradient:  0.3788172367676447
iteration : 1846
train acc:  0.8203125
train loss:  0.43338391184806824
train gradient:  0.43271930284532
iteration : 1847
train acc:  0.8125
train loss:  0.39780741930007935
train gradient:  0.4566154973591611
iteration : 1848
train acc:  0.796875
train loss:  0.45179182291030884
train gradient:  0.45519589119079823
iteration : 1849
train acc:  0.84375
train loss:  0.42953425645828247
train gradient:  0.42708731530293925
iteration : 1850
train acc:  0.78125
train loss:  0.46997305750846863
train gradient:  0.5179635772350664
iteration : 1851
train acc:  0.8359375
train loss:  0.3616737425327301
train gradient:  0.29598427423175
iteration : 1852
train acc:  0.8125
train loss:  0.3729017376899719
train gradient:  0.3650169610115372
iteration : 1853
train acc:  0.8046875
train loss:  0.4229874014854431
train gradient:  0.43857720187656873
iteration : 1854
train acc:  0.796875
train loss:  0.45182114839553833
train gradient:  0.47444545696769597
iteration : 1855
train acc:  0.8125
train loss:  0.44532299041748047
train gradient:  0.5563321610813398
iteration : 1856
train acc:  0.8828125
train loss:  0.3150356113910675
train gradient:  0.348187869746371
iteration : 1857
train acc:  0.765625
train loss:  0.45762038230895996
train gradient:  0.7158093660528312
iteration : 1858
train acc:  0.8203125
train loss:  0.38441145420074463
train gradient:  0.3390106614006231
iteration : 1859
train acc:  0.8046875
train loss:  0.4192776679992676
train gradient:  0.5708337314169631
iteration : 1860
train acc:  0.8046875
train loss:  0.4550914764404297
train gradient:  0.4345518147903585
iteration : 1861
train acc:  0.8046875
train loss:  0.3859419822692871
train gradient:  0.36425523921678116
iteration : 1862
train acc:  0.8125
train loss:  0.39626428484916687
train gradient:  0.3166359784572074
iteration : 1863
train acc:  0.765625
train loss:  0.441142737865448
train gradient:  0.37654111148686736
iteration : 1864
train acc:  0.8125
train loss:  0.4167766869068146
train gradient:  0.3797980553003599
iteration : 1865
train acc:  0.8046875
train loss:  0.44387876987457275
train gradient:  0.3916279274208752
iteration : 1866
train acc:  0.796875
train loss:  0.40089574456214905
train gradient:  0.3456052602926594
iteration : 1867
train acc:  0.828125
train loss:  0.4622255563735962
train gradient:  0.3674425057134549
iteration : 1868
train acc:  0.84375
train loss:  0.3855714201927185
train gradient:  0.39747594457564694
iteration : 1869
train acc:  0.7890625
train loss:  0.35103967785835266
train gradient:  0.2923682554084402
iteration : 1870
train acc:  0.84375
train loss:  0.3662564158439636
train gradient:  0.4969617250065163
iteration : 1871
train acc:  0.8203125
train loss:  0.4221577048301697
train gradient:  1.9616543757275042
iteration : 1872
train acc:  0.78125
train loss:  0.4682045578956604
train gradient:  0.5213034128778825
iteration : 1873
train acc:  0.8046875
train loss:  0.4301714599132538
train gradient:  0.39403378451600585
iteration : 1874
train acc:  0.7734375
train loss:  0.4230462610721588
train gradient:  0.5234755504413164
iteration : 1875
train acc:  0.828125
train loss:  0.4534534811973572
train gradient:  0.5041768134091311
iteration : 1876
train acc:  0.828125
train loss:  0.3918454647064209
train gradient:  0.30865790447505154
iteration : 1877
train acc:  0.8125
train loss:  0.38825252652168274
train gradient:  0.514385111994881
iteration : 1878
train acc:  0.7578125
train loss:  0.4558662176132202
train gradient:  0.6629759337068796
iteration : 1879
train acc:  0.8359375
train loss:  0.386152058839798
train gradient:  0.42510969925422204
iteration : 1880
train acc:  0.828125
train loss:  0.39442282915115356
train gradient:  0.4059113813045412
iteration : 1881
train acc:  0.8125
train loss:  0.40948566794395447
train gradient:  0.4730110825836289
iteration : 1882
train acc:  0.796875
train loss:  0.4235069751739502
train gradient:  0.7004560071546193
iteration : 1883
train acc:  0.7890625
train loss:  0.4220331311225891
train gradient:  0.34118425000636826
iteration : 1884
train acc:  0.859375
train loss:  0.3738541603088379
train gradient:  0.42747075180599403
iteration : 1885
train acc:  0.84375
train loss:  0.39210397005081177
train gradient:  0.4005940994061697
iteration : 1886
train acc:  0.7890625
train loss:  0.40752100944519043
train gradient:  0.4742491462852125
iteration : 1887
train acc:  0.859375
train loss:  0.3358830213546753
train gradient:  0.3712975265413994
iteration : 1888
train acc:  0.8046875
train loss:  0.37808260321617126
train gradient:  0.34204253475191243
iteration : 1889
train acc:  0.8515625
train loss:  0.3324427604675293
train gradient:  0.3819896563032111
iteration : 1890
train acc:  0.796875
train loss:  0.4262203574180603
train gradient:  0.4769523474117182
iteration : 1891
train acc:  0.828125
train loss:  0.3593793511390686
train gradient:  0.347188531975861
iteration : 1892
train acc:  0.8203125
train loss:  0.4007869362831116
train gradient:  0.35647700428849655
iteration : 1893
train acc:  0.84375
train loss:  0.38787996768951416
train gradient:  0.38106714883851306
iteration : 1894
train acc:  0.796875
train loss:  0.3880517780780792
train gradient:  0.48964574700653374
iteration : 1895
train acc:  0.8046875
train loss:  0.3822176456451416
train gradient:  0.3699318551613922
iteration : 1896
train acc:  0.7890625
train loss:  0.41308045387268066
train gradient:  0.42271258201860307
iteration : 1897
train acc:  0.8046875
train loss:  0.4273785352706909
train gradient:  0.5276528221649657
iteration : 1898
train acc:  0.765625
train loss:  0.41062602400779724
train gradient:  0.41801320724109686
iteration : 1899
train acc:  0.828125
train loss:  0.36507952213287354
train gradient:  0.2945899337601762
iteration : 1900
train acc:  0.8515625
train loss:  0.37057921290397644
train gradient:  0.3357070981236528
iteration : 1901
train acc:  0.78125
train loss:  0.4526427984237671
train gradient:  0.40401844327741504
iteration : 1902
train acc:  0.828125
train loss:  0.3924134373664856
train gradient:  0.5247553448342632
iteration : 1903
train acc:  0.828125
train loss:  0.40245139598846436
train gradient:  0.4443249849372994
iteration : 1904
train acc:  0.7890625
train loss:  0.4828585982322693
train gradient:  0.48871651162113977
iteration : 1905
train acc:  0.84375
train loss:  0.41673919558525085
train gradient:  0.46477602659336503
iteration : 1906
train acc:  0.7890625
train loss:  0.4213767647743225
train gradient:  0.49282432088809774
iteration : 1907
train acc:  0.8359375
train loss:  0.35960280895233154
train gradient:  0.38747594336826086
iteration : 1908
train acc:  0.7734375
train loss:  0.4405859708786011
train gradient:  0.5167321511459635
iteration : 1909
train acc:  0.796875
train loss:  0.46002882719039917
train gradient:  0.6814286725591868
iteration : 1910
train acc:  0.7890625
train loss:  0.38269370794296265
train gradient:  0.40798341694548707
iteration : 1911
train acc:  0.875
train loss:  0.3560695946216583
train gradient:  0.3215846301250873
iteration : 1912
train acc:  0.8046875
train loss:  0.46185046434402466
train gradient:  0.5392930004664348
iteration : 1913
train acc:  0.8046875
train loss:  0.3950923681259155
train gradient:  0.4704421729795157
iteration : 1914
train acc:  0.7734375
train loss:  0.4586792588233948
train gradient:  0.7813567217279745
iteration : 1915
train acc:  0.8125
train loss:  0.3893747329711914
train gradient:  0.4949404094509395
iteration : 1916
train acc:  0.8515625
train loss:  0.3850691318511963
train gradient:  0.2943760948241299
iteration : 1917
train acc:  0.8828125
train loss:  0.37523916363716125
train gradient:  0.33773193161543147
iteration : 1918
train acc:  0.796875
train loss:  0.44438281655311584
train gradient:  0.49371481024836605
iteration : 1919
train acc:  0.8515625
train loss:  0.3837554454803467
train gradient:  0.46272394707260206
iteration : 1920
train acc:  0.8359375
train loss:  0.4333633780479431
train gradient:  0.5018937578222084
iteration : 1921
train acc:  0.8515625
train loss:  0.37888938188552856
train gradient:  0.27509018218073866
iteration : 1922
train acc:  0.8046875
train loss:  0.501704216003418
train gradient:  0.6618654838103224
iteration : 1923
train acc:  0.796875
train loss:  0.44471287727355957
train gradient:  0.5652106331791543
iteration : 1924
train acc:  0.7265625
train loss:  0.5255882143974304
train gradient:  0.5946603987194554
iteration : 1925
train acc:  0.8046875
train loss:  0.4267449676990509
train gradient:  0.6032487114256422
iteration : 1926
train acc:  0.84375
train loss:  0.3838070034980774
train gradient:  0.3449979471081519
iteration : 1927
train acc:  0.796875
train loss:  0.4062729477882385
train gradient:  0.46976205126940257
iteration : 1928
train acc:  0.8203125
train loss:  0.3912639617919922
train gradient:  0.36638145471876515
iteration : 1929
train acc:  0.890625
train loss:  0.2892872989177704
train gradient:  0.304016607134689
iteration : 1930
train acc:  0.84375
train loss:  0.34714511036872864
train gradient:  0.2910068041790834
iteration : 1931
train acc:  0.8203125
train loss:  0.4049304723739624
train gradient:  0.369923028144947
iteration : 1932
train acc:  0.8125
train loss:  0.39288824796676636
train gradient:  0.34423867904930383
iteration : 1933
train acc:  0.8046875
train loss:  0.422416627407074
train gradient:  0.3877894858636503
iteration : 1934
train acc:  0.8203125
train loss:  0.42892903089523315
train gradient:  0.4710842313263568
iteration : 1935
train acc:  0.84375
train loss:  0.34998318552970886
train gradient:  0.2737716656906336
iteration : 1936
train acc:  0.78125
train loss:  0.41251856088638306
train gradient:  0.38933487608885115
iteration : 1937
train acc:  0.875
train loss:  0.3593311309814453
train gradient:  0.33764414790183056
iteration : 1938
train acc:  0.8515625
train loss:  0.3521086573600769
train gradient:  0.2505752522190309
iteration : 1939
train acc:  0.8203125
train loss:  0.40695276856422424
train gradient:  0.30031496369726696
iteration : 1940
train acc:  0.828125
train loss:  0.403110533952713
train gradient:  0.3014875153951637
iteration : 1941
train acc:  0.8203125
train loss:  0.3877549171447754
train gradient:  0.4458209741688785
iteration : 1942
train acc:  0.875
train loss:  0.3415641784667969
train gradient:  0.2926570254453156
iteration : 1943
train acc:  0.828125
train loss:  0.3671759068965912
train gradient:  0.32233923757571836
iteration : 1944
train acc:  0.796875
train loss:  0.4077109098434448
train gradient:  0.47631404266279115
iteration : 1945
train acc:  0.828125
train loss:  0.39378613233566284
train gradient:  0.38677851813312975
iteration : 1946
train acc:  0.75
train loss:  0.5137335658073425
train gradient:  0.49410336209234296
iteration : 1947
train acc:  0.8359375
train loss:  0.37742358446121216
train gradient:  0.3199865047655771
iteration : 1948
train acc:  0.8046875
train loss:  0.4270176291465759
train gradient:  0.4080694487739773
iteration : 1949
train acc:  0.84375
train loss:  0.36998921632766724
train gradient:  0.42387101815693634
iteration : 1950
train acc:  0.828125
train loss:  0.33936867117881775
train gradient:  0.342306790906521
iteration : 1951
train acc:  0.9140625
train loss:  0.31364548206329346
train gradient:  0.31952070695029744
iteration : 1952
train acc:  0.8359375
train loss:  0.3603760004043579
train gradient:  0.43344963389751473
iteration : 1953
train acc:  0.8203125
train loss:  0.4039827287197113
train gradient:  0.35684465604264387
iteration : 1954
train acc:  0.7734375
train loss:  0.47223517298698425
train gradient:  0.7239000286270058
iteration : 1955
train acc:  0.8203125
train loss:  0.33047622442245483
train gradient:  0.2788897067802886
iteration : 1956
train acc:  0.84375
train loss:  0.40104299783706665
train gradient:  0.410746028429772
iteration : 1957
train acc:  0.84375
train loss:  0.4076385200023651
train gradient:  0.31143637874969843
iteration : 1958
train acc:  0.8046875
train loss:  0.4193795621395111
train gradient:  0.37173869202369025
iteration : 1959
train acc:  0.7890625
train loss:  0.4494616985321045
train gradient:  0.4118548461810208
iteration : 1960
train acc:  0.828125
train loss:  0.39686280488967896
train gradient:  0.40374003560100097
iteration : 1961
train acc:  0.7421875
train loss:  0.4212481379508972
train gradient:  0.48745441903576836
iteration : 1962
train acc:  0.8203125
train loss:  0.36925017833709717
train gradient:  0.38821723418504744
iteration : 1963
train acc:  0.84375
train loss:  0.37131547927856445
train gradient:  0.28200130213191543
iteration : 1964
train acc:  0.8359375
train loss:  0.4046138525009155
train gradient:  0.5450553665545607
iteration : 1965
train acc:  0.828125
train loss:  0.3794655203819275
train gradient:  0.4257360916951776
iteration : 1966
train acc:  0.859375
train loss:  0.3423013389110565
train gradient:  0.33577545935295067
iteration : 1967
train acc:  0.78125
train loss:  0.47867587208747864
train gradient:  0.6231060663227929
iteration : 1968
train acc:  0.7890625
train loss:  0.3947891592979431
train gradient:  0.4944337632513664
iteration : 1969
train acc:  0.7265625
train loss:  0.4577067196369171
train gradient:  0.5401795201668818
iteration : 1970
train acc:  0.875
train loss:  0.33675798773765564
train gradient:  0.4081522851900455
iteration : 1971
train acc:  0.8125
train loss:  0.43981459736824036
train gradient:  0.5614045277248795
iteration : 1972
train acc:  0.8671875
train loss:  0.3869447112083435
train gradient:  0.4823589743146187
iteration : 1973
train acc:  0.8671875
train loss:  0.3039380609989166
train gradient:  0.2532111927163406
iteration : 1974
train acc:  0.8828125
train loss:  0.3170546591281891
train gradient:  0.24332554776613388
iteration : 1975
train acc:  0.8125
train loss:  0.3988726735115051
train gradient:  0.30767723855764006
iteration : 1976
train acc:  0.859375
train loss:  0.406499445438385
train gradient:  0.4653567601008738
iteration : 1977
train acc:  0.84375
train loss:  0.3775135278701782
train gradient:  0.4228668853192499
iteration : 1978
train acc:  0.828125
train loss:  0.3805515766143799
train gradient:  0.33343369733420203
iteration : 1979
train acc:  0.8359375
train loss:  0.3955090045928955
train gradient:  0.5007172309341943
iteration : 1980
train acc:  0.765625
train loss:  0.4729600250720978
train gradient:  0.5937954110634105
iteration : 1981
train acc:  0.8203125
train loss:  0.3925725817680359
train gradient:  0.405071603059255
iteration : 1982
train acc:  0.828125
train loss:  0.43877947330474854
train gradient:  0.49951942277950145
iteration : 1983
train acc:  0.7890625
train loss:  0.41790494322776794
train gradient:  0.4757115619117497
iteration : 1984
train acc:  0.828125
train loss:  0.3465995192527771
train gradient:  0.2865138578757439
iteration : 1985
train acc:  0.8125
train loss:  0.3815779983997345
train gradient:  0.3993443827066927
iteration : 1986
train acc:  0.84375
train loss:  0.3576720356941223
train gradient:  0.35774026034752965
iteration : 1987
train acc:  0.8046875
train loss:  0.43777740001678467
train gradient:  0.4022022799510788
iteration : 1988
train acc:  0.84375
train loss:  0.3885635733604431
train gradient:  0.29031576023749495
iteration : 1989
train acc:  0.84375
train loss:  0.31708043813705444
train gradient:  0.34232654535838825
iteration : 1990
train acc:  0.84375
train loss:  0.3427484631538391
train gradient:  0.2803877043411529
iteration : 1991
train acc:  0.8203125
train loss:  0.36079850792884827
train gradient:  0.3315176716237596
iteration : 1992
train acc:  0.7890625
train loss:  0.4391636252403259
train gradient:  0.5436818936961174
iteration : 1993
train acc:  0.84375
train loss:  0.3758700489997864
train gradient:  0.33096373914376087
iteration : 1994
train acc:  0.84375
train loss:  0.39868906140327454
train gradient:  0.36557877446732356
iteration : 1995
train acc:  0.8125
train loss:  0.3914211690425873
train gradient:  0.4148216276772097
iteration : 1996
train acc:  0.828125
train loss:  0.35532262921333313
train gradient:  0.4582894465462892
iteration : 1997
train acc:  0.8046875
train loss:  0.40388748049736023
train gradient:  0.44193988901073095
iteration : 1998
train acc:  0.7578125
train loss:  0.4802708625793457
train gradient:  0.5802297625901518
iteration : 1999
train acc:  0.78125
train loss:  0.49130773544311523
train gradient:  0.6333722737675571
iteration : 2000
train acc:  0.7734375
train loss:  0.4499945044517517
train gradient:  0.5192962533610335
iteration : 2001
train acc:  0.828125
train loss:  0.429656058549881
train gradient:  0.7480287965372219
iteration : 2002
train acc:  0.7734375
train loss:  0.3798596262931824
train gradient:  0.35168896569669794
iteration : 2003
train acc:  0.8125
train loss:  0.4572249948978424
train gradient:  0.446178599653292
iteration : 2004
train acc:  0.75
train loss:  0.5217623710632324
train gradient:  0.5544788559799222
iteration : 2005
train acc:  0.8125
train loss:  0.3958471119403839
train gradient:  0.3531711891846131
iteration : 2006
train acc:  0.875
train loss:  0.3744231164455414
train gradient:  0.37216237315844425
iteration : 2007
train acc:  0.8046875
train loss:  0.42994898557662964
train gradient:  0.4252863132282645
iteration : 2008
train acc:  0.8203125
train loss:  0.37844496965408325
train gradient:  0.35169257036806867
iteration : 2009
train acc:  0.828125
train loss:  0.40808314085006714
train gradient:  0.40064841598817147
iteration : 2010
train acc:  0.84375
train loss:  0.3602098524570465
train gradient:  0.332622930584554
iteration : 2011
train acc:  0.84375
train loss:  0.4140728712081909
train gradient:  0.44532037093308335
iteration : 2012
train acc:  0.8203125
train loss:  0.3727087378501892
train gradient:  0.39662195446201487
iteration : 2013
train acc:  0.828125
train loss:  0.36037516593933105
train gradient:  0.4334619391894295
iteration : 2014
train acc:  0.828125
train loss:  0.3708701729774475
train gradient:  0.49128758124988425
iteration : 2015
train acc:  0.75
train loss:  0.451880544424057
train gradient:  0.6522481963503814
iteration : 2016
train acc:  0.828125
train loss:  0.43368011713027954
train gradient:  0.48938729337655656
iteration : 2017
train acc:  0.7890625
train loss:  0.4494606852531433
train gradient:  0.4337137226320868
iteration : 2018
train acc:  0.8359375
train loss:  0.37612485885620117
train gradient:  0.45942206831074267
iteration : 2019
train acc:  0.8203125
train loss:  0.38397347927093506
train gradient:  0.40493196756745103
iteration : 2020
train acc:  0.765625
train loss:  0.4757544994354248
train gradient:  0.5523581779734337
iteration : 2021
train acc:  0.7890625
train loss:  0.41345834732055664
train gradient:  0.4177018842922381
iteration : 2022
train acc:  0.828125
train loss:  0.41837289929389954
train gradient:  0.37643388171239833
iteration : 2023
train acc:  0.8046875
train loss:  0.4640020430088043
train gradient:  0.4859489039303854
iteration : 2024
train acc:  0.78125
train loss:  0.46870511770248413
train gradient:  0.48431548134415675
iteration : 2025
train acc:  0.84375
train loss:  0.4214555025100708
train gradient:  0.4146037968941032
iteration : 2026
train acc:  0.8359375
train loss:  0.4378505349159241
train gradient:  0.4149600127297252
iteration : 2027
train acc:  0.78125
train loss:  0.40098267793655396
train gradient:  0.36223092118417266
iteration : 2028
train acc:  0.859375
train loss:  0.3277309834957123
train gradient:  0.3166914506951672
iteration : 2029
train acc:  0.8125
train loss:  0.3699913024902344
train gradient:  0.27377776811173077
iteration : 2030
train acc:  0.859375
train loss:  0.4260939955711365
train gradient:  0.30839583608525056
iteration : 2031
train acc:  0.796875
train loss:  0.41082853078842163
train gradient:  0.2985003625195557
iteration : 2032
train acc:  0.8203125
train loss:  0.39387643337249756
train gradient:  0.359752077793975
iteration : 2033
train acc:  0.8203125
train loss:  0.35557687282562256
train gradient:  0.30422493616362534
iteration : 2034
train acc:  0.875
train loss:  0.30845123529434204
train gradient:  0.23606323108848587
iteration : 2035
train acc:  0.8125
train loss:  0.40497517585754395
train gradient:  0.3648564142182366
iteration : 2036
train acc:  0.765625
train loss:  0.44271472096443176
train gradient:  0.5216747208232468
iteration : 2037
train acc:  0.8125
train loss:  0.41236358880996704
train gradient:  0.4275057921667664
iteration : 2038
train acc:  0.8046875
train loss:  0.36998456716537476
train gradient:  0.40820778682984743
iteration : 2039
train acc:  0.8515625
train loss:  0.39987683296203613
train gradient:  0.35105390930862124
iteration : 2040
train acc:  0.828125
train loss:  0.42597270011901855
train gradient:  0.5170440659472781
iteration : 2041
train acc:  0.84375
train loss:  0.36683955788612366
train gradient:  0.3933244155172243
iteration : 2042
train acc:  0.859375
train loss:  0.3305593729019165
train gradient:  0.2078692565775496
iteration : 2043
train acc:  0.828125
train loss:  0.3851829767227173
train gradient:  0.4680346012216438
iteration : 2044
train acc:  0.7890625
train loss:  0.46066755056381226
train gradient:  0.44746412000864744
iteration : 2045
train acc:  0.859375
train loss:  0.340713232755661
train gradient:  0.3709456544920672
iteration : 2046
train acc:  0.7734375
train loss:  0.4526534974575043
train gradient:  0.5003923168997362
iteration : 2047
train acc:  0.8046875
train loss:  0.4040127098560333
train gradient:  0.358190266171281
iteration : 2048
train acc:  0.7890625
train loss:  0.4527042508125305
train gradient:  0.5410934521960848
iteration : 2049
train acc:  0.78125
train loss:  0.4037691354751587
train gradient:  0.3260211683697746
iteration : 2050
train acc:  0.796875
train loss:  0.3984358310699463
train gradient:  0.41920622917657036
iteration : 2051
train acc:  0.8203125
train loss:  0.3901495933532715
train gradient:  0.4259480190948508
iteration : 2052
train acc:  0.8203125
train loss:  0.40963906049728394
train gradient:  0.45849734626147476
iteration : 2053
train acc:  0.8359375
train loss:  0.3469623327255249
train gradient:  0.2793766043745725
iteration : 2054
train acc:  0.84375
train loss:  0.35906511545181274
train gradient:  0.3082382117965425
iteration : 2055
train acc:  0.75
train loss:  0.4829774498939514
train gradient:  0.5073428026707276
iteration : 2056
train acc:  0.8359375
train loss:  0.3917502164840698
train gradient:  0.37188606100345895
iteration : 2057
train acc:  0.8359375
train loss:  0.38138437271118164
train gradient:  0.3731836301891979
iteration : 2058
train acc:  0.8046875
train loss:  0.41530081629753113
train gradient:  0.4610744123943128
iteration : 2059
train acc:  0.8203125
train loss:  0.42953991889953613
train gradient:  0.47186536437790333
iteration : 2060
train acc:  0.84375
train loss:  0.3520020544528961
train gradient:  0.49056110246690027
iteration : 2061
train acc:  0.8359375
train loss:  0.4083392024040222
train gradient:  0.43483000447258163
iteration : 2062
train acc:  0.84375
train loss:  0.35336291790008545
train gradient:  0.3111659769987572
iteration : 2063
train acc:  0.8515625
train loss:  0.36966535449028015
train gradient:  0.22669584086067235
iteration : 2064
train acc:  0.7578125
train loss:  0.5177739262580872
train gradient:  0.607983920303665
iteration : 2065
train acc:  0.78125
train loss:  0.4782542288303375
train gradient:  0.49947247512116605
iteration : 2066
train acc:  0.8359375
train loss:  0.36002811789512634
train gradient:  0.28882177384234325
iteration : 2067
train acc:  0.796875
train loss:  0.38341403007507324
train gradient:  0.3892584209396568
iteration : 2068
train acc:  0.7890625
train loss:  0.4127837121486664
train gradient:  0.37621045241816276
iteration : 2069
train acc:  0.828125
train loss:  0.42571431398391724
train gradient:  0.5851105390297379
iteration : 2070
train acc:  0.90625
train loss:  0.296774297952652
train gradient:  0.21788904292150726
iteration : 2071
train acc:  0.8125
train loss:  0.40158140659332275
train gradient:  0.3328416357490867
iteration : 2072
train acc:  0.78125
train loss:  0.47001999616622925
train gradient:  0.5140683501195189
iteration : 2073
train acc:  0.8203125
train loss:  0.4179929494857788
train gradient:  0.5027244999562384
iteration : 2074
train acc:  0.828125
train loss:  0.42301130294799805
train gradient:  0.3479895749891671
iteration : 2075
train acc:  0.84375
train loss:  0.3582063317298889
train gradient:  0.31159164751041407
iteration : 2076
train acc:  0.7890625
train loss:  0.418194979429245
train gradient:  0.45983429131508835
iteration : 2077
train acc:  0.859375
train loss:  0.3352787494659424
train gradient:  0.2985498812760289
iteration : 2078
train acc:  0.78125
train loss:  0.39865952730178833
train gradient:  0.49143376976706554
iteration : 2079
train acc:  0.859375
train loss:  0.36204564571380615
train gradient:  0.25190343070527377
iteration : 2080
train acc:  0.7421875
train loss:  0.4897225499153137
train gradient:  0.411721792585409
iteration : 2081
train acc:  0.8125
train loss:  0.4073861837387085
train gradient:  0.3485842745834261
iteration : 2082
train acc:  0.875
train loss:  0.35738617181777954
train gradient:  0.3589937546050018
iteration : 2083
train acc:  0.875
train loss:  0.3283643126487732
train gradient:  0.3087462163434391
iteration : 2084
train acc:  0.8359375
train loss:  0.3414872884750366
train gradient:  0.32375411632558765
iteration : 2085
train acc:  0.8359375
train loss:  0.36889374256134033
train gradient:  0.303879527571396
iteration : 2086
train acc:  0.8046875
train loss:  0.47212645411491394
train gradient:  0.5222509085380768
iteration : 2087
train acc:  0.84375
train loss:  0.39880502223968506
train gradient:  0.33363615546282044
iteration : 2088
train acc:  0.8046875
train loss:  0.3696163296699524
train gradient:  0.43170730824290454
iteration : 2089
train acc:  0.765625
train loss:  0.4546836018562317
train gradient:  0.5036192682433746
iteration : 2090
train acc:  0.8125
train loss:  0.39305639266967773
train gradient:  0.29012346207785694
iteration : 2091
train acc:  0.8359375
train loss:  0.3737233281135559
train gradient:  0.33592683093399767
iteration : 2092
train acc:  0.78125
train loss:  0.43545961380004883
train gradient:  0.465665258227916
iteration : 2093
train acc:  0.8359375
train loss:  0.35050469636917114
train gradient:  0.40644453020683063
iteration : 2094
train acc:  0.8203125
train loss:  0.40172815322875977
train gradient:  0.422301223126055
iteration : 2095
train acc:  0.8515625
train loss:  0.33705222606658936
train gradient:  0.2972468485583859
iteration : 2096
train acc:  0.8828125
train loss:  0.30850639939308167
train gradient:  0.21187163948818544
iteration : 2097
train acc:  0.8359375
train loss:  0.3555145263671875
train gradient:  0.41245300522893585
iteration : 2098
train acc:  0.8203125
train loss:  0.4225223958492279
train gradient:  0.34895758098102725
iteration : 2099
train acc:  0.7890625
train loss:  0.45424988865852356
train gradient:  0.5826656287697151
iteration : 2100
train acc:  0.8203125
train loss:  0.4252680540084839
train gradient:  0.40061894563383327
iteration : 2101
train acc:  0.8203125
train loss:  0.36862218379974365
train gradient:  0.2907603879050462
iteration : 2102
train acc:  0.765625
train loss:  0.4723336100578308
train gradient:  0.5631337354439168
iteration : 2103
train acc:  0.828125
train loss:  0.38442105054855347
train gradient:  0.3625399086413879
iteration : 2104
train acc:  0.7890625
train loss:  0.40978577733039856
train gradient:  0.4257259599660826
iteration : 2105
train acc:  0.7890625
train loss:  0.46348780393600464
train gradient:  0.557565738950547
iteration : 2106
train acc:  0.828125
train loss:  0.3902851343154907
train gradient:  0.3832238394417149
iteration : 2107
train acc:  0.8203125
train loss:  0.37004080414772034
train gradient:  0.4448739299187302
iteration : 2108
train acc:  0.7890625
train loss:  0.4495326280593872
train gradient:  0.41113194564949057
iteration : 2109
train acc:  0.828125
train loss:  0.3940214514732361
train gradient:  0.47403640084121085
iteration : 2110
train acc:  0.8515625
train loss:  0.34793561697006226
train gradient:  0.4054497052511729
iteration : 2111
train acc:  0.7734375
train loss:  0.467833012342453
train gradient:  0.4655372574352152
iteration : 2112
train acc:  0.8046875
train loss:  0.40251854062080383
train gradient:  0.38610443656034243
iteration : 2113
train acc:  0.8359375
train loss:  0.350626677274704
train gradient:  0.44259814459951186
iteration : 2114
train acc:  0.84375
train loss:  0.36751431226730347
train gradient:  0.34058553443255535
iteration : 2115
train acc:  0.796875
train loss:  0.3835483193397522
train gradient:  0.3884043997688981
iteration : 2116
train acc:  0.84375
train loss:  0.3788154721260071
train gradient:  0.26358923705569137
iteration : 2117
train acc:  0.7421875
train loss:  0.512484073638916
train gradient:  0.7052609052312084
iteration : 2118
train acc:  0.8046875
train loss:  0.39607715606689453
train gradient:  0.5263523588415164
iteration : 2119
train acc:  0.828125
train loss:  0.38132116198539734
train gradient:  0.5355352670206408
iteration : 2120
train acc:  0.8515625
train loss:  0.38616853952407837
train gradient:  0.399081391043968
iteration : 2121
train acc:  0.796875
train loss:  0.45218855142593384
train gradient:  0.4770119937255151
iteration : 2122
train acc:  0.765625
train loss:  0.41499435901641846
train gradient:  0.4088595804912112
iteration : 2123
train acc:  0.8515625
train loss:  0.3546641170978546
train gradient:  0.3149024886999639
iteration : 2124
train acc:  0.828125
train loss:  0.41140443086624146
train gradient:  0.321960048796753
iteration : 2125
train acc:  0.84375
train loss:  0.3914472460746765
train gradient:  0.4066749663340247
iteration : 2126
train acc:  0.78125
train loss:  0.4429747462272644
train gradient:  0.5157533166489329
iteration : 2127
train acc:  0.78125
train loss:  0.3663500249385834
train gradient:  0.36437252427600275
iteration : 2128
train acc:  0.8828125
train loss:  0.3191385269165039
train gradient:  0.23259249353418868
iteration : 2129
train acc:  0.8203125
train loss:  0.4232464134693146
train gradient:  0.44296143112082076
iteration : 2130
train acc:  0.8203125
train loss:  0.34043100476264954
train gradient:  0.3054197144522795
iteration : 2131
train acc:  0.8359375
train loss:  0.3582044541835785
train gradient:  0.35125212911661424
iteration : 2132
train acc:  0.8671875
train loss:  0.28850096464157104
train gradient:  0.22114897037505774
iteration : 2133
train acc:  0.8828125
train loss:  0.35808444023132324
train gradient:  0.3563594485365696
iteration : 2134
train acc:  0.8203125
train loss:  0.3779759109020233
train gradient:  0.6847071181139173
iteration : 2135
train acc:  0.8828125
train loss:  0.31786757707595825
train gradient:  0.2720635870654796
iteration : 2136
train acc:  0.8203125
train loss:  0.3371480107307434
train gradient:  0.27829565506612886
iteration : 2137
train acc:  0.8515625
train loss:  0.36002862453460693
train gradient:  0.3502341979310435
iteration : 2138
train acc:  0.7890625
train loss:  0.4111543893814087
train gradient:  0.40494915666061126
iteration : 2139
train acc:  0.7890625
train loss:  0.49175307154655457
train gradient:  0.5466405787522981
iteration : 2140
train acc:  0.7578125
train loss:  0.4719598591327667
train gradient:  0.5386874383689728
iteration : 2141
train acc:  0.8203125
train loss:  0.4090386629104614
train gradient:  0.4503297566881706
iteration : 2142
train acc:  0.8125
train loss:  0.40753641724586487
train gradient:  0.29306397946156115
iteration : 2143
train acc:  0.7734375
train loss:  0.49829569458961487
train gradient:  0.6050323876486534
iteration : 2144
train acc:  0.78125
train loss:  0.41188591718673706
train gradient:  0.4993285211490328
iteration : 2145
train acc:  0.796875
train loss:  0.4269716739654541
train gradient:  0.46808686606775485
iteration : 2146
train acc:  0.8828125
train loss:  0.32940050959587097
train gradient:  0.26176426033003125
iteration : 2147
train acc:  0.7734375
train loss:  0.47596338391304016
train gradient:  0.4242876424062141
iteration : 2148
train acc:  0.8515625
train loss:  0.31175050139427185
train gradient:  0.2718879848063446
iteration : 2149
train acc:  0.734375
train loss:  0.4330178499221802
train gradient:  0.6303287618557795
iteration : 2150
train acc:  0.859375
train loss:  0.370789498090744
train gradient:  0.4510178361908636
iteration : 2151
train acc:  0.84375
train loss:  0.34116360545158386
train gradient:  0.3428214350314329
iteration : 2152
train acc:  0.8828125
train loss:  0.3170173764228821
train gradient:  0.31525197262549864
iteration : 2153
train acc:  0.90625
train loss:  0.2686968147754669
train gradient:  0.24963285376751387
iteration : 2154
train acc:  0.8203125
train loss:  0.4268497824668884
train gradient:  0.3441387697621877
iteration : 2155
train acc:  0.84375
train loss:  0.3763369619846344
train gradient:  0.3877154075567467
iteration : 2156
train acc:  0.7734375
train loss:  0.394581139087677
train gradient:  0.36866700826979837
iteration : 2157
train acc:  0.84375
train loss:  0.4065057039260864
train gradient:  0.44424825118639205
iteration : 2158
train acc:  0.828125
train loss:  0.36902374029159546
train gradient:  0.3092258695791882
iteration : 2159
train acc:  0.8671875
train loss:  0.32962217926979065
train gradient:  0.4554789537971935
iteration : 2160
train acc:  0.8984375
train loss:  0.3248535990715027
train gradient:  0.45144719480869466
iteration : 2161
train acc:  0.8515625
train loss:  0.3701649606227875
train gradient:  0.3595838490694289
iteration : 2162
train acc:  0.8203125
train loss:  0.37501388788223267
train gradient:  0.35441315579624644
iteration : 2163
train acc:  0.7890625
train loss:  0.381998747587204
train gradient:  0.48930855679354124
iteration : 2164
train acc:  0.8515625
train loss:  0.3382073938846588
train gradient:  0.3922628900565524
iteration : 2165
train acc:  0.7734375
train loss:  0.4484732449054718
train gradient:  0.5096878213559286
iteration : 2166
train acc:  0.8203125
train loss:  0.4494258463382721
train gradient:  0.5429567627973289
iteration : 2167
train acc:  0.8046875
train loss:  0.34394437074661255
train gradient:  0.44618348930479107
iteration : 2168
train acc:  0.84375
train loss:  0.3454449772834778
train gradient:  0.20557465725498253
iteration : 2169
train acc:  0.828125
train loss:  0.40924060344696045
train gradient:  0.43319338306015515
iteration : 2170
train acc:  0.84375
train loss:  0.39181554317474365
train gradient:  0.332238987649574
iteration : 2171
train acc:  0.8203125
train loss:  0.3835309147834778
train gradient:  0.567403062161267
iteration : 2172
train acc:  0.8671875
train loss:  0.3743673264980316
train gradient:  0.3733256990301855
iteration : 2173
train acc:  0.828125
train loss:  0.41887959837913513
train gradient:  0.5750503909703525
iteration : 2174
train acc:  0.8125
train loss:  0.36934894323349
train gradient:  0.3747455496300867
iteration : 2175
train acc:  0.7734375
train loss:  0.439255952835083
train gradient:  0.46311106594048695
iteration : 2176
train acc:  0.7890625
train loss:  0.4158306121826172
train gradient:  0.5254641349476885
iteration : 2177
train acc:  0.8046875
train loss:  0.4087941348552704
train gradient:  0.46403615312671614
iteration : 2178
train acc:  0.84375
train loss:  0.3759460151195526
train gradient:  0.44544043109536235
iteration : 2179
train acc:  0.8203125
train loss:  0.44198524951934814
train gradient:  0.5688798012425061
iteration : 2180
train acc:  0.828125
train loss:  0.38736391067504883
train gradient:  0.45666321238282
iteration : 2181
train acc:  0.8515625
train loss:  0.3524896800518036
train gradient:  0.34725806256501646
iteration : 2182
train acc:  0.78125
train loss:  0.463168203830719
train gradient:  0.49823938871721957
iteration : 2183
train acc:  0.78125
train loss:  0.4709544777870178
train gradient:  0.6206130838423056
iteration : 2184
train acc:  0.828125
train loss:  0.3719452917575836
train gradient:  0.32843349096194135
iteration : 2185
train acc:  0.8203125
train loss:  0.40085726976394653
train gradient:  0.4052548191064936
iteration : 2186
train acc:  0.828125
train loss:  0.36723771691322327
train gradient:  0.5047356195115159
iteration : 2187
train acc:  0.828125
train loss:  0.4788164496421814
train gradient:  0.5488758341389574
iteration : 2188
train acc:  0.7578125
train loss:  0.44691863656044006
train gradient:  0.6095330015412221
iteration : 2189
train acc:  0.8515625
train loss:  0.3443467319011688
train gradient:  0.5041766522033226
iteration : 2190
train acc:  0.8046875
train loss:  0.40536582469940186
train gradient:  0.4312614750167296
iteration : 2191
train acc:  0.78125
train loss:  0.44088563323020935
train gradient:  0.6195530082310892
iteration : 2192
train acc:  0.7578125
train loss:  0.49255430698394775
train gradient:  0.543930546009265
iteration : 2193
train acc:  0.7109375
train loss:  0.5253748893737793
train gradient:  0.7281528638149033
iteration : 2194
train acc:  0.7890625
train loss:  0.4254109263420105
train gradient:  0.43070599135160614
iteration : 2195
train acc:  0.8359375
train loss:  0.360842227935791
train gradient:  0.36616149146625904
iteration : 2196
train acc:  0.859375
train loss:  0.3692578673362732
train gradient:  0.33640878426446924
iteration : 2197
train acc:  0.8125
train loss:  0.37343087792396545
train gradient:  0.30107905794422657
iteration : 2198
train acc:  0.8515625
train loss:  0.33574116230010986
train gradient:  0.30637214061868345
iteration : 2199
train acc:  0.7578125
train loss:  0.4533187747001648
train gradient:  0.4442742586611119
iteration : 2200
train acc:  0.734375
train loss:  0.47402939200401306
train gradient:  0.3890572379740444
iteration : 2201
train acc:  0.7734375
train loss:  0.4609552025794983
train gradient:  0.3630030641107265
iteration : 2202
train acc:  0.828125
train loss:  0.42695721983909607
train gradient:  0.4653487049711556
iteration : 2203
train acc:  0.890625
train loss:  0.31373947858810425
train gradient:  0.23331112961995926
iteration : 2204
train acc:  0.8125
train loss:  0.3627665042877197
train gradient:  0.2957928612508348
iteration : 2205
train acc:  0.828125
train loss:  0.36669790744781494
train gradient:  0.30495122881636555
iteration : 2206
train acc:  0.8671875
train loss:  0.3125314712524414
train gradient:  0.21291890768881938
iteration : 2207
train acc:  0.78125
train loss:  0.4742535352706909
train gradient:  0.4981514966388439
iteration : 2208
train acc:  0.7890625
train loss:  0.38783177733421326
train gradient:  0.3281121126098227
iteration : 2209
train acc:  0.8359375
train loss:  0.31511610746383667
train gradient:  0.22555805388973155
iteration : 2210
train acc:  0.828125
train loss:  0.3633405864238739
train gradient:  0.3660025475005598
iteration : 2211
train acc:  0.828125
train loss:  0.40502026677131653
train gradient:  0.42606758179578225
iteration : 2212
train acc:  0.8515625
train loss:  0.34167248010635376
train gradient:  0.3375914089240278
iteration : 2213
train acc:  0.78125
train loss:  0.3964577317237854
train gradient:  0.339257312022511
iteration : 2214
train acc:  0.8515625
train loss:  0.34231895208358765
train gradient:  0.2844459202858559
iteration : 2215
train acc:  0.84375
train loss:  0.39886772632598877
train gradient:  0.3496892464292086
iteration : 2216
train acc:  0.8125
train loss:  0.37770092487335205
train gradient:  0.37507035450420634
iteration : 2217
train acc:  0.7734375
train loss:  0.45072492957115173
train gradient:  0.4116984609800114
iteration : 2218
train acc:  0.8203125
train loss:  0.3878088593482971
train gradient:  0.4897404156929481
iteration : 2219
train acc:  0.8046875
train loss:  0.40286850929260254
train gradient:  0.3478531334229439
iteration : 2220
train acc:  0.890625
train loss:  0.3033924996852875
train gradient:  0.29283581389038943
iteration : 2221
train acc:  0.796875
train loss:  0.4081658124923706
train gradient:  0.4646813545994359
iteration : 2222
train acc:  0.8984375
train loss:  0.3606381416320801
train gradient:  0.3374439025769772
iteration : 2223
train acc:  0.84375
train loss:  0.37637966871261597
train gradient:  0.3825429574539779
iteration : 2224
train acc:  0.8046875
train loss:  0.41267895698547363
train gradient:  0.3764234688107178
iteration : 2225
train acc:  0.8125
train loss:  0.3733879327774048
train gradient:  0.3556904552248538
iteration : 2226
train acc:  0.8125
train loss:  0.41186362504959106
train gradient:  0.4739498149357129
iteration : 2227
train acc:  0.8359375
train loss:  0.3408104181289673
train gradient:  0.2881077487728369
iteration : 2228
train acc:  0.7734375
train loss:  0.449238121509552
train gradient:  0.47325432413792495
iteration : 2229
train acc:  0.828125
train loss:  0.3880513906478882
train gradient:  0.661324909551982
iteration : 2230
train acc:  0.90625
train loss:  0.2970616817474365
train gradient:  0.3518792264782142
iteration : 2231
train acc:  0.8046875
train loss:  0.4161350727081299
train gradient:  0.4656130965896873
iteration : 2232
train acc:  0.84375
train loss:  0.40147003531455994
train gradient:  0.42868667088766244
iteration : 2233
train acc:  0.875
train loss:  0.33272796869277954
train gradient:  0.2544768816063435
iteration : 2234
train acc:  0.8359375
train loss:  0.348940908908844
train gradient:  0.32623797489798023
iteration : 2235
train acc:  0.828125
train loss:  0.3883054852485657
train gradient:  0.29129681962563286
iteration : 2236
train acc:  0.7890625
train loss:  0.4692356288433075
train gradient:  0.3621662672759554
iteration : 2237
train acc:  0.828125
train loss:  0.4047674238681793
train gradient:  0.5363057483662534
iteration : 2238
train acc:  0.84375
train loss:  0.3596479892730713
train gradient:  0.42097364695596906
iteration : 2239
train acc:  0.8515625
train loss:  0.3278593420982361
train gradient:  0.2989640485389096
iteration : 2240
train acc:  0.7265625
train loss:  0.5109920501708984
train gradient:  0.4729093964739316
iteration : 2241
train acc:  0.8125
train loss:  0.42207103967666626
train gradient:  0.38898951805848614
iteration : 2242
train acc:  0.8046875
train loss:  0.3798350393772125
train gradient:  0.24301873478203068
iteration : 2243
train acc:  0.875
train loss:  0.3623894155025482
train gradient:  0.24503080269682634
iteration : 2244
train acc:  0.828125
train loss:  0.40205687284469604
train gradient:  0.48931095642637673
iteration : 2245
train acc:  0.8515625
train loss:  0.33324962854385376
train gradient:  0.2513912376936929
iteration : 2246
train acc:  0.765625
train loss:  0.46078282594680786
train gradient:  0.4941589537713222
iteration : 2247
train acc:  0.765625
train loss:  0.4403982162475586
train gradient:  0.5620832879154477
iteration : 2248
train acc:  0.828125
train loss:  0.3865605890750885
train gradient:  0.47889382246717765
iteration : 2249
train acc:  0.859375
train loss:  0.31578874588012695
train gradient:  0.29959353176509507
iteration : 2250
train acc:  0.796875
train loss:  0.40445300936698914
train gradient:  0.4249423089128243
iteration : 2251
train acc:  0.7890625
train loss:  0.4527987837791443
train gradient:  0.4439943654684118
iteration : 2252
train acc:  0.7734375
train loss:  0.44306978583335876
train gradient:  0.4723388622664633
iteration : 2253
train acc:  0.765625
train loss:  0.5038713812828064
train gradient:  0.48581441426255395
iteration : 2254
train acc:  0.8203125
train loss:  0.37224632501602173
train gradient:  0.42927743512742705
iteration : 2255
train acc:  0.7890625
train loss:  0.4634271264076233
train gradient:  0.39324611234043405
iteration : 2256
train acc:  0.8671875
train loss:  0.33940452337265015
train gradient:  0.26322662116019424
iteration : 2257
train acc:  0.875
train loss:  0.3263317942619324
train gradient:  0.25857756841357993
iteration : 2258
train acc:  0.796875
train loss:  0.44129782915115356
train gradient:  0.7608263345024128
iteration : 2259
train acc:  0.8046875
train loss:  0.40809381008148193
train gradient:  0.30063314867298324
iteration : 2260
train acc:  0.875
train loss:  0.3156312108039856
train gradient:  0.2644376818253178
iteration : 2261
train acc:  0.8359375
train loss:  0.36096107959747314
train gradient:  0.34002219038526293
iteration : 2262
train acc:  0.78125
train loss:  0.4373861849308014
train gradient:  0.4436435734902674
iteration : 2263
train acc:  0.8125
train loss:  0.39090844988822937
train gradient:  0.3153379145607382
iteration : 2264
train acc:  0.875
train loss:  0.38673466444015503
train gradient:  0.36746600150979003
iteration : 2265
train acc:  0.8203125
train loss:  0.4109317362308502
train gradient:  0.48077876553523313
iteration : 2266
train acc:  0.8046875
train loss:  0.4241602420806885
train gradient:  0.48288351291087855
iteration : 2267
train acc:  0.765625
train loss:  0.4331931471824646
train gradient:  0.44189559661783384
iteration : 2268
train acc:  0.84375
train loss:  0.38387221097946167
train gradient:  0.3996314267572824
iteration : 2269
train acc:  0.8359375
train loss:  0.3367154002189636
train gradient:  0.30848766410234785
iteration : 2270
train acc:  0.828125
train loss:  0.41887980699539185
train gradient:  0.6033227300275332
iteration : 2271
train acc:  0.8125
train loss:  0.4228792190551758
train gradient:  0.5428506742365418
iteration : 2272
train acc:  0.8125
train loss:  0.39509937167167664
train gradient:  0.3657505128524228
iteration : 2273
train acc:  0.7890625
train loss:  0.3741874098777771
train gradient:  0.38732595721016133
iteration : 2274
train acc:  0.8359375
train loss:  0.3947785496711731
train gradient:  0.550215714327798
iteration : 2275
train acc:  0.8671875
train loss:  0.2808264493942261
train gradient:  0.27327955783001656
iteration : 2276
train acc:  0.828125
train loss:  0.39026159048080444
train gradient:  0.31858583848112937
iteration : 2277
train acc:  0.84375
train loss:  0.4280269145965576
train gradient:  0.33659705595119355
iteration : 2278
train acc:  0.890625
train loss:  0.29446443915367126
train gradient:  0.2915262901657555
iteration : 2279
train acc:  0.828125
train loss:  0.42428112030029297
train gradient:  0.4474923752854276
iteration : 2280
train acc:  0.8359375
train loss:  0.4057316780090332
train gradient:  0.5700062725398118
iteration : 2281
train acc:  0.8359375
train loss:  0.37669992446899414
train gradient:  0.2939913127788254
iteration : 2282
train acc:  0.8359375
train loss:  0.37262195348739624
train gradient:  0.3370593022050681
iteration : 2283
train acc:  0.8671875
train loss:  0.3694286346435547
train gradient:  0.39376099188403163
iteration : 2284
train acc:  0.828125
train loss:  0.406989187002182
train gradient:  0.3028147524257823
iteration : 2285
train acc:  0.8203125
train loss:  0.3481954336166382
train gradient:  0.25515868215805193
iteration : 2286
train acc:  0.828125
train loss:  0.33656930923461914
train gradient:  0.32280859851733673
iteration : 2287
train acc:  0.84375
train loss:  0.43903762102127075
train gradient:  0.4137705357766029
iteration : 2288
train acc:  0.828125
train loss:  0.38521313667297363
train gradient:  0.37200918225073004
iteration : 2289
train acc:  0.8046875
train loss:  0.41530436277389526
train gradient:  0.3548087624761158
iteration : 2290
train acc:  0.7734375
train loss:  0.43742966651916504
train gradient:  0.43177626215274817
iteration : 2291
train acc:  0.8203125
train loss:  0.35312607884407043
train gradient:  0.29437732144562767
iteration : 2292
train acc:  0.8359375
train loss:  0.3570098280906677
train gradient:  0.29993413286388043
iteration : 2293
train acc:  0.8828125
train loss:  0.33851632475852966
train gradient:  0.27876118421255347
iteration : 2294
train acc:  0.8203125
train loss:  0.4160004258155823
train gradient:  0.3528834615189567
iteration : 2295
train acc:  0.8671875
train loss:  0.3533207178115845
train gradient:  0.2584775143570166
iteration : 2296
train acc:  0.8359375
train loss:  0.3501214385032654
train gradient:  0.3068833364194609
iteration : 2297
train acc:  0.875
train loss:  0.29843801259994507
train gradient:  0.21551565377994053
iteration : 2298
train acc:  0.7734375
train loss:  0.4488217234611511
train gradient:  0.5225978141845287
iteration : 2299
train acc:  0.890625
train loss:  0.33546561002731323
train gradient:  0.2780286953808139
iteration : 2300
train acc:  0.8125
train loss:  0.3827529549598694
train gradient:  0.4464116905182103
iteration : 2301
train acc:  0.828125
train loss:  0.40927785634994507
train gradient:  0.6204759528586621
iteration : 2302
train acc:  0.8125
train loss:  0.38719090819358826
train gradient:  0.4547892060816196
iteration : 2303
train acc:  0.828125
train loss:  0.42859864234924316
train gradient:  0.6000550298034011
iteration : 2304
train acc:  0.8828125
train loss:  0.3170287609100342
train gradient:  0.3650408902892475
iteration : 2305
train acc:  0.8515625
train loss:  0.3906833529472351
train gradient:  0.42124128886508017
iteration : 2306
train acc:  0.84375
train loss:  0.3382246494293213
train gradient:  0.42213117600313865
iteration : 2307
train acc:  0.7734375
train loss:  0.3966825306415558
train gradient:  0.4639354804244116
iteration : 2308
train acc:  0.84375
train loss:  0.3523816168308258
train gradient:  0.30134701309985684
iteration : 2309
train acc:  0.7734375
train loss:  0.5071865320205688
train gradient:  0.6012757911503379
iteration : 2310
train acc:  0.796875
train loss:  0.39308685064315796
train gradient:  0.4794838784064266
iteration : 2311
train acc:  0.84375
train loss:  0.35730355978012085
train gradient:  0.3927472652258469
iteration : 2312
train acc:  0.8203125
train loss:  0.4134943187236786
train gradient:  0.4711472141335509
iteration : 2313
train acc:  0.796875
train loss:  0.39248889684677124
train gradient:  0.37003663696940353
iteration : 2314
train acc:  0.8515625
train loss:  0.35518500208854675
train gradient:  0.3227347428204706
iteration : 2315
train acc:  0.8515625
train loss:  0.35996538400650024
train gradient:  0.3384597020204943
iteration : 2316
train acc:  0.875
train loss:  0.31652283668518066
train gradient:  0.3965769054837188
iteration : 2317
train acc:  0.8046875
train loss:  0.37657010555267334
train gradient:  0.46528365767971475
iteration : 2318
train acc:  0.8125
train loss:  0.44246190786361694
train gradient:  0.5880796429959757
iteration : 2319
train acc:  0.8828125
train loss:  0.33412373065948486
train gradient:  0.3401405035569983
iteration : 2320
train acc:  0.8203125
train loss:  0.4002710282802582
train gradient:  0.37297174798294125
iteration : 2321
train acc:  0.84375
train loss:  0.3432524502277374
train gradient:  0.3158669033591074
iteration : 2322
train acc:  0.828125
train loss:  0.36221277713775635
train gradient:  0.32815158847076953
iteration : 2323
train acc:  0.828125
train loss:  0.37498730421066284
train gradient:  0.27302536855650944
iteration : 2324
train acc:  0.84375
train loss:  0.40269532799720764
train gradient:  0.5026296384164342
iteration : 2325
train acc:  0.8046875
train loss:  0.3801639676094055
train gradient:  0.35755134553044926
iteration : 2326
train acc:  0.8359375
train loss:  0.4363763928413391
train gradient:  0.5868483358404395
iteration : 2327
train acc:  0.7578125
train loss:  0.4929010272026062
train gradient:  0.5114175834787076
iteration : 2328
train acc:  0.8125
train loss:  0.3482005298137665
train gradient:  0.34794341656199423
iteration : 2329
train acc:  0.84375
train loss:  0.3304259777069092
train gradient:  0.30756391093057467
iteration : 2330
train acc:  0.859375
train loss:  0.34996384382247925
train gradient:  0.2675215029484352
iteration : 2331
train acc:  0.7890625
train loss:  0.501973032951355
train gradient:  0.555878871186775
iteration : 2332
train acc:  0.828125
train loss:  0.3494739830493927
train gradient:  0.40881317756665686
iteration : 2333
train acc:  0.8671875
train loss:  0.30386659502983093
train gradient:  0.3643210360609717
iteration : 2334
train acc:  0.7890625
train loss:  0.4359574317932129
train gradient:  0.35998483226264466
iteration : 2335
train acc:  0.7890625
train loss:  0.4172350764274597
train gradient:  0.3432238542989179
iteration : 2336
train acc:  0.8984375
train loss:  0.32702744007110596
train gradient:  0.33557149423429883
iteration : 2337
train acc:  0.859375
train loss:  0.3565139174461365
train gradient:  0.33544062567710015
iteration : 2338
train acc:  0.796875
train loss:  0.4201591908931732
train gradient:  0.5592690824228672
iteration : 2339
train acc:  0.8203125
train loss:  0.37006545066833496
train gradient:  0.42772850920463096
iteration : 2340
train acc:  0.875
train loss:  0.3918589651584625
train gradient:  0.344383080401666
iteration : 2341
train acc:  0.7890625
train loss:  0.4445208013057709
train gradient:  0.3843294043227762
iteration : 2342
train acc:  0.8203125
train loss:  0.35788920521736145
train gradient:  0.2867240021573444
iteration : 2343
train acc:  0.8125
train loss:  0.45405399799346924
train gradient:  0.44159270905392667
iteration : 2344
train acc:  0.8359375
train loss:  0.3949015736579895
train gradient:  0.35393965444828657
iteration : 2345
train acc:  0.796875
train loss:  0.3959994316101074
train gradient:  0.5127598306457158
iteration : 2346
train acc:  0.8359375
train loss:  0.327629029750824
train gradient:  0.18659600817381777
iteration : 2347
train acc:  0.8203125
train loss:  0.470868319272995
train gradient:  0.6392186071395466
iteration : 2348
train acc:  0.8359375
train loss:  0.3908734917640686
train gradient:  0.3318052289655415
iteration : 2349
train acc:  0.828125
train loss:  0.3770139813423157
train gradient:  0.5210658503524817
iteration : 2350
train acc:  0.859375
train loss:  0.3357531428337097
train gradient:  0.4003742571324263
iteration : 2351
train acc:  0.8046875
train loss:  0.38446348905563354
train gradient:  0.4376902481602188
iteration : 2352
train acc:  0.8359375
train loss:  0.39078348875045776
train gradient:  0.462147647973536
iteration : 2353
train acc:  0.8046875
train loss:  0.4232896566390991
train gradient:  0.4112702582836658
iteration : 2354
train acc:  0.7578125
train loss:  0.5240285992622375
train gradient:  0.6890272262145007
iteration : 2355
train acc:  0.8203125
train loss:  0.35591498017311096
train gradient:  0.37348152255878375
iteration : 2356
train acc:  0.828125
train loss:  0.38739389181137085
train gradient:  0.358954749635592
iteration : 2357
train acc:  0.8125
train loss:  0.4006294310092926
train gradient:  0.5702716398839577
iteration : 2358
train acc:  0.8203125
train loss:  0.37077128887176514
train gradient:  0.3338167140018102
iteration : 2359
train acc:  0.8046875
train loss:  0.425712525844574
train gradient:  0.5293413997130876
iteration : 2360
train acc:  0.7890625
train loss:  0.43474555015563965
train gradient:  0.5388381696067207
iteration : 2361
train acc:  0.796875
train loss:  0.41954904794692993
train gradient:  0.453895392746118
iteration : 2362
train acc:  0.828125
train loss:  0.36637625098228455
train gradient:  0.41539168659728676
iteration : 2363
train acc:  0.8203125
train loss:  0.3931235671043396
train gradient:  0.3068512115200753
iteration : 2364
train acc:  0.859375
train loss:  0.34536999464035034
train gradient:  0.2888193786410558
iteration : 2365
train acc:  0.9140625
train loss:  0.2874663174152374
train gradient:  0.2487543769643402
iteration : 2366
train acc:  0.8125
train loss:  0.4081264138221741
train gradient:  0.3848005300338965
iteration : 2367
train acc:  0.828125
train loss:  0.3885296881198883
train gradient:  0.39240419066560683
iteration : 2368
train acc:  0.8671875
train loss:  0.2816779315471649
train gradient:  0.22506397574736536
iteration : 2369
train acc:  0.8828125
train loss:  0.3008083403110504
train gradient:  0.2622439684700781
iteration : 2370
train acc:  0.796875
train loss:  0.39538174867630005
train gradient:  0.2683859612729689
iteration : 2371
train acc:  0.7421875
train loss:  0.4704163372516632
train gradient:  0.4984979077646025
iteration : 2372
train acc:  0.7890625
train loss:  0.4068449139595032
train gradient:  0.42563499079464895
iteration : 2373
train acc:  0.8671875
train loss:  0.3288426399230957
train gradient:  0.23022983349922618
iteration : 2374
train acc:  0.8515625
train loss:  0.3628048598766327
train gradient:  0.2984616023121696
iteration : 2375
train acc:  0.8125
train loss:  0.40343177318573
train gradient:  0.43623686615169327
iteration : 2376
train acc:  0.859375
train loss:  0.3225495219230652
train gradient:  0.31225173969259934
iteration : 2377
train acc:  0.84375
train loss:  0.34376636147499084
train gradient:  0.42949816460302237
iteration : 2378
train acc:  0.84375
train loss:  0.353005588054657
train gradient:  0.29996565351916477
iteration : 2379
train acc:  0.7421875
train loss:  0.4689449369907379
train gradient:  0.505668500101883
iteration : 2380
train acc:  0.8046875
train loss:  0.3899754285812378
train gradient:  0.5203885671327546
iteration : 2381
train acc:  0.75
train loss:  0.4941375255584717
train gradient:  0.523179407029105
iteration : 2382
train acc:  0.8828125
train loss:  0.32704076170921326
train gradient:  0.2694602861196426
iteration : 2383
train acc:  0.8046875
train loss:  0.43139684200286865
train gradient:  0.49040219202450214
iteration : 2384
train acc:  0.84375
train loss:  0.3324759304523468
train gradient:  0.3592434057595849
iteration : 2385
train acc:  0.8203125
train loss:  0.3750431537628174
train gradient:  0.3639555927053592
iteration : 2386
train acc:  0.84375
train loss:  0.40875959396362305
train gradient:  0.3733302305234674
iteration : 2387
train acc:  0.7890625
train loss:  0.4372565746307373
train gradient:  0.462660431178787
iteration : 2388
train acc:  0.84375
train loss:  0.4019436836242676
train gradient:  0.32856527693448295
iteration : 2389
train acc:  0.875
train loss:  0.32909539341926575
train gradient:  0.3516377350176233
iteration : 2390
train acc:  0.828125
train loss:  0.3516160249710083
train gradient:  0.27830928331869903
iteration : 2391
train acc:  0.7890625
train loss:  0.3835437297821045
train gradient:  0.4005866027618248
iteration : 2392
train acc:  0.8125
train loss:  0.39079684019088745
train gradient:  0.3983664433221752
iteration : 2393
train acc:  0.765625
train loss:  0.6065412759780884
train gradient:  0.7518530290542371
iteration : 2394
train acc:  0.8203125
train loss:  0.40019166469573975
train gradient:  0.3202473926747828
iteration : 2395
train acc:  0.8046875
train loss:  0.3765181005001068
train gradient:  0.30350219306378134
iteration : 2396
train acc:  0.875
train loss:  0.33535701036453247
train gradient:  0.28694937876929555
iteration : 2397
train acc:  0.78125
train loss:  0.5036250948905945
train gradient:  0.6172305556922215
iteration : 2398
train acc:  0.8359375
train loss:  0.3744913637638092
train gradient:  0.451885406714486
iteration : 2399
train acc:  0.8125
train loss:  0.3946252167224884
train gradient:  0.485170400661697
iteration : 2400
train acc:  0.7890625
train loss:  0.3849790096282959
train gradient:  0.3773920584166493
iteration : 2401
train acc:  0.8046875
train loss:  0.4055960178375244
train gradient:  0.46340740097835986
iteration : 2402
train acc:  0.84375
train loss:  0.34892523288726807
train gradient:  0.37339220886570135
iteration : 2403
train acc:  0.7890625
train loss:  0.4633095860481262
train gradient:  0.43135354770798656
iteration : 2404
train acc:  0.796875
train loss:  0.3747501075267792
train gradient:  0.2584037895538356
iteration : 2405
train acc:  0.8359375
train loss:  0.3814319372177124
train gradient:  0.39779538661454134
iteration : 2406
train acc:  0.8046875
train loss:  0.40264272689819336
train gradient:  0.506278054574024
iteration : 2407
train acc:  0.828125
train loss:  0.3406836688518524
train gradient:  0.26201698252170685
iteration : 2408
train acc:  0.8359375
train loss:  0.39056631922721863
train gradient:  0.3626588678995194
iteration : 2409
train acc:  0.78125
train loss:  0.4618804156780243
train gradient:  0.6385309055232582
iteration : 2410
train acc:  0.8515625
train loss:  0.31521669030189514
train gradient:  0.21344805731694858
iteration : 2411
train acc:  0.8671875
train loss:  0.3678834140300751
train gradient:  0.3114117070089435
iteration : 2412
train acc:  0.8671875
train loss:  0.3479848802089691
train gradient:  0.3418278181290097
iteration : 2413
train acc:  0.75
train loss:  0.46119236946105957
train gradient:  1.0247817039359726
iteration : 2414
train acc:  0.859375
train loss:  0.3294224739074707
train gradient:  0.20986742028011673
iteration : 2415
train acc:  0.859375
train loss:  0.369181752204895
train gradient:  0.3279854427483493
iteration : 2416
train acc:  0.828125
train loss:  0.3498793840408325
train gradient:  0.5072475561435243
iteration : 2417
train acc:  0.8515625
train loss:  0.34313055872917175
train gradient:  0.373838815384711
iteration : 2418
train acc:  0.78125
train loss:  0.4205685257911682
train gradient:  0.40623697104665113
iteration : 2419
train acc:  0.8984375
train loss:  0.2462705820798874
train gradient:  0.24216289116252068
iteration : 2420
train acc:  0.859375
train loss:  0.34111911058425903
train gradient:  0.44204414234918965
iteration : 2421
train acc:  0.7890625
train loss:  0.443369597196579
train gradient:  0.45877670676396065
iteration : 2422
train acc:  0.8828125
train loss:  0.3557031750679016
train gradient:  0.4444721477706066
iteration : 2423
train acc:  0.828125
train loss:  0.42007628083229065
train gradient:  0.37819275461982316
iteration : 2424
train acc:  0.859375
train loss:  0.3598015308380127
train gradient:  0.3030707400548835
iteration : 2425
train acc:  0.796875
train loss:  0.4968958795070648
train gradient:  0.7336763876595118
iteration : 2426
train acc:  0.796875
train loss:  0.4322841167449951
train gradient:  0.506650719574576
iteration : 2427
train acc:  0.8671875
train loss:  0.3226809501647949
train gradient:  0.21460748605437535
iteration : 2428
train acc:  0.796875
train loss:  0.41141825914382935
train gradient:  0.4784034310643924
iteration : 2429
train acc:  0.8046875
train loss:  0.43365079164505005
train gradient:  0.47598386143013316
iteration : 2430
train acc:  0.796875
train loss:  0.479909747838974
train gradient:  0.5302020945565572
iteration : 2431
train acc:  0.8203125
train loss:  0.42855921387672424
train gradient:  0.46126233134270234
iteration : 2432
train acc:  0.84375
train loss:  0.3314555287361145
train gradient:  0.2793584692786874
iteration : 2433
train acc:  0.828125
train loss:  0.3683965802192688
train gradient:  0.3896518695972386
iteration : 2434
train acc:  0.8515625
train loss:  0.36995628476142883
train gradient:  0.3349803593350255
iteration : 2435
train acc:  0.8046875
train loss:  0.46269479393959045
train gradient:  0.40592128666553634
iteration : 2436
train acc:  0.8203125
train loss:  0.3933519124984741
train gradient:  0.3688545556027178
iteration : 2437
train acc:  0.875
train loss:  0.3534398674964905
train gradient:  0.33343768282831127
iteration : 2438
train acc:  0.8671875
train loss:  0.33113324642181396
train gradient:  0.30400542573570194
iteration : 2439
train acc:  0.7578125
train loss:  0.5016714334487915
train gradient:  0.6284605825997831
iteration : 2440
train acc:  0.8359375
train loss:  0.3870770037174225
train gradient:  0.2804233432930497
iteration : 2441
train acc:  0.84375
train loss:  0.3661092519760132
train gradient:  0.2256597049084646
iteration : 2442
train acc:  0.8359375
train loss:  0.3643779754638672
train gradient:  0.3308166338394809
iteration : 2443
train acc:  0.84375
train loss:  0.37274205684661865
train gradient:  0.34154779499433924
iteration : 2444
train acc:  0.8359375
train loss:  0.3798903822898865
train gradient:  0.3086443597522569
iteration : 2445
train acc:  0.859375
train loss:  0.3610496520996094
train gradient:  0.49113244184888033
iteration : 2446
train acc:  0.8046875
train loss:  0.4122517704963684
train gradient:  0.3689776129890415
iteration : 2447
train acc:  0.8125
train loss:  0.4436364471912384
train gradient:  0.49805375353371084
iteration : 2448
train acc:  0.8046875
train loss:  0.38012635707855225
train gradient:  0.30488686540179805
iteration : 2449
train acc:  0.8359375
train loss:  0.373293936252594
train gradient:  0.3895847336138357
iteration : 2450
train acc:  0.8203125
train loss:  0.42599380016326904
train gradient:  0.3833839020419906
iteration : 2451
train acc:  0.859375
train loss:  0.3459089398384094
train gradient:  0.25114518494545546
iteration : 2452
train acc:  0.8828125
train loss:  0.3681487441062927
train gradient:  0.40042593888197925
iteration : 2453
train acc:  0.875
train loss:  0.31280386447906494
train gradient:  0.2648811822414902
iteration : 2454
train acc:  0.8203125
train loss:  0.34641095995903015
train gradient:  0.32444121338944365
iteration : 2455
train acc:  0.875
train loss:  0.3574889302253723
train gradient:  0.43547586908579217
iteration : 2456
train acc:  0.8359375
train loss:  0.39658427238464355
train gradient:  0.481895442378691
iteration : 2457
train acc:  0.8671875
train loss:  0.3587862551212311
train gradient:  0.37831880005577434
iteration : 2458
train acc:  0.8359375
train loss:  0.3680577278137207
train gradient:  0.41002514717500327
iteration : 2459
train acc:  0.8515625
train loss:  0.3605073094367981
train gradient:  0.30416398126496685
iteration : 2460
train acc:  0.890625
train loss:  0.331043541431427
train gradient:  0.2690082349936652
iteration : 2461
train acc:  0.859375
train loss:  0.34390830993652344
train gradient:  0.27993672829967065
iteration : 2462
train acc:  0.84375
train loss:  0.41612374782562256
train gradient:  0.4064433784675482
iteration : 2463
train acc:  0.8515625
train loss:  0.35692232847213745
train gradient:  0.346000660519696
iteration : 2464
train acc:  0.78125
train loss:  0.46430572867393494
train gradient:  0.5308547462103227
iteration : 2465
train acc:  0.8203125
train loss:  0.3870447278022766
train gradient:  0.4199872172098065
iteration : 2466
train acc:  0.859375
train loss:  0.3350090980529785
train gradient:  0.247893913015919
iteration : 2467
train acc:  0.796875
train loss:  0.4198095500469208
train gradient:  0.34006009799577425
iteration : 2468
train acc:  0.8125
train loss:  0.453543484210968
train gradient:  0.5105558815292728
iteration : 2469
train acc:  0.8125
train loss:  0.3685934543609619
train gradient:  0.516649706600853
iteration : 2470
train acc:  0.8125
train loss:  0.3932042121887207
train gradient:  0.6145724204049615
iteration : 2471
train acc:  0.796875
train loss:  0.460683137178421
train gradient:  0.4900108827503754
iteration : 2472
train acc:  0.8203125
train loss:  0.3336170017719269
train gradient:  0.2823301916122596
iteration : 2473
train acc:  0.828125
train loss:  0.3701415956020355
train gradient:  0.5144068250682978
iteration : 2474
train acc:  0.828125
train loss:  0.3888015151023865
train gradient:  0.30318242955609226
iteration : 2475
train acc:  0.859375
train loss:  0.44383013248443604
train gradient:  0.47823462706712877
iteration : 2476
train acc:  0.796875
train loss:  0.37219154834747314
train gradient:  0.38185552706499676
iteration : 2477
train acc:  0.8359375
train loss:  0.34945571422576904
train gradient:  0.27022163883186007
iteration : 2478
train acc:  0.7890625
train loss:  0.4376145005226135
train gradient:  0.4416711493210245
iteration : 2479
train acc:  0.8671875
train loss:  0.3356197476387024
train gradient:  0.3732618483890447
iteration : 2480
train acc:  0.9140625
train loss:  0.2899248003959656
train gradient:  0.3493022641634366
iteration : 2481
train acc:  0.78125
train loss:  0.4423035979270935
train gradient:  0.5180238842727607
iteration : 2482
train acc:  0.8125
train loss:  0.3401939570903778
train gradient:  0.31021293577557885
iteration : 2483
train acc:  0.78125
train loss:  0.42410218715667725
train gradient:  0.4719722484827743
iteration : 2484
train acc:  0.8203125
train loss:  0.41256964206695557
train gradient:  0.4237513544786596
iteration : 2485
train acc:  0.84375
train loss:  0.3472636640071869
train gradient:  0.4277035738957365
iteration : 2486
train acc:  0.8125
train loss:  0.365788072347641
train gradient:  0.21759277510086716
iteration : 2487
train acc:  0.859375
train loss:  0.3388022184371948
train gradient:  0.31578970996220534
iteration : 2488
train acc:  0.8203125
train loss:  0.39705196022987366
train gradient:  0.274369783680486
iteration : 2489
train acc:  0.8203125
train loss:  0.41206833720207214
train gradient:  0.34769093682149516
iteration : 2490
train acc:  0.828125
train loss:  0.34433889389038086
train gradient:  0.29097765963404637
iteration : 2491
train acc:  0.796875
train loss:  0.37720394134521484
train gradient:  0.299216568367497
iteration : 2492
train acc:  0.78125
train loss:  0.4864565134048462
train gradient:  0.5615791437405881
iteration : 2493
train acc:  0.8359375
train loss:  0.35249924659729004
train gradient:  0.35490899110449675
iteration : 2494
train acc:  0.84375
train loss:  0.4019398093223572
train gradient:  0.3621203752379542
iteration : 2495
train acc:  0.8515625
train loss:  0.36652112007141113
train gradient:  0.2653424898313258
iteration : 2496
train acc:  0.84375
train loss:  0.3773210048675537
train gradient:  0.46018270781088966
iteration : 2497
train acc:  0.859375
train loss:  0.3111398220062256
train gradient:  0.39905012711690724
iteration : 2498
train acc:  0.8125
train loss:  0.38527995347976685
train gradient:  0.5358417151545927
iteration : 2499
train acc:  0.71875
train loss:  0.5320861339569092
train gradient:  0.5383331912905601
iteration : 2500
train acc:  0.765625
train loss:  0.4266943335533142
train gradient:  0.4625240550041282
iteration : 2501
train acc:  0.8515625
train loss:  0.34526538848876953
train gradient:  0.27944328701389537
iteration : 2502
train acc:  0.8125
train loss:  0.42235052585601807
train gradient:  0.5451486290929501
iteration : 2503
train acc:  0.8046875
train loss:  0.4001934826374054
train gradient:  0.35396927454585947
iteration : 2504
train acc:  0.796875
train loss:  0.4702453315258026
train gradient:  0.4688566029252325
iteration : 2505
train acc:  0.765625
train loss:  0.4680149257183075
train gradient:  0.3905835277091903
iteration : 2506
train acc:  0.828125
train loss:  0.39067769050598145
train gradient:  0.4260530336228648
iteration : 2507
train acc:  0.828125
train loss:  0.40380004048347473
train gradient:  0.36191349100097864
iteration : 2508
train acc:  0.8515625
train loss:  0.39761584997177124
train gradient:  0.29713629898760624
iteration : 2509
train acc:  0.8046875
train loss:  0.41608917713165283
train gradient:  0.40880643934989486
iteration : 2510
train acc:  0.8125
train loss:  0.4303438663482666
train gradient:  0.3868883777614945
iteration : 2511
train acc:  0.828125
train loss:  0.3407207727432251
train gradient:  0.3485249147860065
iteration : 2512
train acc:  0.8046875
train loss:  0.36374497413635254
train gradient:  0.26298235831080397
iteration : 2513
train acc:  0.859375
train loss:  0.3466891944408417
train gradient:  0.2231489155342442
iteration : 2514
train acc:  0.9140625
train loss:  0.28939706087112427
train gradient:  0.23110251081993072
iteration : 2515
train acc:  0.8046875
train loss:  0.3786071538925171
train gradient:  0.441358865842184
iteration : 2516
train acc:  0.84375
train loss:  0.4449028968811035
train gradient:  0.5331287035412291
iteration : 2517
train acc:  0.8515625
train loss:  0.3873404562473297
train gradient:  0.2933939380414158
iteration : 2518
train acc:  0.859375
train loss:  0.3251441717147827
train gradient:  0.3224095872166256
iteration : 2519
train acc:  0.7734375
train loss:  0.4444480836391449
train gradient:  0.3854938311018511
iteration : 2520
train acc:  0.8671875
train loss:  0.36823737621307373
train gradient:  0.3163531978909715
iteration : 2521
train acc:  0.828125
train loss:  0.35466185212135315
train gradient:  0.28339297669633695
iteration : 2522
train acc:  0.84375
train loss:  0.4029214680194855
train gradient:  0.3979076014092737
iteration : 2523
train acc:  0.828125
train loss:  0.3646830916404724
train gradient:  0.2717657950134621
iteration : 2524
train acc:  0.8203125
train loss:  0.3952440619468689
train gradient:  0.2793456029800617
iteration : 2525
train acc:  0.8125
train loss:  0.4076153635978699
train gradient:  0.44902646145050923
iteration : 2526
train acc:  0.875
train loss:  0.3028928339481354
train gradient:  0.3361719822227907
iteration : 2527
train acc:  0.78125
train loss:  0.4475700259208679
train gradient:  0.4699482973679105
iteration : 2528
train acc:  0.8359375
train loss:  0.3688269853591919
train gradient:  0.2761373320330132
iteration : 2529
train acc:  0.859375
train loss:  0.3497205376625061
train gradient:  0.2979149462699935
iteration : 2530
train acc:  0.8828125
train loss:  0.34187501668930054
train gradient:  0.29234563713075434
iteration : 2531
train acc:  0.875
train loss:  0.3411720395088196
train gradient:  0.2777933965957582
iteration : 2532
train acc:  0.828125
train loss:  0.3753816783428192
train gradient:  0.39122863317724765
iteration : 2533
train acc:  0.8671875
train loss:  0.3229731321334839
train gradient:  0.2920043972705944
iteration : 2534
train acc:  0.8515625
train loss:  0.4437355399131775
train gradient:  0.37134007024012894
iteration : 2535
train acc:  0.7578125
train loss:  0.42524978518486023
train gradient:  0.3467387700598807
iteration : 2536
train acc:  0.8828125
train loss:  0.288256973028183
train gradient:  0.27895971513329676
iteration : 2537
train acc:  0.7890625
train loss:  0.40759947896003723
train gradient:  0.3304022065815743
iteration : 2538
train acc:  0.8046875
train loss:  0.4324723482131958
train gradient:  0.3327701842189204
iteration : 2539
train acc:  0.8203125
train loss:  0.39421090483665466
train gradient:  0.44994926951668285
iteration : 2540
train acc:  0.8125
train loss:  0.3999573588371277
train gradient:  0.519498189804176
iteration : 2541
train acc:  0.796875
train loss:  0.41842973232269287
train gradient:  0.2961355317386484
iteration : 2542
train acc:  0.8203125
train loss:  0.39311516284942627
train gradient:  0.42306335420671015
iteration : 2543
train acc:  0.78125
train loss:  0.4105904996395111
train gradient:  0.3447648483194219
iteration : 2544
train acc:  0.8203125
train loss:  0.37359684705734253
train gradient:  0.4228137748480454
iteration : 2545
train acc:  0.8046875
train loss:  0.34834444522857666
train gradient:  0.2923024963101131
iteration : 2546
train acc:  0.8671875
train loss:  0.3845565915107727
train gradient:  0.3245319441979454
iteration : 2547
train acc:  0.8359375
train loss:  0.400601327419281
train gradient:  0.28616663028639777
iteration : 2548
train acc:  0.8359375
train loss:  0.3378838300704956
train gradient:  0.22449910033353282
iteration : 2549
train acc:  0.8203125
train loss:  0.3817703127861023
train gradient:  0.4892183958896023
iteration : 2550
train acc:  0.828125
train loss:  0.36449772119522095
train gradient:  0.2797000289446337
iteration : 2551
train acc:  0.8359375
train loss:  0.3823767602443695
train gradient:  0.46554951937358746
iteration : 2552
train acc:  0.8125
train loss:  0.44150879979133606
train gradient:  0.4180830774913755
iteration : 2553
train acc:  0.7890625
train loss:  0.40869060158729553
train gradient:  0.44042464306139817
iteration : 2554
train acc:  0.8203125
train loss:  0.4027700424194336
train gradient:  0.42604614592960427
iteration : 2555
train acc:  0.8203125
train loss:  0.37406980991363525
train gradient:  0.3261555734508489
iteration : 2556
train acc:  0.796875
train loss:  0.451713502407074
train gradient:  0.40805059604871363
iteration : 2557
train acc:  0.859375
train loss:  0.3042093515396118
train gradient:  0.2023687697972009
iteration : 2558
train acc:  0.859375
train loss:  0.38812685012817383
train gradient:  0.39072899196615585
iteration : 2559
train acc:  0.8203125
train loss:  0.3475455641746521
train gradient:  0.25840904930384284
iteration : 2560
train acc:  0.8359375
train loss:  0.3636297583580017
train gradient:  0.3571585150835015
iteration : 2561
train acc:  0.8046875
train loss:  0.3897969722747803
train gradient:  0.3098273451455287
iteration : 2562
train acc:  0.8203125
train loss:  0.41215020418167114
train gradient:  0.350925011811112
iteration : 2563
train acc:  0.828125
train loss:  0.44476908445358276
train gradient:  0.4115427559020776
iteration : 2564
train acc:  0.8203125
train loss:  0.41512835025787354
train gradient:  0.32889567256782837
iteration : 2565
train acc:  0.84375
train loss:  0.32401028275489807
train gradient:  0.3437308465780938
iteration : 2566
train acc:  0.890625
train loss:  0.303600013256073
train gradient:  0.2254436448370491
iteration : 2567
train acc:  0.8515625
train loss:  0.33810269832611084
train gradient:  0.28748617176423197
iteration : 2568
train acc:  0.796875
train loss:  0.4003636837005615
train gradient:  0.3771114148763656
iteration : 2569
train acc:  0.8359375
train loss:  0.45288097858428955
train gradient:  0.44762698784882515
iteration : 2570
train acc:  0.8359375
train loss:  0.3840214014053345
train gradient:  0.27998451432737687
iteration : 2571
train acc:  0.828125
train loss:  0.3815472722053528
train gradient:  0.3379683349467127
iteration : 2572
train acc:  0.78125
train loss:  0.47992098331451416
train gradient:  0.4657139616172522
iteration : 2573
train acc:  0.8828125
train loss:  0.3031073808670044
train gradient:  0.21428588284328465
iteration : 2574
train acc:  0.859375
train loss:  0.3665199279785156
train gradient:  0.45309774685773957
iteration : 2575
train acc:  0.875
train loss:  0.3412921726703644
train gradient:  0.3841290983823895
iteration : 2576
train acc:  0.84375
train loss:  0.40148231387138367
train gradient:  0.41649594021152864
iteration : 2577
train acc:  0.796875
train loss:  0.3853433132171631
train gradient:  0.32577380014125507
iteration : 2578
train acc:  0.84375
train loss:  0.32626307010650635
train gradient:  0.27261039376065144
iteration : 2579
train acc:  0.859375
train loss:  0.34177035093307495
train gradient:  0.25999263546531143
iteration : 2580
train acc:  0.75
train loss:  0.4567180573940277
train gradient:  0.4619097717581729
iteration : 2581
train acc:  0.8515625
train loss:  0.32027897238731384
train gradient:  0.29239229670418015
iteration : 2582
train acc:  0.796875
train loss:  0.4197874665260315
train gradient:  0.4664158842696299
iteration : 2583
train acc:  0.7890625
train loss:  0.4688691198825836
train gradient:  0.6970924180322411
iteration : 2584
train acc:  0.8203125
train loss:  0.3705092966556549
train gradient:  0.34277094045981565
iteration : 2585
train acc:  0.8203125
train loss:  0.44735273718833923
train gradient:  0.3626217324244069
iteration : 2586
train acc:  0.78125
train loss:  0.4992179572582245
train gradient:  0.6568728006101388
iteration : 2587
train acc:  0.8203125
train loss:  0.3450997769832611
train gradient:  0.2650441616806794
iteration : 2588
train acc:  0.859375
train loss:  0.3756806254386902
train gradient:  0.18947679600419415
iteration : 2589
train acc:  0.828125
train loss:  0.37655168771743774
train gradient:  0.2738586433919657
iteration : 2590
train acc:  0.8203125
train loss:  0.3428472876548767
train gradient:  0.3386658996808519
iteration : 2591
train acc:  0.8359375
train loss:  0.417197048664093
train gradient:  0.5693858256214253
iteration : 2592
train acc:  0.828125
train loss:  0.38128262758255005
train gradient:  0.3886730249116084
iteration : 2593
train acc:  0.8828125
train loss:  0.32905271649360657
train gradient:  0.2723619346977839
iteration : 2594
train acc:  0.828125
train loss:  0.4359273910522461
train gradient:  0.42990826835248647
iteration : 2595
train acc:  0.828125
train loss:  0.37494903802871704
train gradient:  0.3365791335025218
iteration : 2596
train acc:  0.796875
train loss:  0.37854722142219543
train gradient:  0.2813074010579199
iteration : 2597
train acc:  0.828125
train loss:  0.4095844030380249
train gradient:  0.3757384687660964
iteration : 2598
train acc:  0.8359375
train loss:  0.4192616939544678
train gradient:  0.3657509350331024
iteration : 2599
train acc:  0.859375
train loss:  0.34336280822753906
train gradient:  0.3780531254794633
iteration : 2600
train acc:  0.859375
train loss:  0.3265066146850586
train gradient:  0.2774692304302855
iteration : 2601
train acc:  0.859375
train loss:  0.3324083089828491
train gradient:  0.31555762921022323
iteration : 2602
train acc:  0.796875
train loss:  0.41186395287513733
train gradient:  0.38467326276108715
iteration : 2603
train acc:  0.828125
train loss:  0.3425714373588562
train gradient:  0.19224260097307633
iteration : 2604
train acc:  0.7421875
train loss:  0.4991512596607208
train gradient:  0.6497631427659946
iteration : 2605
train acc:  0.75
train loss:  0.5261138081550598
train gradient:  0.539040086221974
iteration : 2606
train acc:  0.828125
train loss:  0.44253894686698914
train gradient:  0.45632177952311986
iteration : 2607
train acc:  0.84375
train loss:  0.3531055450439453
train gradient:  0.26085617168425473
iteration : 2608
train acc:  0.84375
train loss:  0.35011518001556396
train gradient:  0.278021112396786
iteration : 2609
train acc:  0.859375
train loss:  0.3279772400856018
train gradient:  0.2828878354375158
iteration : 2610
train acc:  0.890625
train loss:  0.3470878005027771
train gradient:  0.27826850696174693
iteration : 2611
train acc:  0.8125
train loss:  0.4527817368507385
train gradient:  0.3543588904892529
iteration : 2612
train acc:  0.8125
train loss:  0.40416932106018066
train gradient:  0.38224145206828336
iteration : 2613
train acc:  0.890625
train loss:  0.30771636962890625
train gradient:  0.25225081989183673
iteration : 2614
train acc:  0.796875
train loss:  0.3950679302215576
train gradient:  0.326190030480727
iteration : 2615
train acc:  0.8671875
train loss:  0.3425372838973999
train gradient:  0.36577158489393724
iteration : 2616
train acc:  0.8359375
train loss:  0.3463361859321594
train gradient:  0.2010417663200858
iteration : 2617
train acc:  0.8359375
train loss:  0.4101003408432007
train gradient:  0.31904780829585705
iteration : 2618
train acc:  0.8515625
train loss:  0.35239797830581665
train gradient:  0.22641748528986705
iteration : 2619
train acc:  0.8515625
train loss:  0.3756696581840515
train gradient:  0.2968314345927013
iteration : 2620
train acc:  0.8125
train loss:  0.37742626667022705
train gradient:  0.2451638651952059
iteration : 2621
train acc:  0.8359375
train loss:  0.36991387605667114
train gradient:  0.254526531759634
iteration : 2622
train acc:  0.7890625
train loss:  0.43004316091537476
train gradient:  0.31787413756846766
iteration : 2623
train acc:  0.8359375
train loss:  0.37612101435661316
train gradient:  0.3429309111208771
iteration : 2624
train acc:  0.8125
train loss:  0.41236332058906555
train gradient:  0.3828198697211107
iteration : 2625
train acc:  0.8671875
train loss:  0.34473758935928345
train gradient:  0.23665496278213877
iteration : 2626
train acc:  0.8046875
train loss:  0.41431888937950134
train gradient:  0.33789635245363286
iteration : 2627
train acc:  0.78125
train loss:  0.4167664647102356
train gradient:  0.2741088455585548
iteration : 2628
train acc:  0.84375
train loss:  0.35972246527671814
train gradient:  0.4102292261852377
iteration : 2629
train acc:  0.859375
train loss:  0.33302268385887146
train gradient:  0.30392061494002953
iteration : 2630
train acc:  0.8515625
train loss:  0.3514982759952545
train gradient:  0.25017060628724935
iteration : 2631
train acc:  0.78125
train loss:  0.42272433638572693
train gradient:  0.4503815885862398
iteration : 2632
train acc:  0.8125
train loss:  0.4403916299343109
train gradient:  0.42770190316307344
iteration : 2633
train acc:  0.8515625
train loss:  0.3619268536567688
train gradient:  0.23711555017020342
iteration : 2634
train acc:  0.8125
train loss:  0.4856771230697632
train gradient:  0.473867301800395
iteration : 2635
train acc:  0.84375
train loss:  0.4054904580116272
train gradient:  0.3163869437163387
iteration : 2636
train acc:  0.7890625
train loss:  0.36872223019599915
train gradient:  0.30478012664742316
iteration : 2637
train acc:  0.7421875
train loss:  0.45320451259613037
train gradient:  0.40369997080079284
iteration : 2638
train acc:  0.8359375
train loss:  0.36310064792633057
train gradient:  0.18960651538177742
iteration : 2639
train acc:  0.890625
train loss:  0.35772326588630676
train gradient:  0.2876644867100914
iteration : 2640
train acc:  0.8203125
train loss:  0.3552720546722412
train gradient:  0.2730186728441151
iteration : 2641
train acc:  0.7578125
train loss:  0.4498322308063507
train gradient:  0.5316755060160223
iteration : 2642
train acc:  0.859375
train loss:  0.41979914903640747
train gradient:  0.34134063256227304
iteration : 2643
train acc:  0.7890625
train loss:  0.45567843317985535
train gradient:  0.40495208807799243
iteration : 2644
train acc:  0.8046875
train loss:  0.4076972007751465
train gradient:  0.47354514914537427
iteration : 2645
train acc:  0.8828125
train loss:  0.34789732098579407
train gradient:  0.36343947587975567
iteration : 2646
train acc:  0.890625
train loss:  0.3458539843559265
train gradient:  0.3172770633286582
iteration : 2647
train acc:  0.8828125
train loss:  0.2819111943244934
train gradient:  0.22967825106223816
iteration : 2648
train acc:  0.859375
train loss:  0.33518436551094055
train gradient:  0.26713727544125315
iteration : 2649
train acc:  0.828125
train loss:  0.36475837230682373
train gradient:  0.30463588214439075
iteration : 2650
train acc:  0.828125
train loss:  0.4276573657989502
train gradient:  0.46958249605625463
iteration : 2651
train acc:  0.890625
train loss:  0.2876316010951996
train gradient:  0.1718752359033813
iteration : 2652
train acc:  0.859375
train loss:  0.3172626495361328
train gradient:  0.2564323694842967
iteration : 2653
train acc:  0.875
train loss:  0.3243713974952698
train gradient:  0.28372403938312296
iteration : 2654
train acc:  0.84375
train loss:  0.3430095911026001
train gradient:  0.3239310278277793
iteration : 2655
train acc:  0.78125
train loss:  0.4765016734600067
train gradient:  0.5525554599759595
iteration : 2656
train acc:  0.828125
train loss:  0.37427204847335815
train gradient:  0.29234374996750734
iteration : 2657
train acc:  0.859375
train loss:  0.3369733691215515
train gradient:  0.23890115893012326
iteration : 2658
train acc:  0.8203125
train loss:  0.35121476650238037
train gradient:  0.3791213441495459
iteration : 2659
train acc:  0.796875
train loss:  0.42305299639701843
train gradient:  0.4144811019996983
iteration : 2660
train acc:  0.84375
train loss:  0.34193873405456543
train gradient:  0.2332680745549699
iteration : 2661
train acc:  0.796875
train loss:  0.4044480621814728
train gradient:  0.3240683906544685
iteration : 2662
train acc:  0.8203125
train loss:  0.38265958428382874
train gradient:  0.4256139059268747
iteration : 2663
train acc:  0.8125
train loss:  0.3926180601119995
train gradient:  0.29064425700386065
iteration : 2664
train acc:  0.875
train loss:  0.36159273982048035
train gradient:  0.2729394392717198
iteration : 2665
train acc:  0.8671875
train loss:  0.34089720249176025
train gradient:  0.3567064776603878
iteration : 2666
train acc:  0.7890625
train loss:  0.42229777574539185
train gradient:  0.5215866244734702
iteration : 2667
train acc:  0.84375
train loss:  0.3886423110961914
train gradient:  0.2981573015090421
iteration : 2668
train acc:  0.890625
train loss:  0.30273526906967163
train gradient:  0.25731636517968104
iteration : 2669
train acc:  0.796875
train loss:  0.4569050967693329
train gradient:  0.5326490293175042
iteration : 2670
train acc:  0.8046875
train loss:  0.41011276841163635
train gradient:  0.32361162972377805
iteration : 2671
train acc:  0.7734375
train loss:  0.4491400122642517
train gradient:  0.7361873650914954
iteration : 2672
train acc:  0.796875
train loss:  0.42499884963035583
train gradient:  0.32844715529403384
iteration : 2673
train acc:  0.78125
train loss:  0.43217235803604126
train gradient:  0.4477226587587061
iteration : 2674
train acc:  0.796875
train loss:  0.450558602809906
train gradient:  0.5019552748497502
iteration : 2675
train acc:  0.828125
train loss:  0.37328243255615234
train gradient:  0.27031777527264805
iteration : 2676
train acc:  0.78125
train loss:  0.43038320541381836
train gradient:  0.37241886302733546
iteration : 2677
train acc:  0.8515625
train loss:  0.38371706008911133
train gradient:  0.37144013598218845
iteration : 2678
train acc:  0.84375
train loss:  0.3892284631729126
train gradient:  0.25914651066030564
iteration : 2679
train acc:  0.8828125
train loss:  0.3248547315597534
train gradient:  0.3530135744317928
iteration : 2680
train acc:  0.8671875
train loss:  0.3216931223869324
train gradient:  0.19213435593447284
iteration : 2681
train acc:  0.796875
train loss:  0.420509934425354
train gradient:  0.395172685998047
iteration : 2682
train acc:  0.8125
train loss:  0.39571020007133484
train gradient:  0.26919785104953853
iteration : 2683
train acc:  0.828125
train loss:  0.3727963864803314
train gradient:  0.2895443029402017
iteration : 2684
train acc:  0.8359375
train loss:  0.3577674925327301
train gradient:  0.23217930159981198
iteration : 2685
train acc:  0.8203125
train loss:  0.43181112408638
train gradient:  0.43016097110468793
iteration : 2686
train acc:  0.8515625
train loss:  0.3125721216201782
train gradient:  0.23467288539504255
iteration : 2687
train acc:  0.8359375
train loss:  0.40474265813827515
train gradient:  0.43405927790822446
iteration : 2688
train acc:  0.8203125
train loss:  0.4175451397895813
train gradient:  0.5735440205695765
iteration : 2689
train acc:  0.84375
train loss:  0.3431531488895416
train gradient:  0.27176654768892955
iteration : 2690
train acc:  0.84375
train loss:  0.39596322178840637
train gradient:  0.37585505260444946
iteration : 2691
train acc:  0.8125
train loss:  0.3961452841758728
train gradient:  0.4132491736790556
iteration : 2692
train acc:  0.875
train loss:  0.3798554837703705
train gradient:  0.33817382813142577
iteration : 2693
train acc:  0.8203125
train loss:  0.37828803062438965
train gradient:  0.33342919311156133
iteration : 2694
train acc:  0.796875
train loss:  0.41078412532806396
train gradient:  0.37448823570282225
iteration : 2695
train acc:  0.8359375
train loss:  0.3539915382862091
train gradient:  0.375045246323458
iteration : 2696
train acc:  0.8203125
train loss:  0.39641129970550537
train gradient:  0.36624937269325153
iteration : 2697
train acc:  0.7890625
train loss:  0.38757461309432983
train gradient:  0.4594079254593549
iteration : 2698
train acc:  0.8125
train loss:  0.3626070022583008
train gradient:  0.3125374998472661
iteration : 2699
train acc:  0.78125
train loss:  0.39642995595932007
train gradient:  0.372311188893369
iteration : 2700
train acc:  0.7890625
train loss:  0.43853476643562317
train gradient:  0.5359627436985288
iteration : 2701
train acc:  0.8359375
train loss:  0.3588351011276245
train gradient:  0.36566297474163195
iteration : 2702
train acc:  0.78125
train loss:  0.4696681499481201
train gradient:  0.5320910471839384
iteration : 2703
train acc:  0.8671875
train loss:  0.34489428997039795
train gradient:  0.3297440677077744
iteration : 2704
train acc:  0.8515625
train loss:  0.3958112895488739
train gradient:  0.33478784716315807
iteration : 2705
train acc:  0.7890625
train loss:  0.4536185562610626
train gradient:  0.5328791338419513
iteration : 2706
train acc:  0.828125
train loss:  0.3553559184074402
train gradient:  0.312056118289485
iteration : 2707
train acc:  0.859375
train loss:  0.3499767780303955
train gradient:  0.3281775990883859
iteration : 2708
train acc:  0.828125
train loss:  0.3379932641983032
train gradient:  0.28716908122054996
iteration : 2709
train acc:  0.84375
train loss:  0.35523927211761475
train gradient:  0.3521953522018277
iteration : 2710
train acc:  0.765625
train loss:  0.4740367531776428
train gradient:  0.6193433826659132
iteration : 2711
train acc:  0.8984375
train loss:  0.35273173451423645
train gradient:  0.3464481627233596
iteration : 2712
train acc:  0.8515625
train loss:  0.42132094502449036
train gradient:  0.410878242315421
iteration : 2713
train acc:  0.8125
train loss:  0.349498987197876
train gradient:  0.28571043350463243
iteration : 2714
train acc:  0.828125
train loss:  0.4340645372867584
train gradient:  0.33707355978847864
iteration : 2715
train acc:  0.9140625
train loss:  0.3282637596130371
train gradient:  0.3098529171745037
iteration : 2716
train acc:  0.8828125
train loss:  0.3294157385826111
train gradient:  0.2622506288460225
iteration : 2717
train acc:  0.8671875
train loss:  0.3924500346183777
train gradient:  0.4708247327240988
iteration : 2718
train acc:  0.8828125
train loss:  0.37905046343803406
train gradient:  0.3309349695400614
iteration : 2719
train acc:  0.8046875
train loss:  0.5356206893920898
train gradient:  0.6495454507555731
iteration : 2720
train acc:  0.7890625
train loss:  0.45229417085647583
train gradient:  0.4581040180211917
iteration : 2721
train acc:  0.7890625
train loss:  0.5067285299301147
train gradient:  0.6122703034321096
iteration : 2722
train acc:  0.859375
train loss:  0.3465099036693573
train gradient:  0.31323784528711557
iteration : 2723
train acc:  0.796875
train loss:  0.411480188369751
train gradient:  0.39294140706127006
iteration : 2724
train acc:  0.8515625
train loss:  0.3282175660133362
train gradient:  0.35652729232766883
iteration : 2725
train acc:  0.828125
train loss:  0.3615019619464874
train gradient:  0.27401651720733633
iteration : 2726
train acc:  0.9140625
train loss:  0.32660481333732605
train gradient:  0.23828357541281192
iteration : 2727
train acc:  0.7890625
train loss:  0.399493932723999
train gradient:  0.30055287781241163
iteration : 2728
train acc:  0.8125
train loss:  0.4499247670173645
train gradient:  0.3767995256015036
iteration : 2729
train acc:  0.84375
train loss:  0.34056511521339417
train gradient:  0.3923544124553769
iteration : 2730
train acc:  0.7890625
train loss:  0.46406489610671997
train gradient:  0.4758459329401712
iteration : 2731
train acc:  0.84375
train loss:  0.3276370167732239
train gradient:  0.2513232659954381
iteration : 2732
train acc:  0.8671875
train loss:  0.3137872815132141
train gradient:  0.23588969078298186
iteration : 2733
train acc:  0.8046875
train loss:  0.4214286804199219
train gradient:  0.3163763111106445
iteration : 2734
train acc:  0.8515625
train loss:  0.3588012158870697
train gradient:  0.33764166895036496
iteration : 2735
train acc:  0.8671875
train loss:  0.309915155172348
train gradient:  0.30508862750298643
iteration : 2736
train acc:  0.8125
train loss:  0.4350036382675171
train gradient:  0.484901521968652
iteration : 2737
train acc:  0.7734375
train loss:  0.4288553297519684
train gradient:  0.48746720155339124
iteration : 2738
train acc:  0.8125
train loss:  0.42603325843811035
train gradient:  0.4771283317663706
iteration : 2739
train acc:  0.875
train loss:  0.33702152967453003
train gradient:  0.3469040243572668
iteration : 2740
train acc:  0.8515625
train loss:  0.3664432764053345
train gradient:  0.2782888168222128
iteration : 2741
train acc:  0.8515625
train loss:  0.3323429226875305
train gradient:  0.2239575251643624
iteration : 2742
train acc:  0.875
train loss:  0.3248387575149536
train gradient:  0.16125671872206832
iteration : 2743
train acc:  0.828125
train loss:  0.3960544168949127
train gradient:  0.37010531802233715
iteration : 2744
train acc:  0.828125
train loss:  0.39391952753067017
train gradient:  0.32153528968335554
iteration : 2745
train acc:  0.8984375
train loss:  0.3054293990135193
train gradient:  0.19552939998941526
iteration : 2746
train acc:  0.828125
train loss:  0.4337509274482727
train gradient:  0.383825360352665
iteration : 2747
train acc:  0.8046875
train loss:  0.4166499376296997
train gradient:  0.44221302930023737
iteration : 2748
train acc:  0.84375
train loss:  0.3491913080215454
train gradient:  0.3252657312619677
iteration : 2749
train acc:  0.8671875
train loss:  0.37554094195365906
train gradient:  0.29157916055022215
iteration : 2750
train acc:  0.84375
train loss:  0.3161999583244324
train gradient:  0.257879683246824
iteration : 2751
train acc:  0.8359375
train loss:  0.404099702835083
train gradient:  0.4352913007855816
iteration : 2752
train acc:  0.8671875
train loss:  0.3655913472175598
train gradient:  0.31108279287372004
iteration : 2753
train acc:  0.8359375
train loss:  0.3880930542945862
train gradient:  0.2626641601363283
iteration : 2754
train acc:  0.828125
train loss:  0.36813461780548096
train gradient:  0.3362736361874703
iteration : 2755
train acc:  0.8671875
train loss:  0.3170284628868103
train gradient:  0.30362223942983035
iteration : 2756
train acc:  0.7890625
train loss:  0.3842754364013672
train gradient:  0.2987154879659188
iteration : 2757
train acc:  0.8203125
train loss:  0.41692569851875305
train gradient:  0.44814623061951175
iteration : 2758
train acc:  0.796875
train loss:  0.4238271415233612
train gradient:  0.3962399716927469
iteration : 2759
train acc:  0.7890625
train loss:  0.523500382900238
train gradient:  0.6619543430186514
iteration : 2760
train acc:  0.84375
train loss:  0.3901090621948242
train gradient:  0.480808716989483
iteration : 2761
train acc:  0.828125
train loss:  0.4186660647392273
train gradient:  0.3225646199153301
iteration : 2762
train acc:  0.875
train loss:  0.3090018332004547
train gradient:  0.2883213937362574
iteration : 2763
train acc:  0.78125
train loss:  0.4480430781841278
train gradient:  0.4383372808278092
iteration : 2764
train acc:  0.8515625
train loss:  0.30743226408958435
train gradient:  0.29294451218561224
iteration : 2765
train acc:  0.8515625
train loss:  0.33273589611053467
train gradient:  0.22561462946012534
iteration : 2766
train acc:  0.84375
train loss:  0.3630979657173157
train gradient:  0.24781751264324592
iteration : 2767
train acc:  0.8515625
train loss:  0.3147907853126526
train gradient:  0.285474473950375
iteration : 2768
train acc:  0.8515625
train loss:  0.41535502672195435
train gradient:  0.39025124995601457
iteration : 2769
train acc:  0.8515625
train loss:  0.37520915269851685
train gradient:  0.3397477837162579
iteration : 2770
train acc:  0.84375
train loss:  0.374468594789505
train gradient:  0.39788527830504916
iteration : 2771
train acc:  0.90625
train loss:  0.2524619698524475
train gradient:  0.23581336633226163
iteration : 2772
train acc:  0.875
train loss:  0.33491507172584534
train gradient:  0.24303248682569595
iteration : 2773
train acc:  0.8203125
train loss:  0.3727000951766968
train gradient:  0.3558244284550658
iteration : 2774
train acc:  0.84375
train loss:  0.41024282574653625
train gradient:  0.40995919190020425
iteration : 2775
train acc:  0.8046875
train loss:  0.4132542014122009
train gradient:  0.5029046451678842
iteration : 2776
train acc:  0.8515625
train loss:  0.3146432042121887
train gradient:  0.21612250519717038
iteration : 2777
train acc:  0.8828125
train loss:  0.3190374970436096
train gradient:  0.23686638420330647
iteration : 2778
train acc:  0.859375
train loss:  0.3101223111152649
train gradient:  0.3016971614841218
iteration : 2779
train acc:  0.7890625
train loss:  0.3970780670642853
train gradient:  0.308878384651444
iteration : 2780
train acc:  0.84375
train loss:  0.40328043699264526
train gradient:  0.47869145054016965
iteration : 2781
train acc:  0.828125
train loss:  0.3881506621837616
train gradient:  0.41599865715772544
iteration : 2782
train acc:  0.8046875
train loss:  0.42170611023902893
train gradient:  0.5107324243144312
iteration : 2783
train acc:  0.875
train loss:  0.32909002900123596
train gradient:  0.2570069426128002
iteration : 2784
train acc:  0.78125
train loss:  0.4170656204223633
train gradient:  0.35047570587231075
iteration : 2785
train acc:  0.7734375
train loss:  0.4813786745071411
train gradient:  0.5758342011309705
iteration : 2786
train acc:  0.734375
train loss:  0.5125232338905334
train gradient:  0.6532655806846459
iteration : 2787
train acc:  0.8359375
train loss:  0.43614399433135986
train gradient:  0.39448703779512884
iteration : 2788
train acc:  0.7734375
train loss:  0.5078839063644409
train gradient:  0.4693357290597549
iteration : 2789
train acc:  0.8359375
train loss:  0.40059125423431396
train gradient:  0.38431496626345
iteration : 2790
train acc:  0.7890625
train loss:  0.4566033184528351
train gradient:  0.3742447979514047
iteration : 2791
train acc:  0.7890625
train loss:  0.39250364899635315
train gradient:  0.38656398743252374
iteration : 2792
train acc:  0.765625
train loss:  0.4672318696975708
train gradient:  0.4039751932038304
iteration : 2793
train acc:  0.875
train loss:  0.31552207469940186
train gradient:  0.32259846694964706
iteration : 2794
train acc:  0.8046875
train loss:  0.4341471493244171
train gradient:  0.33211842149340454
iteration : 2795
train acc:  0.8125
train loss:  0.4185647666454315
train gradient:  0.543599426387996
iteration : 2796
train acc:  0.8125
train loss:  0.3699963390827179
train gradient:  0.28932102041455465
iteration : 2797
train acc:  0.828125
train loss:  0.38118112087249756
train gradient:  0.2678589979603501
iteration : 2798
train acc:  0.8515625
train loss:  0.36096835136413574
train gradient:  0.21860251167916522
iteration : 2799
train acc:  0.8203125
train loss:  0.39970663189888
train gradient:  0.4995210250569501
iteration : 2800
train acc:  0.84375
train loss:  0.38230767846107483
train gradient:  0.25762165582905977
iteration : 2801
train acc:  0.8203125
train loss:  0.34584981203079224
train gradient:  0.35273095412569383
iteration : 2802
train acc:  0.8125
train loss:  0.4125291705131531
train gradient:  0.3269917281931171
iteration : 2803
train acc:  0.8203125
train loss:  0.4106886386871338
train gradient:  0.29491205472873117
iteration : 2804
train acc:  0.8828125
train loss:  0.27611368894577026
train gradient:  0.18448589205797133
iteration : 2805
train acc:  0.8125
train loss:  0.4455960690975189
train gradient:  0.37781033375005635
iteration : 2806
train acc:  0.8828125
train loss:  0.340700626373291
train gradient:  0.2735618013795675
iteration : 2807
train acc:  0.8515625
train loss:  0.3775017559528351
train gradient:  0.25313482238803764
iteration : 2808
train acc:  0.8125
train loss:  0.4046907424926758
train gradient:  0.39839546735525705
iteration : 2809
train acc:  0.84375
train loss:  0.38193267583847046
train gradient:  0.24537234699099353
iteration : 2810
train acc:  0.8359375
train loss:  0.3576447665691376
train gradient:  0.3432293780546008
iteration : 2811
train acc:  0.7890625
train loss:  0.3841601312160492
train gradient:  0.34795632263167053
iteration : 2812
train acc:  0.78125
train loss:  0.48977792263031006
train gradient:  0.4010957143970234
iteration : 2813
train acc:  0.8125
train loss:  0.4361371695995331
train gradient:  0.3344194511157593
iteration : 2814
train acc:  0.7890625
train loss:  0.4209407567977905
train gradient:  0.4296214055871008
iteration : 2815
train acc:  0.78125
train loss:  0.3866216540336609
train gradient:  0.25665572935033637
iteration : 2816
train acc:  0.7734375
train loss:  0.4335918426513672
train gradient:  0.4103667409769885
iteration : 2817
train acc:  0.859375
train loss:  0.3394724130630493
train gradient:  0.2397754670109704
iteration : 2818
train acc:  0.828125
train loss:  0.39821791648864746
train gradient:  0.32655829383704105
iteration : 2819
train acc:  0.8671875
train loss:  0.3392154276371002
train gradient:  0.2784702879858616
iteration : 2820
train acc:  0.8046875
train loss:  0.37854769825935364
train gradient:  0.30193262667740894
iteration : 2821
train acc:  0.8671875
train loss:  0.33973050117492676
train gradient:  0.27951678809737635
iteration : 2822
train acc:  0.8046875
train loss:  0.4151320457458496
train gradient:  0.4410552896444874
iteration : 2823
train acc:  0.8203125
train loss:  0.35725030303001404
train gradient:  0.36591147958178843
iteration : 2824
train acc:  0.8359375
train loss:  0.32268226146698
train gradient:  0.17827415519128245
iteration : 2825
train acc:  0.8046875
train loss:  0.39744266867637634
train gradient:  0.34203814812234956
iteration : 2826
train acc:  0.8046875
train loss:  0.4205947518348694
train gradient:  0.3892188241648845
iteration : 2827
train acc:  0.8671875
train loss:  0.34344151616096497
train gradient:  0.2796372038218172
iteration : 2828
train acc:  0.828125
train loss:  0.37662598490715027
train gradient:  0.3186192185107306
iteration : 2829
train acc:  0.8671875
train loss:  0.3187790811061859
train gradient:  0.19390220326895025
iteration : 2830
train acc:  0.828125
train loss:  0.3944177031517029
train gradient:  0.31828574396023507
iteration : 2831
train acc:  0.8359375
train loss:  0.40006116032600403
train gradient:  0.3567058841835573
iteration : 2832
train acc:  0.859375
train loss:  0.3415794372558594
train gradient:  0.20801954356643065
iteration : 2833
train acc:  0.8046875
train loss:  0.4464912414550781
train gradient:  0.39632788518591905
iteration : 2834
train acc:  0.8671875
train loss:  0.3478931784629822
train gradient:  0.24667649399116004
iteration : 2835
train acc:  0.8046875
train loss:  0.3735334873199463
train gradient:  0.4043353714618662
iteration : 2836
train acc:  0.8203125
train loss:  0.3925098776817322
train gradient:  0.2701443070520492
iteration : 2837
train acc:  0.796875
train loss:  0.4508071839809418
train gradient:  0.4281862707118342
iteration : 2838
train acc:  0.8125
train loss:  0.4160081744194031
train gradient:  0.4639611607578723
iteration : 2839
train acc:  0.796875
train loss:  0.4353870749473572
train gradient:  0.3933091182951147
iteration : 2840
train acc:  0.921875
train loss:  0.2855890989303589
train gradient:  0.2544569660202545
iteration : 2841
train acc:  0.796875
train loss:  0.4380374848842621
train gradient:  0.2593740987375983
iteration : 2842
train acc:  0.7578125
train loss:  0.43336957693099976
train gradient:  0.34762228292551745
iteration : 2843
train acc:  0.8125
train loss:  0.46102380752563477
train gradient:  0.3612264963891602
iteration : 2844
train acc:  0.8671875
train loss:  0.3473242521286011
train gradient:  0.27035895229022083
iteration : 2845
train acc:  0.8046875
train loss:  0.3941114544868469
train gradient:  0.5879918027041653
iteration : 2846
train acc:  0.828125
train loss:  0.3708961009979248
train gradient:  0.34431235932031157
iteration : 2847
train acc:  0.796875
train loss:  0.3753214180469513
train gradient:  0.35578011556300715
iteration : 2848
train acc:  0.8203125
train loss:  0.3580975830554962
train gradient:  0.26093006609663677
iteration : 2849
train acc:  0.8671875
train loss:  0.39635103940963745
train gradient:  0.3802708432602455
iteration : 2850
train acc:  0.765625
train loss:  0.5249354243278503
train gradient:  0.5865892044030194
iteration : 2851
train acc:  0.8125
train loss:  0.36486637592315674
train gradient:  0.3009090793319051
iteration : 2852
train acc:  0.84375
train loss:  0.32335084676742554
train gradient:  0.24428023995405979
iteration : 2853
train acc:  0.8828125
train loss:  0.291249543428421
train gradient:  0.20584326662023794
iteration : 2854
train acc:  0.796875
train loss:  0.43938755989074707
train gradient:  0.4783465643199354
iteration : 2855
train acc:  0.8359375
train loss:  0.347043514251709
train gradient:  0.24433343389994203
iteration : 2856
train acc:  0.8125
train loss:  0.5124303102493286
train gradient:  0.5501467811388163
iteration : 2857
train acc:  0.8046875
train loss:  0.4123743176460266
train gradient:  0.3789913581604047
iteration : 2858
train acc:  0.890625
train loss:  0.2936019003391266
train gradient:  0.23795537473225498
iteration : 2859
train acc:  0.78125
train loss:  0.43436747789382935
train gradient:  0.3841973635708796
iteration : 2860
train acc:  0.8125
train loss:  0.3719947934150696
train gradient:  0.29203778597693525
iteration : 2861
train acc:  0.8359375
train loss:  0.3087117075920105
train gradient:  0.26378319374233966
iteration : 2862
train acc:  0.890625
train loss:  0.2841259837150574
train gradient:  0.2851553611105178
iteration : 2863
train acc:  0.8125
train loss:  0.45852693915367126
train gradient:  0.3743642080412566
iteration : 2864
train acc:  0.8203125
train loss:  0.40909260511398315
train gradient:  0.41555311884191504
iteration : 2865
train acc:  0.828125
train loss:  0.4219454526901245
train gradient:  0.7446101037332813
iteration : 2866
train acc:  0.828125
train loss:  0.3815010190010071
train gradient:  0.3528996089742511
iteration : 2867
train acc:  0.7578125
train loss:  0.4915059506893158
train gradient:  0.4754120976437307
iteration : 2868
train acc:  0.828125
train loss:  0.40085458755493164
train gradient:  0.3461585444501013
iteration : 2869
train acc:  0.8515625
train loss:  0.33601683378219604
train gradient:  0.38445473402450586
iteration : 2870
train acc:  0.8515625
train loss:  0.36170995235443115
train gradient:  0.25154661195723366
iteration : 2871
train acc:  0.78125
train loss:  0.4010055661201477
train gradient:  0.3777060140207547
iteration : 2872
train acc:  0.84375
train loss:  0.3587026298046112
train gradient:  0.24085426268522897
iteration : 2873
train acc:  0.8203125
train loss:  0.4166604280471802
train gradient:  0.4963614505025088
iteration : 2874
train acc:  0.875
train loss:  0.31875479221343994
train gradient:  0.2447470247058141
iteration : 2875
train acc:  0.796875
train loss:  0.4087617099285126
train gradient:  0.3395581825880949
iteration : 2876
train acc:  0.78125
train loss:  0.4311606287956238
train gradient:  0.32848417595682344
iteration : 2877
train acc:  0.8515625
train loss:  0.3367219567298889
train gradient:  0.19326326984567452
iteration : 2878
train acc:  0.8046875
train loss:  0.38879716396331787
train gradient:  0.3242987966576407
iteration : 2879
train acc:  0.8203125
train loss:  0.4098292589187622
train gradient:  0.3757630789474722
iteration : 2880
train acc:  0.8359375
train loss:  0.3449832797050476
train gradient:  0.20233563390035353
iteration : 2881
train acc:  0.8671875
train loss:  0.34697937965393066
train gradient:  0.2968583118838348
iteration : 2882
train acc:  0.84375
train loss:  0.36347198486328125
train gradient:  0.2822817036402945
iteration : 2883
train acc:  0.890625
train loss:  0.36701321601867676
train gradient:  0.2858853238369964
iteration : 2884
train acc:  0.8828125
train loss:  0.3445102274417877
train gradient:  0.29161453994314285
iteration : 2885
train acc:  0.875
train loss:  0.33188319206237793
train gradient:  0.23825549227461187
iteration : 2886
train acc:  0.859375
train loss:  0.34084099531173706
train gradient:  0.4313386300736726
iteration : 2887
train acc:  0.8125
train loss:  0.3856239914894104
train gradient:  0.27955188385164326
iteration : 2888
train acc:  0.859375
train loss:  0.38344064354896545
train gradient:  0.32263123345415273
iteration : 2889
train acc:  0.828125
train loss:  0.3811801075935364
train gradient:  0.4316413068979825
iteration : 2890
train acc:  0.828125
train loss:  0.4347161054611206
train gradient:  0.45328334920874835
iteration : 2891
train acc:  0.84375
train loss:  0.38225311040878296
train gradient:  0.27665628184750396
iteration : 2892
train acc:  0.8046875
train loss:  0.4043670892715454
train gradient:  0.28178884281014377
iteration : 2893
train acc:  0.84375
train loss:  0.46091967821121216
train gradient:  0.5400076194127299
iteration : 2894
train acc:  0.828125
train loss:  0.3697237968444824
train gradient:  0.2709976309632294
iteration : 2895
train acc:  0.8203125
train loss:  0.39893102645874023
train gradient:  0.3413765837294361
iteration : 2896
train acc:  0.875
train loss:  0.33472782373428345
train gradient:  0.24012828034789493
iteration : 2897
train acc:  0.8046875
train loss:  0.4758960008621216
train gradient:  0.44053617662138317
iteration : 2898
train acc:  0.8515625
train loss:  0.34153100848197937
train gradient:  0.24173950957217957
iteration : 2899
train acc:  0.8671875
train loss:  0.34364235401153564
train gradient:  0.2035508873340928
iteration : 2900
train acc:  0.8515625
train loss:  0.32228967547416687
train gradient:  0.2788473346284814
iteration : 2901
train acc:  0.78125
train loss:  0.44915884733200073
train gradient:  0.5690928248070355
iteration : 2902
train acc:  0.8203125
train loss:  0.39844194054603577
train gradient:  0.33086694101694697
iteration : 2903
train acc:  0.8203125
train loss:  0.4572042226791382
train gradient:  0.5538438418432282
iteration : 2904
train acc:  0.7734375
train loss:  0.4736286699771881
train gradient:  0.47078208459071663
iteration : 2905
train acc:  0.8515625
train loss:  0.3076930642127991
train gradient:  0.31177280332311325
iteration : 2906
train acc:  0.828125
train loss:  0.39795151352882385
train gradient:  0.30293255443761385
iteration : 2907
train acc:  0.859375
train loss:  0.38857877254486084
train gradient:  0.33889799078748495
iteration : 2908
train acc:  0.8828125
train loss:  0.3260631561279297
train gradient:  0.26743569309806287
iteration : 2909
train acc:  0.8203125
train loss:  0.3976646065711975
train gradient:  0.3971050962625023
iteration : 2910
train acc:  0.8125
train loss:  0.4043651223182678
train gradient:  0.33063914304286485
iteration : 2911
train acc:  0.8046875
train loss:  0.41246163845062256
train gradient:  0.24212278965993764
iteration : 2912
train acc:  0.828125
train loss:  0.4217846393585205
train gradient:  0.30211800087678514
iteration : 2913
train acc:  0.875
train loss:  0.3198070526123047
train gradient:  0.19370240336572037
iteration : 2914
train acc:  0.8828125
train loss:  0.32565128803253174
train gradient:  0.27086026220821946
iteration : 2915
train acc:  0.875
train loss:  0.33990365266799927
train gradient:  0.23909165011533906
iteration : 2916
train acc:  0.8125
train loss:  0.4272184371948242
train gradient:  0.33989122333932353
iteration : 2917
train acc:  0.8828125
train loss:  0.29516923427581787
train gradient:  0.2133248972681196
iteration : 2918
train acc:  0.7890625
train loss:  0.4140356779098511
train gradient:  0.3139975274146512
iteration : 2919
train acc:  0.8203125
train loss:  0.4718039333820343
train gradient:  0.4464468510692984
iteration : 2920
train acc:  0.796875
train loss:  0.379336953163147
train gradient:  0.27626175849690604
iteration : 2921
train acc:  0.8515625
train loss:  0.3153885006904602
train gradient:  0.3278963256466117
iteration : 2922
train acc:  0.796875
train loss:  0.4194142818450928
train gradient:  0.32335131362275676
iteration : 2923
train acc:  0.7890625
train loss:  0.4303240180015564
train gradient:  0.35613382290387213
iteration : 2924
train acc:  0.828125
train loss:  0.3403830826282501
train gradient:  0.2642651054118692
iteration : 2925
train acc:  0.8515625
train loss:  0.379089891910553
train gradient:  0.2851483032178284
iteration : 2926
train acc:  0.7578125
train loss:  0.5069285035133362
train gradient:  0.5141393933211276
iteration : 2927
train acc:  0.796875
train loss:  0.38649648427963257
train gradient:  0.3456069683159945
iteration : 2928
train acc:  0.8515625
train loss:  0.32127058506011963
train gradient:  0.20231739313201436
iteration : 2929
train acc:  0.8828125
train loss:  0.2866634726524353
train gradient:  0.2476030388910031
iteration : 2930
train acc:  0.796875
train loss:  0.4500461518764496
train gradient:  0.38052122068356886
iteration : 2931
train acc:  0.859375
train loss:  0.28344905376434326
train gradient:  0.22230735150074799
iteration : 2932
train acc:  0.78125
train loss:  0.406837522983551
train gradient:  0.3071427484878369
iteration : 2933
train acc:  0.8515625
train loss:  0.3110268712043762
train gradient:  0.19800205535191168
iteration : 2934
train acc:  0.9296875
train loss:  0.2665359377861023
train gradient:  0.13806720486639223
iteration : 2935
train acc:  0.84375
train loss:  0.33798980712890625
train gradient:  0.21768114805066097
iteration : 2936
train acc:  0.8046875
train loss:  0.42884060740470886
train gradient:  0.40209019878372415
iteration : 2937
train acc:  0.8046875
train loss:  0.3905735909938812
train gradient:  0.3596256248449099
iteration : 2938
train acc:  0.859375
train loss:  0.3140224814414978
train gradient:  0.19387396952998825
iteration : 2939
train acc:  0.890625
train loss:  0.2863071858882904
train gradient:  0.23540247862117047
iteration : 2940
train acc:  0.8359375
train loss:  0.3849964737892151
train gradient:  0.31852859121182947
iteration : 2941
train acc:  0.78125
train loss:  0.44571584463119507
train gradient:  0.40432530015233714
iteration : 2942
train acc:  0.8515625
train loss:  0.34986650943756104
train gradient:  0.30482304996543974
iteration : 2943
train acc:  0.859375
train loss:  0.33695799112319946
train gradient:  0.2947114320315946
iteration : 2944
train acc:  0.8515625
train loss:  0.31810852885246277
train gradient:  0.2609649916750251
iteration : 2945
train acc:  0.828125
train loss:  0.32571882009506226
train gradient:  0.2431986400502001
iteration : 2946
train acc:  0.875
train loss:  0.3119211792945862
train gradient:  0.19975103667924246
iteration : 2947
train acc:  0.796875
train loss:  0.4550431966781616
train gradient:  0.3585739247798924
iteration : 2948
train acc:  0.8359375
train loss:  0.3350925147533417
train gradient:  0.2744791872971353
iteration : 2949
train acc:  0.8359375
train loss:  0.377284437417984
train gradient:  0.32893470044477063
iteration : 2950
train acc:  0.8125
train loss:  0.37777912616729736
train gradient:  0.32602068017392516
iteration : 2951
train acc:  0.7890625
train loss:  0.45237383246421814
train gradient:  0.5267597015430381
iteration : 2952
train acc:  0.828125
train loss:  0.35529643297195435
train gradient:  0.24377477605315434
iteration : 2953
train acc:  0.8046875
train loss:  0.3917525112628937
train gradient:  0.4857921104218644
iteration : 2954
train acc:  0.828125
train loss:  0.34082430601119995
train gradient:  0.34446400455997583
iteration : 2955
train acc:  0.8046875
train loss:  0.4179418087005615
train gradient:  0.35786353934043796
iteration : 2956
train acc:  0.8828125
train loss:  0.28133803606033325
train gradient:  0.25016118809530213
iteration : 2957
train acc:  0.8125
train loss:  0.44561031460762024
train gradient:  0.6080104693375373
iteration : 2958
train acc:  0.8359375
train loss:  0.3841707110404968
train gradient:  0.3270088675291778
iteration : 2959
train acc:  0.875
train loss:  0.32132354378700256
train gradient:  0.28368320634766736
iteration : 2960
train acc:  0.890625
train loss:  0.33571723103523254
train gradient:  0.2809552019149239
iteration : 2961
train acc:  0.7734375
train loss:  0.4246981143951416
train gradient:  0.35018648497234406
iteration : 2962
train acc:  0.8359375
train loss:  0.37957876920700073
train gradient:  0.32084052366886257
iteration : 2963
train acc:  0.8203125
train loss:  0.37525734305381775
train gradient:  0.3440635093728399
iteration : 2964
train acc:  0.8515625
train loss:  0.3858625888824463
train gradient:  0.3833496045972338
iteration : 2965
train acc:  0.8359375
train loss:  0.34497159719467163
train gradient:  0.31099445425798855
iteration : 2966
train acc:  0.8515625
train loss:  0.30400100350379944
train gradient:  0.24953505251806152
iteration : 2967
train acc:  0.84375
train loss:  0.3466900885105133
train gradient:  0.30627397429683684
iteration : 2968
train acc:  0.75
train loss:  0.49772343039512634
train gradient:  0.4643181847667815
iteration : 2969
train acc:  0.84375
train loss:  0.37066715955734253
train gradient:  0.2385542592019274
iteration : 2970
train acc:  0.8203125
train loss:  0.36003580689430237
train gradient:  0.30807429588267077
iteration : 2971
train acc:  0.875
train loss:  0.30524659156799316
train gradient:  0.20473206122369417
iteration : 2972
train acc:  0.8671875
train loss:  0.3468809127807617
train gradient:  0.3916358321413785
iteration : 2973
train acc:  0.828125
train loss:  0.37828609347343445
train gradient:  0.3054629755400876
iteration : 2974
train acc:  0.875
train loss:  0.3198174834251404
train gradient:  0.3322451971404457
iteration : 2975
train acc:  0.8046875
train loss:  0.3566911816596985
train gradient:  0.252477434011344
iteration : 2976
train acc:  0.78125
train loss:  0.4287818968296051
train gradient:  0.4052850506765667
iteration : 2977
train acc:  0.8359375
train loss:  0.327206552028656
train gradient:  0.26335412618673604
iteration : 2978
train acc:  0.8515625
train loss:  0.34381091594696045
train gradient:  0.2896203492185991
iteration : 2979
train acc:  0.8125
train loss:  0.47336673736572266
train gradient:  0.5507805134719468
iteration : 2980
train acc:  0.7890625
train loss:  0.4214947819709778
train gradient:  0.41488512615193807
iteration : 2981
train acc:  0.7890625
train loss:  0.4549286961555481
train gradient:  0.3883250975191193
iteration : 2982
train acc:  0.8671875
train loss:  0.3570060133934021
train gradient:  0.27165992293415575
iteration : 2983
train acc:  0.8046875
train loss:  0.43462902307510376
train gradient:  0.3482084588892706
iteration : 2984
train acc:  0.8046875
train loss:  0.44585728645324707
train gradient:  0.34766228130080806
iteration : 2985
train acc:  0.8046875
train loss:  0.459067702293396
train gradient:  0.539558058775135
iteration : 2986
train acc:  0.8515625
train loss:  0.3201714754104614
train gradient:  0.28694243525802193
iteration : 2987
train acc:  0.796875
train loss:  0.4675745964050293
train gradient:  0.46407910321182483
iteration : 2988
train acc:  0.8515625
train loss:  0.37851443886756897
train gradient:  0.3652574243414046
iteration : 2989
train acc:  0.8671875
train loss:  0.3534132242202759
train gradient:  0.30941243611909536
iteration : 2990
train acc:  0.8671875
train loss:  0.34903600811958313
train gradient:  0.26617908942431917
iteration : 2991
train acc:  0.78125
train loss:  0.43763986229896545
train gradient:  0.3297814359647825
iteration : 2992
train acc:  0.8125
train loss:  0.43020379543304443
train gradient:  0.6008238222782478
iteration : 2993
train acc:  0.859375
train loss:  0.39239197969436646
train gradient:  0.31627414845668894
iteration : 2994
train acc:  0.7890625
train loss:  0.40040963888168335
train gradient:  0.33583192989707067
iteration : 2995
train acc:  0.8515625
train loss:  0.39342594146728516
train gradient:  0.28738767689626454
iteration : 2996
train acc:  0.8828125
train loss:  0.33077725768089294
train gradient:  0.26168631782818885
iteration : 2997
train acc:  0.8671875
train loss:  0.3999265730381012
train gradient:  0.2909951795271083
iteration : 2998
train acc:  0.8671875
train loss:  0.3242332339286804
train gradient:  0.25206425791492765
iteration : 2999
train acc:  0.8828125
train loss:  0.3212645351886749
train gradient:  0.1594197673196249
iteration : 3000
train acc:  0.828125
train loss:  0.3720483183860779
train gradient:  0.23049236132119588
iteration : 3001
train acc:  0.8203125
train loss:  0.3965611457824707
train gradient:  0.40930741674151966
iteration : 3002
train acc:  0.8125
train loss:  0.40529972314834595
train gradient:  0.380972403391615
iteration : 3003
train acc:  0.828125
train loss:  0.35204818844795227
train gradient:  0.19476875719060985
iteration : 3004
train acc:  0.765625
train loss:  0.46235138177871704
train gradient:  0.3526782552865723
iteration : 3005
train acc:  0.7421875
train loss:  0.45152556896209717
train gradient:  0.36536675654944534
iteration : 3006
train acc:  0.7890625
train loss:  0.3948909640312195
train gradient:  0.3515897938037845
iteration : 3007
train acc:  0.859375
train loss:  0.3812696635723114
train gradient:  0.2748798940230005
iteration : 3008
train acc:  0.7890625
train loss:  0.3854626417160034
train gradient:  0.29878237436112115
iteration : 3009
train acc:  0.859375
train loss:  0.33581945300102234
train gradient:  0.21291833731954984
iteration : 3010
train acc:  0.8046875
train loss:  0.38364508748054504
train gradient:  0.34762531658763374
iteration : 3011
train acc:  0.8203125
train loss:  0.4293426275253296
train gradient:  0.3239927299833163
iteration : 3012
train acc:  0.8515625
train loss:  0.3611224293708801
train gradient:  0.31603685200770665
iteration : 3013
train acc:  0.8203125
train loss:  0.3996298313140869
train gradient:  0.4365057151427189
iteration : 3014
train acc:  0.8828125
train loss:  0.28194522857666016
train gradient:  0.16149361499615394
iteration : 3015
train acc:  0.8671875
train loss:  0.30750158429145813
train gradient:  0.18734112577351958
iteration : 3016
train acc:  0.796875
train loss:  0.44148287177085876
train gradient:  0.2804699325204974
iteration : 3017
train acc:  0.7890625
train loss:  0.42290717363357544
train gradient:  0.3778944599611106
iteration : 3018
train acc:  0.8203125
train loss:  0.43338143825531006
train gradient:  0.2657759375729353
iteration : 3019
train acc:  0.8046875
train loss:  0.47049474716186523
train gradient:  0.48593003724309414
iteration : 3020
train acc:  0.8203125
train loss:  0.36886656284332275
train gradient:  0.3283161101894552
iteration : 3021
train acc:  0.8046875
train loss:  0.3887813091278076
train gradient:  0.3280803865776006
iteration : 3022
train acc:  0.8203125
train loss:  0.3660169243812561
train gradient:  0.3362480379844507
iteration : 3023
train acc:  0.796875
train loss:  0.40610018372535706
train gradient:  0.34593454793767964
iteration : 3024
train acc:  0.84375
train loss:  0.4066263437271118
train gradient:  0.29537696692095305
iteration : 3025
train acc:  0.875
train loss:  0.3741533160209656
train gradient:  0.34849040929052744
iteration : 3026
train acc:  0.8359375
train loss:  0.3832002282142639
train gradient:  0.37136731322994304
iteration : 3027
train acc:  0.90625
train loss:  0.2822088897228241
train gradient:  0.1967242763391637
iteration : 3028
train acc:  0.8828125
train loss:  0.30537378787994385
train gradient:  0.2308377106024418
iteration : 3029
train acc:  0.8671875
train loss:  0.32153505086898804
train gradient:  0.212652965098063
iteration : 3030
train acc:  0.796875
train loss:  0.3993959128856659
train gradient:  0.24134823127513066
iteration : 3031
train acc:  0.8359375
train loss:  0.3675343096256256
train gradient:  0.33954551628944407
iteration : 3032
train acc:  0.84375
train loss:  0.3773103952407837
train gradient:  0.38956054190230066
iteration : 3033
train acc:  0.8515625
train loss:  0.3332301378250122
train gradient:  0.22309797972558854
iteration : 3034
train acc:  0.8203125
train loss:  0.3993428945541382
train gradient:  0.40602021218087897
iteration : 3035
train acc:  0.8984375
train loss:  0.2794207036495209
train gradient:  0.3633439846902165
iteration : 3036
train acc:  0.8125
train loss:  0.37952184677124023
train gradient:  0.3204543648678979
iteration : 3037
train acc:  0.796875
train loss:  0.4231896698474884
train gradient:  0.3103483966195603
iteration : 3038
train acc:  0.828125
train loss:  0.35951516032218933
train gradient:  0.29366922811861457
iteration : 3039
train acc:  0.8125
train loss:  0.3755178451538086
train gradient:  0.2881571527097071
iteration : 3040
train acc:  0.84375
train loss:  0.3461996018886566
train gradient:  0.1990561754219735
iteration : 3041
train acc:  0.8359375
train loss:  0.3528928756713867
train gradient:  0.2957482937004661
iteration : 3042
train acc:  0.828125
train loss:  0.3597390651702881
train gradient:  0.23796434685903756
iteration : 3043
train acc:  0.8671875
train loss:  0.32363319396972656
train gradient:  0.2796544722211256
iteration : 3044
train acc:  0.8828125
train loss:  0.31714123487472534
train gradient:  0.1905818932189624
iteration : 3045
train acc:  0.7421875
train loss:  0.49675464630126953
train gradient:  0.42361707945516497
iteration : 3046
train acc:  0.8203125
train loss:  0.3656730651855469
train gradient:  0.3215973721095459
iteration : 3047
train acc:  0.828125
train loss:  0.37488076090812683
train gradient:  0.3665371154432011
iteration : 3048
train acc:  0.7890625
train loss:  0.4219389855861664
train gradient:  0.557459798549023
iteration : 3049
train acc:  0.8125
train loss:  0.3589664399623871
train gradient:  0.33894377529692227
iteration : 3050
train acc:  0.8359375
train loss:  0.3400227427482605
train gradient:  0.33330644110240176
iteration : 3051
train acc:  0.8359375
train loss:  0.342399001121521
train gradient:  0.3200618603744754
iteration : 3052
train acc:  0.8515625
train loss:  0.37374651432037354
train gradient:  0.3634048646307289
iteration : 3053
train acc:  0.8828125
train loss:  0.32906991243362427
train gradient:  0.2908300418435097
iteration : 3054
train acc:  0.8203125
train loss:  0.44837266206741333
train gradient:  0.3456198825794916
iteration : 3055
train acc:  0.84375
train loss:  0.3572368323802948
train gradient:  0.22989511104405935
iteration : 3056
train acc:  0.8046875
train loss:  0.36746710538864136
train gradient:  0.283902154896728
iteration : 3057
train acc:  0.765625
train loss:  0.4551399350166321
train gradient:  0.3701754487208967
iteration : 3058
train acc:  0.84375
train loss:  0.3737819790840149
train gradient:  0.3548744243181753
iteration : 3059
train acc:  0.859375
train loss:  0.3342447876930237
train gradient:  0.27443146073785496
iteration : 3060
train acc:  0.84375
train loss:  0.36890342831611633
train gradient:  0.2347640180707592
iteration : 3061
train acc:  0.875
train loss:  0.3380029499530792
train gradient:  0.28847126062950973
iteration : 3062
train acc:  0.875
train loss:  0.33033230900764465
train gradient:  0.3163495262153039
iteration : 3063
train acc:  0.8203125
train loss:  0.4151054620742798
train gradient:  0.42129197865383133
iteration : 3064
train acc:  0.859375
train loss:  0.3589754104614258
train gradient:  0.5110192315242328
iteration : 3065
train acc:  0.796875
train loss:  0.37404391169548035
train gradient:  0.3982803463949589
iteration : 3066
train acc:  0.84375
train loss:  0.4171668291091919
train gradient:  0.34580781901046265
iteration : 3067
train acc:  0.8828125
train loss:  0.3319907486438751
train gradient:  0.28598272840361155
iteration : 3068
train acc:  0.8515625
train loss:  0.37182730436325073
train gradient:  0.29681132573856
iteration : 3069
train acc:  0.8359375
train loss:  0.38309556245803833
train gradient:  0.35505852451659214
iteration : 3070
train acc:  0.875
train loss:  0.314358115196228
train gradient:  0.22780069287649574
iteration : 3071
train acc:  0.828125
train loss:  0.4201442003250122
train gradient:  0.36079061425509823
iteration : 3072
train acc:  0.890625
train loss:  0.3132602572441101
train gradient:  0.23976294496165707
iteration : 3073
train acc:  0.8515625
train loss:  0.3991341292858124
train gradient:  0.5822076229444897
iteration : 3074
train acc:  0.8359375
train loss:  0.37448006868362427
train gradient:  0.29603049664873204
iteration : 3075
train acc:  0.828125
train loss:  0.3757985830307007
train gradient:  0.3799395200995284
iteration : 3076
train acc:  0.796875
train loss:  0.43975019454956055
train gradient:  0.29804366019146356
iteration : 3077
train acc:  0.8515625
train loss:  0.3463813066482544
train gradient:  0.3655094221229586
iteration : 3078
train acc:  0.8828125
train loss:  0.31469783186912537
train gradient:  0.3173654093444909
iteration : 3079
train acc:  0.8203125
train loss:  0.3945019841194153
train gradient:  0.3255509722058103
iteration : 3080
train acc:  0.8046875
train loss:  0.43700897693634033
train gradient:  0.4852183958856392
iteration : 3081
train acc:  0.859375
train loss:  0.3435829281806946
train gradient:  0.24185141752922096
iteration : 3082
train acc:  0.8046875
train loss:  0.4563663601875305
train gradient:  0.3950976850660055
iteration : 3083
train acc:  0.8046875
train loss:  0.4486772418022156
train gradient:  0.38122769103381654
iteration : 3084
train acc:  0.765625
train loss:  0.45779356360435486
train gradient:  0.30738335227549035
iteration : 3085
train acc:  0.90625
train loss:  0.2873110771179199
train gradient:  0.26380090428188946
iteration : 3086
train acc:  0.8125
train loss:  0.42729881405830383
train gradient:  0.87477575233268
iteration : 3087
train acc:  0.796875
train loss:  0.3675313889980316
train gradient:  0.3261231933009586
iteration : 3088
train acc:  0.84375
train loss:  0.35745781660079956
train gradient:  0.3007864067301163
iteration : 3089
train acc:  0.84375
train loss:  0.352450966835022
train gradient:  0.2682276681895522
iteration : 3090
train acc:  0.859375
train loss:  0.38966241478919983
train gradient:  0.3596225914639881
iteration : 3091
train acc:  0.859375
train loss:  0.3896383047103882
train gradient:  0.31804139662994996
iteration : 3092
train acc:  0.859375
train loss:  0.3124370574951172
train gradient:  0.26412013053146177
iteration : 3093
train acc:  0.8359375
train loss:  0.4098164439201355
train gradient:  0.5707413916898179
iteration : 3094
train acc:  0.875
train loss:  0.32790082693099976
train gradient:  0.2183900305697597
iteration : 3095
train acc:  0.84375
train loss:  0.36319035291671753
train gradient:  0.35339111924731503
iteration : 3096
train acc:  0.859375
train loss:  0.3799901008605957
train gradient:  0.4220620834887349
iteration : 3097
train acc:  0.8359375
train loss:  0.3480272889137268
train gradient:  0.3583366325948073
iteration : 3098
train acc:  0.78125
train loss:  0.4353192448616028
train gradient:  0.2755743494311301
iteration : 3099
train acc:  0.875
train loss:  0.3722744286060333
train gradient:  0.608629519364101
iteration : 3100
train acc:  0.8125
train loss:  0.392965704202652
train gradient:  0.3600367541161089
iteration : 3101
train acc:  0.8125
train loss:  0.3659633994102478
train gradient:  0.3073459913701353
iteration : 3102
train acc:  0.8671875
train loss:  0.35246115922927856
train gradient:  0.1982239360585281
iteration : 3103
train acc:  0.796875
train loss:  0.4415667951107025
train gradient:  0.42046124840439264
iteration : 3104
train acc:  0.84375
train loss:  0.36910414695739746
train gradient:  0.21880937414998353
iteration : 3105
train acc:  0.8125
train loss:  0.47616422176361084
train gradient:  0.6672418858568319
iteration : 3106
train acc:  0.8359375
train loss:  0.4191606640815735
train gradient:  0.2942166179519673
iteration : 3107
train acc:  0.8359375
train loss:  0.32734203338623047
train gradient:  0.2726666007201901
iteration : 3108
train acc:  0.78125
train loss:  0.48913004994392395
train gradient:  0.4320374630118977
iteration : 3109
train acc:  0.828125
train loss:  0.3978595733642578
train gradient:  0.3672062951968301
iteration : 3110
train acc:  0.8125
train loss:  0.4359770715236664
train gradient:  0.4247794365248572
iteration : 3111
train acc:  0.796875
train loss:  0.381470263004303
train gradient:  0.3304127700295746
iteration : 3112
train acc:  0.8515625
train loss:  0.3595294952392578
train gradient:  0.38600893539648445
iteration : 3113
train acc:  0.8359375
train loss:  0.38833096623420715
train gradient:  0.3519437736385292
iteration : 3114
train acc:  0.796875
train loss:  0.39849168062210083
train gradient:  0.36253900651179605
iteration : 3115
train acc:  0.7578125
train loss:  0.46647918224334717
train gradient:  0.5194225291248412
iteration : 3116
train acc:  0.828125
train loss:  0.36832869052886963
train gradient:  0.3565855677176622
iteration : 3117
train acc:  0.78125
train loss:  0.4509299397468567
train gradient:  0.406091602192764
iteration : 3118
train acc:  0.8828125
train loss:  0.3136773109436035
train gradient:  0.2810890627170214
iteration : 3119
train acc:  0.8359375
train loss:  0.3721005916595459
train gradient:  0.4283089177397201
iteration : 3120
train acc:  0.8203125
train loss:  0.38119566440582275
train gradient:  0.30719515901160094
iteration : 3121
train acc:  0.84375
train loss:  0.31528377532958984
train gradient:  0.3286281439097211
iteration : 3122
train acc:  0.7890625
train loss:  0.4520065486431122
train gradient:  0.3780648712516488
iteration : 3123
train acc:  0.8515625
train loss:  0.345875084400177
train gradient:  0.27635395665021745
iteration : 3124
train acc:  0.8046875
train loss:  0.36989158391952515
train gradient:  0.26544315574018146
iteration : 3125
train acc:  0.8359375
train loss:  0.31806933879852295
train gradient:  0.27003588634975917
iteration : 3126
train acc:  0.7734375
train loss:  0.46996796131134033
train gradient:  0.5110159281447627
iteration : 3127
train acc:  0.796875
train loss:  0.4546731114387512
train gradient:  0.41310337153492294
iteration : 3128
train acc:  0.90625
train loss:  0.282555490732193
train gradient:  0.21972562581483862
iteration : 3129
train acc:  0.796875
train loss:  0.44282156229019165
train gradient:  0.443657568440531
iteration : 3130
train acc:  0.859375
train loss:  0.3327832818031311
train gradient:  0.27973053212833376
iteration : 3131
train acc:  0.9140625
train loss:  0.3498170077800751
train gradient:  0.26811948807729896
iteration : 3132
train acc:  0.8203125
train loss:  0.37159228324890137
train gradient:  0.30617698706470897
iteration : 3133
train acc:  0.8125
train loss:  0.4058706760406494
train gradient:  0.2725867974181812
iteration : 3134
train acc:  0.8515625
train loss:  0.3725093901157379
train gradient:  0.22684886148966565
iteration : 3135
train acc:  0.8671875
train loss:  0.2854296863079071
train gradient:  0.21183678192573568
iteration : 3136
train acc:  0.8359375
train loss:  0.3609961271286011
train gradient:  0.2590483080895343
iteration : 3137
train acc:  0.78125
train loss:  0.493684858083725
train gradient:  0.5758559707426621
iteration : 3138
train acc:  0.8359375
train loss:  0.39360079169273376
train gradient:  0.2641744808191972
iteration : 3139
train acc:  0.796875
train loss:  0.4580141007900238
train gradient:  0.4490640794017194
iteration : 3140
train acc:  0.875
train loss:  0.32876354455947876
train gradient:  0.20313410902190537
iteration : 3141
train acc:  0.84375
train loss:  0.32741793990135193
train gradient:  0.3697220329664316
iteration : 3142
train acc:  0.828125
train loss:  0.3165278434753418
train gradient:  0.18473475864751765
iteration : 3143
train acc:  0.8671875
train loss:  0.31925299763679504
train gradient:  0.2458264774440973
iteration : 3144
train acc:  0.859375
train loss:  0.3284396827220917
train gradient:  0.2623688036277297
iteration : 3145
train acc:  0.84375
train loss:  0.4309370219707489
train gradient:  0.5138537764042859
iteration : 3146
train acc:  0.8125
train loss:  0.41231769323349
train gradient:  0.5003069967374706
iteration : 3147
train acc:  0.8828125
train loss:  0.29706525802612305
train gradient:  0.24020845292557008
iteration : 3148
train acc:  0.8828125
train loss:  0.32238179445266724
train gradient:  0.27191058103372473
iteration : 3149
train acc:  0.8203125
train loss:  0.39859694242477417
train gradient:  0.29962129339059934
iteration : 3150
train acc:  0.8125
train loss:  0.4058725833892822
train gradient:  0.4197864181394318
iteration : 3151
train acc:  0.8671875
train loss:  0.31686028838157654
train gradient:  0.3039088141714663
iteration : 3152
train acc:  0.859375
train loss:  0.33287563920021057
train gradient:  0.2938648339568376
iteration : 3153
train acc:  0.8359375
train loss:  0.3252737522125244
train gradient:  0.2837372947387911
iteration : 3154
train acc:  0.8203125
train loss:  0.3726271986961365
train gradient:  0.27278139844050714
iteration : 3155
train acc:  0.875
train loss:  0.3305671811103821
train gradient:  0.30857489240580976
iteration : 3156
train acc:  0.8359375
train loss:  0.34296083450317383
train gradient:  0.3336714440448596
iteration : 3157
train acc:  0.8671875
train loss:  0.2971044182777405
train gradient:  0.24932549856573052
iteration : 3158
train acc:  0.8515625
train loss:  0.36129429936408997
train gradient:  0.2529205245594734
iteration : 3159
train acc:  0.84375
train loss:  0.3390069007873535
train gradient:  0.32122695522847494
iteration : 3160
train acc:  0.84375
train loss:  0.3473074436187744
train gradient:  0.48430023699722485
iteration : 3161
train acc:  0.828125
train loss:  0.4063173532485962
train gradient:  0.3995099547617816
iteration : 3162
train acc:  0.7734375
train loss:  0.48085135221481323
train gradient:  0.3874957064606421
iteration : 3163
train acc:  0.8203125
train loss:  0.34931737184524536
train gradient:  0.26889604122688415
iteration : 3164
train acc:  0.84375
train loss:  0.3645980954170227
train gradient:  0.30603848187243343
iteration : 3165
train acc:  0.828125
train loss:  0.4407839775085449
train gradient:  0.34572907255052354
iteration : 3166
train acc:  0.8125
train loss:  0.44223058223724365
train gradient:  0.3580231251941448
iteration : 3167
train acc:  0.765625
train loss:  0.43063122034072876
train gradient:  0.5050673272961425
iteration : 3168
train acc:  0.890625
train loss:  0.3062308728694916
train gradient:  0.29225335687666
iteration : 3169
train acc:  0.8046875
train loss:  0.3881494998931885
train gradient:  0.3619115462790662
iteration : 3170
train acc:  0.828125
train loss:  0.34959402680397034
train gradient:  0.3454016538048838
iteration : 3171
train acc:  0.9296875
train loss:  0.2552911937236786
train gradient:  0.14089319771415593
iteration : 3172
train acc:  0.8671875
train loss:  0.3539291024208069
train gradient:  0.3577062601222364
iteration : 3173
train acc:  0.828125
train loss:  0.3670933246612549
train gradient:  0.33634955715732534
iteration : 3174
train acc:  0.875
train loss:  0.3204139173030853
train gradient:  0.19047959361522698
iteration : 3175
train acc:  0.8125
train loss:  0.4722602963447571
train gradient:  0.47127239239193675
iteration : 3176
train acc:  0.8515625
train loss:  0.3689778745174408
train gradient:  0.3244854478024525
iteration : 3177
train acc:  0.8359375
train loss:  0.3748587369918823
train gradient:  0.4154859520481657
iteration : 3178
train acc:  0.859375
train loss:  0.40522682666778564
train gradient:  0.6199303509966396
iteration : 3179
train acc:  0.8203125
train loss:  0.3768819272518158
train gradient:  0.26127457439716845
iteration : 3180
train acc:  0.8046875
train loss:  0.405877947807312
train gradient:  0.34145414106223115
iteration : 3181
train acc:  0.8671875
train loss:  0.27276766300201416
train gradient:  0.24030892022182493
iteration : 3182
train acc:  0.8046875
train loss:  0.42638105154037476
train gradient:  0.40603601076544293
iteration : 3183
train acc:  0.8359375
train loss:  0.33225196599960327
train gradient:  0.22559913402113224
iteration : 3184
train acc:  0.7890625
train loss:  0.4503931403160095
train gradient:  0.5287247615903554
iteration : 3185
train acc:  0.828125
train loss:  0.3590322434902191
train gradient:  0.3467885465288054
iteration : 3186
train acc:  0.7734375
train loss:  0.4522416591644287
train gradient:  0.3429102515752727
iteration : 3187
train acc:  0.8515625
train loss:  0.3253127932548523
train gradient:  0.2356272492434014
iteration : 3188
train acc:  0.8515625
train loss:  0.3397116959095001
train gradient:  0.2820085530211597
iteration : 3189
train acc:  0.875
train loss:  0.3544374406337738
train gradient:  0.27907866755719163
iteration : 3190
train acc:  0.90625
train loss:  0.3276000916957855
train gradient:  0.36058898696331737
iteration : 3191
train acc:  0.7890625
train loss:  0.48826780915260315
train gradient:  0.46380123546759616
iteration : 3192
train acc:  0.84375
train loss:  0.4343629479408264
train gradient:  0.4127873090976563
iteration : 3193
train acc:  0.8046875
train loss:  0.40971124172210693
train gradient:  0.3475263479827164
iteration : 3194
train acc:  0.8203125
train loss:  0.39560115337371826
train gradient:  0.3629994899909725
iteration : 3195
train acc:  0.8984375
train loss:  0.31085771322250366
train gradient:  0.17460498075137487
iteration : 3196
train acc:  0.828125
train loss:  0.4195300340652466
train gradient:  0.3264050037090527
iteration : 3197
train acc:  0.78125
train loss:  0.4232967495918274
train gradient:  0.42670187198078896
iteration : 3198
train acc:  0.890625
train loss:  0.33258259296417236
train gradient:  0.25370068434954474
iteration : 3199
train acc:  0.890625
train loss:  0.318170428276062
train gradient:  0.23132721927838476
iteration : 3200
train acc:  0.8125
train loss:  0.4223287105560303
train gradient:  0.33510756927827
iteration : 3201
train acc:  0.8046875
train loss:  0.3895423412322998
train gradient:  0.4026679012973206
iteration : 3202
train acc:  0.84375
train loss:  0.32817327976226807
train gradient:  0.3116468248259798
iteration : 3203
train acc:  0.828125
train loss:  0.3804957866668701
train gradient:  0.3327001016259307
iteration : 3204
train acc:  0.8515625
train loss:  0.37285277247428894
train gradient:  0.2834497560551478
iteration : 3205
train acc:  0.828125
train loss:  0.3724459409713745
train gradient:  0.2929104889226246
iteration : 3206
train acc:  0.8203125
train loss:  0.40842849016189575
train gradient:  0.3970967417277905
iteration : 3207
train acc:  0.8359375
train loss:  0.35762640833854675
train gradient:  0.3056844219328208
iteration : 3208
train acc:  0.8046875
train loss:  0.40124067664146423
train gradient:  0.4537501987201341
iteration : 3209
train acc:  0.8203125
train loss:  0.4187706708908081
train gradient:  0.379922705834396
iteration : 3210
train acc:  0.8984375
train loss:  0.29863715171813965
train gradient:  0.2573707631929929
iteration : 3211
train acc:  0.828125
train loss:  0.37597769498825073
train gradient:  0.46256683807045834
iteration : 3212
train acc:  0.7890625
train loss:  0.41938644647598267
train gradient:  0.30509281669165944
iteration : 3213
train acc:  0.84375
train loss:  0.35720714926719666
train gradient:  0.2905513826185716
iteration : 3214
train acc:  0.8515625
train loss:  0.3570675849914551
train gradient:  0.19916283531914966
iteration : 3215
train acc:  0.859375
train loss:  0.326436847448349
train gradient:  0.19475226620093877
iteration : 3216
train acc:  0.890625
train loss:  0.3259369730949402
train gradient:  0.24992043768729705
iteration : 3217
train acc:  0.84375
train loss:  0.36491382122039795
train gradient:  0.3413418292945085
iteration : 3218
train acc:  0.8671875
train loss:  0.3303075134754181
train gradient:  0.28115931289416257
iteration : 3219
train acc:  0.8203125
train loss:  0.38908651471138
train gradient:  0.3000171631349044
iteration : 3220
train acc:  0.78125
train loss:  0.40037015080451965
train gradient:  0.38096658120219573
iteration : 3221
train acc:  0.796875
train loss:  0.3883454203605652
train gradient:  0.34317848165768233
iteration : 3222
train acc:  0.7890625
train loss:  0.3958803713321686
train gradient:  0.31438570777186126
iteration : 3223
train acc:  0.8671875
train loss:  0.3262539207935333
train gradient:  0.3308409059712916
iteration : 3224
train acc:  0.8046875
train loss:  0.40967512130737305
train gradient:  0.41927330766700716
iteration : 3225
train acc:  0.8359375
train loss:  0.3568055331707001
train gradient:  0.342142309359279
iteration : 3226
train acc:  0.828125
train loss:  0.41331562399864197
train gradient:  0.42845255444021896
iteration : 3227
train acc:  0.875
train loss:  0.32987526059150696
train gradient:  0.3262000475873444
iteration : 3228
train acc:  0.8203125
train loss:  0.35217294096946716
train gradient:  0.2501900857502063
iteration : 3229
train acc:  0.84375
train loss:  0.3834775388240814
train gradient:  0.3460333013614232
iteration : 3230
train acc:  0.7890625
train loss:  0.44890856742858887
train gradient:  0.4368393333630168
iteration : 3231
train acc:  0.84375
train loss:  0.3465525209903717
train gradient:  0.2213668648738535
iteration : 3232
train acc:  0.8046875
train loss:  0.4679147005081177
train gradient:  0.49744852879024026
iteration : 3233
train acc:  0.8515625
train loss:  0.33081695437431335
train gradient:  0.25775709561146776
iteration : 3234
train acc:  0.8671875
train loss:  0.30304259061813354
train gradient:  0.2145965198428213
iteration : 3235
train acc:  0.796875
train loss:  0.4640732705593109
train gradient:  0.4607694872688161
iteration : 3236
train acc:  0.8203125
train loss:  0.43426018953323364
train gradient:  0.3760668669010422
iteration : 3237
train acc:  0.859375
train loss:  0.3409580588340759
train gradient:  0.2788616112260131
iteration : 3238
train acc:  0.890625
train loss:  0.3127366900444031
train gradient:  0.21955950960622
iteration : 3239
train acc:  0.84375
train loss:  0.3187965154647827
train gradient:  0.4122142596892923
iteration : 3240
train acc:  0.7890625
train loss:  0.4805135428905487
train gradient:  0.3854734876982441
iteration : 3241
train acc:  0.8359375
train loss:  0.35973861813545227
train gradient:  0.25543473864754457
iteration : 3242
train acc:  0.796875
train loss:  0.4032432436943054
train gradient:  0.3422286507278692
iteration : 3243
train acc:  0.78125
train loss:  0.4543067216873169
train gradient:  0.7367407539752757
iteration : 3244
train acc:  0.8671875
train loss:  0.31461605429649353
train gradient:  0.2176741700593315
iteration : 3245
train acc:  0.859375
train loss:  0.341521680355072
train gradient:  0.2363345443311367
iteration : 3246
train acc:  0.8125
train loss:  0.35364848375320435
train gradient:  0.4121004624795481
iteration : 3247
train acc:  0.8828125
train loss:  0.27630615234375
train gradient:  0.21522568707527606
iteration : 3248
train acc:  0.90625
train loss:  0.2909664511680603
train gradient:  0.16032531558695912
iteration : 3249
train acc:  0.8203125
train loss:  0.4017876982688904
train gradient:  0.3403624028008208
iteration : 3250
train acc:  0.8046875
train loss:  0.39620542526245117
train gradient:  0.39826770263900035
iteration : 3251
train acc:  0.8359375
train loss:  0.3780079483985901
train gradient:  0.23723711060031524
iteration : 3252
train acc:  0.8046875
train loss:  0.4204520285129547
train gradient:  0.35673206619599857
iteration : 3253
train acc:  0.84375
train loss:  0.38323748111724854
train gradient:  0.27453708166676016
iteration : 3254
train acc:  0.8359375
train loss:  0.3612293004989624
train gradient:  0.4433855448459307
iteration : 3255
train acc:  0.8046875
train loss:  0.43773937225341797
train gradient:  0.3401059879475485
iteration : 3256
train acc:  0.8359375
train loss:  0.3572285771369934
train gradient:  0.2624183196394391
iteration : 3257
train acc:  0.875
train loss:  0.3371303081512451
train gradient:  0.31526230537132094
iteration : 3258
train acc:  0.8125
train loss:  0.43872976303100586
train gradient:  0.4491384833164794
iteration : 3259
train acc:  0.859375
train loss:  0.3111841678619385
train gradient:  0.2557234283793074
iteration : 3260
train acc:  0.8515625
train loss:  0.3592967391014099
train gradient:  0.30471173942021595
iteration : 3261
train acc:  0.84375
train loss:  0.37888312339782715
train gradient:  0.3176091924004723
iteration : 3262
train acc:  0.8046875
train loss:  0.39431363344192505
train gradient:  0.3913133761078866
iteration : 3263
train acc:  0.8203125
train loss:  0.4096667766571045
train gradient:  0.34366993379033345
iteration : 3264
train acc:  0.8125
train loss:  0.41461023688316345
train gradient:  0.3581455119944753
iteration : 3265
train acc:  0.84375
train loss:  0.2942701280117035
train gradient:  0.3383639598119694
iteration : 3266
train acc:  0.765625
train loss:  0.4640541970729828
train gradient:  0.3124932984468181
iteration : 3267
train acc:  0.7890625
train loss:  0.43808507919311523
train gradient:  0.3602816435890564
iteration : 3268
train acc:  0.78125
train loss:  0.45060086250305176
train gradient:  0.42654489543178725
iteration : 3269
train acc:  0.859375
train loss:  0.3758484721183777
train gradient:  0.25824459328799687
iteration : 3270
train acc:  0.828125
train loss:  0.3849232792854309
train gradient:  0.2229438665940033
iteration : 3271
train acc:  0.875
train loss:  0.32813912630081177
train gradient:  0.36608310644571546
iteration : 3272
train acc:  0.890625
train loss:  0.30302709341049194
train gradient:  0.30223558182293736
iteration : 3273
train acc:  0.765625
train loss:  0.49394458532333374
train gradient:  0.43649537949871836
iteration : 3274
train acc:  0.8046875
train loss:  0.43342435359954834
train gradient:  0.348747644979599
iteration : 3275
train acc:  0.8671875
train loss:  0.34685346484184265
train gradient:  0.30100719402670145
iteration : 3276
train acc:  0.8359375
train loss:  0.38111236691474915
train gradient:  0.28791203544563426
iteration : 3277
train acc:  0.8515625
train loss:  0.3673078119754791
train gradient:  0.27594704481650845
iteration : 3278
train acc:  0.8515625
train loss:  0.33316171169281006
train gradient:  0.21125829384836156
iteration : 3279
train acc:  0.796875
train loss:  0.43697959184646606
train gradient:  0.38076341656831675
iteration : 3280
train acc:  0.8125
train loss:  0.3928009867668152
train gradient:  0.3374842562690124
iteration : 3281
train acc:  0.90625
train loss:  0.3075743317604065
train gradient:  0.24470961446913922
iteration : 3282
train acc:  0.7578125
train loss:  0.4870092570781708
train gradient:  0.42353497467608675
iteration : 3283
train acc:  0.8359375
train loss:  0.41521772742271423
train gradient:  0.456133943045568
iteration : 3284
train acc:  0.8046875
train loss:  0.3864127993583679
train gradient:  0.25342848077177343
iteration : 3285
train acc:  0.859375
train loss:  0.33479100465774536
train gradient:  0.32874662038746616
iteration : 3286
train acc:  0.84375
train loss:  0.35271090269088745
train gradient:  0.2802467384107287
iteration : 3287
train acc:  0.8984375
train loss:  0.29596146941185
train gradient:  0.17380968807240985
iteration : 3288
train acc:  0.8125
train loss:  0.40525299310684204
train gradient:  0.46186954030781646
iteration : 3289
train acc:  0.828125
train loss:  0.3959747552871704
train gradient:  0.2828753570622867
iteration : 3290
train acc:  0.8203125
train loss:  0.38114815950393677
train gradient:  0.2454153798193629
iteration : 3291
train acc:  0.8203125
train loss:  0.398115873336792
train gradient:  0.41771106225106813
iteration : 3292
train acc:  0.7890625
train loss:  0.38927578926086426
train gradient:  0.23691959982947064
iteration : 3293
train acc:  0.8671875
train loss:  0.34935683012008667
train gradient:  0.336301689742499
iteration : 3294
train acc:  0.8046875
train loss:  0.3865600824356079
train gradient:  0.3142786860545417
iteration : 3295
train acc:  0.875
train loss:  0.31360453367233276
train gradient:  0.2558158319758936
iteration : 3296
train acc:  0.8359375
train loss:  0.4235331118106842
train gradient:  0.32673800920295465
iteration : 3297
train acc:  0.8125
train loss:  0.37522953748703003
train gradient:  0.21464158071939154
iteration : 3298
train acc:  0.828125
train loss:  0.4084985554218292
train gradient:  0.3187133024138356
iteration : 3299
train acc:  0.890625
train loss:  0.3206997513771057
train gradient:  0.25882840594024714
iteration : 3300
train acc:  0.84375
train loss:  0.34733641147613525
train gradient:  0.28107614211469606
iteration : 3301
train acc:  0.84375
train loss:  0.34471577405929565
train gradient:  0.3317538725226079
iteration : 3302
train acc:  0.84375
train loss:  0.3265428841114044
train gradient:  0.24354151845165647
iteration : 3303
train acc:  0.8515625
train loss:  0.33202606439590454
train gradient:  0.2275314611549206
iteration : 3304
train acc:  0.8515625
train loss:  0.32285362482070923
train gradient:  0.18124297606744755
iteration : 3305
train acc:  0.8046875
train loss:  0.40216946601867676
train gradient:  0.27970162544467114
iteration : 3306
train acc:  0.8515625
train loss:  0.33979159593582153
train gradient:  0.2228776576390028
iteration : 3307
train acc:  0.828125
train loss:  0.37026357650756836
train gradient:  0.26821656762878826
iteration : 3308
train acc:  0.8203125
train loss:  0.32812660932540894
train gradient:  0.292920523309141
iteration : 3309
train acc:  0.828125
train loss:  0.3827633261680603
train gradient:  0.3064963925432866
iteration : 3310
train acc:  0.828125
train loss:  0.3582983613014221
train gradient:  0.2224396498973505
iteration : 3311
train acc:  0.8671875
train loss:  0.3307693600654602
train gradient:  0.2393553305188918
iteration : 3312
train acc:  0.875
train loss:  0.31032440066337585
train gradient:  0.2702009366957116
iteration : 3313
train acc:  0.859375
train loss:  0.34819296002388
train gradient:  0.2552275481194068
iteration : 3314
train acc:  0.78125
train loss:  0.4412688612937927
train gradient:  0.35637181668041373
iteration : 3315
train acc:  0.8828125
train loss:  0.32684949040412903
train gradient:  0.2763213189298712
iteration : 3316
train acc:  0.890625
train loss:  0.3026334047317505
train gradient:  0.18702301379039374
iteration : 3317
train acc:  0.8046875
train loss:  0.4285244941711426
train gradient:  0.5262655406952512
iteration : 3318
train acc:  0.8671875
train loss:  0.30463212728500366
train gradient:  0.2589038675126007
iteration : 3319
train acc:  0.796875
train loss:  0.42956846952438354
train gradient:  0.49989683130786705
iteration : 3320
train acc:  0.8359375
train loss:  0.38492870330810547
train gradient:  0.3556142906804164
iteration : 3321
train acc:  0.84375
train loss:  0.31954532861709595
train gradient:  0.2260391285844314
iteration : 3322
train acc:  0.8515625
train loss:  0.4138362407684326
train gradient:  0.39634467729252004
iteration : 3323
train acc:  0.796875
train loss:  0.5014765858650208
train gradient:  0.46186410702388014
iteration : 3324
train acc:  0.765625
train loss:  0.503273069858551
train gradient:  0.6980706723532693
iteration : 3325
train acc:  0.84375
train loss:  0.31971147656440735
train gradient:  0.25564837898685405
iteration : 3326
train acc:  0.828125
train loss:  0.4019815921783447
train gradient:  0.31214576910524117
iteration : 3327
train acc:  0.8203125
train loss:  0.3878307640552521
train gradient:  0.32833288176771036
iteration : 3328
train acc:  0.84375
train loss:  0.3598833680152893
train gradient:  0.36135131159917394
iteration : 3329
train acc:  0.796875
train loss:  0.37125110626220703
train gradient:  0.26735590480361393
iteration : 3330
train acc:  0.8984375
train loss:  0.28468817472457886
train gradient:  0.3037164137229733
iteration : 3331
train acc:  0.8046875
train loss:  0.44213593006134033
train gradient:  0.4092578852438278
iteration : 3332
train acc:  0.8359375
train loss:  0.3824491500854492
train gradient:  0.34917317782292223
iteration : 3333
train acc:  0.8515625
train loss:  0.32466569542884827
train gradient:  0.18096913300782555
iteration : 3334
train acc:  0.859375
train loss:  0.32819032669067383
train gradient:  0.22077885268577088
iteration : 3335
train acc:  0.7890625
train loss:  0.4499756097793579
train gradient:  0.4558945130584657
iteration : 3336
train acc:  0.859375
train loss:  0.35441336035728455
train gradient:  0.4364111867803924
iteration : 3337
train acc:  0.8125
train loss:  0.42606592178344727
train gradient:  0.3252862480286885
iteration : 3338
train acc:  0.8671875
train loss:  0.35192763805389404
train gradient:  0.33620962630500034
iteration : 3339
train acc:  0.8125
train loss:  0.4003991186618805
train gradient:  0.3544733840683698
iteration : 3340
train acc:  0.84375
train loss:  0.347348153591156
train gradient:  0.2863724880212801
iteration : 3341
train acc:  0.8515625
train loss:  0.34074145555496216
train gradient:  0.2791838540425875
iteration : 3342
train acc:  0.8671875
train loss:  0.31703001260757446
train gradient:  0.28595681537366385
iteration : 3343
train acc:  0.8203125
train loss:  0.388175368309021
train gradient:  0.41861890475827246
iteration : 3344
train acc:  0.8046875
train loss:  0.3899388313293457
train gradient:  0.4233686870209454
iteration : 3345
train acc:  0.859375
train loss:  0.3131796717643738
train gradient:  0.2620373217673092
iteration : 3346
train acc:  0.796875
train loss:  0.454848051071167
train gradient:  0.7479600192044069
iteration : 3347
train acc:  0.84375
train loss:  0.345309317111969
train gradient:  0.35272609824019713
iteration : 3348
train acc:  0.875
train loss:  0.32477471232414246
train gradient:  0.2866508997960366
iteration : 3349
train acc:  0.84375
train loss:  0.3790488839149475
train gradient:  0.3343145932845728
iteration : 3350
train acc:  0.8359375
train loss:  0.31840431690216064
train gradient:  0.2286489762107464
iteration : 3351
train acc:  0.8046875
train loss:  0.3921523988246918
train gradient:  0.3523074852982028
iteration : 3352
train acc:  0.8515625
train loss:  0.36020439863204956
train gradient:  0.5043115086857128
iteration : 3353
train acc:  0.8203125
train loss:  0.40462470054626465
train gradient:  0.43420868830276566
iteration : 3354
train acc:  0.875
train loss:  0.31260693073272705
train gradient:  0.35475613171271997
iteration : 3355
train acc:  0.8203125
train loss:  0.3570857644081116
train gradient:  0.32060410560243247
iteration : 3356
train acc:  0.796875
train loss:  0.4408183693885803
train gradient:  0.36686015026913615
iteration : 3357
train acc:  0.828125
train loss:  0.3727066218852997
train gradient:  0.3387389052686032
iteration : 3358
train acc:  0.828125
train loss:  0.40688398480415344
train gradient:  0.4441000169917389
iteration : 3359
train acc:  0.8828125
train loss:  0.3157365620136261
train gradient:  0.2624331265024879
iteration : 3360
train acc:  0.765625
train loss:  0.4357604384422302
train gradient:  0.349781485363757
iteration : 3361
train acc:  0.8515625
train loss:  0.3409900367259979
train gradient:  0.3893878776644384
iteration : 3362
train acc:  0.875
train loss:  0.3233954906463623
train gradient:  0.36235388068774127
iteration : 3363
train acc:  0.8828125
train loss:  0.3394319713115692
train gradient:  0.29843885867765346
iteration : 3364
train acc:  0.796875
train loss:  0.37353426218032837
train gradient:  0.32480420290856943
iteration : 3365
train acc:  0.8125
train loss:  0.37904101610183716
train gradient:  0.34975027111037044
iteration : 3366
train acc:  0.84375
train loss:  0.37945130467414856
train gradient:  0.2749451384187416
iteration : 3367
train acc:  0.8046875
train loss:  0.4579727053642273
train gradient:  0.4622758342204957
iteration : 3368
train acc:  0.828125
train loss:  0.41921448707580566
train gradient:  0.5610103261494974
iteration : 3369
train acc:  0.78125
train loss:  0.42115020751953125
train gradient:  0.4832611184357324
iteration : 3370
train acc:  0.8359375
train loss:  0.34021610021591187
train gradient:  0.27053439775385757
iteration : 3371
train acc:  0.8046875
train loss:  0.46079879999160767
train gradient:  0.4393624088843852
iteration : 3372
train acc:  0.8984375
train loss:  0.26789820194244385
train gradient:  0.14780902816608804
iteration : 3373
train acc:  0.859375
train loss:  0.38960281014442444
train gradient:  0.47201377395696303
iteration : 3374
train acc:  0.8046875
train loss:  0.38901859521865845
train gradient:  0.5555035386148413
iteration : 3375
train acc:  0.859375
train loss:  0.3798680305480957
train gradient:  0.25286384552244284
iteration : 3376
train acc:  0.8671875
train loss:  0.33456361293792725
train gradient:  0.2548288038567951
iteration : 3377
train acc:  0.875
train loss:  0.3283957839012146
train gradient:  0.20913971064457745
iteration : 3378
train acc:  0.8671875
train loss:  0.3183497488498688
train gradient:  0.2356167206953709
iteration : 3379
train acc:  0.8515625
train loss:  0.31608378887176514
train gradient:  0.18224211955991548
iteration : 3380
train acc:  0.796875
train loss:  0.489436537027359
train gradient:  0.44073013667880867
iteration : 3381
train acc:  0.7890625
train loss:  0.377368688583374
train gradient:  0.23692952362883232
iteration : 3382
train acc:  0.765625
train loss:  0.4643724858760834
train gradient:  0.36267995514371965
iteration : 3383
train acc:  0.9140625
train loss:  0.2733747363090515
train gradient:  0.1535013890306597
iteration : 3384
train acc:  0.828125
train loss:  0.3807522654533386
train gradient:  0.2767593016679331
iteration : 3385
train acc:  0.8671875
train loss:  0.34119847416877747
train gradient:  0.3744574772072631
iteration : 3386
train acc:  0.84375
train loss:  0.41300520300865173
train gradient:  0.2805232153457938
iteration : 3387
train acc:  0.8046875
train loss:  0.4084674119949341
train gradient:  0.31284560384177146
iteration : 3388
train acc:  0.8046875
train loss:  0.42029014229774475
train gradient:  0.33500545647342017
iteration : 3389
train acc:  0.84375
train loss:  0.3583813011646271
train gradient:  0.24750467781010846
iteration : 3390
train acc:  0.8515625
train loss:  0.36044442653656006
train gradient:  0.3096915804142893
iteration : 3391
train acc:  0.84375
train loss:  0.39404982328414917
train gradient:  0.32834400714892825
iteration : 3392
train acc:  0.8046875
train loss:  0.3645462989807129
train gradient:  0.2853059840684967
iteration : 3393
train acc:  0.8203125
train loss:  0.37565869092941284
train gradient:  0.25489454733395134
iteration : 3394
train acc:  0.9140625
train loss:  0.26529660820961
train gradient:  0.15132657197504054
iteration : 3395
train acc:  0.859375
train loss:  0.3654824495315552
train gradient:  0.22827265712267053
iteration : 3396
train acc:  0.8203125
train loss:  0.38488757610321045
train gradient:  0.5414148957989369
iteration : 3397
train acc:  0.8828125
train loss:  0.3116503953933716
train gradient:  0.1679328285338596
iteration : 3398
train acc:  0.828125
train loss:  0.3930695056915283
train gradient:  0.32242014224202453
iteration : 3399
train acc:  0.8828125
train loss:  0.3058483600616455
train gradient:  0.26967120899621816
iteration : 3400
train acc:  0.828125
train loss:  0.3533255457878113
train gradient:  0.2774049138834571
iteration : 3401
train acc:  0.8671875
train loss:  0.33315056562423706
train gradient:  0.22853082654178175
iteration : 3402
train acc:  0.8046875
train loss:  0.3757875859737396
train gradient:  0.39892255647862307
iteration : 3403
train acc:  0.8125
train loss:  0.4046604037284851
train gradient:  0.30252114241787004
iteration : 3404
train acc:  0.8828125
train loss:  0.3442683517932892
train gradient:  0.2463584212831064
iteration : 3405
train acc:  0.8046875
train loss:  0.4446081519126892
train gradient:  0.38500491415040167
iteration : 3406
train acc:  0.8515625
train loss:  0.3467235267162323
train gradient:  0.2488085876011005
iteration : 3407
train acc:  0.8359375
train loss:  0.3322763442993164
train gradient:  0.20099906269610307
iteration : 3408
train acc:  0.796875
train loss:  0.4402005672454834
train gradient:  0.35488827065459794
iteration : 3409
train acc:  0.8046875
train loss:  0.4011193811893463
train gradient:  0.2907402080388446
iteration : 3410
train acc:  0.796875
train loss:  0.4144732356071472
train gradient:  0.3854203379049052
iteration : 3411
train acc:  0.8828125
train loss:  0.2853851318359375
train gradient:  0.1671638450281237
iteration : 3412
train acc:  0.828125
train loss:  0.3957662582397461
train gradient:  0.3189214961130248
iteration : 3413
train acc:  0.8046875
train loss:  0.45504534244537354
train gradient:  0.35003343152169447
iteration : 3414
train acc:  0.8359375
train loss:  0.3365571200847626
train gradient:  0.2532182278902917
iteration : 3415
train acc:  0.828125
train loss:  0.32182836532592773
train gradient:  0.2440111015031984
iteration : 3416
train acc:  0.8359375
train loss:  0.41125020384788513
train gradient:  0.3857247459263003
iteration : 3417
train acc:  0.8359375
train loss:  0.3241499364376068
train gradient:  0.31076403989981394
iteration : 3418
train acc:  0.8515625
train loss:  0.3995777666568756
train gradient:  0.44318279107405933
iteration : 3419
train acc:  0.765625
train loss:  0.4182621240615845
train gradient:  0.3420312460199735
iteration : 3420
train acc:  0.8359375
train loss:  0.3898508548736572
train gradient:  0.37754701385675676
iteration : 3421
train acc:  0.8046875
train loss:  0.36764174699783325
train gradient:  0.2901096752185799
iteration : 3422
train acc:  0.7890625
train loss:  0.44000789523124695
train gradient:  0.38166843215934754
iteration : 3423
train acc:  0.7890625
train loss:  0.48807281255722046
train gradient:  0.4699787093739313
iteration : 3424
train acc:  0.8515625
train loss:  0.3628539443016052
train gradient:  0.2522300936902467
iteration : 3425
train acc:  0.8203125
train loss:  0.4020111560821533
train gradient:  0.23818940086538767
iteration : 3426
train acc:  0.8203125
train loss:  0.34441691637039185
train gradient:  0.20071024485277045
iteration : 3427
train acc:  0.84375
train loss:  0.3431105613708496
train gradient:  0.3871193837251455
iteration : 3428
train acc:  0.8828125
train loss:  0.3316761553287506
train gradient:  0.21234153505017375
iteration : 3429
train acc:  0.8515625
train loss:  0.35096293687820435
train gradient:  0.3088917482529323
iteration : 3430
train acc:  0.8984375
train loss:  0.3005533814430237
train gradient:  0.18957781967706372
iteration : 3431
train acc:  0.84375
train loss:  0.29327404499053955
train gradient:  0.252316292079089
iteration : 3432
train acc:  0.875
train loss:  0.35640376806259155
train gradient:  0.3442648070870597
iteration : 3433
train acc:  0.7890625
train loss:  0.42113256454467773
train gradient:  0.3193422570671599
iteration : 3434
train acc:  0.8515625
train loss:  0.37131738662719727
train gradient:  0.28121696395696427
iteration : 3435
train acc:  0.8359375
train loss:  0.3592897951602936
train gradient:  0.21385676884730453
iteration : 3436
train acc:  0.8359375
train loss:  0.38407862186431885
train gradient:  0.2573894233907482
iteration : 3437
train acc:  0.8125
train loss:  0.33771318197250366
train gradient:  0.2676866093336274
iteration : 3438
train acc:  0.8125
train loss:  0.34490638971328735
train gradient:  0.26953778494799074
iteration : 3439
train acc:  0.84375
train loss:  0.3473314046859741
train gradient:  0.2254197448382347
iteration : 3440
train acc:  0.8828125
train loss:  0.2724824547767639
train gradient:  0.2090988275911238
iteration : 3441
train acc:  0.828125
train loss:  0.3541690707206726
train gradient:  0.2787346905608718
iteration : 3442
train acc:  0.8515625
train loss:  0.3325458765029907
train gradient:  0.21040436612623853
iteration : 3443
train acc:  0.8359375
train loss:  0.3689855933189392
train gradient:  0.32540757565861317
iteration : 3444
train acc:  0.796875
train loss:  0.4748433828353882
train gradient:  0.3413732572820694
iteration : 3445
train acc:  0.8671875
train loss:  0.32018381357192993
train gradient:  0.21684828477805815
iteration : 3446
train acc:  0.8828125
train loss:  0.3072506785392761
train gradient:  0.3229019897248513
iteration : 3447
train acc:  0.84375
train loss:  0.33920353651046753
train gradient:  0.2248840483582898
iteration : 3448
train acc:  0.8125
train loss:  0.3662205636501312
train gradient:  0.27373150186682005
iteration : 3449
train acc:  0.84375
train loss:  0.3548949360847473
train gradient:  0.3074338968179939
iteration : 3450
train acc:  0.84375
train loss:  0.3558500409126282
train gradient:  0.24926174927776895
iteration : 3451
train acc:  0.8046875
train loss:  0.41033631563186646
train gradient:  0.34441374541728476
iteration : 3452
train acc:  0.8046875
train loss:  0.40714311599731445
train gradient:  0.36992368917331137
iteration : 3453
train acc:  0.828125
train loss:  0.34934478998184204
train gradient:  0.24089007458083417
iteration : 3454
train acc:  0.8515625
train loss:  0.3489813804626465
train gradient:  0.24929178696956897
iteration : 3455
train acc:  0.8359375
train loss:  0.35802575945854187
train gradient:  0.2489176700613798
iteration : 3456
train acc:  0.859375
train loss:  0.33647391200065613
train gradient:  0.2085372580735783
iteration : 3457
train acc:  0.796875
train loss:  0.44881510734558105
train gradient:  0.3813924566351838
iteration : 3458
train acc:  0.90625
train loss:  0.28005415201187134
train gradient:  0.23310357912959495
iteration : 3459
train acc:  0.859375
train loss:  0.35763269662857056
train gradient:  0.231840155904035
iteration : 3460
train acc:  0.765625
train loss:  0.4946455657482147
train gradient:  0.4563563155726826
iteration : 3461
train acc:  0.859375
train loss:  0.3051918148994446
train gradient:  0.20954028274775893
iteration : 3462
train acc:  0.8203125
train loss:  0.41441166400909424
train gradient:  0.414359540943874
iteration : 3463
train acc:  0.8671875
train loss:  0.28412458300590515
train gradient:  0.3563054844505841
iteration : 3464
train acc:  0.796875
train loss:  0.4706273078918457
train gradient:  0.4999643283611045
iteration : 3465
train acc:  0.859375
train loss:  0.39124298095703125
train gradient:  0.29714537402498475
iteration : 3466
train acc:  0.8515625
train loss:  0.35207533836364746
train gradient:  0.36951946950865383
iteration : 3467
train acc:  0.8359375
train loss:  0.35965120792388916
train gradient:  0.2726014617959264
iteration : 3468
train acc:  0.8828125
train loss:  0.27505722641944885
train gradient:  0.21001323157292265
iteration : 3469
train acc:  0.8828125
train loss:  0.3091203570365906
train gradient:  0.21826626749737063
iteration : 3470
train acc:  0.7890625
train loss:  0.38760852813720703
train gradient:  0.31680113937434984
iteration : 3471
train acc:  0.8515625
train loss:  0.3883405029773712
train gradient:  0.2676381077477499
iteration : 3472
train acc:  0.8359375
train loss:  0.47954684495925903
train gradient:  0.4065641416115655
iteration : 3473
train acc:  0.8359375
train loss:  0.4524843096733093
train gradient:  0.3138860227236748
iteration : 3474
train acc:  0.828125
train loss:  0.4018396735191345
train gradient:  0.25265859494199244
iteration : 3475
train acc:  0.8125
train loss:  0.4923444092273712
train gradient:  0.5859199484159741
iteration : 3476
train acc:  0.8125
train loss:  0.39262670278549194
train gradient:  0.2282350230559979
iteration : 3477
train acc:  0.796875
train loss:  0.4809298515319824
train gradient:  0.46593709395450483
iteration : 3478
train acc:  0.8515625
train loss:  0.34374451637268066
train gradient:  0.28559459084429534
iteration : 3479
train acc:  0.890625
train loss:  0.3060914874076843
train gradient:  0.25477513768356835
iteration : 3480
train acc:  0.8515625
train loss:  0.3722451627254486
train gradient:  0.2881691026545339
iteration : 3481
train acc:  0.8203125
train loss:  0.42361781001091003
train gradient:  0.34000295116075036
iteration : 3482
train acc:  0.8046875
train loss:  0.4360840916633606
train gradient:  0.44254422707058305
iteration : 3483
train acc:  0.859375
train loss:  0.29707953333854675
train gradient:  0.22074816135437375
iteration : 3484
train acc:  0.8046875
train loss:  0.40172767639160156
train gradient:  0.31276828437338994
iteration : 3485
train acc:  0.7734375
train loss:  0.48212113976478577
train gradient:  0.354626367022377
iteration : 3486
train acc:  0.8359375
train loss:  0.312181293964386
train gradient:  0.19050008047937828
iteration : 3487
train acc:  0.8359375
train loss:  0.3177669644355774
train gradient:  0.255727731194416
iteration : 3488
train acc:  0.8515625
train loss:  0.3929532766342163
train gradient:  0.28231483287007786
iteration : 3489
train acc:  0.859375
train loss:  0.35102880001068115
train gradient:  0.19248406027253812
iteration : 3490
train acc:  0.84375
train loss:  0.4125995934009552
train gradient:  0.33768974334347074
iteration : 3491
train acc:  0.84375
train loss:  0.3774304986000061
train gradient:  0.3307870466180217
iteration : 3492
train acc:  0.796875
train loss:  0.38703107833862305
train gradient:  0.19854522565100235
iteration : 3493
train acc:  0.8515625
train loss:  0.3571573793888092
train gradient:  0.36762519778815195
iteration : 3494
train acc:  0.84375
train loss:  0.36185917258262634
train gradient:  0.26961132519537395
iteration : 3495
train acc:  0.828125
train loss:  0.3387146294116974
train gradient:  0.2107625318145201
iteration : 3496
train acc:  0.84375
train loss:  0.3906802535057068
train gradient:  0.3140014375656009
iteration : 3497
train acc:  0.828125
train loss:  0.34863120317459106
train gradient:  0.3064635814204989
iteration : 3498
train acc:  0.859375
train loss:  0.3232954144477844
train gradient:  0.27515493056931456
iteration : 3499
train acc:  0.7578125
train loss:  0.49666136503219604
train gradient:  0.4677401976419811
iteration : 3500
train acc:  0.8515625
train loss:  0.3444903492927551
train gradient:  0.22858107570173408
iteration : 3501
train acc:  0.8515625
train loss:  0.3793275058269501
train gradient:  0.25326403665231795
iteration : 3502
train acc:  0.828125
train loss:  0.3579890727996826
train gradient:  0.30417966103439537
iteration : 3503
train acc:  0.8515625
train loss:  0.3475331962108612
train gradient:  0.4362932573463544
iteration : 3504
train acc:  0.8984375
train loss:  0.2909923195838928
train gradient:  0.1973389632802805
iteration : 3505
train acc:  0.7890625
train loss:  0.4430215656757355
train gradient:  0.4209505161129326
iteration : 3506
train acc:  0.796875
train loss:  0.5117350220680237
train gradient:  0.41983132875995016
iteration : 3507
train acc:  0.8125
train loss:  0.40140047669410706
train gradient:  0.3459467516677013
iteration : 3508
train acc:  0.875
train loss:  0.3178156316280365
train gradient:  0.20598413321920145
iteration : 3509
train acc:  0.8046875
train loss:  0.40199923515319824
train gradient:  0.33244691299664764
iteration : 3510
train acc:  0.8359375
train loss:  0.35557734966278076
train gradient:  0.22325249113779727
iteration : 3511
train acc:  0.828125
train loss:  0.3686191737651825
train gradient:  0.2348357386499776
iteration : 3512
train acc:  0.8203125
train loss:  0.39961451292037964
train gradient:  0.46907354727554673
iteration : 3513
train acc:  0.8046875
train loss:  0.3880424201488495
train gradient:  0.31335588188998775
iteration : 3514
train acc:  0.8203125
train loss:  0.41121983528137207
train gradient:  0.3286155250631417
iteration : 3515
train acc:  0.8671875
train loss:  0.3713357150554657
train gradient:  0.3779045493507555
iteration : 3516
train acc:  0.875
train loss:  0.30861926078796387
train gradient:  0.18092771301110494
iteration : 3517
train acc:  0.828125
train loss:  0.3813980519771576
train gradient:  0.2971654095249167
iteration : 3518
train acc:  0.859375
train loss:  0.3536340594291687
train gradient:  0.3039746178327588
iteration : 3519
train acc:  0.84375
train loss:  0.3707082271575928
train gradient:  0.29088737676666815
iteration : 3520
train acc:  0.828125
train loss:  0.3734230399131775
train gradient:  0.35121935127797693
iteration : 3521
train acc:  0.859375
train loss:  0.32892829179763794
train gradient:  0.2905128436548621
iteration : 3522
train acc:  0.796875
train loss:  0.4370781183242798
train gradient:  0.5054173214987506
iteration : 3523
train acc:  0.84375
train loss:  0.35066819190979004
train gradient:  0.34077244397105316
iteration : 3524
train acc:  0.8359375
train loss:  0.4085387885570526
train gradient:  0.41024812653672776
iteration : 3525
train acc:  0.84375
train loss:  0.3456684947013855
train gradient:  0.29383957296715346
iteration : 3526
train acc:  0.84375
train loss:  0.37838202714920044
train gradient:  0.28603171931101773
iteration : 3527
train acc:  0.84375
train loss:  0.3795824646949768
train gradient:  0.40784641098482555
iteration : 3528
train acc:  0.84375
train loss:  0.36417514085769653
train gradient:  0.188970361587401
iteration : 3529
train acc:  0.8359375
train loss:  0.31780126690864563
train gradient:  0.165509258480755
iteration : 3530
train acc:  0.828125
train loss:  0.4376389682292938
train gradient:  0.3534185806595703
iteration : 3531
train acc:  0.875
train loss:  0.29735711216926575
train gradient:  0.2292902004333763
iteration : 3532
train acc:  0.859375
train loss:  0.32890790700912476
train gradient:  0.23267913899347642
iteration : 3533
train acc:  0.859375
train loss:  0.3581887483596802
train gradient:  0.25885833180162254
iteration : 3534
train acc:  0.8125
train loss:  0.36055728793144226
train gradient:  0.3070824839708087
iteration : 3535
train acc:  0.859375
train loss:  0.32039350271224976
train gradient:  0.24417575630146673
iteration : 3536
train acc:  0.75
train loss:  0.506470799446106
train gradient:  0.4842865232168319
iteration : 3537
train acc:  0.8046875
train loss:  0.3951704800128937
train gradient:  0.3417461950459912
iteration : 3538
train acc:  0.875
train loss:  0.2853286564350128
train gradient:  0.24489350775084515
iteration : 3539
train acc:  0.8671875
train loss:  0.284727543592453
train gradient:  0.18757271594780905
iteration : 3540
train acc:  0.8828125
train loss:  0.32967013120651245
train gradient:  0.21148805086169917
iteration : 3541
train acc:  0.8203125
train loss:  0.41876131296157837
train gradient:  0.27527877243930526
iteration : 3542
train acc:  0.8515625
train loss:  0.332153856754303
train gradient:  0.42120063885823034
iteration : 3543
train acc:  0.796875
train loss:  0.4021565318107605
train gradient:  0.3051653708052804
iteration : 3544
train acc:  0.828125
train loss:  0.39585942029953003
train gradient:  0.31157900052607823
iteration : 3545
train acc:  0.796875
train loss:  0.48629188537597656
train gradient:  0.411810740976877
iteration : 3546
train acc:  0.8515625
train loss:  0.35481053590774536
train gradient:  0.32725392194550645
iteration : 3547
train acc:  0.8515625
train loss:  0.3058166205883026
train gradient:  0.18893475692957173
iteration : 3548
train acc:  0.8203125
train loss:  0.4754113256931305
train gradient:  0.49435384851987935
iteration : 3549
train acc:  0.8984375
train loss:  0.3165278732776642
train gradient:  0.23950806678118958
iteration : 3550
train acc:  0.828125
train loss:  0.39417019486427307
train gradient:  0.35518728853391635
iteration : 3551
train acc:  0.9140625
train loss:  0.2745298743247986
train gradient:  0.16806032146148464
iteration : 3552
train acc:  0.8203125
train loss:  0.38958507776260376
train gradient:  0.26115605705188294
iteration : 3553
train acc:  0.8046875
train loss:  0.4100579619407654
train gradient:  0.4444245077017482
iteration : 3554
train acc:  0.9296875
train loss:  0.24385195970535278
train gradient:  0.17008679509056687
iteration : 3555
train acc:  0.859375
train loss:  0.3104323148727417
train gradient:  0.2330558979775536
iteration : 3556
train acc:  0.8359375
train loss:  0.35784709453582764
train gradient:  0.32852356764410573
iteration : 3557
train acc:  0.8046875
train loss:  0.4815996587276459
train gradient:  0.5329132596199606
iteration : 3558
train acc:  0.8671875
train loss:  0.29983294010162354
train gradient:  0.26161950926069405
iteration : 3559
train acc:  0.8125
train loss:  0.3631824553012848
train gradient:  0.2740219695471088
iteration : 3560
train acc:  0.890625
train loss:  0.305999219417572
train gradient:  0.2084241724490845
iteration : 3561
train acc:  0.84375
train loss:  0.41663506627082825
train gradient:  0.3363127717909589
iteration : 3562
train acc:  0.828125
train loss:  0.44387876987457275
train gradient:  0.3920174178506229
iteration : 3563
train acc:  0.890625
train loss:  0.28806525468826294
train gradient:  0.28049335753925847
iteration : 3564
train acc:  0.890625
train loss:  0.33463141322135925
train gradient:  0.21453618447371472
iteration : 3565
train acc:  0.8671875
train loss:  0.29243069887161255
train gradient:  0.22709874596096052
iteration : 3566
train acc:  0.90625
train loss:  0.281302273273468
train gradient:  0.1927708142540927
iteration : 3567
train acc:  0.8125
train loss:  0.3805009126663208
train gradient:  0.3647709438085572
iteration : 3568
train acc:  0.7734375
train loss:  0.4416942000389099
train gradient:  0.3963103960776314
iteration : 3569
train acc:  0.8671875
train loss:  0.3934539258480072
train gradient:  0.2798686482442137
iteration : 3570
train acc:  0.8203125
train loss:  0.3524288833141327
train gradient:  0.34614284763914144
iteration : 3571
train acc:  0.8515625
train loss:  0.4011782705783844
train gradient:  0.35068511848916306
iteration : 3572
train acc:  0.828125
train loss:  0.38573014736175537
train gradient:  0.5154396043781733
iteration : 3573
train acc:  0.8125
train loss:  0.4090847074985504
train gradient:  0.4258220545268122
iteration : 3574
train acc:  0.796875
train loss:  0.4397561848163605
train gradient:  0.43154490103902743
iteration : 3575
train acc:  0.8046875
train loss:  0.3916057050228119
train gradient:  0.3409705177520302
iteration : 3576
train acc:  0.8515625
train loss:  0.36746686697006226
train gradient:  0.30615612374675655
iteration : 3577
train acc:  0.8984375
train loss:  0.3087300658226013
train gradient:  0.2490503251094982
iteration : 3578
train acc:  0.828125
train loss:  0.37010258436203003
train gradient:  0.31703315631251855
iteration : 3579
train acc:  0.8203125
train loss:  0.34054791927337646
train gradient:  0.3674874618175636
iteration : 3580
train acc:  0.8046875
train loss:  0.38586875796318054
train gradient:  0.3301900731960482
iteration : 3581
train acc:  0.875
train loss:  0.3055756688117981
train gradient:  0.22763983404877872
iteration : 3582
train acc:  0.875
train loss:  0.3168943524360657
train gradient:  0.2873300155562906
iteration : 3583
train acc:  0.8359375
train loss:  0.3649287223815918
train gradient:  0.23649105537705561
iteration : 3584
train acc:  0.875
train loss:  0.2830531597137451
train gradient:  0.185544928267824
iteration : 3585
train acc:  0.84375
train loss:  0.42008525133132935
train gradient:  0.40540021621068373
iteration : 3586
train acc:  0.8515625
train loss:  0.37503209710121155
train gradient:  0.41858965138266596
iteration : 3587
train acc:  0.875
train loss:  0.3403228521347046
train gradient:  0.3033569563614279
iteration : 3588
train acc:  0.859375
train loss:  0.3417511284351349
train gradient:  0.2296411645261906
iteration : 3589
train acc:  0.8359375
train loss:  0.35105541348457336
train gradient:  0.29516733653094057
iteration : 3590
train acc:  0.8203125
train loss:  0.4116387963294983
train gradient:  0.30064567400813913
iteration : 3591
train acc:  0.8125
train loss:  0.36310747265815735
train gradient:  0.2704107545923367
iteration : 3592
train acc:  0.8203125
train loss:  0.38424432277679443
train gradient:  0.3092380952840319
iteration : 3593
train acc:  0.8515625
train loss:  0.3629074692726135
train gradient:  0.37297529713915206
iteration : 3594
train acc:  0.875
train loss:  0.38203856348991394
train gradient:  0.323613660774139
iteration : 3595
train acc:  0.8203125
train loss:  0.38450995087623596
train gradient:  0.322113425268071
iteration : 3596
train acc:  0.7890625
train loss:  0.4650123119354248
train gradient:  0.6549028985240626
iteration : 3597
train acc:  0.796875
train loss:  0.43650758266448975
train gradient:  0.3334950585653905
iteration : 3598
train acc:  0.8359375
train loss:  0.38359224796295166
train gradient:  0.4057551247665402
iteration : 3599
train acc:  0.84375
train loss:  0.3337820768356323
train gradient:  0.19053363739991167
iteration : 3600
train acc:  0.859375
train loss:  0.35631781816482544
train gradient:  0.2980804901558899
iteration : 3601
train acc:  0.84375
train loss:  0.33173948526382446
train gradient:  0.28944126928251723
iteration : 3602
train acc:  0.84375
train loss:  0.42701107263565063
train gradient:  0.4330969824851538
iteration : 3603
train acc:  0.8359375
train loss:  0.390328586101532
train gradient:  0.2853548561529619
iteration : 3604
train acc:  0.8515625
train loss:  0.3489142060279846
train gradient:  0.27633244668566403
iteration : 3605
train acc:  0.8203125
train loss:  0.3965710401535034
train gradient:  0.3383491791963787
iteration : 3606
train acc:  0.8671875
train loss:  0.34362131357192993
train gradient:  0.328257338722994
iteration : 3607
train acc:  0.7890625
train loss:  0.4942272901535034
train gradient:  0.4568671220615384
iteration : 3608
train acc:  0.8125
train loss:  0.40318793058395386
train gradient:  0.3701432436463526
iteration : 3609
train acc:  0.84375
train loss:  0.3676915466785431
train gradient:  0.27297150357195066
iteration : 3610
train acc:  0.8515625
train loss:  0.41950321197509766
train gradient:  0.29621463466345893
iteration : 3611
train acc:  0.8046875
train loss:  0.39034807682037354
train gradient:  0.33061612754209607
iteration : 3612
train acc:  0.8828125
train loss:  0.2894662320613861
train gradient:  0.2142575841775755
iteration : 3613
train acc:  0.8125
train loss:  0.38906943798065186
train gradient:  0.3196973737154935
iteration : 3614
train acc:  0.8671875
train loss:  0.32420575618743896
train gradient:  0.36868781747004903
iteration : 3615
train acc:  0.8046875
train loss:  0.38181498646736145
train gradient:  0.2694011411712834
iteration : 3616
train acc:  0.8671875
train loss:  0.32447540760040283
train gradient:  0.22301124833095912
iteration : 3617
train acc:  0.890625
train loss:  0.2864859998226166
train gradient:  0.1814478160767251
iteration : 3618
train acc:  0.8828125
train loss:  0.3368871510028839
train gradient:  0.2337740178269642
iteration : 3619
train acc:  0.8359375
train loss:  0.3944183886051178
train gradient:  0.24106652144623436
iteration : 3620
train acc:  0.8515625
train loss:  0.3326823115348816
train gradient:  0.24241016722008374
iteration : 3621
train acc:  0.7890625
train loss:  0.39638829231262207
train gradient:  0.2378565435114237
iteration : 3622
train acc:  0.84375
train loss:  0.3579418659210205
train gradient:  0.2840377137689399
iteration : 3623
train acc:  0.8203125
train loss:  0.3853750228881836
train gradient:  0.3088756949105107
iteration : 3624
train acc:  0.859375
train loss:  0.36833620071411133
train gradient:  0.2876534422266331
iteration : 3625
train acc:  0.8515625
train loss:  0.3262861669063568
train gradient:  0.3113361254713982
iteration : 3626
train acc:  0.859375
train loss:  0.3301364481449127
train gradient:  0.23313431255794886
iteration : 3627
train acc:  0.875
train loss:  0.3562675714492798
train gradient:  0.3373261217657486
iteration : 3628
train acc:  0.8359375
train loss:  0.3518369197845459
train gradient:  0.2992578547325041
iteration : 3629
train acc:  0.859375
train loss:  0.3497815728187561
train gradient:  0.22788230393414488
iteration : 3630
train acc:  0.8515625
train loss:  0.3541937470436096
train gradient:  0.2440032743107739
iteration : 3631
train acc:  0.828125
train loss:  0.44637981057167053
train gradient:  0.45150752921610365
iteration : 3632
train acc:  0.8671875
train loss:  0.32370448112487793
train gradient:  0.3434596494034676
iteration : 3633
train acc:  0.8515625
train loss:  0.3257575035095215
train gradient:  0.23518740003450067
iteration : 3634
train acc:  0.8515625
train loss:  0.38002878427505493
train gradient:  0.2364829053190614
iteration : 3635
train acc:  0.8515625
train loss:  0.382293701171875
train gradient:  0.34928852356798845
iteration : 3636
train acc:  0.78125
train loss:  0.48878368735313416
train gradient:  0.44184878097800806
iteration : 3637
train acc:  0.875
train loss:  0.3050166964530945
train gradient:  0.3400627889629369
iteration : 3638
train acc:  0.84375
train loss:  0.3138856887817383
train gradient:  0.18946523863469344
iteration : 3639
train acc:  0.7734375
train loss:  0.4704549312591553
train gradient:  0.39937263539629025
iteration : 3640
train acc:  0.859375
train loss:  0.3820268511772156
train gradient:  0.26619189844661995
iteration : 3641
train acc:  0.875
train loss:  0.2982965409755707
train gradient:  0.17422595276696579
iteration : 3642
train acc:  0.8671875
train loss:  0.300836980342865
train gradient:  0.2141433839872334
iteration : 3643
train acc:  0.890625
train loss:  0.3311157822608948
train gradient:  0.28083032582808587
iteration : 3644
train acc:  0.875
train loss:  0.3053378164768219
train gradient:  0.1982429748087911
iteration : 3645
train acc:  0.8671875
train loss:  0.32219353318214417
train gradient:  0.2218976803664668
iteration : 3646
train acc:  0.78125
train loss:  0.48806971311569214
train gradient:  0.43916763149207505
iteration : 3647
train acc:  0.8203125
train loss:  0.4078163206577301
train gradient:  0.3454298214814124
iteration : 3648
train acc:  0.859375
train loss:  0.3376160264015198
train gradient:  0.2655074113060818
iteration : 3649
train acc:  0.8828125
train loss:  0.35216736793518066
train gradient:  0.2978456930778848
iteration : 3650
train acc:  0.7890625
train loss:  0.4106810688972473
train gradient:  0.5255320668964265
iteration : 3651
train acc:  0.8203125
train loss:  0.3927263021469116
train gradient:  0.269803382508019
iteration : 3652
train acc:  0.8203125
train loss:  0.3561186194419861
train gradient:  0.28437845424084235
iteration : 3653
train acc:  0.8515625
train loss:  0.3103640675544739
train gradient:  0.23464477877542017
iteration : 3654
train acc:  0.78125
train loss:  0.42240795493125916
train gradient:  0.4154210239562519
iteration : 3655
train acc:  0.8359375
train loss:  0.3511374294757843
train gradient:  0.34868660335530854
iteration : 3656
train acc:  0.90625
train loss:  0.26907676458358765
train gradient:  0.15197335679077367
iteration : 3657
train acc:  0.8515625
train loss:  0.36645248532295227
train gradient:  0.33612662228749146
iteration : 3658
train acc:  0.8515625
train loss:  0.39592689275741577
train gradient:  0.32317502570183154
iteration : 3659
train acc:  0.8515625
train loss:  0.32962363958358765
train gradient:  0.24681251485735112
iteration : 3660
train acc:  0.8125
train loss:  0.39977309107780457
train gradient:  0.3041967595968469
iteration : 3661
train acc:  0.796875
train loss:  0.4009072184562683
train gradient:  0.2952879882223116
iteration : 3662
train acc:  0.8359375
train loss:  0.4103662371635437
train gradient:  0.344688572248787
iteration : 3663
train acc:  0.8203125
train loss:  0.3919910788536072
train gradient:  0.29640263633225794
iteration : 3664
train acc:  0.8515625
train loss:  0.36358606815338135
train gradient:  0.2551881982596015
iteration : 3665
train acc:  0.796875
train loss:  0.5034518241882324
train gradient:  0.3687943052645856
iteration : 3666
train acc:  0.8125
train loss:  0.3998008370399475
train gradient:  0.4140967016357745
iteration : 3667
train acc:  0.8203125
train loss:  0.40393632650375366
train gradient:  0.3014333487288129
iteration : 3668
train acc:  0.8359375
train loss:  0.3962630033493042
train gradient:  0.3216951123608848
iteration : 3669
train acc:  0.8125
train loss:  0.3749776780605316
train gradient:  0.25754521049346374
iteration : 3670
train acc:  0.890625
train loss:  0.33030128479003906
train gradient:  0.2761072137822654
iteration : 3671
train acc:  0.8828125
train loss:  0.38204455375671387
train gradient:  0.2848680318564983
iteration : 3672
train acc:  0.796875
train loss:  0.3940308094024658
train gradient:  0.3308904073984663
iteration : 3673
train acc:  0.859375
train loss:  0.33480778336524963
train gradient:  0.22188259289487883
iteration : 3674
train acc:  0.859375
train loss:  0.3149919807910919
train gradient:  0.2364901257018548
iteration : 3675
train acc:  0.8203125
train loss:  0.3697893023490906
train gradient:  0.26865987791588547
iteration : 3676
train acc:  0.7890625
train loss:  0.3905922472476959
train gradient:  0.32607711801565925
iteration : 3677
train acc:  0.8359375
train loss:  0.4091198444366455
train gradient:  0.4922358691537784
iteration : 3678
train acc:  0.859375
train loss:  0.33411017060279846
train gradient:  0.224143782266299
iteration : 3679
train acc:  0.8125
train loss:  0.41310396790504456
train gradient:  0.26161737803600293
iteration : 3680
train acc:  0.8359375
train loss:  0.39224570989608765
train gradient:  0.25201779745415775
iteration : 3681
train acc:  0.7890625
train loss:  0.4833862781524658
train gradient:  0.40353022894524165
iteration : 3682
train acc:  0.859375
train loss:  0.3357778787612915
train gradient:  0.20906562265732695
iteration : 3683
train acc:  0.8671875
train loss:  0.32655075192451477
train gradient:  0.21724054679129415
iteration : 3684
train acc:  0.828125
train loss:  0.3754843473434448
train gradient:  0.28371609910049095
iteration : 3685
train acc:  0.828125
train loss:  0.3114786148071289
train gradient:  0.27329154056252225
iteration : 3686
train acc:  0.890625
train loss:  0.2670568823814392
train gradient:  0.16104791042686012
iteration : 3687
train acc:  0.8515625
train loss:  0.36342519521713257
train gradient:  0.1892028215757791
iteration : 3688
train acc:  0.8359375
train loss:  0.30133652687072754
train gradient:  0.18565231898034845
iteration : 3689
train acc:  0.8203125
train loss:  0.4005213975906372
train gradient:  0.3210705274576622
iteration : 3690
train acc:  0.859375
train loss:  0.29456669092178345
train gradient:  0.22909601670043472
iteration : 3691
train acc:  0.8359375
train loss:  0.4149445593357086
train gradient:  0.31007452329516405
iteration : 3692
train acc:  0.84375
train loss:  0.3642728924751282
train gradient:  0.2000827084862289
iteration : 3693
train acc:  0.828125
train loss:  0.37382856011390686
train gradient:  0.23492957840062634
iteration : 3694
train acc:  0.84375
train loss:  0.43229037523269653
train gradient:  0.25886377665523047
iteration : 3695
train acc:  0.8359375
train loss:  0.34234052896499634
train gradient:  0.18873407120053867
iteration : 3696
train acc:  0.84375
train loss:  0.30301448702812195
train gradient:  0.16509162109714498
iteration : 3697
train acc:  0.8359375
train loss:  0.3565734028816223
train gradient:  0.22729377306944853
iteration : 3698
train acc:  0.859375
train loss:  0.3353402018547058
train gradient:  0.1991749265013839
iteration : 3699
train acc:  0.8125
train loss:  0.4159294366836548
train gradient:  0.47522706051452845
iteration : 3700
train acc:  0.8359375
train loss:  0.3544469475746155
train gradient:  0.27980353526925544
iteration : 3701
train acc:  0.90625
train loss:  0.27354297041893005
train gradient:  0.27446774756554
iteration : 3702
train acc:  0.796875
train loss:  0.45844054222106934
train gradient:  0.39951925184650794
iteration : 3703
train acc:  0.921875
train loss:  0.28597337007522583
train gradient:  0.1627845108815178
iteration : 3704
train acc:  0.875
train loss:  0.3446599543094635
train gradient:  0.2207630940835432
iteration : 3705
train acc:  0.8359375
train loss:  0.3672276735305786
train gradient:  0.23043211640664021
iteration : 3706
train acc:  0.890625
train loss:  0.31948432326316833
train gradient:  0.1933291560905944
iteration : 3707
train acc:  0.71875
train loss:  0.4909537434577942
train gradient:  0.5442650340868387
iteration : 3708
train acc:  0.8828125
train loss:  0.3234826326370239
train gradient:  0.1957434737160612
iteration : 3709
train acc:  0.7421875
train loss:  0.5119485259056091
train gradient:  0.49543013145792153
iteration : 3710
train acc:  0.859375
train loss:  0.37612396478652954
train gradient:  0.2708636812790017
iteration : 3711
train acc:  0.8359375
train loss:  0.3629191517829895
train gradient:  0.23577796093968809
iteration : 3712
train acc:  0.8515625
train loss:  0.3330603837966919
train gradient:  0.2440524176443052
iteration : 3713
train acc:  0.796875
train loss:  0.41099315881729126
train gradient:  0.4536756726393159
iteration : 3714
train acc:  0.859375
train loss:  0.34535694122314453
train gradient:  0.29082782420934333
iteration : 3715
train acc:  0.8046875
train loss:  0.42128485441207886
train gradient:  0.36016876198188563
iteration : 3716
train acc:  0.8515625
train loss:  0.35308247804641724
train gradient:  0.34930033286597456
iteration : 3717
train acc:  0.828125
train loss:  0.3594783544540405
train gradient:  0.2851027341912837
iteration : 3718
train acc:  0.828125
train loss:  0.3662611246109009
train gradient:  0.272267246162187
iteration : 3719
train acc:  0.828125
train loss:  0.4256185293197632
train gradient:  0.29391468974271373
iteration : 3720
train acc:  0.7109375
train loss:  0.5365546345710754
train gradient:  0.5122068483437211
iteration : 3721
train acc:  0.8515625
train loss:  0.3325006365776062
train gradient:  0.26568451191559705
iteration : 3722
train acc:  0.828125
train loss:  0.38642585277557373
train gradient:  0.33763683203354816
iteration : 3723
train acc:  0.890625
train loss:  0.2751923203468323
train gradient:  0.20473408609968907
iteration : 3724
train acc:  0.8828125
train loss:  0.32731789350509644
train gradient:  0.4875342358820591
iteration : 3725
train acc:  0.84375
train loss:  0.3576229214668274
train gradient:  0.23956223393363496
iteration : 3726
train acc:  0.7890625
train loss:  0.3772633671760559
train gradient:  0.3311951317237634
iteration : 3727
train acc:  0.828125
train loss:  0.36231663823127747
train gradient:  0.21242474406345788
iteration : 3728
train acc:  0.8828125
train loss:  0.29441073536872864
train gradient:  0.2260655777153482
iteration : 3729
train acc:  0.8046875
train loss:  0.4098263084888458
train gradient:  0.3168953862403273
iteration : 3730
train acc:  0.875
train loss:  0.319113165140152
train gradient:  0.22581127198399617
iteration : 3731
train acc:  0.8203125
train loss:  0.35990267992019653
train gradient:  0.21646996631153007
iteration : 3732
train acc:  0.8046875
train loss:  0.48303937911987305
train gradient:  0.3693987169045926
iteration : 3733
train acc:  0.90625
train loss:  0.2875238358974457
train gradient:  0.22413517129216154
iteration : 3734
train acc:  0.8046875
train loss:  0.39471155405044556
train gradient:  0.21267794445702068
iteration : 3735
train acc:  0.8046875
train loss:  0.4409315288066864
train gradient:  0.45074834381326856
iteration : 3736
train acc:  0.8046875
train loss:  0.40244820713996887
train gradient:  0.44929481789182163
iteration : 3737
train acc:  0.84375
train loss:  0.34776413440704346
train gradient:  0.24600391120186194
iteration : 3738
train acc:  0.84375
train loss:  0.37309497594833374
train gradient:  0.2614749654517675
iteration : 3739
train acc:  0.8125
train loss:  0.363267183303833
train gradient:  0.31727808856538536
iteration : 3740
train acc:  0.875
train loss:  0.36495935916900635
train gradient:  0.3661504622836153
iteration : 3741
train acc:  0.8203125
train loss:  0.384091317653656
train gradient:  0.31958436726030215
iteration : 3742
train acc:  0.875
train loss:  0.33962875604629517
train gradient:  0.2883084476896098
iteration : 3743
train acc:  0.859375
train loss:  0.3989838659763336
train gradient:  0.22691215932774084
iteration : 3744
train acc:  0.890625
train loss:  0.37410593032836914
train gradient:  0.19393734470984925
iteration : 3745
train acc:  0.8359375
train loss:  0.35900869965553284
train gradient:  0.19988895175252486
iteration : 3746
train acc:  0.8125
train loss:  0.36243143677711487
train gradient:  0.32927290786565333
iteration : 3747
train acc:  0.8046875
train loss:  0.41227784752845764
train gradient:  0.3710417873648361
iteration : 3748
train acc:  0.8828125
train loss:  0.31474795937538147
train gradient:  0.16608195030013284
iteration : 3749
train acc:  0.8125
train loss:  0.3567296862602234
train gradient:  0.3134015941544471
iteration : 3750
train acc:  0.8359375
train loss:  0.37085503339767456
train gradient:  0.4123886765718795
iteration : 3751
train acc:  0.859375
train loss:  0.3497096300125122
train gradient:  0.2537404120931816
iteration : 3752
train acc:  0.8671875
train loss:  0.29896241426467896
train gradient:  0.16018627821970116
iteration : 3753
train acc:  0.8671875
train loss:  0.3280477821826935
train gradient:  0.23242507501205303
iteration : 3754
train acc:  0.8515625
train loss:  0.34667477011680603
train gradient:  0.19853918554963076
iteration : 3755
train acc:  0.8671875
train loss:  0.32452261447906494
train gradient:  0.215182764629741
iteration : 3756
train acc:  0.8828125
train loss:  0.28712955117225647
train gradient:  0.22586570722525912
iteration : 3757
train acc:  0.8046875
train loss:  0.38549259305000305
train gradient:  0.29351267642495493
iteration : 3758
train acc:  0.8125
train loss:  0.40442848205566406
train gradient:  0.33737324658961865
iteration : 3759
train acc:  0.8984375
train loss:  0.2901014983654022
train gradient:  0.20518083108616228
iteration : 3760
train acc:  0.828125
train loss:  0.35736745595932007
train gradient:  0.21599457437731628
iteration : 3761
train acc:  0.8515625
train loss:  0.3205825686454773
train gradient:  0.2243747995363672
iteration : 3762
train acc:  0.8125
train loss:  0.4330572485923767
train gradient:  0.3884338997208611
iteration : 3763
train acc:  0.8671875
train loss:  0.31821146607398987
train gradient:  0.25080700197004585
iteration : 3764
train acc:  0.7890625
train loss:  0.44557470083236694
train gradient:  0.5089190162685688
iteration : 3765
train acc:  0.828125
train loss:  0.343539297580719
train gradient:  0.27647373638278017
iteration : 3766
train acc:  0.84375
train loss:  0.3831098973751068
train gradient:  0.40839161713309535
iteration : 3767
train acc:  0.8125
train loss:  0.430073082447052
train gradient:  0.29283755567662995
iteration : 3768
train acc:  0.8828125
train loss:  0.29442548751831055
train gradient:  0.17828194186358454
iteration : 3769
train acc:  0.828125
train loss:  0.3868580460548401
train gradient:  0.2388909556711999
iteration : 3770
train acc:  0.84375
train loss:  0.34004873037338257
train gradient:  0.41082082019169125
iteration : 3771
train acc:  0.8515625
train loss:  0.4108524024486542
train gradient:  0.33051642847545826
iteration : 3772
train acc:  0.8203125
train loss:  0.391454815864563
train gradient:  0.34934729086091215
iteration : 3773
train acc:  0.8515625
train loss:  0.34574636816978455
train gradient:  0.2784058126161393
iteration : 3774
train acc:  0.8515625
train loss:  0.3631490468978882
train gradient:  0.3951985686297595
iteration : 3775
train acc:  0.828125
train loss:  0.4056105315685272
train gradient:  0.27280501979117955
iteration : 3776
train acc:  0.84375
train loss:  0.30948948860168457
train gradient:  0.2794122265984129
iteration : 3777
train acc:  0.8515625
train loss:  0.3279581665992737
train gradient:  0.2251073928607818
iteration : 3778
train acc:  0.8046875
train loss:  0.3631868362426758
train gradient:  0.2934858798956378
iteration : 3779
train acc:  0.859375
train loss:  0.3019601106643677
train gradient:  0.2959138623901119
iteration : 3780
train acc:  0.765625
train loss:  0.44337278604507446
train gradient:  0.3827504864623677
iteration : 3781
train acc:  0.796875
train loss:  0.4656413793563843
train gradient:  0.6123248635613288
iteration : 3782
train acc:  0.859375
train loss:  0.3094669580459595
train gradient:  0.29554189633393707
iteration : 3783
train acc:  0.8359375
train loss:  0.30976539850234985
train gradient:  0.2953141423805494
iteration : 3784
train acc:  0.859375
train loss:  0.3590424060821533
train gradient:  0.18308312335154003
iteration : 3785
train acc:  0.8671875
train loss:  0.3462751507759094
train gradient:  0.2123168834434311
iteration : 3786
train acc:  0.8515625
train loss:  0.3189910054206848
train gradient:  0.21831113446489006
iteration : 3787
train acc:  0.8671875
train loss:  0.3124021291732788
train gradient:  0.21792952440354418
iteration : 3788
train acc:  0.828125
train loss:  0.370377779006958
train gradient:  0.27715202234322184
iteration : 3789
train acc:  0.8125
train loss:  0.5084019303321838
train gradient:  0.38632106843164365
iteration : 3790
train acc:  0.828125
train loss:  0.38681554794311523
train gradient:  0.3200641380156917
iteration : 3791
train acc:  0.8671875
train loss:  0.3332297205924988
train gradient:  0.30438317054740643
iteration : 3792
train acc:  0.8046875
train loss:  0.38684338331222534
train gradient:  0.2936724154712479
iteration : 3793
train acc:  0.8046875
train loss:  0.38013187050819397
train gradient:  0.3122078895687556
iteration : 3794
train acc:  0.859375
train loss:  0.35800063610076904
train gradient:  0.29700798476102114
iteration : 3795
train acc:  0.8359375
train loss:  0.37275123596191406
train gradient:  0.3481788654461517
iteration : 3796
train acc:  0.8125
train loss:  0.41074275970458984
train gradient:  0.3145928719074717
iteration : 3797
train acc:  0.8046875
train loss:  0.3846062421798706
train gradient:  0.3169783710515804
iteration : 3798
train acc:  0.828125
train loss:  0.32741469144821167
train gradient:  0.25425982963421906
iteration : 3799
train acc:  0.828125
train loss:  0.37608131766319275
train gradient:  0.44019065253693485
iteration : 3800
train acc:  0.796875
train loss:  0.3929949402809143
train gradient:  0.35629984653647684
iteration : 3801
train acc:  0.859375
train loss:  0.35893961787223816
train gradient:  0.22925288104472213
iteration : 3802
train acc:  0.8359375
train loss:  0.3891803026199341
train gradient:  0.2630234371928701
iteration : 3803
train acc:  0.9375
train loss:  0.2558397352695465
train gradient:  0.13865487996493248
iteration : 3804
train acc:  0.8359375
train loss:  0.32184651494026184
train gradient:  0.18941587867934878
iteration : 3805
train acc:  0.8125
train loss:  0.39652711153030396
train gradient:  0.30830739264543694
iteration : 3806
train acc:  0.8359375
train loss:  0.37765875458717346
train gradient:  0.4776199410625777
iteration : 3807
train acc:  0.84375
train loss:  0.3207108676433563
train gradient:  0.19176682144168922
iteration : 3808
train acc:  0.84375
train loss:  0.30373328924179077
train gradient:  0.2616316904462057
iteration : 3809
train acc:  0.84375
train loss:  0.34656891226768494
train gradient:  0.2276379375766014
iteration : 3810
train acc:  0.8359375
train loss:  0.34767043590545654
train gradient:  0.42341778318085777
iteration : 3811
train acc:  0.796875
train loss:  0.3981182873249054
train gradient:  0.3454581177233422
iteration : 3812
train acc:  0.859375
train loss:  0.32784849405288696
train gradient:  0.21144953014294904
iteration : 3813
train acc:  0.828125
train loss:  0.39701664447784424
train gradient:  0.34950416420534397
iteration : 3814
train acc:  0.859375
train loss:  0.343994677066803
train gradient:  0.34370159360433467
iteration : 3815
train acc:  0.875
train loss:  0.3164491653442383
train gradient:  0.2855643727507847
iteration : 3816
train acc:  0.8515625
train loss:  0.342425137758255
train gradient:  0.2019763327799858
iteration : 3817
train acc:  0.8515625
train loss:  0.342872679233551
train gradient:  0.2486599614897216
iteration : 3818
train acc:  0.8515625
train loss:  0.3912563920021057
train gradient:  0.3168918126360851
iteration : 3819
train acc:  0.8125
train loss:  0.375735342502594
train gradient:  0.2962778269865228
iteration : 3820
train acc:  0.8515625
train loss:  0.34200698137283325
train gradient:  0.2327566544484799
iteration : 3821
train acc:  0.84375
train loss:  0.3567478656768799
train gradient:  0.3353232488996231
iteration : 3822
train acc:  0.8125
train loss:  0.3933570086956024
train gradient:  0.28800458055987554
iteration : 3823
train acc:  0.8125
train loss:  0.4404343366622925
train gradient:  0.3483198552437573
iteration : 3824
train acc:  0.8671875
train loss:  0.2951982319355011
train gradient:  0.22709085888344258
iteration : 3825
train acc:  0.84375
train loss:  0.32705551385879517
train gradient:  0.1571560109486846
iteration : 3826
train acc:  0.859375
train loss:  0.33889973163604736
train gradient:  0.31884986749301086
iteration : 3827
train acc:  0.8515625
train loss:  0.2954670786857605
train gradient:  0.16836564289703507
iteration : 3828
train acc:  0.859375
train loss:  0.35658255219459534
train gradient:  0.22002432049910697
iteration : 3829
train acc:  0.8203125
train loss:  0.3784635066986084
train gradient:  0.27660272214537507
iteration : 3830
train acc:  0.7890625
train loss:  0.43375855684280396
train gradient:  0.3642088407484052
iteration : 3831
train acc:  0.8515625
train loss:  0.34629037976264954
train gradient:  0.2556602631086248
iteration : 3832
train acc:  0.8203125
train loss:  0.4172917604446411
train gradient:  0.40201334129476324
iteration : 3833
train acc:  0.828125
train loss:  0.3751406669616699
train gradient:  0.23883314709188136
iteration : 3834
train acc:  0.828125
train loss:  0.3622676730155945
train gradient:  0.2602233778560074
iteration : 3835
train acc:  0.8515625
train loss:  0.35257387161254883
train gradient:  0.26032944486657855
iteration : 3836
train acc:  0.8359375
train loss:  0.40686678886413574
train gradient:  0.3760765425512134
iteration : 3837
train acc:  0.84375
train loss:  0.3493165075778961
train gradient:  0.2830052162545189
iteration : 3838
train acc:  0.796875
train loss:  0.40452706813812256
train gradient:  0.40964232425058034
iteration : 3839
train acc:  0.8515625
train loss:  0.3329910933971405
train gradient:  0.2650231570144889
iteration : 3840
train acc:  0.859375
train loss:  0.31456902623176575
train gradient:  0.23159870179323017
iteration : 3841
train acc:  0.8515625
train loss:  0.40720081329345703
train gradient:  0.45027287747039396
iteration : 3842
train acc:  0.8125
train loss:  0.4177425503730774
train gradient:  0.4110927830875721
iteration : 3843
train acc:  0.8359375
train loss:  0.39772742986679077
train gradient:  0.3599995956587911
iteration : 3844
train acc:  0.8125
train loss:  0.42820805311203003
train gradient:  0.357768925155584
iteration : 3845
train acc:  0.84375
train loss:  0.448952317237854
train gradient:  0.3955438484423428
iteration : 3846
train acc:  0.859375
train loss:  0.32273048162460327
train gradient:  0.21456338948219464
iteration : 3847
train acc:  0.7890625
train loss:  0.3870674967765808
train gradient:  0.2773967828413681
iteration : 3848
train acc:  0.8984375
train loss:  0.30287790298461914
train gradient:  0.19746311076038742
iteration : 3849
train acc:  0.8828125
train loss:  0.31297099590301514
train gradient:  0.1703621247514991
iteration : 3850
train acc:  0.78125
train loss:  0.4737608730792999
train gradient:  0.3740175485557734
iteration : 3851
train acc:  0.859375
train loss:  0.3845823407173157
train gradient:  0.2621631930126715
iteration : 3852
train acc:  0.84375
train loss:  0.3726232945919037
train gradient:  0.24253063342494588
iteration : 3853
train acc:  0.8515625
train loss:  0.3329883813858032
train gradient:  0.28634618663040656
iteration : 3854
train acc:  0.8203125
train loss:  0.35939332842826843
train gradient:  0.22959524747959958
iteration : 3855
train acc:  0.78125
train loss:  0.4286086857318878
train gradient:  0.286710677222476
iteration : 3856
train acc:  0.8515625
train loss:  0.35233283042907715
train gradient:  0.21103788762007542
iteration : 3857
train acc:  0.859375
train loss:  0.33464550971984863
train gradient:  0.17572882268132028
iteration : 3858
train acc:  0.84375
train loss:  0.36511778831481934
train gradient:  0.19322513953787188
iteration : 3859
train acc:  0.8984375
train loss:  0.26439547538757324
train gradient:  0.18211385246417355
iteration : 3860
train acc:  0.8671875
train loss:  0.3215849697589874
train gradient:  0.16861649844420137
iteration : 3861
train acc:  0.8359375
train loss:  0.366091787815094
train gradient:  0.291804954575022
iteration : 3862
train acc:  0.8359375
train loss:  0.3277515172958374
train gradient:  0.26094046073195626
iteration : 3863
train acc:  0.828125
train loss:  0.34542399644851685
train gradient:  0.17866503076454568
iteration : 3864
train acc:  0.859375
train loss:  0.35226279497146606
train gradient:  0.28247102632295096
iteration : 3865
train acc:  0.78125
train loss:  0.4334610402584076
train gradient:  0.44007346636824746
iteration : 3866
train acc:  0.8359375
train loss:  0.3452742099761963
train gradient:  0.2213889208626251
iteration : 3867
train acc:  0.8359375
train loss:  0.3489135503768921
train gradient:  0.2685141782004714
iteration : 3868
train acc:  0.8359375
train loss:  0.3606192469596863
train gradient:  0.1987781381288755
iteration : 3869
train acc:  0.8515625
train loss:  0.386079341173172
train gradient:  0.2864348191563447
iteration : 3870
train acc:  0.7421875
train loss:  0.4539962708950043
train gradient:  0.42231990853440243
iteration : 3871
train acc:  0.859375
train loss:  0.3600732386112213
train gradient:  0.2634283446207908
iteration : 3872
train acc:  0.8203125
train loss:  0.4140888452529907
train gradient:  0.32067502163254186
iteration : 3873
train acc:  0.8671875
train loss:  0.3600728511810303
train gradient:  0.29381490778432584
iteration : 3874
train acc:  0.8203125
train loss:  0.4108906090259552
train gradient:  0.28662268485289616
iteration : 3875
train acc:  0.8203125
train loss:  0.3794781565666199
train gradient:  0.2866715956933114
iteration : 3876
train acc:  0.8046875
train loss:  0.4390154480934143
train gradient:  0.3350779998017425
iteration : 3877
train acc:  0.8125
train loss:  0.3667243719100952
train gradient:  0.33818341338014474
iteration : 3878
train acc:  0.8203125
train loss:  0.42008358240127563
train gradient:  0.35640846957702926
iteration : 3879
train acc:  0.8359375
train loss:  0.35700392723083496
train gradient:  0.20262219184310054
iteration : 3880
train acc:  0.859375
train loss:  0.39751997590065
train gradient:  0.2837412850464934
iteration : 3881
train acc:  0.8671875
train loss:  0.3737131953239441
train gradient:  0.3419658931006314
iteration : 3882
train acc:  0.8671875
train loss:  0.2816966772079468
train gradient:  0.16502888004829935
iteration : 3883
train acc:  0.78125
train loss:  0.43728742003440857
train gradient:  0.28413469103879474
iteration : 3884
train acc:  0.8359375
train loss:  0.3192642331123352
train gradient:  0.34808412618565276
iteration : 3885
train acc:  0.8515625
train loss:  0.39395517110824585
train gradient:  0.2953807405111671
iteration : 3886
train acc:  0.8359375
train loss:  0.35552388429641724
train gradient:  0.25853242635191015
iteration : 3887
train acc:  0.8046875
train loss:  0.3715834319591522
train gradient:  0.2317403992401264
iteration : 3888
train acc:  0.8203125
train loss:  0.3851449191570282
train gradient:  0.27746660854035665
iteration : 3889
train acc:  0.859375
train loss:  0.3782128691673279
train gradient:  0.2081626606403306
iteration : 3890
train acc:  0.875
train loss:  0.31427985429763794
train gradient:  0.185719946150532
iteration : 3891
train acc:  0.8046875
train loss:  0.45483094453811646
train gradient:  0.3548939774089821
iteration : 3892
train acc:  0.875
train loss:  0.30243468284606934
train gradient:  0.24661224855876995
iteration : 3893
train acc:  0.84375
train loss:  0.36725619435310364
train gradient:  0.201354769663756
iteration : 3894
train acc:  0.8359375
train loss:  0.3738047778606415
train gradient:  0.31339802605474365
iteration : 3895
train acc:  0.8125
train loss:  0.40154579281806946
train gradient:  0.3109370786491922
iteration : 3896
train acc:  0.8515625
train loss:  0.3144083023071289
train gradient:  0.33165254692037544
iteration : 3897
train acc:  0.8125
train loss:  0.3911961615085602
train gradient:  0.28144437263519856
iteration : 3898
train acc:  0.8203125
train loss:  0.4244975447654724
train gradient:  0.4253108543105939
iteration : 3899
train acc:  0.84375
train loss:  0.3377172350883484
train gradient:  0.17808466475257806
iteration : 3900
train acc:  0.8203125
train loss:  0.4122626483440399
train gradient:  0.28436730556505996
iteration : 3901
train acc:  0.8515625
train loss:  0.33715060353279114
train gradient:  0.343281877253423
iteration : 3902
train acc:  0.84375
train loss:  0.34641504287719727
train gradient:  0.17788492189751065
iteration : 3903
train acc:  0.828125
train loss:  0.41859132051467896
train gradient:  0.4317656566446977
iteration : 3904
train acc:  0.8359375
train loss:  0.3218536376953125
train gradient:  0.17658154241679458
iteration : 3905
train acc:  0.8515625
train loss:  0.345109224319458
train gradient:  0.18837668027587012
iteration : 3906
train acc:  0.7890625
train loss:  0.4618237018585205
train gradient:  0.46544466434015624
iteration : 3907
train acc:  0.8125
train loss:  0.37937259674072266
train gradient:  0.301686438947109
iteration : 3908
train acc:  0.8359375
train loss:  0.3770967125892639
train gradient:  0.37894728320378773
iteration : 3909
train acc:  0.7890625
train loss:  0.37553226947784424
train gradient:  0.3054008998864147
iteration : 3910
train acc:  0.8203125
train loss:  0.42463982105255127
train gradient:  0.3597830402408798
iteration : 3911
train acc:  0.859375
train loss:  0.3402305841445923
train gradient:  0.22287055226263006
iteration : 3912
train acc:  0.8515625
train loss:  0.32288429141044617
train gradient:  0.27385628172008486
iteration : 3913
train acc:  0.875
train loss:  0.3033899664878845
train gradient:  0.285673792237511
iteration : 3914
train acc:  0.828125
train loss:  0.39660006761550903
train gradient:  0.2904825878168918
iteration : 3915
train acc:  0.8125
train loss:  0.46025770902633667
train gradient:  0.30928975337777886
iteration : 3916
train acc:  0.8671875
train loss:  0.31611567735671997
train gradient:  0.1898046935420934
iteration : 3917
train acc:  0.859375
train loss:  0.3553582429885864
train gradient:  0.2841775903918187
iteration : 3918
train acc:  0.84375
train loss:  0.32585597038269043
train gradient:  0.20831199432780392
iteration : 3919
train acc:  0.8671875
train loss:  0.34526628255844116
train gradient:  0.28600864034873114
iteration : 3920
train acc:  0.859375
train loss:  0.3624291718006134
train gradient:  0.37254560860070185
iteration : 3921
train acc:  0.828125
train loss:  0.39573851227760315
train gradient:  0.31070527224449956
iteration : 3922
train acc:  0.8515625
train loss:  0.39641687273979187
train gradient:  0.28941041416768803
iteration : 3923
train acc:  0.8359375
train loss:  0.3895997107028961
train gradient:  0.25667588662556995
iteration : 3924
train acc:  0.84375
train loss:  0.4150972068309784
train gradient:  0.32516182335377086
iteration : 3925
train acc:  0.796875
train loss:  0.4059966504573822
train gradient:  0.30754365213801815
iteration : 3926
train acc:  0.828125
train loss:  0.35424482822418213
train gradient:  0.24705720619114702
iteration : 3927
train acc:  0.8046875
train loss:  0.3627186417579651
train gradient:  0.2558024939870005
iteration : 3928
train acc:  0.84375
train loss:  0.3629181981086731
train gradient:  0.30631453572607276
iteration : 3929
train acc:  0.84375
train loss:  0.34187203645706177
train gradient:  0.2778202803889303
iteration : 3930
train acc:  0.796875
train loss:  0.4144211411476135
train gradient:  0.3492892603422791
iteration : 3931
train acc:  0.828125
train loss:  0.3405955135822296
train gradient:  0.21346769760187975
iteration : 3932
train acc:  0.8203125
train loss:  0.36355239152908325
train gradient:  0.2920111065532054
iteration : 3933
train acc:  0.84375
train loss:  0.391410231590271
train gradient:  0.43239146949450624
iteration : 3934
train acc:  0.8671875
train loss:  0.30813246965408325
train gradient:  0.2804753552992064
iteration : 3935
train acc:  0.84375
train loss:  0.3151235580444336
train gradient:  0.3063450404629766
iteration : 3936
train acc:  0.8203125
train loss:  0.40379393100738525
train gradient:  0.2755557820216082
iteration : 3937
train acc:  0.890625
train loss:  0.35255923867225647
train gradient:  0.24578636534473325
iteration : 3938
train acc:  0.8828125
train loss:  0.2899332344532013
train gradient:  0.20054243817701453
iteration : 3939
train acc:  0.8671875
train loss:  0.3017927408218384
train gradient:  0.2206437922780537
iteration : 3940
train acc:  0.8671875
train loss:  0.3176853656768799
train gradient:  0.26429417905957137
iteration : 3941
train acc:  0.8828125
train loss:  0.34481996297836304
train gradient:  0.22940929134761717
iteration : 3942
train acc:  0.859375
train loss:  0.3353758156299591
train gradient:  0.2460703514616756
iteration : 3943
train acc:  0.8125
train loss:  0.36784714460372925
train gradient:  0.27797639528734275
iteration : 3944
train acc:  0.7734375
train loss:  0.47595855593681335
train gradient:  0.6506038383307519
iteration : 3945
train acc:  0.7734375
train loss:  0.4545900821685791
train gradient:  0.40679485683614064
iteration : 3946
train acc:  0.84375
train loss:  0.3296623229980469
train gradient:  0.27325742148462245
iteration : 3947
train acc:  0.84375
train loss:  0.33877307176589966
train gradient:  0.23181692068976745
iteration : 3948
train acc:  0.859375
train loss:  0.3193145990371704
train gradient:  0.16121775290553492
iteration : 3949
train acc:  0.8359375
train loss:  0.3561023771762848
train gradient:  0.2494156511143813
iteration : 3950
train acc:  0.859375
train loss:  0.3814302980899811
train gradient:  0.4415322185387571
iteration : 3951
train acc:  0.890625
train loss:  0.3151486814022064
train gradient:  0.20291956657133137
iteration : 3952
train acc:  0.8125
train loss:  0.42787712812423706
train gradient:  0.32187837480854165
iteration : 3953
train acc:  0.8203125
train loss:  0.40413713455200195
train gradient:  0.2810906045120258
iteration : 3954
train acc:  0.796875
train loss:  0.40697649121284485
train gradient:  0.3545111088932836
iteration : 3955
train acc:  0.7578125
train loss:  0.44725751876831055
train gradient:  0.3342118806310812
iteration : 3956
train acc:  0.8046875
train loss:  0.38972872495651245
train gradient:  0.40357419500296793
iteration : 3957
train acc:  0.8203125
train loss:  0.3643643856048584
train gradient:  0.24107154506828726
iteration : 3958
train acc:  0.78125
train loss:  0.4812319874763489
train gradient:  0.42850452608950024
iteration : 3959
train acc:  0.828125
train loss:  0.37373626232147217
train gradient:  0.20865412266433145
iteration : 3960
train acc:  0.8203125
train loss:  0.41166406869888306
train gradient:  0.3073206282116925
iteration : 3961
train acc:  0.828125
train loss:  0.4285125136375427
train gradient:  0.3539593622324339
iteration : 3962
train acc:  0.8203125
train loss:  0.38197314739227295
train gradient:  0.4318121726597332
iteration : 3963
train acc:  0.828125
train loss:  0.405773401260376
train gradient:  0.3543220267189794
iteration : 3964
train acc:  0.8203125
train loss:  0.3376889228820801
train gradient:  0.20036476258938518
iteration : 3965
train acc:  0.8671875
train loss:  0.3648577332496643
train gradient:  0.2570706422040566
iteration : 3966
train acc:  0.84375
train loss:  0.3487098813056946
train gradient:  0.23311213690022728
iteration : 3967
train acc:  0.8515625
train loss:  0.2982357442378998
train gradient:  0.24500243450481193
iteration : 3968
train acc:  0.828125
train loss:  0.4058845341205597
train gradient:  0.21929425862216917
iteration : 3969
train acc:  0.859375
train loss:  0.34850144386291504
train gradient:  0.21720638546813295
iteration : 3970
train acc:  0.890625
train loss:  0.29375019669532776
train gradient:  0.15601210437042518
iteration : 3971
train acc:  0.859375
train loss:  0.33762961626052856
train gradient:  0.2961647404442483
iteration : 3972
train acc:  0.8359375
train loss:  0.3422837555408478
train gradient:  0.29075407486284016
iteration : 3973
train acc:  0.875
train loss:  0.299147367477417
train gradient:  0.16326527322412332
iteration : 3974
train acc:  0.8515625
train loss:  0.3477773368358612
train gradient:  0.4440367562863158
iteration : 3975
train acc:  0.8515625
train loss:  0.3233903646469116
train gradient:  0.23325553988206796
iteration : 3976
train acc:  0.859375
train loss:  0.3322458863258362
train gradient:  0.19690666134342202
iteration : 3977
train acc:  0.796875
train loss:  0.4280446767807007
train gradient:  0.44603389619421335
iteration : 3978
train acc:  0.859375
train loss:  0.2987569570541382
train gradient:  0.20838432117064218
iteration : 3979
train acc:  0.8046875
train loss:  0.38828885555267334
train gradient:  0.23432588410799635
iteration : 3980
train acc:  0.8203125
train loss:  0.4040147662162781
train gradient:  0.29063055003460175
iteration : 3981
train acc:  0.8671875
train loss:  0.3506505489349365
train gradient:  0.26590069308674413
iteration : 3982
train acc:  0.8671875
train loss:  0.2654164731502533
train gradient:  0.23494786116419678
iteration : 3983
train acc:  0.8671875
train loss:  0.33068418502807617
train gradient:  0.2641956167135554
iteration : 3984
train acc:  0.8359375
train loss:  0.3149932324886322
train gradient:  0.14558795389236265
iteration : 3985
train acc:  0.8828125
train loss:  0.32378414273262024
train gradient:  0.20918388163318916
iteration : 3986
train acc:  0.84375
train loss:  0.314436674118042
train gradient:  0.19206366490307886
iteration : 3987
train acc:  0.8359375
train loss:  0.41391104459762573
train gradient:  0.2922415596320145
iteration : 3988
train acc:  0.8828125
train loss:  0.2977924346923828
train gradient:  0.1930326854508933
iteration : 3989
train acc:  0.8828125
train loss:  0.32579460740089417
train gradient:  0.20610236107128346
iteration : 3990
train acc:  0.90625
train loss:  0.2658379375934601
train gradient:  0.19622159218264076
iteration : 3991
train acc:  0.875
train loss:  0.33080804347991943
train gradient:  0.2609824742587262
iteration : 3992
train acc:  0.828125
train loss:  0.3928423523902893
train gradient:  0.45256246748229384
iteration : 3993
train acc:  0.828125
train loss:  0.4173814058303833
train gradient:  0.550242960701807
iteration : 3994
train acc:  0.8203125
train loss:  0.40103501081466675
train gradient:  0.4837989663668262
iteration : 3995
train acc:  0.78125
train loss:  0.47701597213745117
train gradient:  0.45967257993187816
iteration : 3996
train acc:  0.8203125
train loss:  0.3631376624107361
train gradient:  0.2584889245638037
iteration : 3997
train acc:  0.8515625
train loss:  0.3296704888343811
train gradient:  0.29001981050766
iteration : 3998
train acc:  0.796875
train loss:  0.43137431144714355
train gradient:  0.4659645204408907
iteration : 3999
train acc:  0.8125
train loss:  0.36195695400238037
train gradient:  0.2309911590943009
iteration : 4000
train acc:  0.8046875
train loss:  0.39290279150009155
train gradient:  0.3968868667348255
iteration : 4001
train acc:  0.8203125
train loss:  0.4102923572063446
train gradient:  0.31501699290401813
iteration : 4002
train acc:  0.859375
train loss:  0.3161027431488037
train gradient:  0.26381474079852657
iteration : 4003
train acc:  0.78125
train loss:  0.46233034133911133
train gradient:  0.41714973683093987
iteration : 4004
train acc:  0.875
train loss:  0.3410828709602356
train gradient:  0.2642805081017445
iteration : 4005
train acc:  0.859375
train loss:  0.3274654448032379
train gradient:  0.17811803154779288
iteration : 4006
train acc:  0.859375
train loss:  0.35921916365623474
train gradient:  0.3194079752906974
iteration : 4007
train acc:  0.875
train loss:  0.3333064317703247
train gradient:  0.29375141339836597
iteration : 4008
train acc:  0.84375
train loss:  0.3300834596157074
train gradient:  0.29845194046685614
iteration : 4009
train acc:  0.859375
train loss:  0.3292870819568634
train gradient:  0.24609588077544023
iteration : 4010
train acc:  0.84375
train loss:  0.37542611360549927
train gradient:  0.3140371374536868
iteration : 4011
train acc:  0.84375
train loss:  0.36520135402679443
train gradient:  0.29079085045089886
iteration : 4012
train acc:  0.8125
train loss:  0.3775458335876465
train gradient:  0.24682143642494844
iteration : 4013
train acc:  0.8359375
train loss:  0.3263291120529175
train gradient:  0.33874741787064777
iteration : 4014
train acc:  0.8984375
train loss:  0.30015596747398376
train gradient:  0.2046481867193356
iteration : 4015
train acc:  0.8984375
train loss:  0.2754337787628174
train gradient:  0.1553509236211998
iteration : 4016
train acc:  0.875
train loss:  0.3583858013153076
train gradient:  0.285585727760271
iteration : 4017
train acc:  0.8046875
train loss:  0.4359658360481262
train gradient:  0.38938461475843517
iteration : 4018
train acc:  0.8125
train loss:  0.35351383686065674
train gradient:  0.23618857218842057
iteration : 4019
train acc:  0.890625
train loss:  0.28160643577575684
train gradient:  0.20946739235042877
iteration : 4020
train acc:  0.8984375
train loss:  0.2982058525085449
train gradient:  0.22241155391201903
iteration : 4021
train acc:  0.8359375
train loss:  0.4179377853870392
train gradient:  0.47133910375687676
iteration : 4022
train acc:  0.8359375
train loss:  0.349689781665802
train gradient:  0.2948393034877339
iteration : 4023
train acc:  0.8203125
train loss:  0.4219300448894501
train gradient:  0.38012836821633533
iteration : 4024
train acc:  0.8359375
train loss:  0.3779701590538025
train gradient:  0.24281913170957298
iteration : 4025
train acc:  0.8359375
train loss:  0.42379483580589294
train gradient:  0.27342418761230913
iteration : 4026
train acc:  0.828125
train loss:  0.36787688732147217
train gradient:  0.3773989272225631
iteration : 4027
train acc:  0.8359375
train loss:  0.37811002135276794
train gradient:  0.25836317792193547
iteration : 4028
train acc:  0.8515625
train loss:  0.3508951961994171
train gradient:  0.20053831713938255
iteration : 4029
train acc:  0.8671875
train loss:  0.31068187952041626
train gradient:  0.17072005245617922
iteration : 4030
train acc:  0.7734375
train loss:  0.4622466564178467
train gradient:  0.4196554997170609
iteration : 4031
train acc:  0.8671875
train loss:  0.326229989528656
train gradient:  0.2662116288495275
iteration : 4032
train acc:  0.84375
train loss:  0.39045482873916626
train gradient:  0.35944082996489984
iteration : 4033
train acc:  0.859375
train loss:  0.38213008642196655
train gradient:  0.21705946643141893
iteration : 4034
train acc:  0.8046875
train loss:  0.42314279079437256
train gradient:  0.3903037376135997
iteration : 4035
train acc:  0.84375
train loss:  0.35132989287376404
train gradient:  0.27883852487475125
iteration : 4036
train acc:  0.828125
train loss:  0.40520283579826355
train gradient:  0.3537636635122172
iteration : 4037
train acc:  0.828125
train loss:  0.4247276782989502
train gradient:  0.3082058620126419
iteration : 4038
train acc:  0.8671875
train loss:  0.33756330609321594
train gradient:  0.16868228834269192
iteration : 4039
train acc:  0.8046875
train loss:  0.3857375383377075
train gradient:  0.27159748446912924
iteration : 4040
train acc:  0.8125
train loss:  0.3769049346446991
train gradient:  0.31658559576254575
iteration : 4041
train acc:  0.859375
train loss:  0.37160244584083557
train gradient:  0.30592583464290335
iteration : 4042
train acc:  0.859375
train loss:  0.341880738735199
train gradient:  0.3489256669044635
iteration : 4043
train acc:  0.796875
train loss:  0.41188082098960876
train gradient:  0.6979512626967255
iteration : 4044
train acc:  0.7890625
train loss:  0.4240179657936096
train gradient:  0.3965689663674496
iteration : 4045
train acc:  0.859375
train loss:  0.324834406375885
train gradient:  0.19596259011089395
iteration : 4046
train acc:  0.8515625
train loss:  0.3490374684333801
train gradient:  0.2540069209202704
iteration : 4047
train acc:  0.84375
train loss:  0.3608587086200714
train gradient:  0.22939399356807016
iteration : 4048
train acc:  0.8671875
train loss:  0.3280797600746155
train gradient:  0.2831344355034162
iteration : 4049
train acc:  0.875
train loss:  0.3126429319381714
train gradient:  0.27822611941504727
iteration : 4050
train acc:  0.90625
train loss:  0.275684654712677
train gradient:  0.2450425533588152
iteration : 4051
train acc:  0.828125
train loss:  0.4206582307815552
train gradient:  0.27792611174488113
iteration : 4052
train acc:  0.7734375
train loss:  0.44251859188079834
train gradient:  0.5330279548782568
iteration : 4053
train acc:  0.78125
train loss:  0.44446003437042236
train gradient:  0.34508501663785124
iteration : 4054
train acc:  0.765625
train loss:  0.44826260209083557
train gradient:  0.440541757304915
iteration : 4055
train acc:  0.8515625
train loss:  0.3464307487010956
train gradient:  0.2196677363156022
iteration : 4056
train acc:  0.8203125
train loss:  0.3974956274032593
train gradient:  0.30516815035895645
iteration : 4057
train acc:  0.8984375
train loss:  0.3614281117916107
train gradient:  0.32961817076239486
iteration : 4058
train acc:  0.8515625
train loss:  0.40952247381210327
train gradient:  0.2709936693965438
iteration : 4059
train acc:  0.7734375
train loss:  0.42897045612335205
train gradient:  0.33102917139420945
iteration : 4060
train acc:  0.84375
train loss:  0.35241150856018066
train gradient:  0.272089490102522
iteration : 4061
train acc:  0.859375
train loss:  0.33745914697647095
train gradient:  0.26683444795983113
iteration : 4062
train acc:  0.8984375
train loss:  0.27962809801101685
train gradient:  0.16474977876146543
iteration : 4063
train acc:  0.8359375
train loss:  0.3020797371864319
train gradient:  0.22353796245186336
iteration : 4064
train acc:  0.8203125
train loss:  0.37396782636642456
train gradient:  0.3259885252979047
iteration : 4065
train acc:  0.8046875
train loss:  0.3692823052406311
train gradient:  0.2396030566564161
iteration : 4066
train acc:  0.859375
train loss:  0.3724094331264496
train gradient:  0.32027245002607063
iteration : 4067
train acc:  0.8203125
train loss:  0.4346298575401306
train gradient:  0.3276721234397178
iteration : 4068
train acc:  0.828125
train loss:  0.40405476093292236
train gradient:  0.3049443443472234
iteration : 4069
train acc:  0.8203125
train loss:  0.36535072326660156
train gradient:  0.22630519125353205
iteration : 4070
train acc:  0.8359375
train loss:  0.46861886978149414
train gradient:  0.41127707143103287
iteration : 4071
train acc:  0.890625
train loss:  0.30787262320518494
train gradient:  0.238951050079931
iteration : 4072
train acc:  0.84375
train loss:  0.3619847595691681
train gradient:  0.2718373916629522
iteration : 4073
train acc:  0.8515625
train loss:  0.3126488924026489
train gradient:  0.17452656079742218
iteration : 4074
train acc:  0.84375
train loss:  0.37586450576782227
train gradient:  0.2924330451228961
iteration : 4075
train acc:  0.8359375
train loss:  0.3247988224029541
train gradient:  0.2330621036638943
iteration : 4076
train acc:  0.8125
train loss:  0.39645591378211975
train gradient:  0.2861365053577878
iteration : 4077
train acc:  0.8046875
train loss:  0.3926599621772766
train gradient:  0.40651849556215464
iteration : 4078
train acc:  0.8828125
train loss:  0.3168194890022278
train gradient:  0.21603093459591094
iteration : 4079
train acc:  0.8984375
train loss:  0.2767750024795532
train gradient:  0.1826412169798154
iteration : 4080
train acc:  0.828125
train loss:  0.37779009342193604
train gradient:  0.32336382378700024
iteration : 4081
train acc:  0.8359375
train loss:  0.3778230845928192
train gradient:  0.2708198547782989
iteration : 4082
train acc:  0.8359375
train loss:  0.3323713541030884
train gradient:  0.20928467367168
iteration : 4083
train acc:  0.890625
train loss:  0.329969584941864
train gradient:  0.2720329877681551
iteration : 4084
train acc:  0.8359375
train loss:  0.3116919696331024
train gradient:  0.23034187147226226
iteration : 4085
train acc:  0.875
train loss:  0.28457576036453247
train gradient:  0.19298191672285422
iteration : 4086
train acc:  0.8359375
train loss:  0.34457099437713623
train gradient:  0.2595983844146818
iteration : 4087
train acc:  0.84375
train loss:  0.33694005012512207
train gradient:  0.5501597794013556
iteration : 4088
train acc:  0.84375
train loss:  0.3660397529602051
train gradient:  0.2674306382292664
iteration : 4089
train acc:  0.796875
train loss:  0.4312976598739624
train gradient:  0.3973687285601111
iteration : 4090
train acc:  0.8515625
train loss:  0.28182828426361084
train gradient:  0.18400414156340436
iteration : 4091
train acc:  0.8125
train loss:  0.3662114441394806
train gradient:  0.2634888089630191
iteration : 4092
train acc:  0.828125
train loss:  0.39123624563217163
train gradient:  0.25211460976235367
iteration : 4093
train acc:  0.859375
train loss:  0.2714879512786865
train gradient:  0.23236427428773626
iteration : 4094
train acc:  0.8359375
train loss:  0.3886932134628296
train gradient:  0.30499919478581167
iteration : 4095
train acc:  0.8671875
train loss:  0.3627046048641205
train gradient:  0.35119546485606196
iteration : 4096
train acc:  0.8359375
train loss:  0.3684934973716736
train gradient:  0.23718450628691692
iteration : 4097
train acc:  0.84375
train loss:  0.3927224576473236
train gradient:  0.26789138176150495
iteration : 4098
train acc:  0.8671875
train loss:  0.35452181100845337
train gradient:  0.3786618949912116
iteration : 4099
train acc:  0.796875
train loss:  0.4064417779445648
train gradient:  0.4219843225296021
iteration : 4100
train acc:  0.8671875
train loss:  0.3215639591217041
train gradient:  0.24325817358461743
iteration : 4101
train acc:  0.84375
train loss:  0.3750564754009247
train gradient:  0.26174102877016314
iteration : 4102
train acc:  0.828125
train loss:  0.39775270223617554
train gradient:  0.3168358433820043
iteration : 4103
train acc:  0.8515625
train loss:  0.34646058082580566
train gradient:  0.2567530884388531
iteration : 4104
train acc:  0.8359375
train loss:  0.40804019570350647
train gradient:  0.3235544106650195
iteration : 4105
train acc:  0.78125
train loss:  0.4601169526576996
train gradient:  0.37753683208840544
iteration : 4106
train acc:  0.8359375
train loss:  0.3397793173789978
train gradient:  0.2651869507548228
iteration : 4107
train acc:  0.859375
train loss:  0.3533567786216736
train gradient:  0.37998845271467074
iteration : 4108
train acc:  0.8125
train loss:  0.3957078456878662
train gradient:  0.3141677545596587
iteration : 4109
train acc:  0.8515625
train loss:  0.4080505967140198
train gradient:  0.30113881555454874
iteration : 4110
train acc:  0.828125
train loss:  0.34932035207748413
train gradient:  0.33377604028589214
iteration : 4111
train acc:  0.765625
train loss:  0.39847975969314575
train gradient:  0.34574819409890034
iteration : 4112
train acc:  0.84375
train loss:  0.36561983823776245
train gradient:  0.28631455328843886
iteration : 4113
train acc:  0.8359375
train loss:  0.34540730714797974
train gradient:  0.2494128422521173
iteration : 4114
train acc:  0.8125
train loss:  0.34547048807144165
train gradient:  0.22905874436459794
iteration : 4115
train acc:  0.890625
train loss:  0.2606360614299774
train gradient:  0.209238943135806
iteration : 4116
train acc:  0.8359375
train loss:  0.3619690239429474
train gradient:  0.4073502935834809
iteration : 4117
train acc:  0.8671875
train loss:  0.37165266275405884
train gradient:  0.22224903106538724
iteration : 4118
train acc:  0.8359375
train loss:  0.36794447898864746
train gradient:  0.36635033726145266
iteration : 4119
train acc:  0.8046875
train loss:  0.37389248609542847
train gradient:  0.34973486973913587
iteration : 4120
train acc:  0.8359375
train loss:  0.41680651903152466
train gradient:  0.45955700873075483
iteration : 4121
train acc:  0.859375
train loss:  0.3504221439361572
train gradient:  0.18834144611560072
iteration : 4122
train acc:  0.8203125
train loss:  0.4009574353694916
train gradient:  0.24087528868964325
iteration : 4123
train acc:  0.890625
train loss:  0.3257100582122803
train gradient:  0.2674819073790442
iteration : 4124
train acc:  0.828125
train loss:  0.3864193558692932
train gradient:  0.33228959864913593
iteration : 4125
train acc:  0.84375
train loss:  0.38819390535354614
train gradient:  0.4175440824477864
iteration : 4126
train acc:  0.875
train loss:  0.3559306561946869
train gradient:  0.2532809681042841
iteration : 4127
train acc:  0.828125
train loss:  0.38305777311325073
train gradient:  0.37773703859627544
iteration : 4128
train acc:  0.828125
train loss:  0.399377703666687
train gradient:  0.31110459304242843
iteration : 4129
train acc:  0.890625
train loss:  0.2851739525794983
train gradient:  0.19644132510148327
iteration : 4130
train acc:  0.84375
train loss:  0.3577131927013397
train gradient:  0.27909207728375107
iteration : 4131
train acc:  0.8203125
train loss:  0.4424608051776886
train gradient:  0.3470125004637354
iteration : 4132
train acc:  0.796875
train loss:  0.39389440417289734
train gradient:  0.3008024787789838
iteration : 4133
train acc:  0.8125
train loss:  0.44420430064201355
train gradient:  0.3301129930477287
iteration : 4134
train acc:  0.8203125
train loss:  0.3946685791015625
train gradient:  0.4204811506885232
iteration : 4135
train acc:  0.8359375
train loss:  0.3000367283821106
train gradient:  0.1979042397257899
iteration : 4136
train acc:  0.8671875
train loss:  0.2765057384967804
train gradient:  0.20459941675875953
iteration : 4137
train acc:  0.875
train loss:  0.3116104006767273
train gradient:  0.22577176122548126
iteration : 4138
train acc:  0.8671875
train loss:  0.39220935106277466
train gradient:  0.31713106519760453
iteration : 4139
train acc:  0.8359375
train loss:  0.34701162576675415
train gradient:  0.48101947632729214
iteration : 4140
train acc:  0.890625
train loss:  0.3317604064941406
train gradient:  0.25044003989314534
iteration : 4141
train acc:  0.8203125
train loss:  0.41779130697250366
train gradient:  0.24925521280392404
iteration : 4142
train acc:  0.8515625
train loss:  0.32333022356033325
train gradient:  0.31323514686620524
iteration : 4143
train acc:  0.8203125
train loss:  0.3900567293167114
train gradient:  0.28178909640531713
iteration : 4144
train acc:  0.8203125
train loss:  0.37201353907585144
train gradient:  0.38054938232629576
iteration : 4145
train acc:  0.8515625
train loss:  0.4028666019439697
train gradient:  0.36711138541814314
iteration : 4146
train acc:  0.859375
train loss:  0.36368659138679504
train gradient:  0.22683339189103685
iteration : 4147
train acc:  0.8046875
train loss:  0.38073763251304626
train gradient:  0.2757848948803631
iteration : 4148
train acc:  0.8125
train loss:  0.3777601718902588
train gradient:  0.2689908822990696
iteration : 4149
train acc:  0.78125
train loss:  0.4413478374481201
train gradient:  0.45812559579789786
iteration : 4150
train acc:  0.875
train loss:  0.3078843951225281
train gradient:  0.20624869761358447
iteration : 4151
train acc:  0.875
train loss:  0.33179545402526855
train gradient:  0.2873893663916658
iteration : 4152
train acc:  0.8046875
train loss:  0.39360079169273376
train gradient:  0.25983805025862483
iteration : 4153
train acc:  0.890625
train loss:  0.2889851927757263
train gradient:  0.17688953901425203
iteration : 4154
train acc:  0.828125
train loss:  0.4140586256980896
train gradient:  0.3517907077515536
iteration : 4155
train acc:  0.84375
train loss:  0.3772888779640198
train gradient:  0.36604117726324564
iteration : 4156
train acc:  0.84375
train loss:  0.3508760929107666
train gradient:  0.27874722828151693
iteration : 4157
train acc:  0.8125
train loss:  0.3914620876312256
train gradient:  0.29641413540941364
iteration : 4158
train acc:  0.8515625
train loss:  0.3558034300804138
train gradient:  0.19637618302488186
iteration : 4159
train acc:  0.8046875
train loss:  0.4295119643211365
train gradient:  0.395548863351619
iteration : 4160
train acc:  0.8671875
train loss:  0.3352838158607483
train gradient:  0.19003314209211364
iteration : 4161
train acc:  0.8125
train loss:  0.4164075553417206
train gradient:  0.2689002055254955
iteration : 4162
train acc:  0.84375
train loss:  0.32773929834365845
train gradient:  0.17772452156021665
iteration : 4163
train acc:  0.8046875
train loss:  0.3716321587562561
train gradient:  0.3941883564314599
iteration : 4164
train acc:  0.828125
train loss:  0.38994109630584717
train gradient:  0.29338118530278123
iteration : 4165
train acc:  0.828125
train loss:  0.39791691303253174
train gradient:  0.24591160176820767
iteration : 4166
train acc:  0.7734375
train loss:  0.483040988445282
train gradient:  0.43234580063902206
iteration : 4167
train acc:  0.8359375
train loss:  0.3311883807182312
train gradient:  0.1890595866110238
iteration : 4168
train acc:  0.8046875
train loss:  0.4621421992778778
train gradient:  0.32886786068840484
iteration : 4169
train acc:  0.7578125
train loss:  0.46873947978019714
train gradient:  0.37297005581241605
iteration : 4170
train acc:  0.8125
train loss:  0.3786221146583557
train gradient:  0.234028518265722
iteration : 4171
train acc:  0.84375
train loss:  0.37702566385269165
train gradient:  0.23558205038073474
iteration : 4172
train acc:  0.8671875
train loss:  0.3345588445663452
train gradient:  0.2995107620685911
iteration : 4173
train acc:  0.8359375
train loss:  0.37003785371780396
train gradient:  0.2643120101268923
iteration : 4174
train acc:  0.8046875
train loss:  0.41569092869758606
train gradient:  0.2877300435731851
iteration : 4175
train acc:  0.890625
train loss:  0.291667640209198
train gradient:  0.18995658281645827
iteration : 4176
train acc:  0.8125
train loss:  0.4284868538379669
train gradient:  0.2378877317997205
iteration : 4177
train acc:  0.890625
train loss:  0.27364808320999146
train gradient:  0.15313230644346387
iteration : 4178
train acc:  0.890625
train loss:  0.27852851152420044
train gradient:  0.21044414117116236
iteration : 4179
train acc:  0.8359375
train loss:  0.41582658886909485
train gradient:  0.2840703211300767
iteration : 4180
train acc:  0.859375
train loss:  0.36451849341392517
train gradient:  0.2479304649464299
iteration : 4181
train acc:  0.8203125
train loss:  0.38912129402160645
train gradient:  0.22732149998922832
iteration : 4182
train acc:  0.8515625
train loss:  0.32545584440231323
train gradient:  0.1726984459747543
iteration : 4183
train acc:  0.78125
train loss:  0.41952234506607056
train gradient:  0.2737272235186986
iteration : 4184
train acc:  0.8515625
train loss:  0.3526511788368225
train gradient:  0.20888592506076215
iteration : 4185
train acc:  0.859375
train loss:  0.335309237241745
train gradient:  0.24340815133970622
iteration : 4186
train acc:  0.8203125
train loss:  0.33601731061935425
train gradient:  0.1931868017814573
iteration : 4187
train acc:  0.8203125
train loss:  0.4008355140686035
train gradient:  0.32358534951801815
iteration : 4188
train acc:  0.859375
train loss:  0.38129138946533203
train gradient:  0.27184338164476535
iteration : 4189
train acc:  0.8046875
train loss:  0.42342057824134827
train gradient:  0.2809864509260285
iteration : 4190
train acc:  0.8515625
train loss:  0.31763842701911926
train gradient:  0.22969734436776879
iteration : 4191
train acc:  0.8125
train loss:  0.43064796924591064
train gradient:  0.26082109445305324
iteration : 4192
train acc:  0.8125
train loss:  0.42179936170578003
train gradient:  0.3252926331897633
iteration : 4193
train acc:  0.8671875
train loss:  0.38640326261520386
train gradient:  0.23662318442572666
iteration : 4194
train acc:  0.84375
train loss:  0.35002440214157104
train gradient:  0.2643270026377432
iteration : 4195
train acc:  0.8046875
train loss:  0.41642484068870544
train gradient:  0.2604577074497455
iteration : 4196
train acc:  0.8359375
train loss:  0.38473787903785706
train gradient:  0.28745839566769354
iteration : 4197
train acc:  0.8671875
train loss:  0.3264598846435547
train gradient:  0.23711562648156337
iteration : 4198
train acc:  0.828125
train loss:  0.348299115896225
train gradient:  0.28299595730540794
iteration : 4199
train acc:  0.765625
train loss:  0.40846961736679077
train gradient:  0.31224645085543395
iteration : 4200
train acc:  0.8671875
train loss:  0.30989551544189453
train gradient:  0.17643463442658072
iteration : 4201
train acc:  0.75
train loss:  0.47513633966445923
train gradient:  0.4414042039018861
iteration : 4202
train acc:  0.84375
train loss:  0.3774852752685547
train gradient:  0.273219534833807
iteration : 4203
train acc:  0.84375
train loss:  0.344429075717926
train gradient:  0.2973105856256035
iteration : 4204
train acc:  0.7734375
train loss:  0.413377583026886
train gradient:  0.35602954219124566
iteration : 4205
train acc:  0.8671875
train loss:  0.31719309091567993
train gradient:  0.3040494985842847
iteration : 4206
train acc:  0.90625
train loss:  0.24762052297592163
train gradient:  0.16412162336380526
iteration : 4207
train acc:  0.8671875
train loss:  0.3449114263057709
train gradient:  0.23884843713366377
iteration : 4208
train acc:  0.8515625
train loss:  0.33326390385627747
train gradient:  0.23090760112051872
iteration : 4209
train acc:  0.875
train loss:  0.32768580317497253
train gradient:  0.22437207739650822
iteration : 4210
train acc:  0.828125
train loss:  0.4114851951599121
train gradient:  0.3064687203837214
iteration : 4211
train acc:  0.8828125
train loss:  0.3509312868118286
train gradient:  0.261490516991407
iteration : 4212
train acc:  0.8125
train loss:  0.3291424810886383
train gradient:  0.17959114466796122
iteration : 4213
train acc:  0.8671875
train loss:  0.3565659523010254
train gradient:  0.3038392703842876
iteration : 4214
train acc:  0.7421875
train loss:  0.4292105436325073
train gradient:  0.41544211482838084
iteration : 4215
train acc:  0.84375
train loss:  0.36499613523483276
train gradient:  0.21035899682598957
iteration : 4216
train acc:  0.875
train loss:  0.2817078232765198
train gradient:  0.22603374697696244
iteration : 4217
train acc:  0.875
train loss:  0.280478835105896
train gradient:  0.1918919794715786
iteration : 4218
train acc:  0.8671875
train loss:  0.35090458393096924
train gradient:  0.20664472668703943
iteration : 4219
train acc:  0.859375
train loss:  0.35524114966392517
train gradient:  0.30456035461027486
iteration : 4220
train acc:  0.8515625
train loss:  0.3232145607471466
train gradient:  0.2545497806206526
iteration : 4221
train acc:  0.875
train loss:  0.3124597668647766
train gradient:  0.2754369167011187
iteration : 4222
train acc:  0.8203125
train loss:  0.40429797768592834
train gradient:  0.21938753348738216
iteration : 4223
train acc:  0.8125
train loss:  0.4294734597206116
train gradient:  0.31509275762087924
iteration : 4224
train acc:  0.875
train loss:  0.29100501537323
train gradient:  0.2898666027575249
iteration : 4225
train acc:  0.8203125
train loss:  0.368948757648468
train gradient:  0.3357957180010718
iteration : 4226
train acc:  0.90625
train loss:  0.286842942237854
train gradient:  0.21849040260939423
iteration : 4227
train acc:  0.84375
train loss:  0.3297019600868225
train gradient:  0.16057013149681337
iteration : 4228
train acc:  0.8359375
train loss:  0.36605948209762573
train gradient:  0.22179716805584498
iteration : 4229
train acc:  0.8671875
train loss:  0.31066709756851196
train gradient:  0.3036177056732847
iteration : 4230
train acc:  0.8125
train loss:  0.4010810852050781
train gradient:  0.3971775696996721
iteration : 4231
train acc:  0.8203125
train loss:  0.34122079610824585
train gradient:  0.34686155923569295
iteration : 4232
train acc:  0.8671875
train loss:  0.2876013219356537
train gradient:  0.20862528550664688
iteration : 4233
train acc:  0.890625
train loss:  0.2817555069923401
train gradient:  0.4544828338364317
iteration : 4234
train acc:  0.8359375
train loss:  0.4168646037578583
train gradient:  0.34133051937682285
iteration : 4235
train acc:  0.875
train loss:  0.33267104625701904
train gradient:  0.2029282787600137
iteration : 4236
train acc:  0.8515625
train loss:  0.29646599292755127
train gradient:  0.20636053554038525
iteration : 4237
train acc:  0.8125
train loss:  0.4204501807689667
train gradient:  0.2926479312652734
iteration : 4238
train acc:  0.8515625
train loss:  0.3181619644165039
train gradient:  0.20614513786974592
iteration : 4239
train acc:  0.8515625
train loss:  0.3331858515739441
train gradient:  0.28115475948096436
iteration : 4240
train acc:  0.8359375
train loss:  0.39464670419692993
train gradient:  0.3639473698234777
iteration : 4241
train acc:  0.8671875
train loss:  0.32181960344314575
train gradient:  0.25520216645234134
iteration : 4242
train acc:  0.8359375
train loss:  0.35513874888420105
train gradient:  0.4326216145442088
iteration : 4243
train acc:  0.8125
train loss:  0.3700975775718689
train gradient:  0.3098099298265339
iteration : 4244
train acc:  0.9140625
train loss:  0.24610291421413422
train gradient:  0.14714279184775156
iteration : 4245
train acc:  0.828125
train loss:  0.34246987104415894
train gradient:  0.443339698175004
iteration : 4246
train acc:  0.890625
train loss:  0.327117383480072
train gradient:  0.18744606565433425
iteration : 4247
train acc:  0.8203125
train loss:  0.30266517400741577
train gradient:  0.21606140623712972
iteration : 4248
train acc:  0.828125
train loss:  0.42326948046684265
train gradient:  0.4083880220997106
iteration : 4249
train acc:  0.84375
train loss:  0.35284897685050964
train gradient:  0.24805439940266213
iteration : 4250
train acc:  0.796875
train loss:  0.4207928776741028
train gradient:  0.3891377962911694
iteration : 4251
train acc:  0.875
train loss:  0.2882941663265228
train gradient:  0.18818189233850702
iteration : 4252
train acc:  0.7734375
train loss:  0.4865558445453644
train gradient:  0.4540374351479831
iteration : 4253
train acc:  0.8515625
train loss:  0.3071961998939514
train gradient:  0.22369521860109415
iteration : 4254
train acc:  0.8984375
train loss:  0.2945023775100708
train gradient:  0.20775873912274406
iteration : 4255
train acc:  0.8203125
train loss:  0.3624613881111145
train gradient:  0.27075694436439707
iteration : 4256
train acc:  0.796875
train loss:  0.4390804171562195
train gradient:  0.2962456208304269
iteration : 4257
train acc:  0.8359375
train loss:  0.37979474663734436
train gradient:  0.31503029746365985
iteration : 4258
train acc:  0.8828125
train loss:  0.30916649103164673
train gradient:  0.23201589074446954
iteration : 4259
train acc:  0.875
train loss:  0.29382383823394775
train gradient:  0.22950200453680175
iteration : 4260
train acc:  0.78125
train loss:  0.4249337613582611
train gradient:  0.3583410049479114
iteration : 4261
train acc:  0.8828125
train loss:  0.27711939811706543
train gradient:  0.1984752045201413
iteration : 4262
train acc:  0.875
train loss:  0.2656264305114746
train gradient:  0.20504448541304293
iteration : 4263
train acc:  0.8046875
train loss:  0.43434077501296997
train gradient:  0.5509725471780587
iteration : 4264
train acc:  0.828125
train loss:  0.3678230047225952
train gradient:  0.33411292943481274
iteration : 4265
train acc:  0.890625
train loss:  0.3006255328655243
train gradient:  0.2501798221780741
iteration : 4266
train acc:  0.859375
train loss:  0.39818301796913147
train gradient:  0.3512511220306214
iteration : 4267
train acc:  0.796875
train loss:  0.44365960359573364
train gradient:  0.5544866762106697
iteration : 4268
train acc:  0.875
train loss:  0.30990391969680786
train gradient:  0.2079652724323548
iteration : 4269
train acc:  0.828125
train loss:  0.4103485345840454
train gradient:  0.34209481257146596
iteration : 4270
train acc:  0.796875
train loss:  0.3857005536556244
train gradient:  0.26912193442449417
iteration : 4271
train acc:  0.8984375
train loss:  0.26338696479797363
train gradient:  0.18373567971142057
iteration : 4272
train acc:  0.8203125
train loss:  0.4576301574707031
train gradient:  0.40851512418300934
iteration : 4273
train acc:  0.8515625
train loss:  0.3260631263256073
train gradient:  0.22496855050761555
iteration : 4274
train acc:  0.8984375
train loss:  0.24087443947792053
train gradient:  0.1459960175971256
iteration : 4275
train acc:  0.78125
train loss:  0.4404541850090027
train gradient:  0.3388306533274355
iteration : 4276
train acc:  0.8515625
train loss:  0.39208289980888367
train gradient:  0.2564065145363773
iteration : 4277
train acc:  0.78125
train loss:  0.4693294167518616
train gradient:  0.5410378722966174
iteration : 4278
train acc:  0.828125
train loss:  0.3139227628707886
train gradient:  0.21988705534876254
iteration : 4279
train acc:  0.8203125
train loss:  0.5453633666038513
train gradient:  0.48790409241281074
iteration : 4280
train acc:  0.8515625
train loss:  0.33808913826942444
train gradient:  0.2879358565116452
iteration : 4281
train acc:  0.8828125
train loss:  0.31626856327056885
train gradient:  0.18251046746588395
iteration : 4282
train acc:  0.828125
train loss:  0.3218855857849121
train gradient:  0.21254387612724682
iteration : 4283
train acc:  0.828125
train loss:  0.3935449719429016
train gradient:  0.44160951940132354
iteration : 4284
train acc:  0.8203125
train loss:  0.42961764335632324
train gradient:  0.3503373182585038
iteration : 4285
train acc:  0.8515625
train loss:  0.3291012644767761
train gradient:  0.18944206051486123
iteration : 4286
train acc:  0.859375
train loss:  0.33777916431427
train gradient:  0.21712989215829437
iteration : 4287
train acc:  0.828125
train loss:  0.32957515120506287
train gradient:  0.34062926432040014
iteration : 4288
train acc:  0.828125
train loss:  0.37763649225234985
train gradient:  0.3638141936561174
iteration : 4289
train acc:  0.875
train loss:  0.3184909522533417
train gradient:  0.23768173589884778
iteration : 4290
train acc:  0.8125
train loss:  0.3969537019729614
train gradient:  0.4773217038658723
iteration : 4291
train acc:  0.84375
train loss:  0.30776500701904297
train gradient:  0.3987995683628837
iteration : 4292
train acc:  0.796875
train loss:  0.40054404735565186
train gradient:  0.3859635883308851
iteration : 4293
train acc:  0.8671875
train loss:  0.34674882888793945
train gradient:  0.3386403475131151
iteration : 4294
train acc:  0.8359375
train loss:  0.30794665217399597
train gradient:  0.24953913572843558
iteration : 4295
train acc:  0.828125
train loss:  0.34519702196121216
train gradient:  0.20081516508213787
iteration : 4296
train acc:  0.8828125
train loss:  0.35542935132980347
train gradient:  0.25024816357009344
iteration : 4297
train acc:  0.765625
train loss:  0.4193086326122284
train gradient:  0.34220274422725333
iteration : 4298
train acc:  0.8515625
train loss:  0.324881911277771
train gradient:  0.22776576348501362
iteration : 4299
train acc:  0.8828125
train loss:  0.30777519941329956
train gradient:  0.20725579838164848
iteration : 4300
train acc:  0.859375
train loss:  0.2861446142196655
train gradient:  0.1862526019019246
iteration : 4301
train acc:  0.8359375
train loss:  0.40236055850982666
train gradient:  0.20630997738236684
iteration : 4302
train acc:  0.78125
train loss:  0.4666762351989746
train gradient:  0.38931728375975405
iteration : 4303
train acc:  0.890625
train loss:  0.3135185241699219
train gradient:  0.24910168801770896
iteration : 4304
train acc:  0.8828125
train loss:  0.32208937406539917
train gradient:  0.1715542847855467
iteration : 4305
train acc:  0.8515625
train loss:  0.36320528388023376
train gradient:  0.2710583785969702
iteration : 4306
train acc:  0.8125
train loss:  0.4024416208267212
train gradient:  0.33349797578869417
iteration : 4307
train acc:  0.8125
train loss:  0.3844700753688812
train gradient:  0.23185479499671569
iteration : 4308
train acc:  0.7890625
train loss:  0.48232409358024597
train gradient:  0.48623604254151714
iteration : 4309
train acc:  0.8359375
train loss:  0.3679341673851013
train gradient:  0.4012078208375824
iteration : 4310
train acc:  0.890625
train loss:  0.2779170274734497
train gradient:  0.16426862218731575
iteration : 4311
train acc:  0.859375
train loss:  0.3637210428714752
train gradient:  0.27053430654309985
iteration : 4312
train acc:  0.875
train loss:  0.3257799446582794
train gradient:  0.15881660390221802
iteration : 4313
train acc:  0.8125
train loss:  0.39634090662002563
train gradient:  0.28573314036668257
iteration : 4314
train acc:  0.8828125
train loss:  0.30459895730018616
train gradient:  0.14719988482924956
iteration : 4315
train acc:  0.8671875
train loss:  0.3396409749984741
train gradient:  0.2248413213577093
iteration : 4316
train acc:  0.8359375
train loss:  0.3777724504470825
train gradient:  0.3331856579529788
iteration : 4317
train acc:  0.8203125
train loss:  0.3849453628063202
train gradient:  0.26546171787625616
iteration : 4318
train acc:  0.859375
train loss:  0.34042224287986755
train gradient:  0.19548191346442367
iteration : 4319
train acc:  0.8828125
train loss:  0.3114136755466461
train gradient:  0.20470463118953433
iteration : 4320
train acc:  0.7890625
train loss:  0.4371908903121948
train gradient:  0.34435559553554584
iteration : 4321
train acc:  0.8984375
train loss:  0.2828025817871094
train gradient:  0.18300002421652634
iteration : 4322
train acc:  0.796875
train loss:  0.4431310296058655
train gradient:  0.39858857974050965
iteration : 4323
train acc:  0.8828125
train loss:  0.3191649913787842
train gradient:  0.21668765801225393
iteration : 4324
train acc:  0.7890625
train loss:  0.4644172191619873
train gradient:  0.4047285534560322
iteration : 4325
train acc:  0.8046875
train loss:  0.3729454278945923
train gradient:  0.2965238759581717
iteration : 4326
train acc:  0.890625
train loss:  0.288457453250885
train gradient:  0.1713101874866896
iteration : 4327
train acc:  0.8359375
train loss:  0.473914235830307
train gradient:  0.3975381919282532
iteration : 4328
train acc:  0.875
train loss:  0.3422069549560547
train gradient:  0.21724892220985734
iteration : 4329
train acc:  0.8671875
train loss:  0.3100805878639221
train gradient:  0.2291920209678765
iteration : 4330
train acc:  0.8125
train loss:  0.3827705979347229
train gradient:  0.35432104590403346
iteration : 4331
train acc:  0.84375
train loss:  0.32853782176971436
train gradient:  0.14189922821963497
iteration : 4332
train acc:  0.8203125
train loss:  0.3508618474006653
train gradient:  0.25702654859942276
iteration : 4333
train acc:  0.8515625
train loss:  0.32449865341186523
train gradient:  0.2516690473171408
iteration : 4334
train acc:  0.859375
train loss:  0.3631066083908081
train gradient:  0.17583016063328888
iteration : 4335
train acc:  0.84375
train loss:  0.3082270622253418
train gradient:  0.21255589518193432
iteration : 4336
train acc:  0.7890625
train loss:  0.45971357822418213
train gradient:  0.41493964291934265
iteration : 4337
train acc:  0.828125
train loss:  0.42501068115234375
train gradient:  0.5051197752617784
iteration : 4338
train acc:  0.8828125
train loss:  0.31197381019592285
train gradient:  0.1543167170779482
iteration : 4339
train acc:  0.859375
train loss:  0.3250378370285034
train gradient:  0.16689941542619352
iteration : 4340
train acc:  0.8515625
train loss:  0.37600213289260864
train gradient:  0.27443412932935085
iteration : 4341
train acc:  0.8515625
train loss:  0.36547696590423584
train gradient:  0.2604394924809752
iteration : 4342
train acc:  0.875
train loss:  0.2918268144130707
train gradient:  0.1765615146099768
iteration : 4343
train acc:  0.8515625
train loss:  0.3297545313835144
train gradient:  0.24572618548082958
iteration : 4344
train acc:  0.8515625
train loss:  0.380149781703949
train gradient:  0.3572033619999501
iteration : 4345
train acc:  0.8515625
train loss:  0.33663296699523926
train gradient:  0.20024937164192969
iteration : 4346
train acc:  0.828125
train loss:  0.38014742732048035
train gradient:  0.2548323741829057
iteration : 4347
train acc:  0.8359375
train loss:  0.3998883366584778
train gradient:  0.2898859103154375
iteration : 4348
train acc:  0.84375
train loss:  0.34187042713165283
train gradient:  0.21655896496843008
iteration : 4349
train acc:  0.7890625
train loss:  0.4400339722633362
train gradient:  0.34957615431523575
iteration : 4350
train acc:  0.8359375
train loss:  0.33466511964797974
train gradient:  0.17851669058105826
iteration : 4351
train acc:  0.8515625
train loss:  0.37678390741348267
train gradient:  0.23246303205517727
iteration : 4352
train acc:  0.84375
train loss:  0.36580193042755127
train gradient:  0.3000656892952278
iteration : 4353
train acc:  0.8984375
train loss:  0.2707217335700989
train gradient:  0.0969573267270546
iteration : 4354
train acc:  0.8515625
train loss:  0.295954167842865
train gradient:  0.15814351728337728
iteration : 4355
train acc:  0.859375
train loss:  0.3472341001033783
train gradient:  0.32051812146696895
iteration : 4356
train acc:  0.921875
train loss:  0.32556894421577454
train gradient:  0.21949802088726614
iteration : 4357
train acc:  0.828125
train loss:  0.368320107460022
train gradient:  0.19409639756360186
iteration : 4358
train acc:  0.890625
train loss:  0.28109419345855713
train gradient:  0.158419159693656
iteration : 4359
train acc:  0.8984375
train loss:  0.2806118130683899
train gradient:  0.15360098020095586
iteration : 4360
train acc:  0.8671875
train loss:  0.2929273247718811
train gradient:  0.2224123976809183
iteration : 4361
train acc:  0.78125
train loss:  0.4607965648174286
train gradient:  0.48524326164281495
iteration : 4362
train acc:  0.7890625
train loss:  0.4331654906272888
train gradient:  0.3205789444765648
iteration : 4363
train acc:  0.8203125
train loss:  0.35548025369644165
train gradient:  0.25909138483725813
iteration : 4364
train acc:  0.828125
train loss:  0.3156495690345764
train gradient:  0.2726079773689248
iteration : 4365
train acc:  0.875
train loss:  0.30216291546821594
train gradient:  0.4009501076763593
iteration : 4366
train acc:  0.828125
train loss:  0.43480008840560913
train gradient:  0.4349842976763417
iteration : 4367
train acc:  0.84375
train loss:  0.29962220788002014
train gradient:  0.226290811690324
iteration : 4368
train acc:  0.90625
train loss:  0.3026915490627289
train gradient:  0.1865194825277266
iteration : 4369
train acc:  0.859375
train loss:  0.3544083833694458
train gradient:  0.30084252908545694
iteration : 4370
train acc:  0.890625
train loss:  0.29147154092788696
train gradient:  0.2004446053984591
iteration : 4371
train acc:  0.84375
train loss:  0.3383216857910156
train gradient:  0.27945819463530247
iteration : 4372
train acc:  0.875
train loss:  0.28419867157936096
train gradient:  0.15183327319330525
iteration : 4373
train acc:  0.8515625
train loss:  0.3556371331214905
train gradient:  0.3214728909627029
iteration : 4374
train acc:  0.890625
train loss:  0.3088141679763794
train gradient:  0.26462077821889357
iteration : 4375
train acc:  0.8359375
train loss:  0.38882362842559814
train gradient:  0.3048451431660918
iteration : 4376
train acc:  0.859375
train loss:  0.3271574378013611
train gradient:  0.24071380654130212
iteration : 4377
train acc:  0.8515625
train loss:  0.3144800662994385
train gradient:  0.2353103267325745
iteration : 4378
train acc:  0.875
train loss:  0.28372809290885925
train gradient:  0.19262863416690312
iteration : 4379
train acc:  0.8359375
train loss:  0.3551637828350067
train gradient:  0.21943971318911537
iteration : 4380
train acc:  0.8046875
train loss:  0.3944125175476074
train gradient:  0.24366292208684465
iteration : 4381
train acc:  0.75
train loss:  0.4359288811683655
train gradient:  0.34353545883805797
iteration : 4382
train acc:  0.8515625
train loss:  0.336555153131485
train gradient:  0.20049175402149555
iteration : 4383
train acc:  0.796875
train loss:  0.3592628538608551
train gradient:  0.2940755883526418
iteration : 4384
train acc:  0.796875
train loss:  0.3748733401298523
train gradient:  0.2755860541232974
iteration : 4385
train acc:  0.8671875
train loss:  0.32887405157089233
train gradient:  0.32060897275913647
iteration : 4386
train acc:  0.8359375
train loss:  0.3488280177116394
train gradient:  0.2860699207374534
iteration : 4387
train acc:  0.8125
train loss:  0.38178929686546326
train gradient:  0.35994922408486596
iteration : 4388
train acc:  0.8828125
train loss:  0.3072630763053894
train gradient:  0.1642128947585911
iteration : 4389
train acc:  0.90625
train loss:  0.265546053647995
train gradient:  0.13550214699016427
iteration : 4390
train acc:  0.8046875
train loss:  0.4143446087837219
train gradient:  0.3416531426383953
iteration : 4391
train acc:  0.8203125
train loss:  0.4197348356246948
train gradient:  0.35812682434492943
iteration : 4392
train acc:  0.84375
train loss:  0.3608718514442444
train gradient:  0.25852440107177765
iteration : 4393
train acc:  0.828125
train loss:  0.36828696727752686
train gradient:  0.33994571091397313
iteration : 4394
train acc:  0.859375
train loss:  0.3484516143798828
train gradient:  0.2649378625306861
iteration : 4395
train acc:  0.8203125
train loss:  0.4052404463291168
train gradient:  0.2918088967723923
iteration : 4396
train acc:  0.84375
train loss:  0.3836824893951416
train gradient:  0.24500114215900176
iteration : 4397
train acc:  0.8125
train loss:  0.3766378164291382
train gradient:  0.27584164081900536
iteration : 4398
train acc:  0.859375
train loss:  0.423907607793808
train gradient:  0.4071581074393636
iteration : 4399
train acc:  0.8515625
train loss:  0.3227621912956238
train gradient:  0.23275436809375613
iteration : 4400
train acc:  0.8828125
train loss:  0.27045127749443054
train gradient:  0.1640653584072174
iteration : 4401
train acc:  0.875
train loss:  0.3508818745613098
train gradient:  0.23150210812381147
iteration : 4402
train acc:  0.84375
train loss:  0.38272738456726074
train gradient:  0.3096571404333941
iteration : 4403
train acc:  0.84375
train loss:  0.42250925302505493
train gradient:  0.3338357116101157
iteration : 4404
train acc:  0.8203125
train loss:  0.3700050115585327
train gradient:  0.25601733029896095
iteration : 4405
train acc:  0.78125
train loss:  0.3884935677051544
train gradient:  0.33290352317673894
iteration : 4406
train acc:  0.828125
train loss:  0.37985676527023315
train gradient:  0.3279314772855752
iteration : 4407
train acc:  0.7890625
train loss:  0.39861512184143066
train gradient:  0.28734612376544044
iteration : 4408
train acc:  0.8203125
train loss:  0.37074577808380127
train gradient:  0.2185203590263837
iteration : 4409
train acc:  0.8046875
train loss:  0.42320722341537476
train gradient:  0.283233624987661
iteration : 4410
train acc:  0.84375
train loss:  0.33253973722457886
train gradient:  0.22165499329745
iteration : 4411
train acc:  0.859375
train loss:  0.3430693447589874
train gradient:  0.2626098388670169
iteration : 4412
train acc:  0.84375
train loss:  0.31584274768829346
train gradient:  0.19272766645058614
iteration : 4413
train acc:  0.8828125
train loss:  0.28141582012176514
train gradient:  0.17653751036492424
iteration : 4414
train acc:  0.84375
train loss:  0.3411647081375122
train gradient:  0.27689626264488104
iteration : 4415
train acc:  0.8671875
train loss:  0.328845351934433
train gradient:  0.18381991932309694
iteration : 4416
train acc:  0.8671875
train loss:  0.3135296702384949
train gradient:  0.22386337245453636
iteration : 4417
train acc:  0.8671875
train loss:  0.43160563707351685
train gradient:  0.3243716474231341
iteration : 4418
train acc:  0.8125
train loss:  0.3526335656642914
train gradient:  0.20027118519078563
iteration : 4419
train acc:  0.8671875
train loss:  0.30777087807655334
train gradient:  0.19055599784973148
iteration : 4420
train acc:  0.84375
train loss:  0.40231961011886597
train gradient:  0.3115822957914627
iteration : 4421
train acc:  0.8515625
train loss:  0.3564755618572235
train gradient:  0.28045068853404254
iteration : 4422
train acc:  0.8671875
train loss:  0.3305780291557312
train gradient:  0.26038371883132577
iteration : 4423
train acc:  0.84375
train loss:  0.36877650022506714
train gradient:  0.33558081217450414
iteration : 4424
train acc:  0.859375
train loss:  0.34478217363357544
train gradient:  0.18737882022836538
iteration : 4425
train acc:  0.8125
train loss:  0.4189662039279938
train gradient:  0.4290052537899417
iteration : 4426
train acc:  0.796875
train loss:  0.38351115584373474
train gradient:  0.277779250485298
iteration : 4427
train acc:  0.828125
train loss:  0.41103595495224
train gradient:  0.3299699555910996
iteration : 4428
train acc:  0.7890625
train loss:  0.4783059358596802
train gradient:  0.5654496227537041
iteration : 4429
train acc:  0.890625
train loss:  0.298774391412735
train gradient:  0.1513993563715632
iteration : 4430
train acc:  0.8984375
train loss:  0.2548348903656006
train gradient:  0.1998440996055219
iteration : 4431
train acc:  0.859375
train loss:  0.3449745178222656
train gradient:  0.2214607654671818
iteration : 4432
train acc:  0.7109375
train loss:  0.5941016674041748
train gradient:  0.7466184971777605
iteration : 4433
train acc:  0.875
train loss:  0.32569003105163574
train gradient:  0.3610406812648475
iteration : 4434
train acc:  0.84375
train loss:  0.3406106233596802
train gradient:  0.18924900997031016
iteration : 4435
train acc:  0.859375
train loss:  0.3781875967979431
train gradient:  0.36876240832422824
iteration : 4436
train acc:  0.8125
train loss:  0.41050606966018677
train gradient:  0.27853918474182987
iteration : 4437
train acc:  0.828125
train loss:  0.3507612347602844
train gradient:  0.22540539982770075
iteration : 4438
train acc:  0.8515625
train loss:  0.3345828652381897
train gradient:  0.24874828546977043
iteration : 4439
train acc:  0.8046875
train loss:  0.44071751832962036
train gradient:  0.33720285832387914
iteration : 4440
train acc:  0.8125
train loss:  0.35702770948410034
train gradient:  0.3014425285428786
iteration : 4441
train acc:  0.78125
train loss:  0.4427959620952606
train gradient:  0.32695559520933065
iteration : 4442
train acc:  0.8671875
train loss:  0.34737247228622437
train gradient:  0.18630010072063533
iteration : 4443
train acc:  0.8515625
train loss:  0.29330572485923767
train gradient:  0.21555999763357264
iteration : 4444
train acc:  0.7734375
train loss:  0.4254298806190491
train gradient:  0.2860886093186628
iteration : 4445
train acc:  0.8515625
train loss:  0.320082426071167
train gradient:  0.28975412044621407
iteration : 4446
train acc:  0.8203125
train loss:  0.3438750207424164
train gradient:  0.23042512394172399
iteration : 4447
train acc:  0.859375
train loss:  0.33697509765625
train gradient:  0.19159454099844017
iteration : 4448
train acc:  0.875
train loss:  0.2938096523284912
train gradient:  0.21756928612179544
iteration : 4449
train acc:  0.8203125
train loss:  0.3606491684913635
train gradient:  0.3239639891896095
iteration : 4450
train acc:  0.7890625
train loss:  0.45093435049057007
train gradient:  0.35685532564662564
iteration : 4451
train acc:  0.828125
train loss:  0.398659884929657
train gradient:  0.2257406084210529
iteration : 4452
train acc:  0.8046875
train loss:  0.41953200101852417
train gradient:  0.37456662784576367
iteration : 4453
train acc:  0.8359375
train loss:  0.3800234794616699
train gradient:  0.3307835032242311
iteration : 4454
train acc:  0.8671875
train loss:  0.29894325137138367
train gradient:  0.20018603464120022
iteration : 4455
train acc:  0.8359375
train loss:  0.329537570476532
train gradient:  0.24010669345984126
iteration : 4456
train acc:  0.8359375
train loss:  0.41037750244140625
train gradient:  0.3882834283666115
iteration : 4457
train acc:  0.796875
train loss:  0.39670342206954956
train gradient:  0.3418451216364378
iteration : 4458
train acc:  0.78125
train loss:  0.4386110007762909
train gradient:  0.41059580127506995
iteration : 4459
train acc:  0.8671875
train loss:  0.32607609033584595
train gradient:  0.2124666957273212
iteration : 4460
train acc:  0.8671875
train loss:  0.31125396490097046
train gradient:  0.20067454602366613
iteration : 4461
train acc:  0.8125
train loss:  0.36727041006088257
train gradient:  0.290922898581387
iteration : 4462
train acc:  0.8203125
train loss:  0.43377405405044556
train gradient:  0.38046972729389644
iteration : 4463
train acc:  0.828125
train loss:  0.42536306381225586
train gradient:  0.4139700659275777
iteration : 4464
train acc:  0.8671875
train loss:  0.2817257344722748
train gradient:  0.21296433590750086
iteration : 4465
train acc:  0.8515625
train loss:  0.3407256603240967
train gradient:  0.33100558278161024
iteration : 4466
train acc:  0.859375
train loss:  0.3515772521495819
train gradient:  0.24461316991100301
iteration : 4467
train acc:  0.875
train loss:  0.3417321443557739
train gradient:  0.3126540138140796
iteration : 4468
train acc:  0.796875
train loss:  0.407085657119751
train gradient:  0.3360166707936119
iteration : 4469
train acc:  0.8515625
train loss:  0.33265841007232666
train gradient:  0.1899020921696732
iteration : 4470
train acc:  0.84375
train loss:  0.3812370300292969
train gradient:  0.32471006470532415
iteration : 4471
train acc:  0.828125
train loss:  0.3662688732147217
train gradient:  0.28438037453113585
iteration : 4472
train acc:  0.828125
train loss:  0.44259190559387207
train gradient:  0.37893669917735345
iteration : 4473
train acc:  0.8671875
train loss:  0.35016265511512756
train gradient:  0.26061350210011214
iteration : 4474
train acc:  0.8125
train loss:  0.4024568498134613
train gradient:  0.2426606763638881
iteration : 4475
train acc:  0.7890625
train loss:  0.42939528822898865
train gradient:  0.25055145514694654
iteration : 4476
train acc:  0.7734375
train loss:  0.4804123044013977
train gradient:  0.4956762513847117
iteration : 4477
train acc:  0.828125
train loss:  0.3267613649368286
train gradient:  0.28643616151375323
iteration : 4478
train acc:  0.859375
train loss:  0.3642345666885376
train gradient:  0.35682145567145035
iteration : 4479
train acc:  0.828125
train loss:  0.35363978147506714
train gradient:  0.2591993139202124
iteration : 4480
train acc:  0.8125
train loss:  0.38295018672943115
train gradient:  0.28049641175107104
iteration : 4481
train acc:  0.8203125
train loss:  0.3635108470916748
train gradient:  0.30826574912578664
iteration : 4482
train acc:  0.8046875
train loss:  0.3879493772983551
train gradient:  0.2878366836993206
iteration : 4483
train acc:  0.8359375
train loss:  0.3205380439758301
train gradient:  0.24281069607123
iteration : 4484
train acc:  0.8359375
train loss:  0.3386234939098358
train gradient:  0.17459290024318908
iteration : 4485
train acc:  0.8359375
train loss:  0.3650791049003601
train gradient:  0.29301562105328266
iteration : 4486
train acc:  0.828125
train loss:  0.36859551072120667
train gradient:  0.23598810909705498
iteration : 4487
train acc:  0.8828125
train loss:  0.29804396629333496
train gradient:  0.21646049553820174
iteration : 4488
train acc:  0.859375
train loss:  0.31637895107269287
train gradient:  0.23782947375942157
iteration : 4489
train acc:  0.8125
train loss:  0.3938698172569275
train gradient:  0.40542487044919384
iteration : 4490
train acc:  0.8359375
train loss:  0.32440605759620667
train gradient:  0.2576577580044423
iteration : 4491
train acc:  0.8203125
train loss:  0.3683848977088928
train gradient:  0.24639474505279046
iteration : 4492
train acc:  0.7890625
train loss:  0.4159778952598572
train gradient:  0.3576793922892407
iteration : 4493
train acc:  0.8515625
train loss:  0.4427717328071594
train gradient:  0.31733389297467307
iteration : 4494
train acc:  0.859375
train loss:  0.3163527548313141
train gradient:  0.21685790902596122
iteration : 4495
train acc:  0.859375
train loss:  0.343704491853714
train gradient:  0.27464882062504986
iteration : 4496
train acc:  0.859375
train loss:  0.3437250852584839
train gradient:  0.2504366567501368
iteration : 4497
train acc:  0.828125
train loss:  0.37979280948638916
train gradient:  0.28567893016513346
iteration : 4498
train acc:  0.8125
train loss:  0.36197346448898315
train gradient:  0.24249125415494738
iteration : 4499
train acc:  0.875
train loss:  0.35483330488204956
train gradient:  0.2534082164769558
iteration : 4500
train acc:  0.8046875
train loss:  0.41029924154281616
train gradient:  0.3719869363406019
iteration : 4501
train acc:  0.8515625
train loss:  0.3027888536453247
train gradient:  0.17322700655696877
iteration : 4502
train acc:  0.828125
train loss:  0.3521949350833893
train gradient:  0.20029355952530387
iteration : 4503
train acc:  0.8984375
train loss:  0.2926561236381531
train gradient:  0.20430423793268557
iteration : 4504
train acc:  0.8203125
train loss:  0.3598310053348541
train gradient:  0.22550696061667133
iteration : 4505
train acc:  0.8828125
train loss:  0.29472753405570984
train gradient:  0.19394678587045353
iteration : 4506
train acc:  0.796875
train loss:  0.38605159521102905
train gradient:  0.25425744704790315
iteration : 4507
train acc:  0.8046875
train loss:  0.43470650911331177
train gradient:  0.3549297362491119
iteration : 4508
train acc:  0.84375
train loss:  0.29016655683517456
train gradient:  0.19353966457261654
iteration : 4509
train acc:  0.8203125
train loss:  0.3712692856788635
train gradient:  0.352990294770946
iteration : 4510
train acc:  0.796875
train loss:  0.3990497291088104
train gradient:  0.2971087538513523
iteration : 4511
train acc:  0.78125
train loss:  0.44407135248184204
train gradient:  0.3103593828945075
iteration : 4512
train acc:  0.8828125
train loss:  0.3079039454460144
train gradient:  0.12541542428180114
iteration : 4513
train acc:  0.8125
train loss:  0.3339739441871643
train gradient:  0.24348224361363419
iteration : 4514
train acc:  0.890625
train loss:  0.3410367965698242
train gradient:  0.22967560797379716
iteration : 4515
train acc:  0.796875
train loss:  0.4335925579071045
train gradient:  0.33774056286587395
iteration : 4516
train acc:  0.828125
train loss:  0.3732971251010895
train gradient:  0.22203572931086163
iteration : 4517
train acc:  0.8359375
train loss:  0.36878716945648193
train gradient:  0.30169348825602316
iteration : 4518
train acc:  0.8515625
train loss:  0.40040820837020874
train gradient:  0.40216136221987364
iteration : 4519
train acc:  0.8203125
train loss:  0.35797300934791565
train gradient:  0.2840981512237435
iteration : 4520
train acc:  0.84375
train loss:  0.36724361777305603
train gradient:  0.24122674190358345
iteration : 4521
train acc:  0.84375
train loss:  0.31437110900878906
train gradient:  0.2919819812899811
iteration : 4522
train acc:  0.765625
train loss:  0.4094509482383728
train gradient:  0.48865818651137444
iteration : 4523
train acc:  0.8515625
train loss:  0.358276903629303
train gradient:  0.27205557098727884
iteration : 4524
train acc:  0.8359375
train loss:  0.3078121542930603
train gradient:  0.195893544656279
iteration : 4525
train acc:  0.8203125
train loss:  0.4228687286376953
train gradient:  0.27764188338110996
iteration : 4526
train acc:  0.828125
train loss:  0.3681798577308655
train gradient:  0.2551757522902489
iteration : 4527
train acc:  0.7734375
train loss:  0.4070836901664734
train gradient:  0.3903199833647054
iteration : 4528
train acc:  0.8359375
train loss:  0.4120866656303406
train gradient:  0.392451333053124
iteration : 4529
train acc:  0.8125
train loss:  0.4027227759361267
train gradient:  0.3386776877997663
iteration : 4530
train acc:  0.8203125
train loss:  0.4200780391693115
train gradient:  0.288195326527847
iteration : 4531
train acc:  0.8828125
train loss:  0.2936166226863861
train gradient:  0.20352846756470455
iteration : 4532
train acc:  0.8359375
train loss:  0.3754134178161621
train gradient:  0.3255832025025804
iteration : 4533
train acc:  0.8984375
train loss:  0.26412487030029297
train gradient:  0.12349993430378883
iteration : 4534
train acc:  0.8359375
train loss:  0.3191937804222107
train gradient:  0.14353388953497156
iteration : 4535
train acc:  0.8046875
train loss:  0.4457108974456787
train gradient:  0.41810568387772995
iteration : 4536
train acc:  0.859375
train loss:  0.3705786466598511
train gradient:  0.24609214040732452
iteration : 4537
train acc:  0.84375
train loss:  0.36414194107055664
train gradient:  0.29916964038744237
iteration : 4538
train acc:  0.875
train loss:  0.32780638337135315
train gradient:  0.22260483044308005
iteration : 4539
train acc:  0.8828125
train loss:  0.3557931184768677
train gradient:  0.31633695214779284
iteration : 4540
train acc:  0.828125
train loss:  0.3500995934009552
train gradient:  0.20692718586719025
iteration : 4541
train acc:  0.859375
train loss:  0.30306553840637207
train gradient:  0.18244530168972403
iteration : 4542
train acc:  0.859375
train loss:  0.35277217626571655
train gradient:  0.24648500801803516
iteration : 4543
train acc:  0.84375
train loss:  0.36515331268310547
train gradient:  0.2176886726616623
iteration : 4544
train acc:  0.8203125
train loss:  0.3573567271232605
train gradient:  0.18951650300583908
iteration : 4545
train acc:  0.8359375
train loss:  0.3752667307853699
train gradient:  0.28705546970189066
iteration : 4546
train acc:  0.859375
train loss:  0.35045555233955383
train gradient:  0.25890616178373904
iteration : 4547
train acc:  0.8671875
train loss:  0.3484591245651245
train gradient:  0.257324084915155
iteration : 4548
train acc:  0.8125
train loss:  0.3996228873729706
train gradient:  0.24162629414286818
iteration : 4549
train acc:  0.8125
train loss:  0.36435389518737793
train gradient:  0.23240649889012852
iteration : 4550
train acc:  0.875
train loss:  0.2793530225753784
train gradient:  0.12395270764273945
iteration : 4551
train acc:  0.859375
train loss:  0.2886824905872345
train gradient:  0.18204894922336368
iteration : 4552
train acc:  0.8515625
train loss:  0.3305119574069977
train gradient:  0.1735821316186663
iteration : 4553
train acc:  0.8515625
train loss:  0.3862747251987457
train gradient:  0.2595183484472201
iteration : 4554
train acc:  0.828125
train loss:  0.3629962205886841
train gradient:  0.2311354494542312
iteration : 4555
train acc:  0.828125
train loss:  0.38719964027404785
train gradient:  0.26063925545220784
iteration : 4556
train acc:  0.859375
train loss:  0.30567729473114014
train gradient:  0.19066971002563018
iteration : 4557
train acc:  0.8203125
train loss:  0.35055816173553467
train gradient:  0.29757220069708695
iteration : 4558
train acc:  0.84375
train loss:  0.3656899333000183
train gradient:  0.2554119902229541
iteration : 4559
train acc:  0.84375
train loss:  0.36630043387413025
train gradient:  0.19543802921110956
iteration : 4560
train acc:  0.8515625
train loss:  0.38642436265945435
train gradient:  0.2944589789729201
iteration : 4561
train acc:  0.84375
train loss:  0.36850911378860474
train gradient:  0.2420236095097828
iteration : 4562
train acc:  0.8984375
train loss:  0.2988055348396301
train gradient:  0.17952723359605166
iteration : 4563
train acc:  0.859375
train loss:  0.318738728761673
train gradient:  0.21331581094406785
iteration : 4564
train acc:  0.8203125
train loss:  0.3877212405204773
train gradient:  0.38804656264539283
iteration : 4565
train acc:  0.84375
train loss:  0.3717680871486664
train gradient:  0.25767610646779304
iteration : 4566
train acc:  0.8203125
train loss:  0.37630176544189453
train gradient:  0.4042134372249506
iteration : 4567
train acc:  0.8671875
train loss:  0.29959172010421753
train gradient:  0.2838109240802914
iteration : 4568
train acc:  0.8671875
train loss:  0.30249249935150146
train gradient:  0.22149381389292913
iteration : 4569
train acc:  0.8515625
train loss:  0.3703954815864563
train gradient:  0.26403439927582906
iteration : 4570
train acc:  0.90625
train loss:  0.2735215425491333
train gradient:  0.1668615175224244
iteration : 4571
train acc:  0.7890625
train loss:  0.42288973927497864
train gradient:  0.4398022817589754
iteration : 4572
train acc:  0.875
train loss:  0.3087872564792633
train gradient:  0.18611832201610268
iteration : 4573
train acc:  0.890625
train loss:  0.2693820595741272
train gradient:  0.2730207842465373
iteration : 4574
train acc:  0.8359375
train loss:  0.35406479239463806
train gradient:  0.2371357001716864
iteration : 4575
train acc:  0.875
train loss:  0.3503585755825043
train gradient:  0.33098031272540773
iteration : 4576
train acc:  0.8203125
train loss:  0.33263784646987915
train gradient:  0.24150292343778662
iteration : 4577
train acc:  0.8203125
train loss:  0.3943745195865631
train gradient:  0.2618065413077848
iteration : 4578
train acc:  0.796875
train loss:  0.5241071581840515
train gradient:  0.39868799059066656
iteration : 4579
train acc:  0.8125
train loss:  0.3781830072402954
train gradient:  0.3139881976885079
iteration : 4580
train acc:  0.9140625
train loss:  0.2494613379240036
train gradient:  0.22609133170155438
iteration : 4581
train acc:  0.875
train loss:  0.3416547477245331
train gradient:  0.24680773136675557
iteration : 4582
train acc:  0.828125
train loss:  0.3698052167892456
train gradient:  0.23891848780849861
iteration : 4583
train acc:  0.8515625
train loss:  0.35656481981277466
train gradient:  0.27977312442400726
iteration : 4584
train acc:  0.8125
train loss:  0.38560327887535095
train gradient:  0.32632496521040516
iteration : 4585
train acc:  0.8046875
train loss:  0.4612363874912262
train gradient:  0.38621731339343973
iteration : 4586
train acc:  0.8359375
train loss:  0.42978471517562866
train gradient:  0.2861704478485221
iteration : 4587
train acc:  0.8359375
train loss:  0.3199130594730377
train gradient:  0.2606535510359774
iteration : 4588
train acc:  0.8359375
train loss:  0.404265820980072
train gradient:  0.3590343859497115
iteration : 4589
train acc:  0.84375
train loss:  0.3406188189983368
train gradient:  0.1964510343386524
iteration : 4590
train acc:  0.828125
train loss:  0.3590962886810303
train gradient:  0.3057059481357744
iteration : 4591
train acc:  0.859375
train loss:  0.38420337438583374
train gradient:  0.2866225498993933
iteration : 4592
train acc:  0.828125
train loss:  0.4507121741771698
train gradient:  0.42156382563424466
iteration : 4593
train acc:  0.8515625
train loss:  0.32042646408081055
train gradient:  0.23061663289557782
iteration : 4594
train acc:  0.796875
train loss:  0.5028223991394043
train gradient:  0.5801037618408396
iteration : 4595
train acc:  0.890625
train loss:  0.3445626199245453
train gradient:  0.2609843210183076
iteration : 4596
train acc:  0.890625
train loss:  0.3175920248031616
train gradient:  0.37930008988534025
iteration : 4597
train acc:  0.859375
train loss:  0.3485568165779114
train gradient:  0.19917423841174714
iteration : 4598
train acc:  0.8203125
train loss:  0.4460088312625885
train gradient:  0.4214511536762613
iteration : 4599
train acc:  0.8046875
train loss:  0.43705177307128906
train gradient:  0.37180694763365196
iteration : 4600
train acc:  0.828125
train loss:  0.37874406576156616
train gradient:  0.3128153948979103
iteration : 4601
train acc:  0.859375
train loss:  0.33680930733680725
train gradient:  0.27639355950673566
iteration : 4602
train acc:  0.828125
train loss:  0.3779873251914978
train gradient:  0.2753204578102815
iteration : 4603
train acc:  0.890625
train loss:  0.2978302836418152
train gradient:  0.19697331129880066
iteration : 4604
train acc:  0.8359375
train loss:  0.3708098232746124
train gradient:  0.3053095311868961
iteration : 4605
train acc:  0.8125
train loss:  0.4049631655216217
train gradient:  0.3928931867550289
iteration : 4606
train acc:  0.859375
train loss:  0.32861047983169556
train gradient:  0.33331476092092965
iteration : 4607
train acc:  0.8828125
train loss:  0.3360297977924347
train gradient:  0.2380482578855864
iteration : 4608
train acc:  0.84375
train loss:  0.3361284136772156
train gradient:  0.19023173764215343
iteration : 4609
train acc:  0.9140625
train loss:  0.30607569217681885
train gradient:  0.21331487580091874
iteration : 4610
train acc:  0.875
train loss:  0.3148738145828247
train gradient:  0.22990195618266165
iteration : 4611
train acc:  0.828125
train loss:  0.37210339307785034
train gradient:  0.305640397939864
iteration : 4612
train acc:  0.8046875
train loss:  0.38078123331069946
train gradient:  0.4100626043600063
iteration : 4613
train acc:  0.84375
train loss:  0.3789527118206024
train gradient:  0.334168832462019
iteration : 4614
train acc:  0.875
train loss:  0.4268140196800232
train gradient:  0.29277092346954553
iteration : 4615
train acc:  0.78125
train loss:  0.35517412424087524
train gradient:  0.2813683365246329
iteration : 4616
train acc:  0.8515625
train loss:  0.3300371766090393
train gradient:  0.19851900237856648
iteration : 4617
train acc:  0.8046875
train loss:  0.41663646697998047
train gradient:  0.3226725344548252
iteration : 4618
train acc:  0.890625
train loss:  0.3191664218902588
train gradient:  0.21897178183089058
iteration : 4619
train acc:  0.8203125
train loss:  0.36335867643356323
train gradient:  0.3925925624989073
iteration : 4620
train acc:  0.859375
train loss:  0.34974586963653564
train gradient:  0.23264546600825234
iteration : 4621
train acc:  0.78125
train loss:  0.43684035539627075
train gradient:  0.3232960525875792
iteration : 4622
train acc:  0.796875
train loss:  0.3991583585739136
train gradient:  0.26538541662875836
iteration : 4623
train acc:  0.8046875
train loss:  0.40552157163619995
train gradient:  0.28103969196367795
iteration : 4624
train acc:  0.8359375
train loss:  0.35674309730529785
train gradient:  0.3283326198233602
iteration : 4625
train acc:  0.875
train loss:  0.2844296097755432
train gradient:  0.19686193344056654
iteration : 4626
train acc:  0.859375
train loss:  0.29274049401283264
train gradient:  0.29556473535715994
iteration : 4627
train acc:  0.8515625
train loss:  0.36324694752693176
train gradient:  0.2942655123628606
iteration : 4628
train acc:  0.859375
train loss:  0.34233564138412476
train gradient:  0.24579684647864344
iteration : 4629
train acc:  0.8359375
train loss:  0.3473213315010071
train gradient:  0.21140754856150573
iteration : 4630
train acc:  0.828125
train loss:  0.35269778966903687
train gradient:  0.24008981060507073
iteration : 4631
train acc:  0.8046875
train loss:  0.41535210609436035
train gradient:  0.44972720506812963
iteration : 4632
train acc:  0.859375
train loss:  0.2949548065662384
train gradient:  0.1690872955361889
iteration : 4633
train acc:  0.9296875
train loss:  0.23857173323631287
train gradient:  0.16031884842314917
iteration : 4634
train acc:  0.8203125
train loss:  0.3779868185520172
train gradient:  0.27394921295802915
iteration : 4635
train acc:  0.859375
train loss:  0.3464061915874481
train gradient:  0.3128764068267638
iteration : 4636
train acc:  0.8046875
train loss:  0.39958685636520386
train gradient:  0.26714567933527394
iteration : 4637
train acc:  0.84375
train loss:  0.3764834702014923
train gradient:  0.268347185842147
iteration : 4638
train acc:  0.8671875
train loss:  0.32220616936683655
train gradient:  0.17709736308915597
iteration : 4639
train acc:  0.8125
train loss:  0.3877180814743042
train gradient:  0.2643635291735925
iteration : 4640
train acc:  0.90625
train loss:  0.29760217666625977
train gradient:  0.19327211685622903
iteration : 4641
train acc:  0.8515625
train loss:  0.431570440530777
train gradient:  0.34243000367524024
iteration : 4642
train acc:  0.8671875
train loss:  0.3108755648136139
train gradient:  0.17284842480487733
iteration : 4643
train acc:  0.8828125
train loss:  0.3295624554157257
train gradient:  0.20354553571315237
iteration : 4644
train acc:  0.828125
train loss:  0.32675790786743164
train gradient:  0.31363383014184937
iteration : 4645
train acc:  0.84375
train loss:  0.30830997228622437
train gradient:  0.2060938167262049
iteration : 4646
train acc:  0.8984375
train loss:  0.3530023694038391
train gradient:  0.3167964495848221
iteration : 4647
train acc:  0.8046875
train loss:  0.390052855014801
train gradient:  0.27325889075507376
iteration : 4648
train acc:  0.859375
train loss:  0.3642175495624542
train gradient:  0.22475198578223982
iteration : 4649
train acc:  0.8828125
train loss:  0.2891574501991272
train gradient:  0.1456519060710182
iteration : 4650
train acc:  0.828125
train loss:  0.35554128885269165
train gradient:  0.23655591829540623
iteration : 4651
train acc:  0.859375
train loss:  0.3169189393520355
train gradient:  0.2202573139923193
iteration : 4652
train acc:  0.828125
train loss:  0.3421415090560913
train gradient:  0.25136212661806684
iteration : 4653
train acc:  0.8125
train loss:  0.35042232275009155
train gradient:  0.43263846472827977
iteration : 4654
train acc:  0.875
train loss:  0.40094053745269775
train gradient:  0.25469485183257484
iteration : 4655
train acc:  0.84375
train loss:  0.3400443196296692
train gradient:  0.26537903181100403
iteration : 4656
train acc:  0.859375
train loss:  0.30707287788391113
train gradient:  0.1666308027240139
iteration : 4657
train acc:  0.796875
train loss:  0.4551166892051697
train gradient:  0.40293704614512393
iteration : 4658
train acc:  0.890625
train loss:  0.2639104425907135
train gradient:  0.18915400824834183
iteration : 4659
train acc:  0.859375
train loss:  0.4033249616622925
train gradient:  0.36113904659592705
iteration : 4660
train acc:  0.828125
train loss:  0.36304670572280884
train gradient:  0.1789012288495062
iteration : 4661
train acc:  0.875
train loss:  0.3278016448020935
train gradient:  0.16008034197421683
iteration : 4662
train acc:  0.859375
train loss:  0.328677237033844
train gradient:  0.20142874857590254
iteration : 4663
train acc:  0.9140625
train loss:  0.24731767177581787
train gradient:  0.13778595719907427
iteration : 4664
train acc:  0.828125
train loss:  0.3283810019493103
train gradient:  0.25734967216478305
iteration : 4665
train acc:  0.8515625
train loss:  0.3671637773513794
train gradient:  0.3245328211022348
iteration : 4666
train acc:  0.859375
train loss:  0.35424983501434326
train gradient:  0.24749309863500135
iteration : 4667
train acc:  0.84375
train loss:  0.34725630283355713
train gradient:  0.27295406611435413
iteration : 4668
train acc:  0.828125
train loss:  0.3674975633621216
train gradient:  0.2734233645095934
iteration : 4669
train acc:  0.7890625
train loss:  0.39847320318222046
train gradient:  0.30213587266955205
iteration : 4670
train acc:  0.796875
train loss:  0.48054173588752747
train gradient:  0.4408194647696517
iteration : 4671
train acc:  0.8359375
train loss:  0.42808789014816284
train gradient:  0.44788745674596653
iteration : 4672
train acc:  0.84375
train loss:  0.35893458127975464
train gradient:  0.3074009807586033
iteration : 4673
train acc:  0.8515625
train loss:  0.3530299961566925
train gradient:  0.32724716510884055
iteration : 4674
train acc:  0.8359375
train loss:  0.35141217708587646
train gradient:  0.22506712886774932
iteration : 4675
train acc:  0.890625
train loss:  0.27211469411849976
train gradient:  0.13083378482045038
iteration : 4676
train acc:  0.8046875
train loss:  0.4060259461402893
train gradient:  0.23255688302579589
iteration : 4677
train acc:  0.84375
train loss:  0.31912630796432495
train gradient:  0.25932345278962626
iteration : 4678
train acc:  0.8359375
train loss:  0.3381458520889282
train gradient:  0.22073283365266572
iteration : 4679
train acc:  0.9140625
train loss:  0.30247026681900024
train gradient:  0.2155928251787574
iteration : 4680
train acc:  0.84375
train loss:  0.3784709870815277
train gradient:  0.2760271440584884
iteration : 4681
train acc:  0.796875
train loss:  0.37557172775268555
train gradient:  0.2877669874771245
iteration : 4682
train acc:  0.828125
train loss:  0.4010259509086609
train gradient:  0.3586962937185436
iteration : 4683
train acc:  0.8203125
train loss:  0.37056446075439453
train gradient:  0.2868255871608467
iteration : 4684
train acc:  0.8046875
train loss:  0.4008069932460785
train gradient:  0.3315433394354191
iteration : 4685
train acc:  0.8984375
train loss:  0.2927635908126831
train gradient:  0.14026166761003417
iteration : 4686
train acc:  0.8671875
train loss:  0.3449590802192688
train gradient:  0.23154249606993696
iteration : 4687
train acc:  0.8671875
train loss:  0.3073476552963257
train gradient:  0.21923168074053806
iteration : 4688
train acc:  0.8203125
train loss:  0.3528427481651306
train gradient:  0.41935627528011094
iteration : 4689
train acc:  0.8359375
train loss:  0.33052486181259155
train gradient:  0.3182241896512174
iteration : 4690
train acc:  0.828125
train loss:  0.37340131402015686
train gradient:  0.23272555748203066
iteration : 4691
train acc:  0.8515625
train loss:  0.3315306603908539
train gradient:  0.2252144701885346
iteration : 4692
train acc:  0.828125
train loss:  0.3241422772407532
train gradient:  0.21327321136651026
iteration : 4693
train acc:  0.859375
train loss:  0.29660287499427795
train gradient:  0.2701139640659268
iteration : 4694
train acc:  0.8671875
train loss:  0.3644498586654663
train gradient:  0.2699426474927039
iteration : 4695
train acc:  0.8515625
train loss:  0.35542112588882446
train gradient:  0.2512333360816226
iteration : 4696
train acc:  0.84375
train loss:  0.318797767162323
train gradient:  0.26635770539295045
iteration : 4697
train acc:  0.765625
train loss:  0.4868253469467163
train gradient:  0.46936526190183187
iteration : 4698
train acc:  0.8125
train loss:  0.35750705003738403
train gradient:  0.26988031408699004
iteration : 4699
train acc:  0.8515625
train loss:  0.32424646615982056
train gradient:  0.25014053839337164
iteration : 4700
train acc:  0.7578125
train loss:  0.5371850728988647
train gradient:  0.5241250237782351
iteration : 4701
train acc:  0.8984375
train loss:  0.29683712124824524
train gradient:  0.18379698534922576
iteration : 4702
train acc:  0.8359375
train loss:  0.34038299322128296
train gradient:  0.36480312782966784
iteration : 4703
train acc:  0.8125
train loss:  0.4583069086074829
train gradient:  0.46864601263540456
iteration : 4704
train acc:  0.828125
train loss:  0.3630428612232208
train gradient:  0.2331314871741797
iteration : 4705
train acc:  0.7890625
train loss:  0.45569849014282227
train gradient:  0.34708558168409104
iteration : 4706
train acc:  0.7734375
train loss:  0.43082481622695923
train gradient:  0.39774362910515965
iteration : 4707
train acc:  0.8203125
train loss:  0.4478296637535095
train gradient:  0.43744401579900316
iteration : 4708
train acc:  0.859375
train loss:  0.319598913192749
train gradient:  0.40133477458969263
iteration : 4709
train acc:  0.890625
train loss:  0.31568607687950134
train gradient:  0.19681974107503056
iteration : 4710
train acc:  0.875
train loss:  0.3375057578086853
train gradient:  0.215770992350877
iteration : 4711
train acc:  0.921875
train loss:  0.3071337938308716
train gradient:  0.18764025963767825
iteration : 4712
train acc:  0.875
train loss:  0.30920594930648804
train gradient:  0.20117594435549213
iteration : 4713
train acc:  0.796875
train loss:  0.4296105206012726
train gradient:  0.31855491837690875
iteration : 4714
train acc:  0.828125
train loss:  0.338070273399353
train gradient:  0.21729787558448954
iteration : 4715
train acc:  0.796875
train loss:  0.36487874388694763
train gradient:  0.2875990519528504
iteration : 4716
train acc:  0.828125
train loss:  0.3891910910606384
train gradient:  0.24538656836697934
iteration : 4717
train acc:  0.8984375
train loss:  0.2938995361328125
train gradient:  0.21763015535739968
iteration : 4718
train acc:  0.8515625
train loss:  0.3639433681964874
train gradient:  0.2201796442841935
iteration : 4719
train acc:  0.828125
train loss:  0.37610146403312683
train gradient:  0.1971053729129216
iteration : 4720
train acc:  0.8203125
train loss:  0.45108330249786377
train gradient:  0.32062927824729076
iteration : 4721
train acc:  0.8671875
train loss:  0.3202968239784241
train gradient:  0.2672964802993817
iteration : 4722
train acc:  0.8125
train loss:  0.3587169647216797
train gradient:  0.2203461170736537
iteration : 4723
train acc:  0.84375
train loss:  0.36840367317199707
train gradient:  0.2609694398521378
iteration : 4724
train acc:  0.8671875
train loss:  0.38320404291152954
train gradient:  0.3012572296755591
iteration : 4725
train acc:  0.84375
train loss:  0.3626159429550171
train gradient:  0.1772807324236417
iteration : 4726
train acc:  0.84375
train loss:  0.3629051446914673
train gradient:  0.30516413137164555
iteration : 4727
train acc:  0.8515625
train loss:  0.32292941212654114
train gradient:  0.19823106607147545
iteration : 4728
train acc:  0.8125
train loss:  0.38499099016189575
train gradient:  0.27135517873486603
iteration : 4729
train acc:  0.8515625
train loss:  0.28213250637054443
train gradient:  0.1941882426569475
iteration : 4730
train acc:  0.8671875
train loss:  0.33062446117401123
train gradient:  0.2601745873956875
iteration : 4731
train acc:  0.875
train loss:  0.35834717750549316
train gradient:  0.2465751297426098
iteration : 4732
train acc:  0.8203125
train loss:  0.3770791292190552
train gradient:  0.25709119700540184
iteration : 4733
train acc:  0.859375
train loss:  0.31611716747283936
train gradient:  0.16518013512641078
iteration : 4734
train acc:  0.8515625
train loss:  0.3688560724258423
train gradient:  0.3047327705935107
iteration : 4735
train acc:  0.828125
train loss:  0.3693947196006775
train gradient:  0.21671469001579624
iteration : 4736
train acc:  0.875
train loss:  0.31662043929100037
train gradient:  0.17434651545810032
iteration : 4737
train acc:  0.84375
train loss:  0.3487980365753174
train gradient:  0.17613127731551675
iteration : 4738
train acc:  0.796875
train loss:  0.39145493507385254
train gradient:  0.33765090162020867
iteration : 4739
train acc:  0.84375
train loss:  0.33011943101882935
train gradient:  0.24165849904302605
iteration : 4740
train acc:  0.8515625
train loss:  0.32058185338974
train gradient:  0.26267302597026404
iteration : 4741
train acc:  0.796875
train loss:  0.365278959274292
train gradient:  0.24816118629394313
iteration : 4742
train acc:  0.78125
train loss:  0.545038104057312
train gradient:  0.531798690251395
iteration : 4743
train acc:  0.8046875
train loss:  0.4323537349700928
train gradient:  0.36114844848558664
iteration : 4744
train acc:  0.828125
train loss:  0.36001914739608765
train gradient:  0.21673478548309158
iteration : 4745
train acc:  0.8125
train loss:  0.3799434304237366
train gradient:  0.25290675398100565
iteration : 4746
train acc:  0.8671875
train loss:  0.3159526586532593
train gradient:  0.20201230957516395
iteration : 4747
train acc:  0.890625
train loss:  0.28717291355133057
train gradient:  0.19212745220843136
iteration : 4748
train acc:  0.875
train loss:  0.32784536480903625
train gradient:  0.23747356221725321
iteration : 4749
train acc:  0.8203125
train loss:  0.4005649983882904
train gradient:  0.40887829150274696
iteration : 4750
train acc:  0.8359375
train loss:  0.3298862874507904
train gradient:  0.22720697154129232
iteration : 4751
train acc:  0.8828125
train loss:  0.3393670320510864
train gradient:  0.2432840445751921
iteration : 4752
train acc:  0.8359375
train loss:  0.3955543637275696
train gradient:  0.2812076851133511
iteration : 4753
train acc:  0.8125
train loss:  0.40396246314048767
train gradient:  0.3075343483024587
iteration : 4754
train acc:  0.8671875
train loss:  0.3319803476333618
train gradient:  0.18840137726697434
iteration : 4755
train acc:  0.9375
train loss:  0.24933229386806488
train gradient:  0.18117083357308
iteration : 4756
train acc:  0.8125
train loss:  0.43620383739471436
train gradient:  0.3335604140963362
iteration : 4757
train acc:  0.8515625
train loss:  0.36322563886642456
train gradient:  0.3103261687266284
iteration : 4758
train acc:  0.8515625
train loss:  0.3551149368286133
train gradient:  0.22829020279463896
iteration : 4759
train acc:  0.8359375
train loss:  0.3881606459617615
train gradient:  0.20466147796984435
iteration : 4760
train acc:  0.8359375
train loss:  0.3503050208091736
train gradient:  0.2530986851618637
iteration : 4761
train acc:  0.828125
train loss:  0.37559211254119873
train gradient:  0.19253254965324046
iteration : 4762
train acc:  0.8046875
train loss:  0.3736925721168518
train gradient:  0.23616276312295925
iteration : 4763
train acc:  0.859375
train loss:  0.31906071305274963
train gradient:  0.1738879405022622
iteration : 4764
train acc:  0.8515625
train loss:  0.36098945140838623
train gradient:  0.2023249818202712
iteration : 4765
train acc:  0.8359375
train loss:  0.32375073432922363
train gradient:  0.2380489409485453
iteration : 4766
train acc:  0.8046875
train loss:  0.41067683696746826
train gradient:  0.28684006326123546
iteration : 4767
train acc:  0.8359375
train loss:  0.42681944370269775
train gradient:  0.508665317176042
iteration : 4768
train acc:  0.890625
train loss:  0.2613966763019562
train gradient:  0.16688183509383367
iteration : 4769
train acc:  0.8125
train loss:  0.37432438135147095
train gradient:  0.24648972841089445
iteration : 4770
train acc:  0.84375
train loss:  0.30114367604255676
train gradient:  0.17787074368105535
iteration : 4771
train acc:  0.8671875
train loss:  0.34699514508247375
train gradient:  0.35700616920280626
iteration : 4772
train acc:  0.875
train loss:  0.2883104681968689
train gradient:  0.1920047847724945
iteration : 4773
train acc:  0.8828125
train loss:  0.27065014839172363
train gradient:  0.13954279054979257
iteration : 4774
train acc:  0.8125
train loss:  0.39524298906326294
train gradient:  0.3198557899286344
iteration : 4775
train acc:  0.875
train loss:  0.3198677897453308
train gradient:  0.26860337417875635
iteration : 4776
train acc:  0.8203125
train loss:  0.3840467631816864
train gradient:  0.18550167394219394
iteration : 4777
train acc:  0.8359375
train loss:  0.31869882345199585
train gradient:  0.2664463477440516
iteration : 4778
train acc:  0.828125
train loss:  0.37193936109542847
train gradient:  0.23750886985743014
iteration : 4779
train acc:  0.8359375
train loss:  0.39248937368392944
train gradient:  0.22137595395607523
iteration : 4780
train acc:  0.78125
train loss:  0.47636258602142334
train gradient:  0.32158232788187996
iteration : 4781
train acc:  0.84375
train loss:  0.39686745405197144
train gradient:  0.2498188907301418
iteration : 4782
train acc:  0.828125
train loss:  0.3621130883693695
train gradient:  0.2760238696056514
iteration : 4783
train acc:  0.90625
train loss:  0.2648635506629944
train gradient:  0.14546630297293273
iteration : 4784
train acc:  0.84375
train loss:  0.34876492619514465
train gradient:  0.21003966357549422
iteration : 4785
train acc:  0.8515625
train loss:  0.36543893814086914
train gradient:  0.24088593760747218
iteration : 4786
train acc:  0.828125
train loss:  0.36065924167633057
train gradient:  0.26684274724718726
iteration : 4787
train acc:  0.84375
train loss:  0.3520222306251526
train gradient:  0.25972644745357
iteration : 4788
train acc:  0.84375
train loss:  0.363959938287735
train gradient:  0.2590424574082888
iteration : 4789
train acc:  0.796875
train loss:  0.4017380177974701
train gradient:  0.3218060382877708
iteration : 4790
train acc:  0.8671875
train loss:  0.3220236003398895
train gradient:  0.22829128315308536
iteration : 4791
train acc:  0.859375
train loss:  0.3421519696712494
train gradient:  0.260729756722008
iteration : 4792
train acc:  0.765625
train loss:  0.52140212059021
train gradient:  0.46652838940186103
iteration : 4793
train acc:  0.890625
train loss:  0.30035632848739624
train gradient:  0.20733786320598552
iteration : 4794
train acc:  0.828125
train loss:  0.35373446345329285
train gradient:  0.2429589509781821
iteration : 4795
train acc:  0.8359375
train loss:  0.36635732650756836
train gradient:  0.29734460668112617
iteration : 4796
train acc:  0.84375
train loss:  0.36370980739593506
train gradient:  0.2443865814473996
iteration : 4797
train acc:  0.8125
train loss:  0.36634787917137146
train gradient:  0.22321540318352576
iteration : 4798
train acc:  0.921875
train loss:  0.25734490156173706
train gradient:  0.20033905735693994
iteration : 4799
train acc:  0.78125
train loss:  0.4453463852405548
train gradient:  0.33129839156636476
iteration : 4800
train acc:  0.8203125
train loss:  0.344726026058197
train gradient:  0.23773449174677036
iteration : 4801
train acc:  0.8515625
train loss:  0.3301098346710205
train gradient:  0.27122369348416464
iteration : 4802
train acc:  0.859375
train loss:  0.3510245680809021
train gradient:  0.2771743209533005
iteration : 4803
train acc:  0.8125
train loss:  0.34638088941574097
train gradient:  0.19701132754723824
iteration : 4804
train acc:  0.78125
train loss:  0.3962273597717285
train gradient:  0.43580635215618047
iteration : 4805
train acc:  0.8046875
train loss:  0.45665672421455383
train gradient:  0.3782588041433558
iteration : 4806
train acc:  0.90625
train loss:  0.3088345229625702
train gradient:  0.18525533317101328
iteration : 4807
train acc:  0.8125
train loss:  0.39734572172164917
train gradient:  0.2086861011870409
iteration : 4808
train acc:  0.7890625
train loss:  0.3993636965751648
train gradient:  0.29889164527668366
iteration : 4809
train acc:  0.8828125
train loss:  0.27779340744018555
train gradient:  0.17322356894367394
iteration : 4810
train acc:  0.84375
train loss:  0.3666943609714508
train gradient:  0.210318011286808
iteration : 4811
train acc:  0.84375
train loss:  0.38710683584213257
train gradient:  0.22987552893269037
iteration : 4812
train acc:  0.8125
train loss:  0.38369685411453247
train gradient:  0.3356979206652888
iteration : 4813
train acc:  0.8125
train loss:  0.3572580814361572
train gradient:  0.22424684760878344
iteration : 4814
train acc:  0.8125
train loss:  0.39559900760650635
train gradient:  0.2937372708935551
iteration : 4815
train acc:  0.8203125
train loss:  0.3925793766975403
train gradient:  0.45678186420868594
iteration : 4816
train acc:  0.8359375
train loss:  0.35889339447021484
train gradient:  0.22948964729914845
iteration : 4817
train acc:  0.8515625
train loss:  0.365641713142395
train gradient:  0.16791735864820526
iteration : 4818
train acc:  0.84375
train loss:  0.328136682510376
train gradient:  0.2189394782511661
iteration : 4819
train acc:  0.890625
train loss:  0.32495826482772827
train gradient:  0.1809598091160101
iteration : 4820
train acc:  0.859375
train loss:  0.31471627950668335
train gradient:  0.31439855481177015
iteration : 4821
train acc:  0.84375
train loss:  0.3092470169067383
train gradient:  0.17727774455093448
iteration : 4822
train acc:  0.7890625
train loss:  0.38752222061157227
train gradient:  0.24411264360849008
iteration : 4823
train acc:  0.8671875
train loss:  0.3286057710647583
train gradient:  0.14546159247670526
iteration : 4824
train acc:  0.8515625
train loss:  0.3621731400489807
train gradient:  0.23652608786445434
iteration : 4825
train acc:  0.8125
train loss:  0.3872174620628357
train gradient:  0.27326468568412554
iteration : 4826
train acc:  0.859375
train loss:  0.35836517810821533
train gradient:  0.17759356996104303
iteration : 4827
train acc:  0.8515625
train loss:  0.3330739140510559
train gradient:  0.171864214128358
iteration : 4828
train acc:  0.8359375
train loss:  0.3755505383014679
train gradient:  0.2204255536118696
iteration : 4829
train acc:  0.828125
train loss:  0.35249173641204834
train gradient:  0.1989262663561967
iteration : 4830
train acc:  0.796875
train loss:  0.36193427443504333
train gradient:  0.21478574402262052
iteration : 4831
train acc:  0.875
train loss:  0.36212074756622314
train gradient:  0.19439882647783366
iteration : 4832
train acc:  0.8203125
train loss:  0.3838101029396057
train gradient:  0.20744240399090494
iteration : 4833
train acc:  0.7890625
train loss:  0.4103612005710602
train gradient:  0.34309725619277065
iteration : 4834
train acc:  0.8515625
train loss:  0.29536890983581543
train gradient:  0.15751802100476867
iteration : 4835
train acc:  0.828125
train loss:  0.38902562856674194
train gradient:  0.40129868649715683
iteration : 4836
train acc:  0.859375
train loss:  0.35442304611206055
train gradient:  0.20061179840083143
iteration : 4837
train acc:  0.8671875
train loss:  0.3399563133716583
train gradient:  0.4188051884757133
iteration : 4838
train acc:  0.828125
train loss:  0.37380993366241455
train gradient:  0.3587578549991864
iteration : 4839
train acc:  0.828125
train loss:  0.3434458076953888
train gradient:  0.247444674795751
iteration : 4840
train acc:  0.875
train loss:  0.327700138092041
train gradient:  0.17714104955622448
iteration : 4841
train acc:  0.8359375
train loss:  0.43339353799819946
train gradient:  0.30914322010590883
iteration : 4842
train acc:  0.8515625
train loss:  0.3148449659347534
train gradient:  0.19150032577012654
iteration : 4843
train acc:  0.828125
train loss:  0.4107022285461426
train gradient:  0.33489912034694025
iteration : 4844
train acc:  0.859375
train loss:  0.4137318730354309
train gradient:  0.21462646897454263
iteration : 4845
train acc:  0.8125
train loss:  0.36093422770500183
train gradient:  0.2463603238432653
iteration : 4846
train acc:  0.8359375
train loss:  0.3674735426902771
train gradient:  0.24245527594312538
iteration : 4847
train acc:  0.8515625
train loss:  0.35530826449394226
train gradient:  0.22046629258274889
iteration : 4848
train acc:  0.7890625
train loss:  0.39358827471733093
train gradient:  0.2539607880483054
iteration : 4849
train acc:  0.8359375
train loss:  0.4349524676799774
train gradient:  0.4076272729104976
iteration : 4850
train acc:  0.8828125
train loss:  0.2643499970436096
train gradient:  0.13460632521377175
iteration : 4851
train acc:  0.8515625
train loss:  0.3719887137413025
train gradient:  0.21289583934814366
iteration : 4852
train acc:  0.796875
train loss:  0.4228675961494446
train gradient:  0.29386751682749657
iteration : 4853
train acc:  0.875
train loss:  0.35554811358451843
train gradient:  0.1785667857421476
iteration : 4854
train acc:  0.8125
train loss:  0.42612332105636597
train gradient:  0.37726451621417373
iteration : 4855
train acc:  0.90625
train loss:  0.2582818865776062
train gradient:  0.13576029663466846
iteration : 4856
train acc:  0.8203125
train loss:  0.39141595363616943
train gradient:  0.2354944758387734
iteration : 4857
train acc:  0.859375
train loss:  0.34622180461883545
train gradient:  0.26038243051604315
iteration : 4858
train acc:  0.8359375
train loss:  0.3922329246997833
train gradient:  0.5030173388535766
iteration : 4859
train acc:  0.8359375
train loss:  0.34152331948280334
train gradient:  0.19032994789125646
iteration : 4860
train acc:  0.828125
train loss:  0.3599192202091217
train gradient:  0.40186590437449177
iteration : 4861
train acc:  0.84375
train loss:  0.37211039662361145
train gradient:  0.3487053818761719
iteration : 4862
train acc:  0.8203125
train loss:  0.3747391104698181
train gradient:  0.28930314440906907
iteration : 4863
train acc:  0.8046875
train loss:  0.3994406461715698
train gradient:  0.28502807471955655
iteration : 4864
train acc:  0.7734375
train loss:  0.43213754892349243
train gradient:  0.32552277489014086
iteration : 4865
train acc:  0.7890625
train loss:  0.3937014937400818
train gradient:  0.30754211296033035
iteration : 4866
train acc:  0.890625
train loss:  0.32132619619369507
train gradient:  0.1776736985371324
iteration : 4867
train acc:  0.8671875
train loss:  0.3423738479614258
train gradient:  0.19983010904291754
iteration : 4868
train acc:  0.890625
train loss:  0.2929933965206146
train gradient:  0.21894280340269862
iteration : 4869
train acc:  0.8203125
train loss:  0.35512232780456543
train gradient:  0.26714610845310577
iteration : 4870
train acc:  0.8125
train loss:  0.37557387351989746
train gradient:  0.25534766317893504
iteration : 4871
train acc:  0.8984375
train loss:  0.2774893343448639
train gradient:  0.1607100300864112
iteration : 4872
train acc:  0.8359375
train loss:  0.33976495265960693
train gradient:  0.24877207774939558
iteration : 4873
train acc:  0.84375
train loss:  0.38138437271118164
train gradient:  0.3418739929388339
iteration : 4874
train acc:  0.8046875
train loss:  0.39013800024986267
train gradient:  0.2647812946741743
iteration : 4875
train acc:  0.7734375
train loss:  0.44266626238822937
train gradient:  0.42196167046994715
iteration : 4876
train acc:  0.8359375
train loss:  0.3585158586502075
train gradient:  0.34053030458918027
iteration : 4877
train acc:  0.84375
train loss:  0.39164674282073975
train gradient:  0.3967173864124813
iteration : 4878
train acc:  0.8359375
train loss:  0.34125036001205444
train gradient:  0.25016558605322914
iteration : 4879
train acc:  0.8515625
train loss:  0.34103894233703613
train gradient:  0.22231703023601732
iteration : 4880
train acc:  0.8125
train loss:  0.3442993462085724
train gradient:  0.22720711203526636
iteration : 4881
train acc:  0.859375
train loss:  0.3479401469230652
train gradient:  0.21819841095710388
iteration : 4882
train acc:  0.8515625
train loss:  0.3066422939300537
train gradient:  0.240239359675059
iteration : 4883
train acc:  0.8671875
train loss:  0.2782468795776367
train gradient:  0.14580539049017716
iteration : 4884
train acc:  0.875
train loss:  0.31663763523101807
train gradient:  0.31333080193482155
iteration : 4885
train acc:  0.859375
train loss:  0.30970075726509094
train gradient:  0.19166536968138614
iteration : 4886
train acc:  0.78125
train loss:  0.43221864104270935
train gradient:  0.32548938603995425
iteration : 4887
train acc:  0.859375
train loss:  0.3458455204963684
train gradient:  0.2707059854459231
iteration : 4888
train acc:  0.8359375
train loss:  0.33844733238220215
train gradient:  0.26102187078562816
iteration : 4889
train acc:  0.8046875
train loss:  0.41954684257507324
train gradient:  0.35256190040343677
iteration : 4890
train acc:  0.8046875
train loss:  0.4293988347053528
train gradient:  0.45144861022719773
iteration : 4891
train acc:  0.8046875
train loss:  0.437760591506958
train gradient:  0.3914996077595903
iteration : 4892
train acc:  0.875
train loss:  0.3093383312225342
train gradient:  0.18070623746619074
iteration : 4893
train acc:  0.875
train loss:  0.32569873332977295
train gradient:  0.2490797637608739
iteration : 4894
train acc:  0.7578125
train loss:  0.4992230534553528
train gradient:  0.41548224070884504
iteration : 4895
train acc:  0.8671875
train loss:  0.2958448529243469
train gradient:  0.17306179820450018
iteration : 4896
train acc:  0.828125
train loss:  0.39503204822540283
train gradient:  0.3539673120593955
iteration : 4897
train acc:  0.8671875
train loss:  0.34603095054626465
train gradient:  0.2526224715737646
iteration : 4898
train acc:  0.890625
train loss:  0.3394748568534851
train gradient:  0.2569347468515712
iteration : 4899
train acc:  0.765625
train loss:  0.45929235219955444
train gradient:  0.4512225561418083
iteration : 4900
train acc:  0.84375
train loss:  0.3321535289287567
train gradient:  0.19408262541422613
iteration : 4901
train acc:  0.8359375
train loss:  0.40258872509002686
train gradient:  0.3688271878839135
iteration : 4902
train acc:  0.84375
train loss:  0.425503134727478
train gradient:  0.30296501361350175
iteration : 4903
train acc:  0.8671875
train loss:  0.30863264203071594
train gradient:  0.18985217611737393
iteration : 4904
train acc:  0.9140625
train loss:  0.2773618996143341
train gradient:  0.17604912465185374
iteration : 4905
train acc:  0.8203125
train loss:  0.38119202852249146
train gradient:  0.19093443588150744
iteration : 4906
train acc:  0.8828125
train loss:  0.2970393896102905
train gradient:  0.24004385307264187
iteration : 4907
train acc:  0.9140625
train loss:  0.26685065031051636
train gradient:  0.16156487141515946
iteration : 4908
train acc:  0.859375
train loss:  0.4084363579750061
train gradient:  0.28778895778377694
iteration : 4909
train acc:  0.8203125
train loss:  0.40435659885406494
train gradient:  0.35702532105013013
iteration : 4910
train acc:  0.8828125
train loss:  0.3056427240371704
train gradient:  0.16917804250568313
iteration : 4911
train acc:  0.796875
train loss:  0.44076570868492126
train gradient:  0.3576900102199141
iteration : 4912
train acc:  0.8671875
train loss:  0.3489822745323181
train gradient:  0.24425883897603706
iteration : 4913
train acc:  0.8046875
train loss:  0.44727474451065063
train gradient:  0.34670863779767297
iteration : 4914
train acc:  0.8828125
train loss:  0.33256077766418457
train gradient:  0.24122029375976126
iteration : 4915
train acc:  0.859375
train loss:  0.28668177127838135
train gradient:  0.18205251896328528
iteration : 4916
train acc:  0.8984375
train loss:  0.29751890897750854
train gradient:  0.1616167288092435
iteration : 4917
train acc:  0.8046875
train loss:  0.4554046392440796
train gradient:  0.45489837143490147
iteration : 4918
train acc:  0.8125
train loss:  0.3778969645500183
train gradient:  0.3018165692992646
iteration : 4919
train acc:  0.859375
train loss:  0.30743592977523804
train gradient:  0.18863967255689373
iteration : 4920
train acc:  0.859375
train loss:  0.342866986989975
train gradient:  0.249923624416251
iteration : 4921
train acc:  0.8203125
train loss:  0.4249753952026367
train gradient:  0.35123712389008244
iteration : 4922
train acc:  0.859375
train loss:  0.36210381984710693
train gradient:  0.22358381973549962
iteration : 4923
train acc:  0.921875
train loss:  0.2693904936313629
train gradient:  0.19715931518168645
iteration : 4924
train acc:  0.8359375
train loss:  0.4147748351097107
train gradient:  0.3005527909156475
iteration : 4925
train acc:  0.875
train loss:  0.35428696870803833
train gradient:  0.2640232816929456
iteration : 4926
train acc:  0.84375
train loss:  0.3812080919742584
train gradient:  0.29164459410259935
iteration : 4927
train acc:  0.8046875
train loss:  0.3305797576904297
train gradient:  0.23180324268820862
iteration : 4928
train acc:  0.875
train loss:  0.29754722118377686
train gradient:  0.2574158468929832
iteration : 4929
train acc:  0.890625
train loss:  0.2940477430820465
train gradient:  0.21766267099737863
iteration : 4930
train acc:  0.84375
train loss:  0.34039410948753357
train gradient:  0.21606204843584476
iteration : 4931
train acc:  0.828125
train loss:  0.3960394859313965
train gradient:  0.27508093398135475
iteration : 4932
train acc:  0.84375
train loss:  0.35394287109375
train gradient:  0.23713726964633808
iteration : 4933
train acc:  0.84375
train loss:  0.3299025595188141
train gradient:  0.21100571734043555
iteration : 4934
train acc:  0.8515625
train loss:  0.36369776725769043
train gradient:  0.2587701685731178
iteration : 4935
train acc:  0.90625
train loss:  0.27056288719177246
train gradient:  0.18384854811246848
iteration : 4936
train acc:  0.84375
train loss:  0.35391080379486084
train gradient:  0.26082978767502396
iteration : 4937
train acc:  0.8828125
train loss:  0.31540989875793457
train gradient:  0.21688607552037353
iteration : 4938
train acc:  0.796875
train loss:  0.45968031883239746
train gradient:  0.3782047225215875
iteration : 4939
train acc:  0.796875
train loss:  0.4129723608493805
train gradient:  0.28453831388211775
iteration : 4940
train acc:  0.84375
train loss:  0.37768036127090454
train gradient:  0.40970321090257056
iteration : 4941
train acc:  0.8515625
train loss:  0.3577062487602234
train gradient:  0.20471851108507427
iteration : 4942
train acc:  0.8359375
train loss:  0.3753153681755066
train gradient:  0.40025163199530095
iteration : 4943
train acc:  0.84375
train loss:  0.29773733019828796
train gradient:  0.17121701286553528
iteration : 4944
train acc:  0.859375
train loss:  0.33875399827957153
train gradient:  0.22736646183279444
iteration : 4945
train acc:  0.8046875
train loss:  0.3906329870223999
train gradient:  0.2317873726219916
iteration : 4946
train acc:  0.859375
train loss:  0.34229525923728943
train gradient:  0.22593376480136057
iteration : 4947
train acc:  0.8359375
train loss:  0.32661664485931396
train gradient:  0.21461996120278692
iteration : 4948
train acc:  0.8203125
train loss:  0.37873032689094543
train gradient:  0.37778238763668204
iteration : 4949
train acc:  0.84375
train loss:  0.33303746581077576
train gradient:  0.2965718956813306
iteration : 4950
train acc:  0.84375
train loss:  0.2775038480758667
train gradient:  0.1820682714908678
iteration : 4951
train acc:  0.828125
train loss:  0.33730533719062805
train gradient:  0.22037129331507022
iteration : 4952
train acc:  0.8359375
train loss:  0.3269086480140686
train gradient:  0.25836829954564
iteration : 4953
train acc:  0.890625
train loss:  0.2506919205188751
train gradient:  0.12353242705872922
iteration : 4954
train acc:  0.8828125
train loss:  0.28807246685028076
train gradient:  0.283989292223318
iteration : 4955
train acc:  0.8359375
train loss:  0.3426424562931061
train gradient:  0.2994132909786254
iteration : 4956
train acc:  0.875
train loss:  0.3574163317680359
train gradient:  0.2587624641988282
iteration : 4957
train acc:  0.921875
train loss:  0.27103501558303833
train gradient:  0.2076278852973448
iteration : 4958
train acc:  0.8515625
train loss:  0.3212026059627533
train gradient:  0.18393972879530024
iteration : 4959
train acc:  0.84375
train loss:  0.3289312422275543
train gradient:  0.2168974033576066
iteration : 4960
train acc:  0.8203125
train loss:  0.36190319061279297
train gradient:  0.234167303037768
iteration : 4961
train acc:  0.8125
train loss:  0.37380391359329224
train gradient:  0.36412923951074627
iteration : 4962
train acc:  0.828125
train loss:  0.39020591974258423
train gradient:  0.29104948685522064
iteration : 4963
train acc:  0.875
train loss:  0.3166167736053467
train gradient:  0.15568891152046466
iteration : 4964
train acc:  0.8515625
train loss:  0.30142349004745483
train gradient:  0.21017355034486424
iteration : 4965
train acc:  0.84375
train loss:  0.32301703095436096
train gradient:  0.23736726760518775
iteration : 4966
train acc:  0.8125
train loss:  0.3716949224472046
train gradient:  0.2632849619922092
iteration : 4967
train acc:  0.796875
train loss:  0.43929141759872437
train gradient:  0.36993049007391743
iteration : 4968
train acc:  0.8828125
train loss:  0.2938380837440491
train gradient:  0.16775931030673374
iteration : 4969
train acc:  0.84375
train loss:  0.34796738624572754
train gradient:  0.301890903023344
iteration : 4970
train acc:  0.84375
train loss:  0.323896586894989
train gradient:  0.2631528618654824
iteration : 4971
train acc:  0.84375
train loss:  0.32214757800102234
train gradient:  0.2344892527348436
iteration : 4972
train acc:  0.84375
train loss:  0.3007141351699829
train gradient:  0.33297479854479234
iteration : 4973
train acc:  0.8046875
train loss:  0.4039124548435211
train gradient:  0.29663022404690914
iteration : 4974
train acc:  0.8125
train loss:  0.3779754936695099
train gradient:  0.2403420755371679
iteration : 4975
train acc:  0.8359375
train loss:  0.3700372576713562
train gradient:  0.3159772724349022
iteration : 4976
train acc:  0.8125
train loss:  0.4023042917251587
train gradient:  0.3520771678918315
iteration : 4977
train acc:  0.8828125
train loss:  0.2850952744483948
train gradient:  0.22850581419165547
iteration : 4978
train acc:  0.8515625
train loss:  0.3539615869522095
train gradient:  0.2523204005928128
iteration : 4979
train acc:  0.8515625
train loss:  0.3609743118286133
train gradient:  0.3324503803721369
iteration : 4980
train acc:  0.828125
train loss:  0.35411396622657776
train gradient:  0.48706106224340967
iteration : 4981
train acc:  0.78125
train loss:  0.4522891640663147
train gradient:  0.35777073165919204
iteration : 4982
train acc:  0.8125
train loss:  0.36066073179244995
train gradient:  0.3203243489617791
iteration : 4983
train acc:  0.8515625
train loss:  0.30141210556030273
train gradient:  0.2602700857039227
iteration : 4984
train acc:  0.8359375
train loss:  0.3673417568206787
train gradient:  0.2655278809226402
iteration : 4985
train acc:  0.8203125
train loss:  0.42043203115463257
train gradient:  0.25771484876715894
iteration : 4986
train acc:  0.8359375
train loss:  0.47386348247528076
train gradient:  0.45959489088922845
iteration : 4987
train acc:  0.890625
train loss:  0.2750459611415863
train gradient:  0.20179973584741198
iteration : 4988
train acc:  0.8515625
train loss:  0.3324103355407715
train gradient:  0.2464236947884577
iteration : 4989
train acc:  0.78125
train loss:  0.426128089427948
train gradient:  0.5326508714961303
iteration : 4990
train acc:  0.8125
train loss:  0.3978506326675415
train gradient:  0.2990349550922635
iteration : 4991
train acc:  0.8515625
train loss:  0.32824400067329407
train gradient:  0.31236094950523957
iteration : 4992
train acc:  0.8125
train loss:  0.395790696144104
train gradient:  0.24285381820112947
iteration : 4993
train acc:  0.796875
train loss:  0.409305602312088
train gradient:  0.42917159918303616
iteration : 4994
train acc:  0.796875
train loss:  0.4582253694534302
train gradient:  0.36704708707747424
iteration : 4995
train acc:  0.8671875
train loss:  0.3621676564216614
train gradient:  0.28409823076113755
iteration : 4996
train acc:  0.8203125
train loss:  0.37836188077926636
train gradient:  0.3542197882750368
iteration : 4997
train acc:  0.8203125
train loss:  0.3768373131752014
train gradient:  0.31339819637101957
iteration : 4998
train acc:  0.8359375
train loss:  0.347761869430542
train gradient:  0.2207993788271082
iteration : 4999
train acc:  0.875
train loss:  0.29137253761291504
train gradient:  0.1789332263143928
iteration : 5000
train acc:  0.8359375
train loss:  0.3924161195755005
train gradient:  0.23758977789361369
iteration : 5001
train acc:  0.84375
train loss:  0.3633694648742676
train gradient:  0.25936493393964355
iteration : 5002
train acc:  0.875
train loss:  0.2792964279651642
train gradient:  0.20202328183751667
iteration : 5003
train acc:  0.78125
train loss:  0.40209639072418213
train gradient:  0.24462837530268966
iteration : 5004
train acc:  0.8046875
train loss:  0.35365939140319824
train gradient:  0.28929552567981254
iteration : 5005
train acc:  0.8203125
train loss:  0.41350817680358887
train gradient:  0.3011629030676008
iteration : 5006
train acc:  0.8984375
train loss:  0.3813323974609375
train gradient:  0.4513123017291811
iteration : 5007
train acc:  0.8203125
train loss:  0.42992520332336426
train gradient:  0.5894144916361355
iteration : 5008
train acc:  0.8515625
train loss:  0.3505433201789856
train gradient:  0.21316390615655845
iteration : 5009
train acc:  0.8359375
train loss:  0.4049174189567566
train gradient:  0.32423826149700785
iteration : 5010
train acc:  0.8515625
train loss:  0.33410823345184326
train gradient:  0.21357012509036344
iteration : 5011
train acc:  0.9140625
train loss:  0.27598121762275696
train gradient:  0.12464548996777819
iteration : 5012
train acc:  0.859375
train loss:  0.35264503955841064
train gradient:  0.3855354427098727
iteration : 5013
train acc:  0.7734375
train loss:  0.4471691846847534
train gradient:  0.39813070309107
iteration : 5014
train acc:  0.8828125
train loss:  0.3141942024230957
train gradient:  0.18414356739447252
iteration : 5015
train acc:  0.828125
train loss:  0.38107699155807495
train gradient:  0.410738864156564
iteration : 5016
train acc:  0.828125
train loss:  0.3656761646270752
train gradient:  0.27609162061578585
iteration : 5017
train acc:  0.8125
train loss:  0.4068470895290375
train gradient:  0.315888877608493
iteration : 5018
train acc:  0.875
train loss:  0.33673912286758423
train gradient:  0.22861215979885305
iteration : 5019
train acc:  0.8515625
train loss:  0.3546779751777649
train gradient:  0.2635527110787354
iteration : 5020
train acc:  0.8828125
train loss:  0.26452481746673584
train gradient:  0.16053575372078024
iteration : 5021
train acc:  0.8203125
train loss:  0.3793266713619232
train gradient:  0.27784357031410917
iteration : 5022
train acc:  0.8359375
train loss:  0.3752031922340393
train gradient:  0.2488658850191398
iteration : 5023
train acc:  0.8203125
train loss:  0.36638644337654114
train gradient:  0.2672797287231487
iteration : 5024
train acc:  0.828125
train loss:  0.35331910848617554
train gradient:  0.334684457012739
iteration : 5025
train acc:  0.8203125
train loss:  0.3558945655822754
train gradient:  0.23087782226981418
iteration : 5026
train acc:  0.8359375
train loss:  0.39028429985046387
train gradient:  0.273242677069045
iteration : 5027
train acc:  0.875
train loss:  0.3206974267959595
train gradient:  0.20365265382066228
iteration : 5028
train acc:  0.8671875
train loss:  0.3952973783016205
train gradient:  0.31097156563369044
iteration : 5029
train acc:  0.9375
train loss:  0.23532085120677948
train gradient:  0.1154618076061329
iteration : 5030
train acc:  0.8359375
train loss:  0.32723504304885864
train gradient:  0.21812270963588154
iteration : 5031
train acc:  0.8515625
train loss:  0.3652554452419281
train gradient:  0.23078428411786528
iteration : 5032
train acc:  0.859375
train loss:  0.3430463373661041
train gradient:  0.22654094283019757
iteration : 5033
train acc:  0.859375
train loss:  0.35524988174438477
train gradient:  0.2992584728280273
iteration : 5034
train acc:  0.84375
train loss:  0.3786062002182007
train gradient:  0.26811397238164364
iteration : 5035
train acc:  0.8359375
train loss:  0.3931666314601898
train gradient:  0.2578234736539522
iteration : 5036
train acc:  0.8515625
train loss:  0.3527781069278717
train gradient:  0.22177058174374908
iteration : 5037
train acc:  0.78125
train loss:  0.40220150351524353
train gradient:  0.2539030727375925
iteration : 5038
train acc:  0.828125
train loss:  0.3915979862213135
train gradient:  0.30521707829882444
iteration : 5039
train acc:  0.765625
train loss:  0.5566680431365967
train gradient:  0.47667041416230643
iteration : 5040
train acc:  0.921875
train loss:  0.31999099254608154
train gradient:  0.22228311980652146
iteration : 5041
train acc:  0.796875
train loss:  0.41798725724220276
train gradient:  0.29965680398899575
iteration : 5042
train acc:  0.8515625
train loss:  0.3346188962459564
train gradient:  0.21677991226671595
iteration : 5043
train acc:  0.875
train loss:  0.29573675990104675
train gradient:  0.20558428941639736
iteration : 5044
train acc:  0.8125
train loss:  0.3778756856918335
train gradient:  0.16305654825135996
iteration : 5045
train acc:  0.8828125
train loss:  0.3184479773044586
train gradient:  0.18955602740453767
iteration : 5046
train acc:  0.8671875
train loss:  0.308589905500412
train gradient:  0.15963246894408412
iteration : 5047
train acc:  0.828125
train loss:  0.47815218567848206
train gradient:  0.349291909544975
iteration : 5048
train acc:  0.8203125
train loss:  0.38366323709487915
train gradient:  0.25474528048497774
iteration : 5049
train acc:  0.8515625
train loss:  0.29799792170524597
train gradient:  0.22207002617406474
iteration : 5050
train acc:  0.875
train loss:  0.35435396432876587
train gradient:  0.2830143803609301
iteration : 5051
train acc:  0.859375
train loss:  0.31028851866722107
train gradient:  0.20720554043536432
iteration : 5052
train acc:  0.7578125
train loss:  0.4638565480709076
train gradient:  0.2789002219644501
iteration : 5053
train acc:  0.8125
train loss:  0.4016727805137634
train gradient:  0.3073354290878159
iteration : 5054
train acc:  0.8359375
train loss:  0.3231028914451599
train gradient:  0.18751721454195863
iteration : 5055
train acc:  0.8046875
train loss:  0.38133466243743896
train gradient:  0.22119706222753432
iteration : 5056
train acc:  0.7890625
train loss:  0.3985496461391449
train gradient:  0.22943368575400006
iteration : 5057
train acc:  0.8671875
train loss:  0.304090678691864
train gradient:  0.2270768859119639
iteration : 5058
train acc:  0.828125
train loss:  0.35867321491241455
train gradient:  0.36629816644591817
iteration : 5059
train acc:  0.8046875
train loss:  0.38460469245910645
train gradient:  0.272276849699864
iteration : 5060
train acc:  0.8984375
train loss:  0.2569596767425537
train gradient:  0.12143403234524845
iteration : 5061
train acc:  0.84375
train loss:  0.4094221293926239
train gradient:  0.3275026488472493
iteration : 5062
train acc:  0.8359375
train loss:  0.36501064896583557
train gradient:  0.23152057078838578
iteration : 5063
train acc:  0.828125
train loss:  0.36481165885925293
train gradient:  0.23013616309971402
iteration : 5064
train acc:  0.84375
train loss:  0.3184671401977539
train gradient:  0.23065099824000873
iteration : 5065
train acc:  0.78125
train loss:  0.4283074736595154
train gradient:  0.27606344174335085
iteration : 5066
train acc:  0.875
train loss:  0.34050390124320984
train gradient:  0.19036589956190345
iteration : 5067
train acc:  0.8828125
train loss:  0.2895362377166748
train gradient:  0.2028703502700915
iteration : 5068
train acc:  0.8046875
train loss:  0.3682844638824463
train gradient:  0.3656753750859275
iteration : 5069
train acc:  0.8671875
train loss:  0.3372565507888794
train gradient:  0.22330273439356524
iteration : 5070
train acc:  0.8984375
train loss:  0.2615761160850525
train gradient:  0.13642104312945097
iteration : 5071
train acc:  0.8828125
train loss:  0.3286721706390381
train gradient:  0.21673598236160013
iteration : 5072
train acc:  0.859375
train loss:  0.35194161534309387
train gradient:  0.2067537164401336
iteration : 5073
train acc:  0.8359375
train loss:  0.39583319425582886
train gradient:  0.21612398476372954
iteration : 5074
train acc:  0.84375
train loss:  0.3453054428100586
train gradient:  0.19696772470676496
iteration : 5075
train acc:  0.859375
train loss:  0.32808297872543335
train gradient:  0.23293839233456481
iteration : 5076
train acc:  0.8671875
train loss:  0.3356684148311615
train gradient:  0.17833705633715533
iteration : 5077
train acc:  0.8828125
train loss:  0.3028818368911743
train gradient:  0.1470642239855393
iteration : 5078
train acc:  0.8203125
train loss:  0.3830185532569885
train gradient:  0.3344189723439251
iteration : 5079
train acc:  0.84375
train loss:  0.3424571752548218
train gradient:  0.23286849689914574
iteration : 5080
train acc:  0.8359375
train loss:  0.360252320766449
train gradient:  0.2562335820585113
iteration : 5081
train acc:  0.8828125
train loss:  0.34875041246414185
train gradient:  0.3483727649862892
iteration : 5082
train acc:  0.8359375
train loss:  0.36547815799713135
train gradient:  0.23433632963391404
iteration : 5083
train acc:  0.8359375
train loss:  0.36343133449554443
train gradient:  0.24150406767691385
iteration : 5084
train acc:  0.8515625
train loss:  0.3572832942008972
train gradient:  0.20994773087538457
iteration : 5085
train acc:  0.78125
train loss:  0.43834683299064636
train gradient:  0.310378650179181
iteration : 5086
train acc:  0.828125
train loss:  0.33027172088623047
train gradient:  0.1925394125549119
iteration : 5087
train acc:  0.8671875
train loss:  0.3109733462333679
train gradient:  0.15530600057173996
iteration : 5088
train acc:  0.8046875
train loss:  0.436920166015625
train gradient:  0.29796446176516217
iteration : 5089
train acc:  0.859375
train loss:  0.33677783608436584
train gradient:  0.14635177198046712
iteration : 5090
train acc:  0.8125
train loss:  0.3620253801345825
train gradient:  0.19543006715490424
iteration : 5091
train acc:  0.8671875
train loss:  0.34575772285461426
train gradient:  0.43985728834406795
iteration : 5092
train acc:  0.8359375
train loss:  0.3323972821235657
train gradient:  0.270302787546977
iteration : 5093
train acc:  0.8828125
train loss:  0.3148309588432312
train gradient:  0.17723286611655983
iteration : 5094
train acc:  0.7890625
train loss:  0.4549953043460846
train gradient:  0.3247153862213619
iteration : 5095
train acc:  0.8125
train loss:  0.3602061867713928
train gradient:  0.23842214729450978
iteration : 5096
train acc:  0.8203125
train loss:  0.3925800919532776
train gradient:  0.2553726107774942
iteration : 5097
train acc:  0.8125
train loss:  0.42789530754089355
train gradient:  0.23442930708356297
iteration : 5098
train acc:  0.828125
train loss:  0.3377268612384796
train gradient:  0.1793795722163841
iteration : 5099
train acc:  0.8828125
train loss:  0.32586440443992615
train gradient:  0.220851035797258
iteration : 5100
train acc:  0.828125
train loss:  0.32134202122688293
train gradient:  0.25102690117600274
iteration : 5101
train acc:  0.8515625
train loss:  0.34548282623291016
train gradient:  0.18685312857780217
iteration : 5102
train acc:  0.84375
train loss:  0.4240872859954834
train gradient:  0.33988737409696174
iteration : 5103
train acc:  0.8515625
train loss:  0.3611428439617157
train gradient:  0.2788791038473562
iteration : 5104
train acc:  0.828125
train loss:  0.37203526496887207
train gradient:  0.23097302670712344
iteration : 5105
train acc:  0.828125
train loss:  0.3765011727809906
train gradient:  0.24543886768472106
iteration : 5106
train acc:  0.78125
train loss:  0.44562503695487976
train gradient:  0.3157204235748759
iteration : 5107
train acc:  0.8359375
train loss:  0.37253865599632263
train gradient:  0.2239219300847395
iteration : 5108
train acc:  0.8203125
train loss:  0.38984376192092896
train gradient:  0.24056181434257728
iteration : 5109
train acc:  0.8125
train loss:  0.35382410883903503
train gradient:  0.26896002305551064
iteration : 5110
train acc:  0.84375
train loss:  0.32177454233169556
train gradient:  0.1599329248498626
iteration : 5111
train acc:  0.859375
train loss:  0.3294065594673157
train gradient:  0.17848906337478188
iteration : 5112
train acc:  0.859375
train loss:  0.2972823977470398
train gradient:  0.165129834811377
iteration : 5113
train acc:  0.8515625
train loss:  0.30468252301216125
train gradient:  0.167653779194613
iteration : 5114
train acc:  0.84375
train loss:  0.32812488079071045
train gradient:  0.21798518429802094
iteration : 5115
train acc:  0.8515625
train loss:  0.3322207033634186
train gradient:  0.29607718947443845
iteration : 5116
train acc:  0.9140625
train loss:  0.2935367524623871
train gradient:  0.17645726299166456
iteration : 5117
train acc:  0.859375
train loss:  0.3301439881324768
train gradient:  0.23616125964679385
iteration : 5118
train acc:  0.859375
train loss:  0.3381389379501343
train gradient:  0.2538703849999584
iteration : 5119
train acc:  0.8515625
train loss:  0.34820252656936646
train gradient:  0.2655327198062359
iteration : 5120
train acc:  0.84375
train loss:  0.35755521059036255
train gradient:  0.31853584836160054
iteration : 5121
train acc:  0.859375
train loss:  0.343600332736969
train gradient:  0.2904387843443978
iteration : 5122
train acc:  0.7890625
train loss:  0.43273597955703735
train gradient:  0.4185772631553223
iteration : 5123
train acc:  0.8359375
train loss:  0.3482649326324463
train gradient:  0.3449066218043373
iteration : 5124
train acc:  0.84375
train loss:  0.32665467262268066
train gradient:  0.21339049796717194
iteration : 5125
train acc:  0.8359375
train loss:  0.3788847327232361
train gradient:  0.24551815022734513
iteration : 5126
train acc:  0.8359375
train loss:  0.34500759840011597
train gradient:  0.21232587369886052
iteration : 5127
train acc:  0.828125
train loss:  0.3980390131473541
train gradient:  0.3339862180421413
iteration : 5128
train acc:  0.828125
train loss:  0.4422604441642761
train gradient:  0.3887180714107334
iteration : 5129
train acc:  0.8359375
train loss:  0.39069026708602905
train gradient:  0.22235363303479883
iteration : 5130
train acc:  0.828125
train loss:  0.3558318614959717
train gradient:  0.19822043948813345
iteration : 5131
train acc:  0.8359375
train loss:  0.33436012268066406
train gradient:  0.40944430228553663
iteration : 5132
train acc:  0.8671875
train loss:  0.363076388835907
train gradient:  0.2929428507190182
iteration : 5133
train acc:  0.828125
train loss:  0.35371947288513184
train gradient:  0.26048490305238287
iteration : 5134
train acc:  0.8515625
train loss:  0.33825409412384033
train gradient:  0.24336771061532836
iteration : 5135
train acc:  0.8671875
train loss:  0.3665715456008911
train gradient:  0.30902250238946516
iteration : 5136
train acc:  0.8828125
train loss:  0.33994996547698975
train gradient:  0.2149644460118341
iteration : 5137
train acc:  0.8359375
train loss:  0.44429537653923035
train gradient:  0.32992022006707267
iteration : 5138
train acc:  0.828125
train loss:  0.3431292474269867
train gradient:  0.18708947871737758
iteration : 5139
train acc:  0.796875
train loss:  0.4329271912574768
train gradient:  0.36573518103587216
iteration : 5140
train acc:  0.84375
train loss:  0.38370081782341003
train gradient:  0.2904929991897757
iteration : 5141
train acc:  0.828125
train loss:  0.36830154061317444
train gradient:  0.21637194208177063
iteration : 5142
train acc:  0.8515625
train loss:  0.35178324580192566
train gradient:  0.22381423452681437
iteration : 5143
train acc:  0.8671875
train loss:  0.31929564476013184
train gradient:  0.17966697207212778
iteration : 5144
train acc:  0.828125
train loss:  0.3615109622478485
train gradient:  0.22296179979095992
iteration : 5145
train acc:  0.890625
train loss:  0.2918229401111603
train gradient:  0.18673088708248015
iteration : 5146
train acc:  0.859375
train loss:  0.3414088785648346
train gradient:  0.25917269214139776
iteration : 5147
train acc:  0.8046875
train loss:  0.39360105991363525
train gradient:  0.3038664453990516
iteration : 5148
train acc:  0.8671875
train loss:  0.3172469139099121
train gradient:  0.19774306026686997
iteration : 5149
train acc:  0.796875
train loss:  0.40881067514419556
train gradient:  0.36658000536600743
iteration : 5150
train acc:  0.8828125
train loss:  0.2684316635131836
train gradient:  0.15396930998698471
iteration : 5151
train acc:  0.8125
train loss:  0.37443387508392334
train gradient:  0.32241990675817306
iteration : 5152
train acc:  0.84375
train loss:  0.3326229751110077
train gradient:  0.24292313540685107
iteration : 5153
train acc:  0.8515625
train loss:  0.31089484691619873
train gradient:  0.14492462443205673
iteration : 5154
train acc:  0.859375
train loss:  0.3533589243888855
train gradient:  0.2742313624708479
iteration : 5155
train acc:  0.7890625
train loss:  0.49214252829551697
train gradient:  0.41952497184221454
iteration : 5156
train acc:  0.859375
train loss:  0.37167418003082275
train gradient:  0.2261424198439772
iteration : 5157
train acc:  0.8046875
train loss:  0.4148412346839905
train gradient:  0.2750491499910733
iteration : 5158
train acc:  0.890625
train loss:  0.2812228202819824
train gradient:  0.13425557025849746
iteration : 5159
train acc:  0.8828125
train loss:  0.30964112281799316
train gradient:  0.16744227410212736
iteration : 5160
train acc:  0.8984375
train loss:  0.2685856521129608
train gradient:  0.11986134532047459
iteration : 5161
train acc:  0.8515625
train loss:  0.3210245370864868
train gradient:  0.24984589972849336
iteration : 5162
train acc:  0.8828125
train loss:  0.28695064783096313
train gradient:  0.15190076632286525
iteration : 5163
train acc:  0.8984375
train loss:  0.25432318449020386
train gradient:  0.15047239444686578
iteration : 5164
train acc:  0.8515625
train loss:  0.3370571434497833
train gradient:  0.20829808946691544
iteration : 5165
train acc:  0.8515625
train loss:  0.3191985487937927
train gradient:  0.210458686961206
iteration : 5166
train acc:  0.8671875
train loss:  0.319580614566803
train gradient:  0.2766802055053791
iteration : 5167
train acc:  0.8203125
train loss:  0.39276325702667236
train gradient:  0.2706824736648089
iteration : 5168
train acc:  0.875
train loss:  0.3102271556854248
train gradient:  0.2369085645285991
iteration : 5169
train acc:  0.84375
train loss:  0.3852649927139282
train gradient:  0.24124676116445387
iteration : 5170
train acc:  0.8671875
train loss:  0.30686163902282715
train gradient:  0.19171498345444254
iteration : 5171
train acc:  0.8671875
train loss:  0.2732408046722412
train gradient:  0.17960287321296042
iteration : 5172
train acc:  0.84375
train loss:  0.3551168441772461
train gradient:  0.18329221161902726
iteration : 5173
train acc:  0.828125
train loss:  0.35545194149017334
train gradient:  0.22360367469419512
iteration : 5174
train acc:  0.890625
train loss:  0.31698668003082275
train gradient:  0.19418232736648455
iteration : 5175
train acc:  0.875
train loss:  0.3609369695186615
train gradient:  0.3560668575226084
iteration : 5176
train acc:  0.828125
train loss:  0.36403006315231323
train gradient:  0.30440016882435283
iteration : 5177
train acc:  0.828125
train loss:  0.38022005558013916
train gradient:  0.20994807346036054
iteration : 5178
train acc:  0.8359375
train loss:  0.3421030342578888
train gradient:  0.25178615319202724
iteration : 5179
train acc:  0.8828125
train loss:  0.3382551670074463
train gradient:  0.2133168474003977
iteration : 5180
train acc:  0.7890625
train loss:  0.43349921703338623
train gradient:  0.3219175957829919
iteration : 5181
train acc:  0.8046875
train loss:  0.42647603154182434
train gradient:  0.3158316500491981
iteration : 5182
train acc:  0.8828125
train loss:  0.32391098141670227
train gradient:  0.31861219334047386
iteration : 5183
train acc:  0.8515625
train loss:  0.34838759899139404
train gradient:  0.22839216304422677
iteration : 5184
train acc:  0.890625
train loss:  0.28485608100891113
train gradient:  0.19682386216425088
iteration : 5185
train acc:  0.8671875
train loss:  0.3305094242095947
train gradient:  0.15960882270512552
iteration : 5186
train acc:  0.84375
train loss:  0.3559568226337433
train gradient:  0.22534816258628365
iteration : 5187
train acc:  0.8203125
train loss:  0.3647131323814392
train gradient:  0.2546975252629195
iteration : 5188
train acc:  0.8359375
train loss:  0.32871079444885254
train gradient:  0.27757946783146675
iteration : 5189
train acc:  0.828125
train loss:  0.3581673502922058
train gradient:  0.1860606072700465
iteration : 5190
train acc:  0.796875
train loss:  0.3888472318649292
train gradient:  0.2316202297027738
iteration : 5191
train acc:  0.8046875
train loss:  0.3784029185771942
train gradient:  0.24852770061911328
iteration : 5192
train acc:  0.890625
train loss:  0.24718396365642548
train gradient:  0.13767926224195987
iteration : 5193
train acc:  0.84375
train loss:  0.34694522619247437
train gradient:  0.21081377595873158
iteration : 5194
train acc:  0.8203125
train loss:  0.37218791246414185
train gradient:  0.23231867440176598
iteration : 5195
train acc:  0.84375
train loss:  0.3362376391887665
train gradient:  0.19799947474589363
iteration : 5196
train acc:  0.8515625
train loss:  0.30875349044799805
train gradient:  0.2392220363255077
iteration : 5197
train acc:  0.796875
train loss:  0.4238266348838806
train gradient:  0.2660321292288827
iteration : 5198
train acc:  0.8125
train loss:  0.3507835268974304
train gradient:  0.2390956235269987
iteration : 5199
train acc:  0.8515625
train loss:  0.3321667015552521
train gradient:  0.17840817152490618
iteration : 5200
train acc:  0.8828125
train loss:  0.2954431474208832
train gradient:  0.1961733076207221
iteration : 5201
train acc:  0.8671875
train loss:  0.31925690174102783
train gradient:  0.21302382988693297
iteration : 5202
train acc:  0.84375
train loss:  0.3615117073059082
train gradient:  0.2558591222711252
iteration : 5203
train acc:  0.8515625
train loss:  0.3151476979255676
train gradient:  0.20925571894254658
iteration : 5204
train acc:  0.890625
train loss:  0.2811509370803833
train gradient:  0.18312646192235804
iteration : 5205
train acc:  0.796875
train loss:  0.3910534977912903
train gradient:  0.253012918540546
iteration : 5206
train acc:  0.84375
train loss:  0.34410080313682556
train gradient:  0.2689978286381059
iteration : 5207
train acc:  0.84375
train loss:  0.366140753030777
train gradient:  0.2338856353289595
iteration : 5208
train acc:  0.8203125
train loss:  0.43020594120025635
train gradient:  0.4183568641160392
iteration : 5209
train acc:  0.8671875
train loss:  0.33649396896362305
train gradient:  0.2256232426143423
iteration : 5210
train acc:  0.84375
train loss:  0.3766990602016449
train gradient:  0.26994516570404403
iteration : 5211
train acc:  0.8515625
train loss:  0.34077075123786926
train gradient:  0.22985790372199297
iteration : 5212
train acc:  0.859375
train loss:  0.31204652786254883
train gradient:  0.1653351957769728
iteration : 5213
train acc:  0.7734375
train loss:  0.45188480615615845
train gradient:  0.39375009623149926
iteration : 5214
train acc:  0.875
train loss:  0.29519122838974
train gradient:  0.147134190468174
iteration : 5215
train acc:  0.875
train loss:  0.28677627444267273
train gradient:  0.1477612117555344
iteration : 5216
train acc:  0.859375
train loss:  0.2990795969963074
train gradient:  0.18294681042160554
iteration : 5217
train acc:  0.84375
train loss:  0.4406420886516571
train gradient:  0.3485891670257586
iteration : 5218
train acc:  0.859375
train loss:  0.3062645196914673
train gradient:  0.18411997720279719
iteration : 5219
train acc:  0.859375
train loss:  0.33771926164627075
train gradient:  0.18847852783936409
iteration : 5220
train acc:  0.8515625
train loss:  0.3617410957813263
train gradient:  0.16787437233483074
iteration : 5221
train acc:  0.859375
train loss:  0.2999149560928345
train gradient:  0.1659746707866845
iteration : 5222
train acc:  0.875
train loss:  0.29183948040008545
train gradient:  0.18224088126252203
iteration : 5223
train acc:  0.796875
train loss:  0.4082338511943817
train gradient:  0.36504095971215716
iteration : 5224
train acc:  0.828125
train loss:  0.4712112247943878
train gradient:  0.45251138893936554
iteration : 5225
train acc:  0.8515625
train loss:  0.34111735224723816
train gradient:  0.1995785359965062
iteration : 5226
train acc:  0.8203125
train loss:  0.3148021697998047
train gradient:  0.27155436194922933
iteration : 5227
train acc:  0.875
train loss:  0.33295679092407227
train gradient:  0.27612136196690357
iteration : 5228
train acc:  0.875
train loss:  0.33713144063949585
train gradient:  0.28959466613310864
iteration : 5229
train acc:  0.8203125
train loss:  0.38100165128707886
train gradient:  0.2238647908830058
iteration : 5230
train acc:  0.84375
train loss:  0.3719351887702942
train gradient:  0.18977548594977414
iteration : 5231
train acc:  0.8828125
train loss:  0.30151617527008057
train gradient:  0.2069245928303344
iteration : 5232
train acc:  0.8671875
train loss:  0.2946159243583679
train gradient:  0.20688252182145703
iteration : 5233
train acc:  0.796875
train loss:  0.40765300393104553
train gradient:  0.31459005844625393
iteration : 5234
train acc:  0.796875
train loss:  0.40330320596694946
train gradient:  0.31048548929407377
iteration : 5235
train acc:  0.8203125
train loss:  0.3866525888442993
train gradient:  0.2608479827324649
iteration : 5236
train acc:  0.8203125
train loss:  0.4082429111003876
train gradient:  0.28490170139698073
iteration : 5237
train acc:  0.8046875
train loss:  0.3942515254020691
train gradient:  0.28826952483268203
iteration : 5238
train acc:  0.8203125
train loss:  0.3582073450088501
train gradient:  0.2911255979109779
iteration : 5239
train acc:  0.859375
train loss:  0.30808985233306885
train gradient:  0.1946317021656729
iteration : 5240
train acc:  0.7890625
train loss:  0.3995211720466614
train gradient:  0.2698326765096314
iteration : 5241
train acc:  0.859375
train loss:  0.2981625199317932
train gradient:  0.13957209501335202
iteration : 5242
train acc:  0.84375
train loss:  0.3173137903213501
train gradient:  0.13684915334200187
iteration : 5243
train acc:  0.859375
train loss:  0.31348657608032227
train gradient:  0.24662793149965134
iteration : 5244
train acc:  0.828125
train loss:  0.39355069398880005
train gradient:  0.3350090661681232
iteration : 5245
train acc:  0.859375
train loss:  0.36629289388656616
train gradient:  0.2964794781714839
iteration : 5246
train acc:  0.78125
train loss:  0.4159121513366699
train gradient:  0.3604129361180625
iteration : 5247
train acc:  0.8671875
train loss:  0.31075048446655273
train gradient:  0.21110797624121172
iteration : 5248
train acc:  0.828125
train loss:  0.4036554992198944
train gradient:  0.305272596525241
iteration : 5249
train acc:  0.8046875
train loss:  0.40054070949554443
train gradient:  0.2956717220497265
iteration : 5250
train acc:  0.8203125
train loss:  0.35993844270706177
train gradient:  0.2599025984606613
iteration : 5251
train acc:  0.8359375
train loss:  0.31988391280174255
train gradient:  0.20613397852447357
iteration : 5252
train acc:  0.8359375
train loss:  0.3318200707435608
train gradient:  0.257974480881936
iteration : 5253
train acc:  0.90625
train loss:  0.29865002632141113
train gradient:  0.2587368444329645
iteration : 5254
train acc:  0.8125
train loss:  0.402704656124115
train gradient:  0.263819130465512
iteration : 5255
train acc:  0.8515625
train loss:  0.3068224787712097
train gradient:  0.1658027878759718
iteration : 5256
train acc:  0.8046875
train loss:  0.36165475845336914
train gradient:  0.2603314823923716
iteration : 5257
train acc:  0.8515625
train loss:  0.34500589966773987
train gradient:  0.27972070854764863
iteration : 5258
train acc:  0.875
train loss:  0.3752933740615845
train gradient:  0.49427635990629454
iteration : 5259
train acc:  0.890625
train loss:  0.29309719800949097
train gradient:  0.1389507298535522
iteration : 5260
train acc:  0.875
train loss:  0.3805120587348938
train gradient:  0.26952700595976486
iteration : 5261
train acc:  0.8515625
train loss:  0.30719637870788574
train gradient:  0.1441364367886592
iteration : 5262
train acc:  0.859375
train loss:  0.3838208317756653
train gradient:  0.2564740433563808
iteration : 5263
train acc:  0.796875
train loss:  0.38430672883987427
train gradient:  0.3673653848235597
iteration : 5264
train acc:  0.84375
train loss:  0.33832353353500366
train gradient:  0.41243861904969487
iteration : 5265
train acc:  0.8828125
train loss:  0.36774739623069763
train gradient:  0.2087703700358226
iteration : 5266
train acc:  0.8984375
train loss:  0.32421430945396423
train gradient:  0.20595988664082854
iteration : 5267
train acc:  0.84375
train loss:  0.3449259400367737
train gradient:  0.22568823648676634
iteration : 5268
train acc:  0.8515625
train loss:  0.3452094495296478
train gradient:  0.22076504121741902
iteration : 5269
train acc:  0.8125
train loss:  0.416207492351532
train gradient:  0.33871161213313244
iteration : 5270
train acc:  0.8828125
train loss:  0.3076247274875641
train gradient:  0.2190002745778192
iteration : 5271
train acc:  0.84375
train loss:  0.3541049361228943
train gradient:  0.37592655264009306
iteration : 5272
train acc:  0.875
train loss:  0.25714656710624695
train gradient:  0.15809709790617432
iteration : 5273
train acc:  0.859375
train loss:  0.3067673444747925
train gradient:  0.15878914906671499
iteration : 5274
train acc:  0.84375
train loss:  0.37488752603530884
train gradient:  0.3022320438617392
iteration : 5275
train acc:  0.8203125
train loss:  0.3387172818183899
train gradient:  0.23521761902648125
iteration : 5276
train acc:  0.8828125
train loss:  0.29774144291877747
train gradient:  0.25244711927486074
iteration : 5277
train acc:  0.84375
train loss:  0.3542172312736511
train gradient:  0.358609794202376
iteration : 5278
train acc:  0.8828125
train loss:  0.28020304441452026
train gradient:  0.2075445218953148
iteration : 5279
train acc:  0.84375
train loss:  0.3293392062187195
train gradient:  0.2369606995582731
iteration : 5280
train acc:  0.859375
train loss:  0.3575085699558258
train gradient:  0.22896076444404423
iteration : 5281
train acc:  0.84375
train loss:  0.36230725049972534
train gradient:  0.29398420091762467
iteration : 5282
train acc:  0.8359375
train loss:  0.3615308403968811
train gradient:  0.23597773915442233
iteration : 5283
train acc:  0.9140625
train loss:  0.26396703720092773
train gradient:  0.16674594907662682
iteration : 5284
train acc:  0.8359375
train loss:  0.39513641595840454
train gradient:  0.3317683412671481
iteration : 5285
train acc:  0.7890625
train loss:  0.425564706325531
train gradient:  0.29052506358286045
iteration : 5286
train acc:  0.828125
train loss:  0.3489724099636078
train gradient:  0.2636344788491634
iteration : 5287
train acc:  0.7578125
train loss:  0.6043398380279541
train gradient:  0.6601145244534927
iteration : 5288
train acc:  0.84375
train loss:  0.31070476770401
train gradient:  0.18692752705738785
iteration : 5289
train acc:  0.875
train loss:  0.3265925645828247
train gradient:  0.1637108612251953
iteration : 5290
train acc:  0.8359375
train loss:  0.32887330651283264
train gradient:  0.20582364181724772
iteration : 5291
train acc:  0.84375
train loss:  0.353060781955719
train gradient:  0.2503363243363579
iteration : 5292
train acc:  0.8671875
train loss:  0.3353576958179474
train gradient:  0.209806154362235
iteration : 5293
train acc:  0.796875
train loss:  0.411687433719635
train gradient:  0.2945885316886305
iteration : 5294
train acc:  0.828125
train loss:  0.33678215742111206
train gradient:  0.20279831739483828
iteration : 5295
train acc:  0.8203125
train loss:  0.3601346015930176
train gradient:  0.3460411391374632
iteration : 5296
train acc:  0.796875
train loss:  0.43529126048088074
train gradient:  0.37841254937804575
iteration : 5297
train acc:  0.890625
train loss:  0.3693282902240753
train gradient:  0.3036814260703908
iteration : 5298
train acc:  0.796875
train loss:  0.46068501472473145
train gradient:  0.5414090548525425
iteration : 5299
train acc:  0.8046875
train loss:  0.43042153120040894
train gradient:  0.43146063367740806
iteration : 5300
train acc:  0.859375
train loss:  0.33604180812835693
train gradient:  0.2127197027403126
iteration : 5301
train acc:  0.8359375
train loss:  0.3603230118751526
train gradient:  0.3862883425800749
iteration : 5302
train acc:  0.75
train loss:  0.5432901978492737
train gradient:  0.490422581066361
iteration : 5303
train acc:  0.859375
train loss:  0.39158928394317627
train gradient:  0.23060137242397133
iteration : 5304
train acc:  0.875
train loss:  0.3497655391693115
train gradient:  0.23630516575023547
iteration : 5305
train acc:  0.828125
train loss:  0.41085708141326904
train gradient:  0.28438233071560626
iteration : 5306
train acc:  0.8359375
train loss:  0.35819923877716064
train gradient:  0.17823594708694046
iteration : 5307
train acc:  0.8125
train loss:  0.333649218082428
train gradient:  0.23301747217674773
iteration : 5308
train acc:  0.84375
train loss:  0.3480865955352783
train gradient:  0.19678323843986298
iteration : 5309
train acc:  0.7890625
train loss:  0.4534531831741333
train gradient:  0.44179084002639774
iteration : 5310
train acc:  0.8125
train loss:  0.38743600249290466
train gradient:  0.26033549494141583
iteration : 5311
train acc:  0.8515625
train loss:  0.32420122623443604
train gradient:  0.21216599685020615
iteration : 5312
train acc:  0.8984375
train loss:  0.28414759039878845
train gradient:  0.22152929794299897
iteration : 5313
train acc:  0.8359375
train loss:  0.4056077301502228
train gradient:  0.2680804521873894
iteration : 5314
train acc:  0.84375
train loss:  0.3368936777114868
train gradient:  0.25902708724415024
iteration : 5315
train acc:  0.875
train loss:  0.3118181824684143
train gradient:  0.30761593284202243
iteration : 5316
train acc:  0.859375
train loss:  0.34206923842430115
train gradient:  0.18155794299145522
iteration : 5317
train acc:  0.8984375
train loss:  0.27731868624687195
train gradient:  0.19516525801517595
iteration : 5318
train acc:  0.84375
train loss:  0.33380550146102905
train gradient:  0.13430299590898861
iteration : 5319
train acc:  0.9140625
train loss:  0.24331319332122803
train gradient:  0.14363519142751974
iteration : 5320
train acc:  0.8203125
train loss:  0.3782617449760437
train gradient:  0.23859646005374777
iteration : 5321
train acc:  0.8828125
train loss:  0.2976253628730774
train gradient:  0.21836656535306734
iteration : 5322
train acc:  0.8046875
train loss:  0.37167197465896606
train gradient:  0.25214043506088085
iteration : 5323
train acc:  0.8828125
train loss:  0.32222115993499756
train gradient:  0.1780266604108139
iteration : 5324
train acc:  0.8359375
train loss:  0.34349191188812256
train gradient:  0.29462752548598753
iteration : 5325
train acc:  0.84375
train loss:  0.35920989513397217
train gradient:  0.2578561136745542
iteration : 5326
train acc:  0.84375
train loss:  0.3314065933227539
train gradient:  0.17667713282109224
iteration : 5327
train acc:  0.859375
train loss:  0.36919885873794556
train gradient:  0.20788277069211686
iteration : 5328
train acc:  0.8203125
train loss:  0.38446977734565735
train gradient:  0.31991319853807487
iteration : 5329
train acc:  0.8828125
train loss:  0.3463810086250305
train gradient:  0.3189753254859004
iteration : 5330
train acc:  0.8515625
train loss:  0.3363456130027771
train gradient:  0.2398586997203135
iteration : 5331
train acc:  0.8125
train loss:  0.42579251527786255
train gradient:  0.5271487854696268
iteration : 5332
train acc:  0.859375
train loss:  0.3364083170890808
train gradient:  0.2566082233819739
iteration : 5333
train acc:  0.828125
train loss:  0.3685888648033142
train gradient:  0.2560999118936395
iteration : 5334
train acc:  0.90625
train loss:  0.29241591691970825
train gradient:  0.1835299670752792
iteration : 5335
train acc:  0.84375
train loss:  0.2959635257720947
train gradient:  0.17485974824494047
iteration : 5336
train acc:  0.828125
train loss:  0.3971061706542969
train gradient:  0.25976689908760453
iteration : 5337
train acc:  0.8046875
train loss:  0.39262259006500244
train gradient:  0.31201503208422177
iteration : 5338
train acc:  0.8671875
train loss:  0.3197324275970459
train gradient:  0.31885991413189246
iteration : 5339
train acc:  0.8125
train loss:  0.425642728805542
train gradient:  0.32395366816495386
iteration : 5340
train acc:  0.8359375
train loss:  0.37018319964408875
train gradient:  0.2147190668878208
iteration : 5341
train acc:  0.8046875
train loss:  0.3767937421798706
train gradient:  0.179052005173018
iteration : 5342
train acc:  0.890625
train loss:  0.2639766037464142
train gradient:  0.18984682794931523
iteration : 5343
train acc:  0.734375
train loss:  0.4614591598510742
train gradient:  0.351876273975473
iteration : 5344
train acc:  0.7890625
train loss:  0.40053829550743103
train gradient:  0.28345033371408235
iteration : 5345
train acc:  0.8203125
train loss:  0.43736666440963745
train gradient:  0.49832939959990963
iteration : 5346
train acc:  0.8359375
train loss:  0.32031112909317017
train gradient:  0.21608177808575685
iteration : 5347
train acc:  0.859375
train loss:  0.30804064869880676
train gradient:  0.20786707441102031
iteration : 5348
train acc:  0.828125
train loss:  0.3445640802383423
train gradient:  0.2914932901846267
iteration : 5349
train acc:  0.8203125
train loss:  0.4427518844604492
train gradient:  0.3098301045184117
iteration : 5350
train acc:  0.890625
train loss:  0.2518209218978882
train gradient:  0.1388923354054863
iteration : 5351
train acc:  0.8203125
train loss:  0.4295380711555481
train gradient:  0.2877397008501876
iteration : 5352
train acc:  0.828125
train loss:  0.34388720989227295
train gradient:  0.23623630137937957
iteration : 5353
train acc:  0.828125
train loss:  0.36053377389907837
train gradient:  0.3073431293489595
iteration : 5354
train acc:  0.890625
train loss:  0.2830924391746521
train gradient:  0.12765558234984603
iteration : 5355
train acc:  0.890625
train loss:  0.3094824552536011
train gradient:  0.18256708112061973
iteration : 5356
train acc:  0.84375
train loss:  0.37901967763900757
train gradient:  0.2971247218628083
iteration : 5357
train acc:  0.859375
train loss:  0.32130658626556396
train gradient:  0.24895956161996374
iteration : 5358
train acc:  0.8515625
train loss:  0.30117282271385193
train gradient:  0.15738469291101959
iteration : 5359
train acc:  0.875
train loss:  0.3737010359764099
train gradient:  0.32189098965892904
iteration : 5360
train acc:  0.796875
train loss:  0.5107815861701965
train gradient:  0.49503861860861176
iteration : 5361
train acc:  0.8046875
train loss:  0.360354483127594
train gradient:  0.2200195892646799
iteration : 5362
train acc:  0.8828125
train loss:  0.2993007302284241
train gradient:  0.14770594702537526
iteration : 5363
train acc:  0.90625
train loss:  0.300140380859375
train gradient:  0.155937684922327
iteration : 5364
train acc:  0.828125
train loss:  0.3253742456436157
train gradient:  0.22718957418420535
iteration : 5365
train acc:  0.796875
train loss:  0.4300538897514343
train gradient:  0.443954764973293
iteration : 5366
train acc:  0.8203125
train loss:  0.43792781233787537
train gradient:  0.25939106823103264
iteration : 5367
train acc:  0.9140625
train loss:  0.26314374804496765
train gradient:  0.14872329328286876
iteration : 5368
train acc:  0.8828125
train loss:  0.33319559693336487
train gradient:  0.2035565908457488
iteration : 5369
train acc:  0.8203125
train loss:  0.4347953796386719
train gradient:  0.4051841433271148
iteration : 5370
train acc:  0.828125
train loss:  0.3494236171245575
train gradient:  0.19194972882897005
iteration : 5371
train acc:  0.84375
train loss:  0.3718101680278778
train gradient:  0.2730783995892336
iteration : 5372
train acc:  0.8203125
train loss:  0.3493052124977112
train gradient:  0.20518039305384345
iteration : 5373
train acc:  0.84375
train loss:  0.3688337206840515
train gradient:  0.15877119653772992
iteration : 5374
train acc:  0.8125
train loss:  0.35958331823349
train gradient:  0.27096248235636117
iteration : 5375
train acc:  0.875
train loss:  0.3038173019886017
train gradient:  0.3824576117591495
iteration : 5376
train acc:  0.9140625
train loss:  0.25535809993743896
train gradient:  0.15209366006429204
iteration : 5377
train acc:  0.8203125
train loss:  0.40858954191207886
train gradient:  0.32740243947018477
iteration : 5378
train acc:  0.8203125
train loss:  0.4440799355506897
train gradient:  0.4630369374443873
iteration : 5379
train acc:  0.8203125
train loss:  0.42176681756973267
train gradient:  0.2654286426640307
iteration : 5380
train acc:  0.875
train loss:  0.3250773549079895
train gradient:  0.20756949792661808
iteration : 5381
train acc:  0.828125
train loss:  0.401988685131073
train gradient:  0.2875762537138516
iteration : 5382
train acc:  0.875
train loss:  0.32569068670272827
train gradient:  0.17620946984775007
iteration : 5383
train acc:  0.8359375
train loss:  0.3196566700935364
train gradient:  0.19801136599385133
iteration : 5384
train acc:  0.84375
train loss:  0.3962157666683197
train gradient:  0.2723334228523259
iteration : 5385
train acc:  0.796875
train loss:  0.35741788148880005
train gradient:  0.24419903116917407
iteration : 5386
train acc:  0.8515625
train loss:  0.33355432748794556
train gradient:  0.2385528806080638
iteration : 5387
train acc:  0.828125
train loss:  0.33118748664855957
train gradient:  0.21944196433302823
iteration : 5388
train acc:  0.8125
train loss:  0.3511139154434204
train gradient:  0.21070302059875073
iteration : 5389
train acc:  0.8828125
train loss:  0.3322191834449768
train gradient:  0.17688826566229043
iteration : 5390
train acc:  0.8515625
train loss:  0.33117300271987915
train gradient:  0.19115774531711593
iteration : 5391
train acc:  0.828125
train loss:  0.33085912466049194
train gradient:  0.15456105178940038
iteration : 5392
train acc:  0.875
train loss:  0.2733756899833679
train gradient:  0.13207652428838262
iteration : 5393
train acc:  0.84375
train loss:  0.39130476117134094
train gradient:  0.2548934079077803
iteration : 5394
train acc:  0.8515625
train loss:  0.33505943417549133
train gradient:  0.24497209664641628
iteration : 5395
train acc:  0.90625
train loss:  0.33819857239723206
train gradient:  0.2545229001309062
iteration : 5396
train acc:  0.828125
train loss:  0.39517489075660706
train gradient:  0.24752039447675667
iteration : 5397
train acc:  0.8828125
train loss:  0.28502845764160156
train gradient:  0.1453566662657501
iteration : 5398
train acc:  0.875
train loss:  0.34949588775634766
train gradient:  0.14894133013444574
iteration : 5399
train acc:  0.90625
train loss:  0.278707355260849
train gradient:  0.2141867612785464
iteration : 5400
train acc:  0.8515625
train loss:  0.3083878755569458
train gradient:  0.1521957883294039
iteration : 5401
train acc:  0.859375
train loss:  0.35945427417755127
train gradient:  0.2798505303731088
iteration : 5402
train acc:  0.8515625
train loss:  0.34226924180984497
train gradient:  0.161444971202453
iteration : 5403
train acc:  0.8671875
train loss:  0.32205480337142944
train gradient:  0.19508314984108996
iteration : 5404
train acc:  0.859375
train loss:  0.3612026870250702
train gradient:  0.16489461391238913
iteration : 5405
train acc:  0.8515625
train loss:  0.3260256350040436
train gradient:  0.17136998283079102
iteration : 5406
train acc:  0.78125
train loss:  0.4188835024833679
train gradient:  0.4111582044699421
iteration : 5407
train acc:  0.8203125
train loss:  0.3731778860092163
train gradient:  0.25695389518266637
iteration : 5408
train acc:  0.8828125
train loss:  0.2636810541152954
train gradient:  0.11404611498696486
iteration : 5409
train acc:  0.8359375
train loss:  0.3511013388633728
train gradient:  0.28954662411799514
iteration : 5410
train acc:  0.890625
train loss:  0.27889585494995117
train gradient:  0.17539088381544737
iteration : 5411
train acc:  0.78125
train loss:  0.47840332984924316
train gradient:  0.3628195227108072
iteration : 5412
train acc:  0.84375
train loss:  0.3322792053222656
train gradient:  0.32031021997097797
iteration : 5413
train acc:  0.7890625
train loss:  0.40412259101867676
train gradient:  0.2665013653886169
iteration : 5414
train acc:  0.84375
train loss:  0.34962689876556396
train gradient:  0.2207801761647631
iteration : 5415
train acc:  0.8203125
train loss:  0.4177868962287903
train gradient:  0.3129150027905054
iteration : 5416
train acc:  0.8359375
train loss:  0.32768240571022034
train gradient:  0.2638418718045876
iteration : 5417
train acc:  0.890625
train loss:  0.32218247652053833
train gradient:  0.23995655819377362
iteration : 5418
train acc:  0.7890625
train loss:  0.5115598440170288
train gradient:  0.4714993655987269
iteration : 5419
train acc:  0.8828125
train loss:  0.36601361632347107
train gradient:  0.1945537061251689
iteration : 5420
train acc:  0.8515625
train loss:  0.34298384189605713
train gradient:  0.1850307197176212
iteration : 5421
train acc:  0.8046875
train loss:  0.42023324966430664
train gradient:  0.291889015044377
iteration : 5422
train acc:  0.890625
train loss:  0.28486916422843933
train gradient:  0.166845787004496
iteration : 5423
train acc:  0.859375
train loss:  0.3186854124069214
train gradient:  0.25219977791634884
iteration : 5424
train acc:  0.8203125
train loss:  0.37417420744895935
train gradient:  0.21895843581044516
iteration : 5425
train acc:  0.8671875
train loss:  0.32131749391555786
train gradient:  0.23246626570832635
iteration : 5426
train acc:  0.859375
train loss:  0.31150558590888977
train gradient:  0.22491149923664588
iteration : 5427
train acc:  0.875
train loss:  0.27337491512298584
train gradient:  0.1365839505791025
iteration : 5428
train acc:  0.8671875
train loss:  0.36088645458221436
train gradient:  0.33102581868066233
iteration : 5429
train acc:  0.8515625
train loss:  0.3279913067817688
train gradient:  0.1575109832777929
iteration : 5430
train acc:  0.859375
train loss:  0.28715306520462036
train gradient:  0.14513166995902993
iteration : 5431
train acc:  0.8671875
train loss:  0.3113054633140564
train gradient:  0.16047421880877297
iteration : 5432
train acc:  0.8125
train loss:  0.39366093277931213
train gradient:  0.29888737256859965
iteration : 5433
train acc:  0.875
train loss:  0.34513059258461
train gradient:  0.1550700183657242
iteration : 5434
train acc:  0.828125
train loss:  0.363278329372406
train gradient:  0.23298550681740082
iteration : 5435
train acc:  0.8125
train loss:  0.37557166814804077
train gradient:  0.1936628256648582
iteration : 5436
train acc:  0.7578125
train loss:  0.4381164610385895
train gradient:  0.25852723657978993
iteration : 5437
train acc:  0.8828125
train loss:  0.28966647386550903
train gradient:  0.17006740304465948
iteration : 5438
train acc:  0.8515625
train loss:  0.3337368965148926
train gradient:  0.15357861211464607
iteration : 5439
train acc:  0.8359375
train loss:  0.3007947504520416
train gradient:  0.21239878558195224
iteration : 5440
train acc:  0.8671875
train loss:  0.2966642379760742
train gradient:  0.17118945386571122
iteration : 5441
train acc:  0.890625
train loss:  0.2878879904747009
train gradient:  0.17317410097138666
iteration : 5442
train acc:  0.8046875
train loss:  0.4569752812385559
train gradient:  0.3859525733642338
iteration : 5443
train acc:  0.8359375
train loss:  0.32668280601501465
train gradient:  0.18795137835621245
iteration : 5444
train acc:  0.796875
train loss:  0.4564800262451172
train gradient:  0.4312570449784035
iteration : 5445
train acc:  0.859375
train loss:  0.33554884791374207
train gradient:  0.16008835673064506
iteration : 5446
train acc:  0.859375
train loss:  0.35597050189971924
train gradient:  0.2144100599658362
iteration : 5447
train acc:  0.8046875
train loss:  0.4319274425506592
train gradient:  0.3883144597187759
iteration : 5448
train acc:  0.828125
train loss:  0.3820911645889282
train gradient:  0.289844329262475
iteration : 5449
train acc:  0.8828125
train loss:  0.2949151396751404
train gradient:  0.21638534599139325
iteration : 5450
train acc:  0.8359375
train loss:  0.4476976692676544
train gradient:  0.39367500190638155
iteration : 5451
train acc:  0.8359375
train loss:  0.33969712257385254
train gradient:  0.22942548832046217
iteration : 5452
train acc:  0.8515625
train loss:  0.3367132246494293
train gradient:  0.2147378506944707
iteration : 5453
train acc:  0.859375
train loss:  0.3350793421268463
train gradient:  0.2500339258260579
iteration : 5454
train acc:  0.84375
train loss:  0.36471855640411377
train gradient:  0.2268189772971058
iteration : 5455
train acc:  0.8359375
train loss:  0.3594124913215637
train gradient:  0.21296053652772334
iteration : 5456
train acc:  0.8671875
train loss:  0.30643391609191895
train gradient:  0.15819318733180532
iteration : 5457
train acc:  0.859375
train loss:  0.32147371768951416
train gradient:  0.19431641988617493
iteration : 5458
train acc:  0.8203125
train loss:  0.3820852041244507
train gradient:  0.3034258387437607
iteration : 5459
train acc:  0.8203125
train loss:  0.3881043791770935
train gradient:  0.4500566891702213
iteration : 5460
train acc:  0.8359375
train loss:  0.4364977777004242
train gradient:  0.3111987216463427
iteration : 5461
train acc:  0.8828125
train loss:  0.2801176905632019
train gradient:  0.19005702888858855
iteration : 5462
train acc:  0.8125
train loss:  0.3585032820701599
train gradient:  0.21860069269243498
iteration : 5463
train acc:  0.78125
train loss:  0.4611847698688507
train gradient:  0.5234632936431172
iteration : 5464
train acc:  0.8359375
train loss:  0.3354761004447937
train gradient:  0.18554142554660907
iteration : 5465
train acc:  0.859375
train loss:  0.33823055028915405
train gradient:  0.23379751234155013
iteration : 5466
train acc:  0.828125
train loss:  0.29882878065109253
train gradient:  0.24196078178263014
iteration : 5467
train acc:  0.8828125
train loss:  0.28577694296836853
train gradient:  0.2272399266134975
iteration : 5468
train acc:  0.84375
train loss:  0.35576364398002625
train gradient:  0.2295733904141552
iteration : 5469
train acc:  0.8203125
train loss:  0.4064948558807373
train gradient:  0.2821849880856863
iteration : 5470
train acc:  0.8125
train loss:  0.33034688234329224
train gradient:  0.23383767652652787
iteration : 5471
train acc:  0.8828125
train loss:  0.31157049536705017
train gradient:  0.2532797020890308
iteration : 5472
train acc:  0.828125
train loss:  0.3906686007976532
train gradient:  0.20067472658524887
iteration : 5473
train acc:  0.875
train loss:  0.35812950134277344
train gradient:  0.23427194600619267
iteration : 5474
train acc:  0.828125
train loss:  0.37353020906448364
train gradient:  0.28211127835844924
iteration : 5475
train acc:  0.875
train loss:  0.3215550184249878
train gradient:  0.1832967539785475
iteration : 5476
train acc:  0.828125
train loss:  0.385492205619812
train gradient:  0.2680217866358051
iteration : 5477
train acc:  0.828125
train loss:  0.36488449573516846
train gradient:  0.2423652189473635
iteration : 5478
train acc:  0.8359375
train loss:  0.34848296642303467
train gradient:  0.21819789081018487
iteration : 5479
train acc:  0.859375
train loss:  0.327039897441864
train gradient:  0.2032197458213486
iteration : 5480
train acc:  0.859375
train loss:  0.37432414293289185
train gradient:  0.21843696869680607
iteration : 5481
train acc:  0.8359375
train loss:  0.31826674938201904
train gradient:  0.19473414657578242
iteration : 5482
train acc:  0.796875
train loss:  0.48443758487701416
train gradient:  0.35972284442326435
iteration : 5483
train acc:  0.8828125
train loss:  0.29885825514793396
train gradient:  0.152698706651206
iteration : 5484
train acc:  0.8203125
train loss:  0.41154050827026367
train gradient:  0.2888699138841109
iteration : 5485
train acc:  0.8125
train loss:  0.39440426230430603
train gradient:  0.3436879823515298
iteration : 5486
train acc:  0.8515625
train loss:  0.3121676445007324
train gradient:  0.2115792370978458
iteration : 5487
train acc:  0.859375
train loss:  0.3294168710708618
train gradient:  0.28460358826562737
iteration : 5488
train acc:  0.78125
train loss:  0.3925829529762268
train gradient:  0.20088102042103623
iteration : 5489
train acc:  0.8515625
train loss:  0.34084784984588623
train gradient:  0.2439641584601826
iteration : 5490
train acc:  0.8203125
train loss:  0.3718149662017822
train gradient:  0.2999871198531329
iteration : 5491
train acc:  0.84375
train loss:  0.32061851024627686
train gradient:  0.19251129544220752
iteration : 5492
train acc:  0.8671875
train loss:  0.3549156188964844
train gradient:  0.2335834545549672
iteration : 5493
train acc:  0.859375
train loss:  0.32331961393356323
train gradient:  0.15826661788989033
iteration : 5494
train acc:  0.8515625
train loss:  0.3321949243545532
train gradient:  0.20043534365363055
iteration : 5495
train acc:  0.828125
train loss:  0.34672099351882935
train gradient:  0.15222329046528904
iteration : 5496
train acc:  0.8828125
train loss:  0.3245660066604614
train gradient:  0.15955335653565345
iteration : 5497
train acc:  0.8359375
train loss:  0.34474503993988037
train gradient:  0.20842003566754724
iteration : 5498
train acc:  0.8671875
train loss:  0.2840661406517029
train gradient:  0.1513242104816794
iteration : 5499
train acc:  0.828125
train loss:  0.36738121509552
train gradient:  0.18494264902649107
iteration : 5500
train acc:  0.8671875
train loss:  0.3217444121837616
train gradient:  0.2553638554744528
iteration : 5501
train acc:  0.78125
train loss:  0.47219133377075195
train gradient:  0.3787926202996705
iteration : 5502
train acc:  0.8203125
train loss:  0.36996546387672424
train gradient:  0.23975595705408176
iteration : 5503
train acc:  0.859375
train loss:  0.29565155506134033
train gradient:  0.32838738230694375
iteration : 5504
train acc:  0.84375
train loss:  0.387027770280838
train gradient:  0.19666628729586955
iteration : 5505
train acc:  0.84375
train loss:  0.31721651554107666
train gradient:  0.2025460638037424
iteration : 5506
train acc:  0.84375
train loss:  0.3392729163169861
train gradient:  0.15425980231382957
iteration : 5507
train acc:  0.828125
train loss:  0.40592676401138306
train gradient:  0.35985215762250095
iteration : 5508
train acc:  0.7890625
train loss:  0.4591677784919739
train gradient:  0.33309354303057426
iteration : 5509
train acc:  0.859375
train loss:  0.36026251316070557
train gradient:  0.18326052747435229
iteration : 5510
train acc:  0.8359375
train loss:  0.360317587852478
train gradient:  0.23674080506759604
iteration : 5511
train acc:  0.890625
train loss:  0.2812851369380951
train gradient:  0.18747603951942857
iteration : 5512
train acc:  0.8203125
train loss:  0.38091304898262024
train gradient:  0.2691162643204711
iteration : 5513
train acc:  0.796875
train loss:  0.36455512046813965
train gradient:  0.2709453877830103
iteration : 5514
train acc:  0.859375
train loss:  0.3257175385951996
train gradient:  0.25054533765015463
iteration : 5515
train acc:  0.84375
train loss:  0.3773697018623352
train gradient:  0.21703611474809384
iteration : 5516
train acc:  0.796875
train loss:  0.3549392819404602
train gradient:  0.18100378129590566
iteration : 5517
train acc:  0.84375
train loss:  0.3277781903743744
train gradient:  0.15671504549050003
iteration : 5518
train acc:  0.8359375
train loss:  0.34785306453704834
train gradient:  0.1899822154202161
iteration : 5519
train acc:  0.828125
train loss:  0.36813849210739136
train gradient:  0.3974543291843652
iteration : 5520
train acc:  0.875
train loss:  0.36584216356277466
train gradient:  0.24847498838572743
iteration : 5521
train acc:  0.8828125
train loss:  0.27503782510757446
train gradient:  0.21818997673764384
iteration : 5522
train acc:  0.859375
train loss:  0.3283759355545044
train gradient:  0.15713290546058803
iteration : 5523
train acc:  0.890625
train loss:  0.2974482774734497
train gradient:  0.18117391296266278
iteration : 5524
train acc:  0.828125
train loss:  0.3814302682876587
train gradient:  0.18754672586154764
iteration : 5525
train acc:  0.8125
train loss:  0.4440309703350067
train gradient:  0.2949349452494943
iteration : 5526
train acc:  0.8671875
train loss:  0.2545073628425598
train gradient:  0.14623453739052894
iteration : 5527
train acc:  0.84375
train loss:  0.3445984125137329
train gradient:  0.21364519253092412
iteration : 5528
train acc:  0.8984375
train loss:  0.2523133158683777
train gradient:  0.11075008078338798
iteration : 5529
train acc:  0.828125
train loss:  0.367770254611969
train gradient:  0.27178807716531755
iteration : 5530
train acc:  0.859375
train loss:  0.31075817346572876
train gradient:  0.22007514366539688
iteration : 5531
train acc:  0.78125
train loss:  0.3647037744522095
train gradient:  0.3293555573094397
iteration : 5532
train acc:  0.8359375
train loss:  0.34727609157562256
train gradient:  0.2031193762608849
iteration : 5533
train acc:  0.90625
train loss:  0.2356213629245758
train gradient:  0.11465544261135621
iteration : 5534
train acc:  0.84375
train loss:  0.3350706100463867
train gradient:  0.1708945403443593
iteration : 5535
train acc:  0.859375
train loss:  0.3905622959136963
train gradient:  0.20441215634517995
iteration : 5536
train acc:  0.859375
train loss:  0.2897065281867981
train gradient:  0.13989123630512978
iteration : 5537
train acc:  0.796875
train loss:  0.47329607605934143
train gradient:  0.44754915646065674
iteration : 5538
train acc:  0.8359375
train loss:  0.34612131118774414
train gradient:  0.20234049068707677
iteration : 5539
train acc:  0.8671875
train loss:  0.3048880696296692
train gradient:  0.15653035582088548
iteration : 5540
train acc:  0.8359375
train loss:  0.32554489374160767
train gradient:  0.1875900042695296
iteration : 5541
train acc:  0.8046875
train loss:  0.38747110962867737
train gradient:  0.22634717451835779
iteration : 5542
train acc:  0.8515625
train loss:  0.3374292254447937
train gradient:  0.18752773834555897
iteration : 5543
train acc:  0.875
train loss:  0.38161754608154297
train gradient:  0.3770319816735086
iteration : 5544
train acc:  0.8046875
train loss:  0.4073123335838318
train gradient:  0.2528915509517147
iteration : 5545
train acc:  0.8984375
train loss:  0.2961866855621338
train gradient:  0.24144803375678314
iteration : 5546
train acc:  0.8359375
train loss:  0.33645519614219666
train gradient:  0.17123088701638467
iteration : 5547
train acc:  0.84375
train loss:  0.3138304352760315
train gradient:  0.2309269215145285
iteration : 5548
train acc:  0.9140625
train loss:  0.25332367420196533
train gradient:  0.1683843440844338
iteration : 5549
train acc:  0.84375
train loss:  0.33186572790145874
train gradient:  0.2167290478735905
iteration : 5550
train acc:  0.8515625
train loss:  0.34589883685112
train gradient:  0.33335163542485746
iteration : 5551
train acc:  0.859375
train loss:  0.3059498369693756
train gradient:  0.18233253376349384
iteration : 5552
train acc:  0.859375
train loss:  0.2822635769844055
train gradient:  0.19364986807294726
iteration : 5553
train acc:  0.8515625
train loss:  0.38139304518699646
train gradient:  0.2612880197305262
iteration : 5554
train acc:  0.7890625
train loss:  0.5046865940093994
train gradient:  0.3919547936675794
iteration : 5555
train acc:  0.828125
train loss:  0.4673537015914917
train gradient:  0.34805104233482126
iteration : 5556
train acc:  0.875
train loss:  0.29053229093551636
train gradient:  0.15130084637025715
iteration : 5557
train acc:  0.84375
train loss:  0.31058448553085327
train gradient:  0.20975854774163857
iteration : 5558
train acc:  0.875
train loss:  0.2949986755847931
train gradient:  0.2271178434563208
iteration : 5559
train acc:  0.84375
train loss:  0.35956934094429016
train gradient:  0.2135898417677154
iteration : 5560
train acc:  0.8203125
train loss:  0.3859363794326782
train gradient:  0.23104917968480748
iteration : 5561
train acc:  0.8359375
train loss:  0.32171106338500977
train gradient:  0.22691500963657135
iteration : 5562
train acc:  0.859375
train loss:  0.3210415244102478
train gradient:  0.2076888198333168
iteration : 5563
train acc:  0.8671875
train loss:  0.3614111542701721
train gradient:  0.14911303857838132
iteration : 5564
train acc:  0.78125
train loss:  0.3897184431552887
train gradient:  0.35360923990678733
iteration : 5565
train acc:  0.8828125
train loss:  0.3270403742790222
train gradient:  0.23127069504462633
iteration : 5566
train acc:  0.8515625
train loss:  0.27759993076324463
train gradient:  0.22926133133192916
iteration : 5567
train acc:  0.8828125
train loss:  0.3337240517139435
train gradient:  0.19607335208180524
iteration : 5568
train acc:  0.8828125
train loss:  0.27719151973724365
train gradient:  0.23347073634251947
iteration : 5569
train acc:  0.8828125
train loss:  0.25367313623428345
train gradient:  0.18283904864993633
iteration : 5570
train acc:  0.78125
train loss:  0.43805745244026184
train gradient:  0.4122101237212453
iteration : 5571
train acc:  0.875
train loss:  0.3263886570930481
train gradient:  0.19141818672485197
iteration : 5572
train acc:  0.875
train loss:  0.3246611952781677
train gradient:  0.20002907211197718
iteration : 5573
train acc:  0.8359375
train loss:  0.33929628133773804
train gradient:  0.22296801192141158
iteration : 5574
train acc:  0.8515625
train loss:  0.3445553481578827
train gradient:  0.19146831149268112
iteration : 5575
train acc:  0.8515625
train loss:  0.33220601081848145
train gradient:  0.23753233368438764
iteration : 5576
train acc:  0.84375
train loss:  0.336344450712204
train gradient:  0.21393589550967346
iteration : 5577
train acc:  0.8515625
train loss:  0.298011839389801
train gradient:  0.16368361026535644
iteration : 5578
train acc:  0.8828125
train loss:  0.27090033888816833
train gradient:  0.18193245951598358
iteration : 5579
train acc:  0.8046875
train loss:  0.33323127031326294
train gradient:  0.235528742903617
iteration : 5580
train acc:  0.828125
train loss:  0.3734201192855835
train gradient:  0.33047510307974326
iteration : 5581
train acc:  0.8203125
train loss:  0.3435263931751251
train gradient:  0.2350706533166256
iteration : 5582
train acc:  0.7890625
train loss:  0.44187426567077637
train gradient:  0.48771321253738625
iteration : 5583
train acc:  0.890625
train loss:  0.27951350808143616
train gradient:  0.1801115657716182
iteration : 5584
train acc:  0.7890625
train loss:  0.4340772330760956
train gradient:  0.3642055146327366
iteration : 5585
train acc:  0.796875
train loss:  0.4227643609046936
train gradient:  0.2847790242562954
iteration : 5586
train acc:  0.8515625
train loss:  0.31232750415802
train gradient:  0.2179687999840296
iteration : 5587
train acc:  0.84375
train loss:  0.30293789505958557
train gradient:  0.18519698556067132
iteration : 5588
train acc:  0.8671875
train loss:  0.2996065020561218
train gradient:  0.14127235028296303
iteration : 5589
train acc:  0.8359375
train loss:  0.2914935350418091
train gradient:  0.13860922920357152
iteration : 5590
train acc:  0.890625
train loss:  0.34123697876930237
train gradient:  0.25123609132506847
iteration : 5591
train acc:  0.8828125
train loss:  0.30063751339912415
train gradient:  0.23272259638442686
iteration : 5592
train acc:  0.796875
train loss:  0.4079185128211975
train gradient:  0.3860277679749597
iteration : 5593
train acc:  0.8125
train loss:  0.38229334354400635
train gradient:  0.3358973383094344
iteration : 5594
train acc:  0.859375
train loss:  0.30700746178627014
train gradient:  0.2208511128714285
iteration : 5595
train acc:  0.84375
train loss:  0.3685196042060852
train gradient:  0.2711646468576728
iteration : 5596
train acc:  0.796875
train loss:  0.42980360984802246
train gradient:  0.3099781811069916
iteration : 5597
train acc:  0.84375
train loss:  0.36096513271331787
train gradient:  0.24554504077667555
iteration : 5598
train acc:  0.859375
train loss:  0.38342130184173584
train gradient:  0.32799175317601426
iteration : 5599
train acc:  0.8203125
train loss:  0.4063713550567627
train gradient:  0.3148602827092223
iteration : 5600
train acc:  0.8515625
train loss:  0.34275394678115845
train gradient:  0.19563605011928112
iteration : 5601
train acc:  0.875
train loss:  0.3450397849082947
train gradient:  0.2834856390228996
iteration : 5602
train acc:  0.765625
train loss:  0.4473690986633301
train gradient:  0.3513397725699671
iteration : 5603
train acc:  0.8203125
train loss:  0.392089307308197
train gradient:  0.26619222706340956
iteration : 5604
train acc:  0.8203125
train loss:  0.3661386966705322
train gradient:  0.18860330709250472
iteration : 5605
train acc:  0.8515625
train loss:  0.3522834777832031
train gradient:  0.21503725584028116
iteration : 5606
train acc:  0.84375
train loss:  0.3750355839729309
train gradient:  0.28670962992067195
iteration : 5607
train acc:  0.875
train loss:  0.3827793002128601
train gradient:  0.2725995425563495
iteration : 5608
train acc:  0.84375
train loss:  0.3686956763267517
train gradient:  0.20618175259047317
iteration : 5609
train acc:  0.8984375
train loss:  0.2883169949054718
train gradient:  0.14255934730562264
iteration : 5610
train acc:  0.8125
train loss:  0.34063851833343506
train gradient:  0.2509156481193548
iteration : 5611
train acc:  0.8125
train loss:  0.41036418080329895
train gradient:  0.3163885846431372
iteration : 5612
train acc:  0.8671875
train loss:  0.34141120314598083
train gradient:  0.20444955067160525
iteration : 5613
train acc:  0.8828125
train loss:  0.3381417393684387
train gradient:  0.24078002016184885
iteration : 5614
train acc:  0.890625
train loss:  0.2615281641483307
train gradient:  0.21471124796988833
iteration : 5615
train acc:  0.859375
train loss:  0.32564765214920044
train gradient:  0.2752085243711221
iteration : 5616
train acc:  0.84375
train loss:  0.39063775539398193
train gradient:  0.24562413212440448
iteration : 5617
train acc:  0.90625
train loss:  0.2899002730846405
train gradient:  0.22059473101999144
iteration : 5618
train acc:  0.8203125
train loss:  0.39980608224868774
train gradient:  0.21378469309467568
iteration : 5619
train acc:  0.8828125
train loss:  0.31310802698135376
train gradient:  0.253784331604838
iteration : 5620
train acc:  0.84375
train loss:  0.37497299909591675
train gradient:  0.24077950261294642
iteration : 5621
train acc:  0.8125
train loss:  0.36685582995414734
train gradient:  0.2580493115398665
iteration : 5622
train acc:  0.7734375
train loss:  0.47875967621803284
train gradient:  0.556673536400416
iteration : 5623
train acc:  0.8203125
train loss:  0.3507224917411804
train gradient:  0.2061749972885114
iteration : 5624
train acc:  0.875
train loss:  0.3661007881164551
train gradient:  0.17796248351134303
iteration : 5625
train acc:  0.8359375
train loss:  0.33880171179771423
train gradient:  0.23068156845335136
iteration : 5626
train acc:  0.8203125
train loss:  0.4213021397590637
train gradient:  0.4541600983467533
iteration : 5627
train acc:  0.875
train loss:  0.3294374346733093
train gradient:  0.22497660101124745
iteration : 5628
train acc:  0.859375
train loss:  0.2969045341014862
train gradient:  0.15357753515260894
iteration : 5629
train acc:  0.828125
train loss:  0.3519514501094818
train gradient:  0.19940426495741048
iteration : 5630
train acc:  0.7890625
train loss:  0.391295850276947
train gradient:  0.3032475980973774
iteration : 5631
train acc:  0.8046875
train loss:  0.3826281428337097
train gradient:  0.26073709967512193
iteration : 5632
train acc:  0.8359375
train loss:  0.34155622124671936
train gradient:  0.2083022896676362
iteration : 5633
train acc:  0.921875
train loss:  0.2599528729915619
train gradient:  0.23187091487388578
iteration : 5634
train acc:  0.859375
train loss:  0.33085525035858154
train gradient:  0.1619203484367065
iteration : 5635
train acc:  0.84375
train loss:  0.32950103282928467
train gradient:  0.2600631618985134
iteration : 5636
train acc:  0.828125
train loss:  0.35364097356796265
train gradient:  0.2950726936139206
iteration : 5637
train acc:  0.875
train loss:  0.3138786554336548
train gradient:  0.15199866563033632
iteration : 5638
train acc:  0.828125
train loss:  0.33267897367477417
train gradient:  0.2279861429981007
iteration : 5639
train acc:  0.875
train loss:  0.307397723197937
train gradient:  0.1979220707263205
iteration : 5640
train acc:  0.8671875
train loss:  0.2768390476703644
train gradient:  0.1686124826324333
iteration : 5641
train acc:  0.8828125
train loss:  0.32745981216430664
train gradient:  0.20066832253614109
iteration : 5642
train acc:  0.8515625
train loss:  0.3989247679710388
train gradient:  0.2793288441875758
iteration : 5643
train acc:  0.8515625
train loss:  0.3509563207626343
train gradient:  0.20294565145700472
iteration : 5644
train acc:  0.7890625
train loss:  0.5462499260902405
train gradient:  0.5647175420232895
iteration : 5645
train acc:  0.84375
train loss:  0.3111151456832886
train gradient:  0.2455941484645463
iteration : 5646
train acc:  0.875
train loss:  0.30916666984558105
train gradient:  0.16274611908805792
iteration : 5647
train acc:  0.828125
train loss:  0.3579179644584656
train gradient:  0.43764141566698245
iteration : 5648
train acc:  0.78125
train loss:  0.4473886787891388
train gradient:  0.4448427488378626
iteration : 5649
train acc:  0.84375
train loss:  0.30708712339401245
train gradient:  0.1527466563795735
iteration : 5650
train acc:  0.8828125
train loss:  0.3344007730484009
train gradient:  0.160657799379226
iteration : 5651
train acc:  0.90625
train loss:  0.24552041292190552
train gradient:  0.12423390872266261
iteration : 5652
train acc:  0.875
train loss:  0.26808086037635803
train gradient:  0.14940161024828919
iteration : 5653
train acc:  0.8359375
train loss:  0.34960830211639404
train gradient:  0.22468299679005435
iteration : 5654
train acc:  0.8359375
train loss:  0.3473566770553589
train gradient:  0.22080665033267197
iteration : 5655
train acc:  0.8359375
train loss:  0.3960403501987457
train gradient:  0.24881198773012703
iteration : 5656
train acc:  0.828125
train loss:  0.3554689884185791
train gradient:  0.2241266194931346
iteration : 5657
train acc:  0.828125
train loss:  0.3864087462425232
train gradient:  0.2187705062591194
iteration : 5658
train acc:  0.7890625
train loss:  0.4245665669441223
train gradient:  0.24838991360004772
iteration : 5659
train acc:  0.8359375
train loss:  0.31716352701187134
train gradient:  0.25301628813857185
iteration : 5660
train acc:  0.7890625
train loss:  0.39278489351272583
train gradient:  0.2781679296250295
iteration : 5661
train acc:  0.8359375
train loss:  0.3976641893386841
train gradient:  0.24960785677167358
iteration : 5662
train acc:  0.84375
train loss:  0.3845701813697815
train gradient:  0.2758762658394767
iteration : 5663
train acc:  0.8828125
train loss:  0.3634752035140991
train gradient:  0.27231786945494924
iteration : 5664
train acc:  0.78125
train loss:  0.4062234163284302
train gradient:  0.2757114886641713
iteration : 5665
train acc:  0.828125
train loss:  0.35465919971466064
train gradient:  0.2575154747127415
iteration : 5666
train acc:  0.7890625
train loss:  0.44153282046318054
train gradient:  0.307403413672164
iteration : 5667
train acc:  0.8515625
train loss:  0.2939619719982147
train gradient:  0.18616649357435266
iteration : 5668
train acc:  0.8515625
train loss:  0.3290555477142334
train gradient:  0.16209426461291165
iteration : 5669
train acc:  0.7890625
train loss:  0.3924802839756012
train gradient:  0.3065802824316906
iteration : 5670
train acc:  0.875
train loss:  0.30926889181137085
train gradient:  0.2289218288190567
iteration : 5671
train acc:  0.859375
train loss:  0.3856833577156067
train gradient:  0.33630527185743075
iteration : 5672
train acc:  0.8125
train loss:  0.4533551335334778
train gradient:  0.4090497137102116
iteration : 5673
train acc:  0.890625
train loss:  0.3143501281738281
train gradient:  0.17107730524519535
iteration : 5674
train acc:  0.84375
train loss:  0.37290236353874207
train gradient:  0.22139004465986076
iteration : 5675
train acc:  0.84375
train loss:  0.34481388330459595
train gradient:  0.20074511195127787
iteration : 5676
train acc:  0.875
train loss:  0.32067227363586426
train gradient:  0.21458788381476682
iteration : 5677
train acc:  0.8515625
train loss:  0.33456793427467346
train gradient:  0.2047347196088022
iteration : 5678
train acc:  0.796875
train loss:  0.3599768280982971
train gradient:  0.2224042621983609
iteration : 5679
train acc:  0.875
train loss:  0.3380386233329773
train gradient:  0.2405547601363528
iteration : 5680
train acc:  0.8671875
train loss:  0.3105105757713318
train gradient:  0.2050787218916933
iteration : 5681
train acc:  0.859375
train loss:  0.3133928179740906
train gradient:  0.17428119804935785
iteration : 5682
train acc:  0.8359375
train loss:  0.3485898971557617
train gradient:  0.25295730332128896
iteration : 5683
train acc:  0.7890625
train loss:  0.4175516963005066
train gradient:  0.3608943115386024
iteration : 5684
train acc:  0.859375
train loss:  0.3345039486885071
train gradient:  0.15553200418397428
iteration : 5685
train acc:  0.8671875
train loss:  0.3702806830406189
train gradient:  0.2194682891429745
iteration : 5686
train acc:  0.8125
train loss:  0.43139272928237915
train gradient:  0.41243353637162866
iteration : 5687
train acc:  0.8046875
train loss:  0.4150595963001251
train gradient:  0.2841439796198975
iteration : 5688
train acc:  0.875
train loss:  0.32123807072639465
train gradient:  0.18442208887248762
iteration : 5689
train acc:  0.8359375
train loss:  0.34842923283576965
train gradient:  0.23268740701626622
iteration : 5690
train acc:  0.828125
train loss:  0.4024035930633545
train gradient:  0.40014504917402677
iteration : 5691
train acc:  0.828125
train loss:  0.35298287868499756
train gradient:  0.20124266794196094
iteration : 5692
train acc:  0.890625
train loss:  0.3096119165420532
train gradient:  0.2732043035679136
iteration : 5693
train acc:  0.8515625
train loss:  0.3547699451446533
train gradient:  0.2956481438885852
iteration : 5694
train acc:  0.875
train loss:  0.31840983033180237
train gradient:  0.1553982561109361
iteration : 5695
train acc:  0.8359375
train loss:  0.3774806261062622
train gradient:  0.2614430606555073
iteration : 5696
train acc:  0.84375
train loss:  0.3779563307762146
train gradient:  0.20771565581284074
iteration : 5697
train acc:  0.78125
train loss:  0.42878997325897217
train gradient:  0.3656584464202712
iteration : 5698
train acc:  0.828125
train loss:  0.38389360904693604
train gradient:  0.2654118282072415
iteration : 5699
train acc:  0.8359375
train loss:  0.3651200234889984
train gradient:  0.2641336212775161
iteration : 5700
train acc:  0.7890625
train loss:  0.4289986789226532
train gradient:  0.30335829807942083
iteration : 5701
train acc:  0.828125
train loss:  0.37565094232559204
train gradient:  0.3053329798668707
iteration : 5702
train acc:  0.8359375
train loss:  0.3625408113002777
train gradient:  0.25069367426392114
iteration : 5703
train acc:  0.890625
train loss:  0.2871396243572235
train gradient:  0.17493528708555697
iteration : 5704
train acc:  0.8125
train loss:  0.35470855236053467
train gradient:  0.26891822882731076
iteration : 5705
train acc:  0.875
train loss:  0.3503427505493164
train gradient:  0.20529657481634206
iteration : 5706
train acc:  0.859375
train loss:  0.32288217544555664
train gradient:  0.2053710119089112
iteration : 5707
train acc:  0.8125
train loss:  0.38290339708328247
train gradient:  0.3224897541010592
iteration : 5708
train acc:  0.8828125
train loss:  0.2993259131908417
train gradient:  0.14862695759346295
iteration : 5709
train acc:  0.8515625
train loss:  0.34563201665878296
train gradient:  0.24872593665230885
iteration : 5710
train acc:  0.8125
train loss:  0.4183061122894287
train gradient:  0.33918260037663667
iteration : 5711
train acc:  0.8125
train loss:  0.39818620681762695
train gradient:  0.33826843331814627
iteration : 5712
train acc:  0.84375
train loss:  0.3278142809867859
train gradient:  0.23686150156686103
iteration : 5713
train acc:  0.8515625
train loss:  0.31859439611434937
train gradient:  0.16103124876781383
iteration : 5714
train acc:  0.859375
train loss:  0.3283560276031494
train gradient:  0.18708622894874233
iteration : 5715
train acc:  0.8359375
train loss:  0.3769800662994385
train gradient:  0.24310050844324488
iteration : 5716
train acc:  0.8828125
train loss:  0.33768633008003235
train gradient:  0.23387920453812777
iteration : 5717
train acc:  0.859375
train loss:  0.3482646644115448
train gradient:  0.1767285422382799
iteration : 5718
train acc:  0.9140625
train loss:  0.25579774379730225
train gradient:  0.1560434635513515
iteration : 5719
train acc:  0.8984375
train loss:  0.3213377594947815
train gradient:  0.16818948091347247
iteration : 5720
train acc:  0.7578125
train loss:  0.49655210971832275
train gradient:  0.39604219395809714
iteration : 5721
train acc:  0.8359375
train loss:  0.3320830464363098
train gradient:  0.19322803733694308
iteration : 5722
train acc:  0.8125
train loss:  0.38496923446655273
train gradient:  0.3974676611725449
iteration : 5723
train acc:  0.890625
train loss:  0.2867565155029297
train gradient:  0.21252324280947518
iteration : 5724
train acc:  0.8125
train loss:  0.4091746211051941
train gradient:  0.2797201552055289
iteration : 5725
train acc:  0.859375
train loss:  0.2721772789955139
train gradient:  0.15300285274620723
iteration : 5726
train acc:  0.8046875
train loss:  0.4056280851364136
train gradient:  0.24853426540077217
iteration : 5727
train acc:  0.890625
train loss:  0.32238534092903137
train gradient:  0.21897307388567394
iteration : 5728
train acc:  0.9140625
train loss:  0.255561888217926
train gradient:  0.11368753561572818
iteration : 5729
train acc:  0.84375
train loss:  0.3506777286529541
train gradient:  0.2830714351318881
iteration : 5730
train acc:  0.890625
train loss:  0.3223267197608948
train gradient:  0.23511701488340947
iteration : 5731
train acc:  0.875
train loss:  0.3593577742576599
train gradient:  0.19002682330328274
iteration : 5732
train acc:  0.828125
train loss:  0.3465045988559723
train gradient:  0.20648520469009551
iteration : 5733
train acc:  0.78125
train loss:  0.4185875356197357
train gradient:  0.4505244248603572
iteration : 5734
train acc:  0.796875
train loss:  0.3561420440673828
train gradient:  0.19232767633155762
iteration : 5735
train acc:  0.8359375
train loss:  0.43095913529396057
train gradient:  0.3605131357376541
iteration : 5736
train acc:  0.859375
train loss:  0.31067955493927
train gradient:  0.154370646564095
iteration : 5737
train acc:  0.8671875
train loss:  0.34091508388519287
train gradient:  0.19286741500028484
iteration : 5738
train acc:  0.859375
train loss:  0.29662519693374634
train gradient:  0.1392594245437313
iteration : 5739
train acc:  0.796875
train loss:  0.43673327565193176
train gradient:  0.4030836194313484
iteration : 5740
train acc:  0.8125
train loss:  0.3987785279750824
train gradient:  0.3392514255810292
iteration : 5741
train acc:  0.8671875
train loss:  0.3438517153263092
train gradient:  0.17713103373136174
iteration : 5742
train acc:  0.8203125
train loss:  0.4255863428115845
train gradient:  0.2705654257234857
iteration : 5743
train acc:  0.8984375
train loss:  0.22973410785198212
train gradient:  0.12594499766016493
iteration : 5744
train acc:  0.8203125
train loss:  0.3778051733970642
train gradient:  0.2945229910507983
iteration : 5745
train acc:  0.859375
train loss:  0.35001182556152344
train gradient:  0.1908971358350336
iteration : 5746
train acc:  0.8359375
train loss:  0.36537647247314453
train gradient:  0.22330642950686727
iteration : 5747
train acc:  0.8828125
train loss:  0.3051506280899048
train gradient:  0.18886941751895597
iteration : 5748
train acc:  0.8125
train loss:  0.3995152711868286
train gradient:  0.25509222534221776
iteration : 5749
train acc:  0.8125
train loss:  0.3583417534828186
train gradient:  0.24149766352815577
iteration : 5750
train acc:  0.875
train loss:  0.30999842286109924
train gradient:  0.21340130756107933
iteration : 5751
train acc:  0.8515625
train loss:  0.3366868495941162
train gradient:  0.17348258624305643
iteration : 5752
train acc:  0.84375
train loss:  0.358079731464386
train gradient:  0.2538101830311043
iteration : 5753
train acc:  0.84375
train loss:  0.3174412250518799
train gradient:  0.2525142139050437
iteration : 5754
train acc:  0.8671875
train loss:  0.3065943121910095
train gradient:  0.19761366713826303
iteration : 5755
train acc:  0.84375
train loss:  0.34220391511917114
train gradient:  0.21625411237230974
iteration : 5756
train acc:  0.8828125
train loss:  0.332109272480011
train gradient:  0.27342317560709817
iteration : 5757
train acc:  0.8515625
train loss:  0.35091283917427063
train gradient:  0.20818897036715264
iteration : 5758
train acc:  0.84375
train loss:  0.3549022078514099
train gradient:  0.23381628115543096
iteration : 5759
train acc:  0.90625
train loss:  0.309341162443161
train gradient:  0.16040628694484593
iteration : 5760
train acc:  0.8828125
train loss:  0.34846389293670654
train gradient:  0.18804999268282532
iteration : 5761
train acc:  0.8671875
train loss:  0.331144779920578
train gradient:  0.1769059794527738
iteration : 5762
train acc:  0.84375
train loss:  0.3559931218624115
train gradient:  0.24050152945469216
iteration : 5763
train acc:  0.875
train loss:  0.3507550358772278
train gradient:  0.22067932927499276
iteration : 5764
train acc:  0.84375
train loss:  0.34641745686531067
train gradient:  0.31027801231777036
iteration : 5765
train acc:  0.8671875
train loss:  0.3160461485385895
train gradient:  0.17059531032748465
iteration : 5766
train acc:  0.875
train loss:  0.3032403886318207
train gradient:  0.2497302272585334
iteration : 5767
train acc:  0.7890625
train loss:  0.406808465719223
train gradient:  0.26170097283776134
iteration : 5768
train acc:  0.859375
train loss:  0.3905385732650757
train gradient:  0.19653940397461261
iteration : 5769
train acc:  0.8203125
train loss:  0.3795018196105957
train gradient:  0.2568187520560483
iteration : 5770
train acc:  0.8828125
train loss:  0.2744511365890503
train gradient:  0.21493169494294107
iteration : 5771
train acc:  0.875
train loss:  0.3066873550415039
train gradient:  0.24451607236653566
iteration : 5772
train acc:  0.8828125
train loss:  0.2823248505592346
train gradient:  0.18351879106303587
iteration : 5773
train acc:  0.9140625
train loss:  0.30069100856781006
train gradient:  0.23243755837175428
iteration : 5774
train acc:  0.890625
train loss:  0.27835631370544434
train gradient:  0.12435923845681235
iteration : 5775
train acc:  0.8203125
train loss:  0.38088980317115784
train gradient:  0.3443828710018598
iteration : 5776
train acc:  0.8125
train loss:  0.39300715923309326
train gradient:  0.24748540195147273
iteration : 5777
train acc:  0.875
train loss:  0.27855172753334045
train gradient:  0.21057618606140904
iteration : 5778
train acc:  0.875
train loss:  0.29623377323150635
train gradient:  0.20573036845199655
iteration : 5779
train acc:  0.8515625
train loss:  0.40355220437049866
train gradient:  0.264236374198939
iteration : 5780
train acc:  0.9296875
train loss:  0.2378990799188614
train gradient:  0.14520355172426713
iteration : 5781
train acc:  0.8046875
train loss:  0.4089486002922058
train gradient:  0.30415853828840056
iteration : 5782
train acc:  0.859375
train loss:  0.35093408823013306
train gradient:  0.2879581378070133
iteration : 5783
train acc:  0.859375
train loss:  0.32202571630477905
train gradient:  0.18746435267841877
iteration : 5784
train acc:  0.8671875
train loss:  0.28031033277511597
train gradient:  0.18161331281877519
iteration : 5785
train acc:  0.8671875
train loss:  0.3293088376522064
train gradient:  0.24737188416408937
iteration : 5786
train acc:  0.8828125
train loss:  0.3583354651927948
train gradient:  0.22087714216192364
iteration : 5787
train acc:  0.875
train loss:  0.31617942452430725
train gradient:  0.19265761182512436
iteration : 5788
train acc:  0.828125
train loss:  0.38665252923965454
train gradient:  0.3026587920074459
iteration : 5789
train acc:  0.8828125
train loss:  0.28846293687820435
train gradient:  0.15176128441148679
iteration : 5790
train acc:  0.8203125
train loss:  0.39375296235084534
train gradient:  0.29835020943958057
iteration : 5791
train acc:  0.84375
train loss:  0.3584004342556
train gradient:  0.22543621335450686
iteration : 5792
train acc:  0.875
train loss:  0.29855167865753174
train gradient:  0.15584963226478235
iteration : 5793
train acc:  0.8515625
train loss:  0.3995940387248993
train gradient:  0.32175481292682345
iteration : 5794
train acc:  0.8828125
train loss:  0.32356977462768555
train gradient:  0.24381128652963713
iteration : 5795
train acc:  0.8125
train loss:  0.38065770268440247
train gradient:  0.2327190591750761
iteration : 5796
train acc:  0.890625
train loss:  0.26371732354164124
train gradient:  0.1915784735019224
iteration : 5797
train acc:  0.8671875
train loss:  0.33291763067245483
train gradient:  0.24863991468291172
iteration : 5798
train acc:  0.7890625
train loss:  0.39979714155197144
train gradient:  0.34078870636424574
iteration : 5799
train acc:  0.828125
train loss:  0.392948716878891
train gradient:  0.28916684583846725
iteration : 5800
train acc:  0.8671875
train loss:  0.3138994574546814
train gradient:  0.199720588736487
iteration : 5801
train acc:  0.796875
train loss:  0.3652712404727936
train gradient:  0.30990445536189254
iteration : 5802
train acc:  0.8515625
train loss:  0.3968747854232788
train gradient:  0.2618468577421374
iteration : 5803
train acc:  0.84375
train loss:  0.3925520181655884
train gradient:  0.3593188753633874
iteration : 5804
train acc:  0.859375
train loss:  0.3486166298389435
train gradient:  0.19375212890691834
iteration : 5805
train acc:  0.8671875
train loss:  0.3119664490222931
train gradient:  0.2490446537002088
iteration : 5806
train acc:  0.875
train loss:  0.2855478525161743
train gradient:  0.23365823868298274
iteration : 5807
train acc:  0.890625
train loss:  0.311508446931839
train gradient:  0.1643651594096382
iteration : 5808
train acc:  0.8203125
train loss:  0.44292888045310974
train gradient:  0.36508525604842607
iteration : 5809
train acc:  0.8359375
train loss:  0.36171698570251465
train gradient:  0.1790711419461222
iteration : 5810
train acc:  0.859375
train loss:  0.31860846281051636
train gradient:  0.1609708939558458
iteration : 5811
train acc:  0.8046875
train loss:  0.33772552013397217
train gradient:  0.2021799684451509
iteration : 5812
train acc:  0.7421875
train loss:  0.5885995030403137
train gradient:  0.49999452613754736
iteration : 5813
train acc:  0.828125
train loss:  0.41293632984161377
train gradient:  0.25808961091416777
iteration : 5814
train acc:  0.8515625
train loss:  0.3048056662082672
train gradient:  0.1892927971342107
iteration : 5815
train acc:  0.8828125
train loss:  0.26171475648880005
train gradient:  0.16054656231526743
iteration : 5816
train acc:  0.859375
train loss:  0.2931525409221649
train gradient:  0.14674675433733722
iteration : 5817
train acc:  0.859375
train loss:  0.30615949630737305
train gradient:  0.1375270798843593
iteration : 5818
train acc:  0.8671875
train loss:  0.32388120889663696
train gradient:  0.19634193154610408
iteration : 5819
train acc:  0.8359375
train loss:  0.344221293926239
train gradient:  0.2177643603184057
iteration : 5820
train acc:  0.84375
train loss:  0.35529831051826477
train gradient:  0.2261792867217629
iteration : 5821
train acc:  0.8984375
train loss:  0.28950631618499756
train gradient:  0.15163770270621604
iteration : 5822
train acc:  0.859375
train loss:  0.3365045487880707
train gradient:  0.21926938036220334
iteration : 5823
train acc:  0.890625
train loss:  0.32537782192230225
train gradient:  0.15058924507705262
iteration : 5824
train acc:  0.859375
train loss:  0.3755660653114319
train gradient:  0.3341505617318686
iteration : 5825
train acc:  0.84375
train loss:  0.2815779447555542
train gradient:  0.15821487776306076
iteration : 5826
train acc:  0.84375
train loss:  0.36409634351730347
train gradient:  0.2018256759151308
iteration : 5827
train acc:  0.8515625
train loss:  0.3170936703681946
train gradient:  0.24412392153058704
iteration : 5828
train acc:  0.875
train loss:  0.34519749879837036
train gradient:  0.3253734151236107
iteration : 5829
train acc:  0.84375
train loss:  0.3762791156768799
train gradient:  0.2328055321767953
iteration : 5830
train acc:  0.875
train loss:  0.3182608485221863
train gradient:  0.20008772491618543
iteration : 5831
train acc:  0.8046875
train loss:  0.3552039861679077
train gradient:  0.2163471798213527
iteration : 5832
train acc:  0.8828125
train loss:  0.2692974805831909
train gradient:  0.13622141300805785
iteration : 5833
train acc:  0.84375
train loss:  0.3708333373069763
train gradient:  0.2156682430145587
iteration : 5834
train acc:  0.84375
train loss:  0.35588690638542175
train gradient:  0.24068650318465518
iteration : 5835
train acc:  0.796875
train loss:  0.3807715177536011
train gradient:  0.26319817212033664
iteration : 5836
train acc:  0.859375
train loss:  0.30777502059936523
train gradient:  0.35170137481442043
iteration : 5837
train acc:  0.890625
train loss:  0.29108864068984985
train gradient:  0.15273791131848768
iteration : 5838
train acc:  0.859375
train loss:  0.29921755194664
train gradient:  0.1483292038422181
iteration : 5839
train acc:  0.828125
train loss:  0.406924843788147
train gradient:  0.47750871575047726
iteration : 5840
train acc:  0.875
train loss:  0.3291209936141968
train gradient:  0.21980451625789918
iteration : 5841
train acc:  0.859375
train loss:  0.33227264881134033
train gradient:  0.15962773369209976
iteration : 5842
train acc:  0.8125
train loss:  0.3848336338996887
train gradient:  0.2488648024832935
iteration : 5843
train acc:  0.8359375
train loss:  0.41821396350860596
train gradient:  0.2923539282502984
iteration : 5844
train acc:  0.84375
train loss:  0.2943108081817627
train gradient:  0.19119557393864403
iteration : 5845
train acc:  0.8125
train loss:  0.38563549518585205
train gradient:  0.2019017672999213
iteration : 5846
train acc:  0.8828125
train loss:  0.2634984254837036
train gradient:  0.2609973754909326
iteration : 5847
train acc:  0.8046875
train loss:  0.39229387044906616
train gradient:  0.42859472220429223
iteration : 5848
train acc:  0.84375
train loss:  0.3984035849571228
train gradient:  0.3749948803645928
iteration : 5849
train acc:  0.8359375
train loss:  0.3548000156879425
train gradient:  0.32130749857160185
iteration : 5850
train acc:  0.828125
train loss:  0.35972216725349426
train gradient:  0.20187365787404582
iteration : 5851
train acc:  0.8203125
train loss:  0.38591691851615906
train gradient:  0.19950241914505498
iteration : 5852
train acc:  0.8046875
train loss:  0.4437442123889923
train gradient:  0.32908484126527116
iteration : 5853
train acc:  0.8203125
train loss:  0.4171198010444641
train gradient:  0.24615872986060475
iteration : 5854
train acc:  0.8828125
train loss:  0.2899624705314636
train gradient:  0.22184364223766273
iteration : 5855
train acc:  0.8046875
train loss:  0.36062943935394287
train gradient:  0.24230739907752516
iteration : 5856
train acc:  0.828125
train loss:  0.35251420736312866
train gradient:  0.178998057628947
iteration : 5857
train acc:  0.8828125
train loss:  0.2926561236381531
train gradient:  0.21232807724468972
iteration : 5858
train acc:  0.875
train loss:  0.31644999980926514
train gradient:  0.17986642230149916
iteration : 5859
train acc:  0.890625
train loss:  0.2930413484573364
train gradient:  0.1681597854558362
iteration : 5860
train acc:  0.8671875
train loss:  0.3357810378074646
train gradient:  0.1928072612452615
iteration : 5861
train acc:  0.8125
train loss:  0.34572046995162964
train gradient:  0.19863443680302106
iteration : 5862
train acc:  0.8984375
train loss:  0.2439557909965515
train gradient:  0.14858394625694493
iteration : 5863
train acc:  0.828125
train loss:  0.39423924684524536
train gradient:  0.24564439634838783
iteration : 5864
train acc:  0.84375
train loss:  0.4553264379501343
train gradient:  0.34433027542642286
iteration : 5865
train acc:  0.828125
train loss:  0.4373924434185028
train gradient:  0.3449811501035719
iteration : 5866
train acc:  0.859375
train loss:  0.3454624116420746
train gradient:  0.3435545347141823
iteration : 5867
train acc:  0.84375
train loss:  0.30945920944213867
train gradient:  0.20990767710576635
iteration : 5868
train acc:  0.828125
train loss:  0.4084617495536804
train gradient:  0.3133744590098189
iteration : 5869
train acc:  0.8671875
train loss:  0.3581152558326721
train gradient:  0.23743780037402903
iteration : 5870
train acc:  0.875
train loss:  0.2800333797931671
train gradient:  0.19136982834579364
iteration : 5871
train acc:  0.859375
train loss:  0.3712710440158844
train gradient:  0.24041641468429403
iteration : 5872
train acc:  0.8359375
train loss:  0.33174967765808105
train gradient:  0.23024649247226645
iteration : 5873
train acc:  0.796875
train loss:  0.39269599318504333
train gradient:  0.19672406725069955
iteration : 5874
train acc:  0.859375
train loss:  0.2798497676849365
train gradient:  0.19122518756134455
iteration : 5875
train acc:  0.828125
train loss:  0.3857523500919342
train gradient:  0.27618176768107255
iteration : 5876
train acc:  0.8828125
train loss:  0.294795423746109
train gradient:  0.18344685869664995
iteration : 5877
train acc:  0.8203125
train loss:  0.4407968819141388
train gradient:  0.33958058726702867
iteration : 5878
train acc:  0.8359375
train loss:  0.3998013138771057
train gradient:  0.2917951272968168
iteration : 5879
train acc:  0.8125
train loss:  0.42643508315086365
train gradient:  0.32014966501422293
iteration : 5880
train acc:  0.8671875
train loss:  0.2905597984790802
train gradient:  0.15235565207502735
iteration : 5881
train acc:  0.875
train loss:  0.3033190369606018
train gradient:  0.21080962516707197
iteration : 5882
train acc:  0.8671875
train loss:  0.2852209508419037
train gradient:  0.16137686432416648
iteration : 5883
train acc:  0.828125
train loss:  0.3577471971511841
train gradient:  0.2779585849496627
iteration : 5884
train acc:  0.8515625
train loss:  0.3747749328613281
train gradient:  0.23534896329334012
iteration : 5885
train acc:  0.8203125
train loss:  0.41373419761657715
train gradient:  0.30315615170384513
iteration : 5886
train acc:  0.875
train loss:  0.30500924587249756
train gradient:  0.1793931920251637
iteration : 5887
train acc:  0.7890625
train loss:  0.4731613099575043
train gradient:  0.34089403488600845
iteration : 5888
train acc:  0.8203125
train loss:  0.37174057960510254
train gradient:  0.4320633744553183
iteration : 5889
train acc:  0.84375
train loss:  0.3434112071990967
train gradient:  0.14234552455454083
iteration : 5890
train acc:  0.84375
train loss:  0.3038225769996643
train gradient:  0.1639008088846115
iteration : 5891
train acc:  0.890625
train loss:  0.3134429156780243
train gradient:  0.20306022668711113
iteration : 5892
train acc:  0.8125
train loss:  0.41035354137420654
train gradient:  0.3750789604068586
iteration : 5893
train acc:  0.828125
train loss:  0.3819449543952942
train gradient:  0.24897297827522197
iteration : 5894
train acc:  0.8671875
train loss:  0.3551506996154785
train gradient:  0.26568564774130465
iteration : 5895
train acc:  0.78125
train loss:  0.42399466037750244
train gradient:  0.3066643166659379
iteration : 5896
train acc:  0.8984375
train loss:  0.28926998376846313
train gradient:  0.24570324884908357
iteration : 5897
train acc:  0.8671875
train loss:  0.32539886236190796
train gradient:  0.2818981778501259
iteration : 5898
train acc:  0.8203125
train loss:  0.3933853209018707
train gradient:  0.31899578293985587
iteration : 5899
train acc:  0.8671875
train loss:  0.3109907805919647
train gradient:  0.20387329014323444
iteration : 5900
train acc:  0.78125
train loss:  0.42820996046066284
train gradient:  0.3160426729304098
iteration : 5901
train acc:  0.8515625
train loss:  0.34202343225479126
train gradient:  0.17112166297925152
iteration : 5902
train acc:  0.84375
train loss:  0.3608757555484772
train gradient:  0.2841005050855815
iteration : 5903
train acc:  0.8671875
train loss:  0.3129972219467163
train gradient:  0.13944068073360028
iteration : 5904
train acc:  0.875
train loss:  0.349663108587265
train gradient:  0.19448374745356062
iteration : 5905
train acc:  0.8515625
train loss:  0.4004130959510803
train gradient:  0.2442135462098719
iteration : 5906
train acc:  0.84375
train loss:  0.3149348795413971
train gradient:  0.20183646044567127
iteration : 5907
train acc:  0.875
train loss:  0.3386131823062897
train gradient:  0.15519383814857068
iteration : 5908
train acc:  0.8828125
train loss:  0.31679096817970276
train gradient:  0.14380874557476583
iteration : 5909
train acc:  0.8046875
train loss:  0.4602457582950592
train gradient:  0.4021480496336223
iteration : 5910
train acc:  0.890625
train loss:  0.3082534372806549
train gradient:  0.23725771133923693
iteration : 5911
train acc:  0.84375
train loss:  0.2930271029472351
train gradient:  0.1648484281951615
iteration : 5912
train acc:  0.859375
train loss:  0.2845413386821747
train gradient:  0.21233964396065363
iteration : 5913
train acc:  0.796875
train loss:  0.40683192014694214
train gradient:  0.2952631536619978
iteration : 5914
train acc:  0.8515625
train loss:  0.38666701316833496
train gradient:  0.28849730158755504
iteration : 5915
train acc:  0.859375
train loss:  0.3536977469921112
train gradient:  0.24230106012534688
iteration : 5916
train acc:  0.875
train loss:  0.27869388461112976
train gradient:  0.13009346231754385
iteration : 5917
train acc:  0.7421875
train loss:  0.4841821491718292
train gradient:  0.3676621817890469
iteration : 5918
train acc:  0.8671875
train loss:  0.36838531494140625
train gradient:  0.2170334373269747
iteration : 5919
train acc:  0.8828125
train loss:  0.32832086086273193
train gradient:  0.21445484080574784
iteration : 5920
train acc:  0.8203125
train loss:  0.41627979278564453
train gradient:  0.20211292870714628
iteration : 5921
train acc:  0.828125
train loss:  0.3902839422225952
train gradient:  0.28323028651267934
iteration : 5922
train acc:  0.8515625
train loss:  0.40970006585121155
train gradient:  0.47780171065147997
iteration : 5923
train acc:  0.875
train loss:  0.2522916793823242
train gradient:  0.16874237808288428
iteration : 5924
train acc:  0.84375
train loss:  0.3752421736717224
train gradient:  0.22273094419935416
iteration : 5925
train acc:  0.90625
train loss:  0.282897025346756
train gradient:  0.16975272188939763
iteration : 5926
train acc:  0.8671875
train loss:  0.3149130344390869
train gradient:  0.22716376887060555
iteration : 5927
train acc:  0.9140625
train loss:  0.2663341164588928
train gradient:  0.19339625269142996
iteration : 5928
train acc:  0.8359375
train loss:  0.367681622505188
train gradient:  0.18367852837514517
iteration : 5929
train acc:  0.828125
train loss:  0.39648061990737915
train gradient:  0.25775364040931464
iteration : 5930
train acc:  0.875
train loss:  0.35129857063293457
train gradient:  0.2751714394145357
iteration : 5931
train acc:  0.8359375
train loss:  0.35119742155075073
train gradient:  0.18459129020159057
iteration : 5932
train acc:  0.890625
train loss:  0.2771944999694824
train gradient:  0.2264013330478616
iteration : 5933
train acc:  0.8984375
train loss:  0.2862395644187927
train gradient:  0.1513559732038954
iteration : 5934
train acc:  0.859375
train loss:  0.3366125822067261
train gradient:  0.22538172088610542
iteration : 5935
train acc:  0.796875
train loss:  0.43149062991142273
train gradient:  0.2673249979423944
iteration : 5936
train acc:  0.875
train loss:  0.274772047996521
train gradient:  0.16149725255782332
iteration : 5937
train acc:  0.859375
train loss:  0.3410751223564148
train gradient:  0.269335916803157
iteration : 5938
train acc:  0.8359375
train loss:  0.35469722747802734
train gradient:  0.2301232520217652
iteration : 5939
train acc:  0.8125
train loss:  0.3720243573188782
train gradient:  0.24851426135905808
iteration : 5940
train acc:  0.890625
train loss:  0.3118939697742462
train gradient:  0.17866809297581712
iteration : 5941
train acc:  0.875
train loss:  0.3443639278411865
train gradient:  0.2087891679587927
iteration : 5942
train acc:  0.859375
train loss:  0.32735103368759155
train gradient:  0.22864990544757488
iteration : 5943
train acc:  0.8984375
train loss:  0.28901219367980957
train gradient:  0.1418749960942714
iteration : 5944
train acc:  0.875
train loss:  0.3152202367782593
train gradient:  0.148627240339555
iteration : 5945
train acc:  0.859375
train loss:  0.3208252191543579
train gradient:  0.19703536153648338
iteration : 5946
train acc:  0.8125
train loss:  0.36606478691101074
train gradient:  0.3057926854364103
iteration : 5947
train acc:  0.8671875
train loss:  0.36170950531959534
train gradient:  0.2566248905157591
iteration : 5948
train acc:  0.8984375
train loss:  0.2775346636772156
train gradient:  0.15876212021310385
iteration : 5949
train acc:  0.890625
train loss:  0.26665085554122925
train gradient:  0.22300074829665212
iteration : 5950
train acc:  0.8515625
train loss:  0.3805122673511505
train gradient:  0.38012090109405067
iteration : 5951
train acc:  0.8359375
train loss:  0.38685256242752075
train gradient:  0.3484272183147879
iteration : 5952
train acc:  0.796875
train loss:  0.4738892912864685
train gradient:  0.33752033920474117
iteration : 5953
train acc:  0.828125
train loss:  0.38155683875083923
train gradient:  0.31486038695797036
iteration : 5954
train acc:  0.8671875
train loss:  0.33866626024246216
train gradient:  0.19051377333760672
iteration : 5955
train acc:  0.8203125
train loss:  0.4157083034515381
train gradient:  0.30656333759610793
iteration : 5956
train acc:  0.8515625
train loss:  0.317190557718277
train gradient:  0.1990695580294259
iteration : 5957
train acc:  0.796875
train loss:  0.41182729601860046
train gradient:  0.2771236492042787
iteration : 5958
train acc:  0.8671875
train loss:  0.31591665744781494
train gradient:  0.1343920168394955
iteration : 5959
train acc:  0.8984375
train loss:  0.24938258528709412
train gradient:  0.12296615560915782
iteration : 5960
train acc:  0.8203125
train loss:  0.34542080760002136
train gradient:  0.2774622844772709
iteration : 5961
train acc:  0.828125
train loss:  0.3244980275630951
train gradient:  0.2427629863155168
iteration : 5962
train acc:  0.84375
train loss:  0.36401012539863586
train gradient:  0.18439966535169347
iteration : 5963
train acc:  0.8203125
train loss:  0.3615628778934479
train gradient:  0.32070787238494874
iteration : 5964
train acc:  0.875
train loss:  0.33505338430404663
train gradient:  0.24275450546989213
iteration : 5965
train acc:  0.8359375
train loss:  0.29144179821014404
train gradient:  0.19057506663913193
iteration : 5966
train acc:  0.8671875
train loss:  0.3017413914203644
train gradient:  0.21774640380022992
iteration : 5967
train acc:  0.84375
train loss:  0.3046053647994995
train gradient:  0.14031932224319943
iteration : 5968
train acc:  0.875
train loss:  0.33586424589157104
train gradient:  0.20882190525806835
iteration : 5969
train acc:  0.8359375
train loss:  0.3678276538848877
train gradient:  0.2699669347332809
iteration : 5970
train acc:  0.84375
train loss:  0.3790391683578491
train gradient:  0.3160913506244903
iteration : 5971
train acc:  0.8828125
train loss:  0.28069037199020386
train gradient:  0.16712271101399134
iteration : 5972
train acc:  0.859375
train loss:  0.3042382597923279
train gradient:  0.1610459161297873
iteration : 5973
train acc:  0.859375
train loss:  0.28982266783714294
train gradient:  0.17229421866392797
iteration : 5974
train acc:  0.8359375
train loss:  0.3411054015159607
train gradient:  0.18419392445344865
iteration : 5975
train acc:  0.84375
train loss:  0.3489047884941101
train gradient:  0.3056294442148741
iteration : 5976
train acc:  0.8203125
train loss:  0.4157513678073883
train gradient:  0.2775890729054241
iteration : 5977
train acc:  0.8828125
train loss:  0.3143860697746277
train gradient:  0.19876005747163933
iteration : 5978
train acc:  0.8671875
train loss:  0.34062981605529785
train gradient:  0.2107997220307664
iteration : 5979
train acc:  0.8828125
train loss:  0.2795753479003906
train gradient:  0.18431315661958542
iteration : 5980
train acc:  0.890625
train loss:  0.3200896084308624
train gradient:  0.2652103152514527
iteration : 5981
train acc:  0.7890625
train loss:  0.39797940850257874
train gradient:  0.3236708063760589
iteration : 5982
train acc:  0.8984375
train loss:  0.2645457983016968
train gradient:  0.15353409747360816
iteration : 5983
train acc:  0.859375
train loss:  0.3051745891571045
train gradient:  0.1438920777078614
iteration : 5984
train acc:  0.84375
train loss:  0.3221951127052307
train gradient:  0.184971357354708
iteration : 5985
train acc:  0.890625
train loss:  0.2930217385292053
train gradient:  0.20204967611228017
iteration : 5986
train acc:  0.796875
train loss:  0.41805779933929443
train gradient:  0.2622630975893358
iteration : 5987
train acc:  0.90625
train loss:  0.25747400522232056
train gradient:  0.16686492600457334
iteration : 5988
train acc:  0.8359375
train loss:  0.33886051177978516
train gradient:  0.28465769229656546
iteration : 5989
train acc:  0.8515625
train loss:  0.3216111660003662
train gradient:  0.25969049978247255
iteration : 5990
train acc:  0.84375
train loss:  0.34144800901412964
train gradient:  0.21845043533808028
iteration : 5991
train acc:  0.8515625
train loss:  0.30793678760528564
train gradient:  0.2325549923538517
iteration : 5992
train acc:  0.7890625
train loss:  0.4285603165626526
train gradient:  0.29913805596214066
iteration : 5993
train acc:  0.828125
train loss:  0.3886668086051941
train gradient:  0.3999487934910564
iteration : 5994
train acc:  0.859375
train loss:  0.3063151240348816
train gradient:  0.24676675701755668
iteration : 5995
train acc:  0.8828125
train loss:  0.3002721071243286
train gradient:  0.18583098342194915
iteration : 5996
train acc:  0.9296875
train loss:  0.25093692541122437
train gradient:  0.1636769442496497
iteration : 5997
train acc:  0.859375
train loss:  0.3288322687149048
train gradient:  0.2611026337890547
iteration : 5998
train acc:  0.8828125
train loss:  0.30977487564086914
train gradient:  0.14501422042308187
iteration : 5999
train acc:  0.84375
train loss:  0.394002228975296
train gradient:  0.25956113539078096
iteration : 6000
train acc:  0.828125
train loss:  0.352140337228775
train gradient:  0.21204680368911866
iteration : 6001
train acc:  0.890625
train loss:  0.3570895791053772
train gradient:  0.2491919225981455
iteration : 6002
train acc:  0.8828125
train loss:  0.3053215742111206
train gradient:  0.1582723072094781
iteration : 6003
train acc:  0.875
train loss:  0.26276278495788574
train gradient:  0.1923663543028637
iteration : 6004
train acc:  0.890625
train loss:  0.33404549956321716
train gradient:  0.27570442802446304
iteration : 6005
train acc:  0.8671875
train loss:  0.31710442900657654
train gradient:  0.27927643994859946
iteration : 6006
train acc:  0.84375
train loss:  0.334089994430542
train gradient:  0.35232875377447326
iteration : 6007
train acc:  0.90625
train loss:  0.29768648743629456
train gradient:  0.15893312337400753
iteration : 6008
train acc:  0.859375
train loss:  0.33871692419052124
train gradient:  0.2806512243065468
iteration : 6009
train acc:  0.8203125
train loss:  0.3209122121334076
train gradient:  0.22088790341339853
iteration : 6010
train acc:  0.8984375
train loss:  0.2865537405014038
train gradient:  0.1653647413701694
iteration : 6011
train acc:  0.8046875
train loss:  0.40168997645378113
train gradient:  0.22656516344420946
iteration : 6012
train acc:  0.875
train loss:  0.30367255210876465
train gradient:  0.18759113418853346
iteration : 6013
train acc:  0.859375
train loss:  0.3516516089439392
train gradient:  0.24011013645020268
iteration : 6014
train acc:  0.8125
train loss:  0.31587404012680054
train gradient:  0.151015820116377
iteration : 6015
train acc:  0.890625
train loss:  0.2572634816169739
train gradient:  0.11615916710900259
iteration : 6016
train acc:  0.8359375
train loss:  0.3528488278388977
train gradient:  0.2018410898428392
iteration : 6017
train acc:  0.859375
train loss:  0.3884057402610779
train gradient:  0.29467519146892046
iteration : 6018
train acc:  0.78125
train loss:  0.4406060576438904
train gradient:  0.4253838898857572
iteration : 6019
train acc:  0.8671875
train loss:  0.373792439699173
train gradient:  0.29603753217287315
iteration : 6020
train acc:  0.8359375
train loss:  0.4036470651626587
train gradient:  0.31382558698317775
iteration : 6021
train acc:  0.8984375
train loss:  0.2989847958087921
train gradient:  0.12796000729204884
iteration : 6022
train acc:  0.8203125
train loss:  0.36099421977996826
train gradient:  0.31404757356302715
iteration : 6023
train acc:  0.8203125
train loss:  0.3822591006755829
train gradient:  0.2759639720045254
iteration : 6024
train acc:  0.8046875
train loss:  0.3826454281806946
train gradient:  0.35131655674841994
iteration : 6025
train acc:  0.8359375
train loss:  0.33344781398773193
train gradient:  0.19201214628848262
iteration : 6026
train acc:  0.8515625
train loss:  0.3203244209289551
train gradient:  0.1881102127057259
iteration : 6027
train acc:  0.8125
train loss:  0.35343417525291443
train gradient:  0.21174619070510728
iteration : 6028
train acc:  0.875
train loss:  0.25140202045440674
train gradient:  0.12789805870113324
iteration : 6029
train acc:  0.8359375
train loss:  0.3556554317474365
train gradient:  0.21064832645603812
iteration : 6030
train acc:  0.875
train loss:  0.29395079612731934
train gradient:  0.18829853429961113
iteration : 6031
train acc:  0.78125
train loss:  0.3849591016769409
train gradient:  0.24631265381463135
iteration : 6032
train acc:  0.8125
train loss:  0.344819039106369
train gradient:  0.16719065473286898
iteration : 6033
train acc:  0.8671875
train loss:  0.26473355293273926
train gradient:  0.15245966354103252
iteration : 6034
train acc:  0.8203125
train loss:  0.4267612397670746
train gradient:  0.324770834271874
iteration : 6035
train acc:  0.8984375
train loss:  0.2719425559043884
train gradient:  0.17000948959878265
iteration : 6036
train acc:  0.875
train loss:  0.32869207859039307
train gradient:  0.19839638332130538
iteration : 6037
train acc:  0.84375
train loss:  0.3250661790370941
train gradient:  0.24390214359066298
iteration : 6038
train acc:  0.8203125
train loss:  0.3881485164165497
train gradient:  0.2568938464122277
iteration : 6039
train acc:  0.859375
train loss:  0.350150465965271
train gradient:  0.18391283162370614
iteration : 6040
train acc:  0.8203125
train loss:  0.35788077116012573
train gradient:  0.27426870167536616
iteration : 6041
train acc:  0.875
train loss:  0.2920456528663635
train gradient:  0.13306691981520255
iteration : 6042
train acc:  0.8671875
train loss:  0.29385703802108765
train gradient:  0.14355988018384225
iteration : 6043
train acc:  0.859375
train loss:  0.3560274839401245
train gradient:  0.2179833045338933
iteration : 6044
train acc:  0.7890625
train loss:  0.4550873935222626
train gradient:  0.31673694859247514
iteration : 6045
train acc:  0.84375
train loss:  0.3298398554325104
train gradient:  0.21630596233004865
iteration : 6046
train acc:  0.875
train loss:  0.3187849521636963
train gradient:  0.1756204988392347
iteration : 6047
train acc:  0.8515625
train loss:  0.4178096652030945
train gradient:  0.2729784688214436
iteration : 6048
train acc:  0.828125
train loss:  0.30220693349838257
train gradient:  0.1805293135177915
iteration : 6049
train acc:  0.875
train loss:  0.28264686465263367
train gradient:  0.16076737690406653
iteration : 6050
train acc:  0.8125
train loss:  0.3616020083427429
train gradient:  0.23170981675680255
iteration : 6051
train acc:  0.8125
train loss:  0.3433802127838135
train gradient:  0.2536756633119907
iteration : 6052
train acc:  0.8359375
train loss:  0.3536175787448883
train gradient:  0.34954909925495464
iteration : 6053
train acc:  0.859375
train loss:  0.2899497151374817
train gradient:  0.17210176905248892
iteration : 6054
train acc:  0.8515625
train loss:  0.3435988426208496
train gradient:  0.17004258885944457
iteration : 6055
train acc:  0.8828125
train loss:  0.2947465479373932
train gradient:  0.1560123795083497
iteration : 6056
train acc:  0.78125
train loss:  0.4123612642288208
train gradient:  0.35286843944640256
iteration : 6057
train acc:  0.84375
train loss:  0.3816421627998352
train gradient:  0.2499435239390321
iteration : 6058
train acc:  0.8828125
train loss:  0.2750071883201599
train gradient:  0.16362740693191924
iteration : 6059
train acc:  0.8125
train loss:  0.47697409987449646
train gradient:  0.5264424833796073
iteration : 6060
train acc:  0.828125
train loss:  0.3857705593109131
train gradient:  0.22211735811511987
iteration : 6061
train acc:  0.796875
train loss:  0.4208186864852905
train gradient:  0.30567768592880024
iteration : 6062
train acc:  0.8203125
train loss:  0.3589833676815033
train gradient:  0.2236032467030936
iteration : 6063
train acc:  0.8515625
train loss:  0.3399837911128998
train gradient:  0.15816991925415777
iteration : 6064
train acc:  0.8515625
train loss:  0.30583202838897705
train gradient:  0.15198231295581385
iteration : 6065
train acc:  0.828125
train loss:  0.359851211309433
train gradient:  0.28162892005856943
iteration : 6066
train acc:  0.90625
train loss:  0.293193519115448
train gradient:  0.18887746193453925
iteration : 6067
train acc:  0.875
train loss:  0.36549460887908936
train gradient:  0.17342867551461721
iteration : 6068
train acc:  0.8125
train loss:  0.3968242108821869
train gradient:  0.28384296636299833
iteration : 6069
train acc:  0.828125
train loss:  0.3562021851539612
train gradient:  0.1834414703204113
iteration : 6070
train acc:  0.8671875
train loss:  0.3838393986225128
train gradient:  0.2514381077931426
iteration : 6071
train acc:  0.875
train loss:  0.2962416708469391
train gradient:  0.15721526349255566
iteration : 6072
train acc:  0.8515625
train loss:  0.3204742968082428
train gradient:  0.2653006572147123
iteration : 6073
train acc:  0.8515625
train loss:  0.33389803767204285
train gradient:  0.17749230112233458
iteration : 6074
train acc:  0.84375
train loss:  0.3053642511367798
train gradient:  0.18666581692789996
iteration : 6075
train acc:  0.859375
train loss:  0.4024912714958191
train gradient:  0.4069544353281948
iteration : 6076
train acc:  0.828125
train loss:  0.37741798162460327
train gradient:  0.37265765921070576
iteration : 6077
train acc:  0.859375
train loss:  0.3091801106929779
train gradient:  0.15275955054858675
iteration : 6078
train acc:  0.7890625
train loss:  0.4443036913871765
train gradient:  0.3128972202618988
iteration : 6079
train acc:  0.859375
train loss:  0.2982853651046753
train gradient:  0.19259284101942806
iteration : 6080
train acc:  0.8359375
train loss:  0.3421318829059601
train gradient:  0.16007039529308323
iteration : 6081
train acc:  0.796875
train loss:  0.45426005125045776
train gradient:  0.3317426893755217
iteration : 6082
train acc:  0.828125
train loss:  0.3522547483444214
train gradient:  0.19037903539498335
iteration : 6083
train acc:  0.921875
train loss:  0.2718082368373871
train gradient:  0.1560453064668071
iteration : 6084
train acc:  0.859375
train loss:  0.3516163229942322
train gradient:  0.17319055431680275
iteration : 6085
train acc:  0.796875
train loss:  0.39569878578186035
train gradient:  0.29645831772701775
iteration : 6086
train acc:  0.84375
train loss:  0.3575080931186676
train gradient:  0.16681346382060253
iteration : 6087
train acc:  0.8671875
train loss:  0.3069502115249634
train gradient:  0.17448894592319575
iteration : 6088
train acc:  0.9140625
train loss:  0.265256404876709
train gradient:  0.1415255772860693
iteration : 6089
train acc:  0.84375
train loss:  0.33535271883010864
train gradient:  0.33591043890851363
iteration : 6090
train acc:  0.8515625
train loss:  0.3595775067806244
train gradient:  0.2268593283593427
iteration : 6091
train acc:  0.90625
train loss:  0.2607654631137848
train gradient:  0.1567716284347467
iteration : 6092
train acc:  0.859375
train loss:  0.2976500689983368
train gradient:  0.16612845142833713
iteration : 6093
train acc:  0.8046875
train loss:  0.4651932120323181
train gradient:  0.35878358359650403
iteration : 6094
train acc:  0.8671875
train loss:  0.30996692180633545
train gradient:  0.17847298315958124
iteration : 6095
train acc:  0.8515625
train loss:  0.3266647756099701
train gradient:  0.23299227424199073
iteration : 6096
train acc:  0.859375
train loss:  0.31684330105781555
train gradient:  0.3096818609878682
iteration : 6097
train acc:  0.8359375
train loss:  0.38439077138900757
train gradient:  0.194865616357235
iteration : 6098
train acc:  0.8125
train loss:  0.4006030559539795
train gradient:  0.25321458136190733
iteration : 6099
train acc:  0.8671875
train loss:  0.3259630501270294
train gradient:  0.24547749124967183
iteration : 6100
train acc:  0.84375
train loss:  0.35964256525039673
train gradient:  0.18524558713059877
iteration : 6101
train acc:  0.859375
train loss:  0.38089844584465027
train gradient:  0.34745390438859014
iteration : 6102
train acc:  0.828125
train loss:  0.35717684030532837
train gradient:  0.2762900835526617
iteration : 6103
train acc:  0.8671875
train loss:  0.32971271872520447
train gradient:  0.2674705044921899
iteration : 6104
train acc:  0.84375
train loss:  0.3212652802467346
train gradient:  0.1757683673824058
iteration : 6105
train acc:  0.8984375
train loss:  0.25378280878067017
train gradient:  0.18042084175446332
iteration : 6106
train acc:  0.8359375
train loss:  0.3502994477748871
train gradient:  0.2694540917207759
iteration : 6107
train acc:  0.8828125
train loss:  0.3427993655204773
train gradient:  0.25748095606491095
iteration : 6108
train acc:  0.90625
train loss:  0.2577536702156067
train gradient:  0.1459727789262084
iteration : 6109
train acc:  0.84375
train loss:  0.3069740831851959
train gradient:  0.168887382091637
iteration : 6110
train acc:  0.8359375
train loss:  0.368389368057251
train gradient:  0.2573017636244808
iteration : 6111
train acc:  0.8671875
train loss:  0.3191961944103241
train gradient:  0.18235894072057146
iteration : 6112
train acc:  0.84375
train loss:  0.39896219968795776
train gradient:  0.2704740504875107
iteration : 6113
train acc:  0.875
train loss:  0.29438233375549316
train gradient:  0.1791923147533982
iteration : 6114
train acc:  0.875
train loss:  0.2717091739177704
train gradient:  0.16274264508095274
iteration : 6115
train acc:  0.875
train loss:  0.2937958240509033
train gradient:  0.20682210757401923
iteration : 6116
train acc:  0.8359375
train loss:  0.4455755054950714
train gradient:  0.3495690597230676
iteration : 6117
train acc:  0.8515625
train loss:  0.3404400944709778
train gradient:  0.26354601538896666
iteration : 6118
train acc:  0.8203125
train loss:  0.30322641134262085
train gradient:  0.2553134114338998
iteration : 6119
train acc:  0.8515625
train loss:  0.34653836488723755
train gradient:  0.20573254461783874
iteration : 6120
train acc:  0.859375
train loss:  0.34872573614120483
train gradient:  0.17523099319193947
iteration : 6121
train acc:  0.84375
train loss:  0.36075612902641296
train gradient:  0.2735714180259034
iteration : 6122
train acc:  0.859375
train loss:  0.317716121673584
train gradient:  0.21151552756030428
iteration : 6123
train acc:  0.8203125
train loss:  0.41148585081100464
train gradient:  0.2857021711753104
iteration : 6124
train acc:  0.890625
train loss:  0.2895315885543823
train gradient:  0.2628875962836263
iteration : 6125
train acc:  0.8125
train loss:  0.3667333126068115
train gradient:  0.23956660822077866
iteration : 6126
train acc:  0.8359375
train loss:  0.34636980295181274
train gradient:  0.22440850938786677
iteration : 6127
train acc:  0.875
train loss:  0.34038209915161133
train gradient:  0.22404950174442845
iteration : 6128
train acc:  0.890625
train loss:  0.28938058018684387
train gradient:  0.16003060023975943
iteration : 6129
train acc:  0.8515625
train loss:  0.3230898976325989
train gradient:  0.21731967948231623
iteration : 6130
train acc:  0.84375
train loss:  0.39550986886024475
train gradient:  0.26741973338668684
iteration : 6131
train acc:  0.8359375
train loss:  0.40091395378112793
train gradient:  0.2354566059293505
iteration : 6132
train acc:  0.78125
train loss:  0.41358238458633423
train gradient:  0.2689907656319832
iteration : 6133
train acc:  0.9140625
train loss:  0.26657629013061523
train gradient:  0.21867738502747736
iteration : 6134
train acc:  0.90625
train loss:  0.23430462181568146
train gradient:  0.11681790416021261
iteration : 6135
train acc:  0.890625
train loss:  0.3336015045642853
train gradient:  0.1908383496346973
iteration : 6136
train acc:  0.828125
train loss:  0.4249246418476105
train gradient:  0.2969609437056341
iteration : 6137
train acc:  0.8203125
train loss:  0.393753319978714
train gradient:  0.21820680319628086
iteration : 6138
train acc:  0.8671875
train loss:  0.3221927583217621
train gradient:  0.21151968048623793
iteration : 6139
train acc:  0.859375
train loss:  0.30871841311454773
train gradient:  0.1832666556547188
iteration : 6140
train acc:  0.8828125
train loss:  0.2706456184387207
train gradient:  0.17450379366547636
iteration : 6141
train acc:  0.8515625
train loss:  0.3201022148132324
train gradient:  0.11739754382450199
iteration : 6142
train acc:  0.828125
train loss:  0.4041031002998352
train gradient:  0.18932816765831792
iteration : 6143
train acc:  0.8515625
train loss:  0.3163388669490814
train gradient:  0.16885386180767828
iteration : 6144
train acc:  0.828125
train loss:  0.3983662724494934
train gradient:  0.22761355319738746
iteration : 6145
train acc:  0.8671875
train loss:  0.32116878032684326
train gradient:  0.17215962861663237
iteration : 6146
train acc:  0.828125
train loss:  0.3674889802932739
train gradient:  0.22575269834754708
iteration : 6147
train acc:  0.8671875
train loss:  0.37221744656562805
train gradient:  0.20879363887278232
iteration : 6148
train acc:  0.8125
train loss:  0.3868483304977417
train gradient:  0.17056606929775375
iteration : 6149
train acc:  0.7890625
train loss:  0.4103296399116516
train gradient:  0.2983232259133381
iteration : 6150
train acc:  0.7890625
train loss:  0.4045635163784027
train gradient:  0.25155658523516267
iteration : 6151
train acc:  0.84375
train loss:  0.43765249848365784
train gradient:  0.3695258434423409
iteration : 6152
train acc:  0.8515625
train loss:  0.3232666552066803
train gradient:  0.22138999601426257
iteration : 6153
train acc:  0.8359375
train loss:  0.3641611933708191
train gradient:  0.1991617390883814
iteration : 6154
train acc:  0.875
train loss:  0.371740460395813
train gradient:  0.3773395720526657
iteration : 6155
train acc:  0.8828125
train loss:  0.3727121353149414
train gradient:  0.23592189380997672
iteration : 6156
train acc:  0.8515625
train loss:  0.3333336114883423
train gradient:  0.2180041604564421
iteration : 6157
train acc:  0.8515625
train loss:  0.35205575823783875
train gradient:  0.17962911559736108
iteration : 6158
train acc:  0.8515625
train loss:  0.34771841764450073
train gradient:  0.21524576835933668
iteration : 6159
train acc:  0.84375
train loss:  0.37358325719833374
train gradient:  0.23392571930774925
iteration : 6160
train acc:  0.8828125
train loss:  0.2863706052303314
train gradient:  0.1685900590237706
iteration : 6161
train acc:  0.8515625
train loss:  0.3925113081932068
train gradient:  0.2334753489913624
iteration : 6162
train acc:  0.7734375
train loss:  0.4291297495365143
train gradient:  0.27192008925605726
iteration : 6163
train acc:  0.8046875
train loss:  0.3594214916229248
train gradient:  0.17059547079765774
iteration : 6164
train acc:  0.8359375
train loss:  0.36333703994750977
train gradient:  0.24225813340674854
iteration : 6165
train acc:  0.8671875
train loss:  0.39685991406440735
train gradient:  0.3583633368717436
iteration : 6166
train acc:  0.859375
train loss:  0.3614515960216522
train gradient:  0.1759276116347558
iteration : 6167
train acc:  0.8125
train loss:  0.35622572898864746
train gradient:  0.27652555416131774
iteration : 6168
train acc:  0.8828125
train loss:  0.34731149673461914
train gradient:  0.23989715402459671
iteration : 6169
train acc:  0.8515625
train loss:  0.3154829740524292
train gradient:  0.25477001212928113
iteration : 6170
train acc:  0.796875
train loss:  0.4289606809616089
train gradient:  0.31601561348208274
iteration : 6171
train acc:  0.8359375
train loss:  0.33231884241104126
train gradient:  0.16739420846211123
iteration : 6172
train acc:  0.859375
train loss:  0.2870320975780487
train gradient:  0.16768520989649843
iteration : 6173
train acc:  0.8046875
train loss:  0.39179885387420654
train gradient:  0.3189776058360511
iteration : 6174
train acc:  0.8125
train loss:  0.3841540217399597
train gradient:  0.23024323014083636
iteration : 6175
train acc:  0.921875
train loss:  0.25676193833351135
train gradient:  0.10728688323118901
iteration : 6176
train acc:  0.875
train loss:  0.3293476700782776
train gradient:  0.16772364090854414
iteration : 6177
train acc:  0.8671875
train loss:  0.3016795516014099
train gradient:  0.15479540152634133
iteration : 6178
train acc:  0.8515625
train loss:  0.3001149594783783
train gradient:  0.15915856172453308
iteration : 6179
train acc:  0.859375
train loss:  0.34961649775505066
train gradient:  0.2164227548122135
iteration : 6180
train acc:  0.796875
train loss:  0.4042009115219116
train gradient:  0.2884681640684499
iteration : 6181
train acc:  0.875
train loss:  0.32216283679008484
train gradient:  0.1789007516529092
iteration : 6182
train acc:  0.90625
train loss:  0.23312130570411682
train gradient:  0.1478793735907873
iteration : 6183
train acc:  0.859375
train loss:  0.30035215616226196
train gradient:  0.17219768755286502
iteration : 6184
train acc:  0.8359375
train loss:  0.3108634948730469
train gradient:  0.19363472525926878
iteration : 6185
train acc:  0.8671875
train loss:  0.319077730178833
train gradient:  0.21816224685201538
iteration : 6186
train acc:  0.796875
train loss:  0.3912864625453949
train gradient:  0.2919272582099452
iteration : 6187
train acc:  0.828125
train loss:  0.3584756553173065
train gradient:  0.3079101535682715
iteration : 6188
train acc:  0.8203125
train loss:  0.41279688477516174
train gradient:  0.24388754817968644
iteration : 6189
train acc:  0.8515625
train loss:  0.3339992165565491
train gradient:  0.17959797886360063
iteration : 6190
train acc:  0.859375
train loss:  0.35350674390792847
train gradient:  0.20386723392530584
iteration : 6191
train acc:  0.8203125
train loss:  0.3999786674976349
train gradient:  0.29104244353149744
iteration : 6192
train acc:  0.8359375
train loss:  0.33844953775405884
train gradient:  0.2028732749698815
iteration : 6193
train acc:  0.8515625
train loss:  0.34478241205215454
train gradient:  0.38472396977173473
iteration : 6194
train acc:  0.859375
train loss:  0.32796138525009155
train gradient:  0.3478444595082675
iteration : 6195
train acc:  0.796875
train loss:  0.4321754574775696
train gradient:  0.2572935920488716
iteration : 6196
train acc:  0.8359375
train loss:  0.39389562606811523
train gradient:  0.22423900845344358
iteration : 6197
train acc:  0.890625
train loss:  0.28592443466186523
train gradient:  0.20025124882388698
iteration : 6198
train acc:  0.8046875
train loss:  0.37859219312667847
train gradient:  0.18248290341660453
iteration : 6199
train acc:  0.8828125
train loss:  0.26691734790802
train gradient:  0.14701593584657724
iteration : 6200
train acc:  0.90625
train loss:  0.22656945884227753
train gradient:  0.0916813723162258
iteration : 6201
train acc:  0.8125
train loss:  0.4584186375141144
train gradient:  0.3531308643961931
iteration : 6202
train acc:  0.84375
train loss:  0.38691574335098267
train gradient:  0.158542524582027
iteration : 6203
train acc:  0.8828125
train loss:  0.307656466960907
train gradient:  0.152492331236654
iteration : 6204
train acc:  0.8125
train loss:  0.35344642400741577
train gradient:  0.2239805815954672
iteration : 6205
train acc:  0.8671875
train loss:  0.3350081145763397
train gradient:  0.2644801784976116
iteration : 6206
train acc:  0.8671875
train loss:  0.34288522601127625
train gradient:  0.17185481239051034
iteration : 6207
train acc:  0.90625
train loss:  0.27883243560791016
train gradient:  0.21944208254183825
iteration : 6208
train acc:  0.8359375
train loss:  0.327464759349823
train gradient:  0.3198716505255318
iteration : 6209
train acc:  0.8046875
train loss:  0.3704773187637329
train gradient:  0.24614915653552136
iteration : 6210
train acc:  0.8515625
train loss:  0.3269510567188263
train gradient:  0.17568171518979098
iteration : 6211
train acc:  0.7890625
train loss:  0.3839302062988281
train gradient:  0.2657002385619688
iteration : 6212
train acc:  0.828125
train loss:  0.35815030336380005
train gradient:  0.21154894771274108
iteration : 6213
train acc:  0.8671875
train loss:  0.29044419527053833
train gradient:  0.13498511768843824
iteration : 6214
train acc:  0.8203125
train loss:  0.33272606134414673
train gradient:  0.17220970298210675
iteration : 6215
train acc:  0.859375
train loss:  0.2982770800590515
train gradient:  0.16771237482214396
iteration : 6216
train acc:  0.8125
train loss:  0.4564242660999298
train gradient:  0.2735775108456173
iteration : 6217
train acc:  0.796875
train loss:  0.4074699580669403
train gradient:  0.23766201367570142
iteration : 6218
train acc:  0.8984375
train loss:  0.27780336141586304
train gradient:  0.1131366834381344
iteration : 6219
train acc:  0.8671875
train loss:  0.3403077721595764
train gradient:  0.1761898083642745
iteration : 6220
train acc:  0.8359375
train loss:  0.3300073444843292
train gradient:  0.15897041404932805
iteration : 6221
train acc:  0.859375
train loss:  0.3304142653942108
train gradient:  0.19150494382320016
iteration : 6222
train acc:  0.8046875
train loss:  0.40119391679763794
train gradient:  0.24021585225025244
iteration : 6223
train acc:  0.8125
train loss:  0.4593938887119293
train gradient:  0.3625757895247335
iteration : 6224
train acc:  0.828125
train loss:  0.399474561214447
train gradient:  0.3622048303403484
iteration : 6225
train acc:  0.8125
train loss:  0.33806124329566956
train gradient:  0.22808832904971674
iteration : 6226
train acc:  0.859375
train loss:  0.33497992157936096
train gradient:  0.17071516097471737
iteration : 6227
train acc:  0.8515625
train loss:  0.339313805103302
train gradient:  0.24952360805304902
iteration : 6228
train acc:  0.8515625
train loss:  0.3564860224723816
train gradient:  0.29846657965914064
iteration : 6229
train acc:  0.8359375
train loss:  0.35822564363479614
train gradient:  0.20995702601018285
iteration : 6230
train acc:  0.796875
train loss:  0.4092858135700226
train gradient:  0.3810406824524592
iteration : 6231
train acc:  0.8515625
train loss:  0.34952598810195923
train gradient:  0.20310225821951022
iteration : 6232
train acc:  0.890625
train loss:  0.31285810470581055
train gradient:  0.19345054223094862
iteration : 6233
train acc:  0.8671875
train loss:  0.2963680028915405
train gradient:  0.1369209135464523
iteration : 6234
train acc:  0.8671875
train loss:  0.32326602935791016
train gradient:  0.22121547310209
iteration : 6235
train acc:  0.90625
train loss:  0.300895094871521
train gradient:  0.17268233635979602
iteration : 6236
train acc:  0.84375
train loss:  0.3643413186073303
train gradient:  0.2456110371698453
iteration : 6237
train acc:  0.875
train loss:  0.3303430676460266
train gradient:  0.17046752619531824
iteration : 6238
train acc:  0.859375
train loss:  0.32507753372192383
train gradient:  0.2527901516116603
iteration : 6239
train acc:  0.875
train loss:  0.290696382522583
train gradient:  0.1269243222120527
iteration : 6240
train acc:  0.8828125
train loss:  0.2600700557231903
train gradient:  0.18405956685005384
iteration : 6241
train acc:  0.828125
train loss:  0.377042680978775
train gradient:  0.32307894603656107
iteration : 6242
train acc:  0.8125
train loss:  0.3945775330066681
train gradient:  0.2802986741648767
iteration : 6243
train acc:  0.8828125
train loss:  0.35014212131500244
train gradient:  0.17500566761770936
iteration : 6244
train acc:  0.8515625
train loss:  0.35871243476867676
train gradient:  0.24015885283221414
iteration : 6245
train acc:  0.796875
train loss:  0.40457645058631897
train gradient:  0.18982651012415605
iteration : 6246
train acc:  0.796875
train loss:  0.38158583641052246
train gradient:  0.26453763536812513
iteration : 6247
train acc:  0.8828125
train loss:  0.28440505266189575
train gradient:  0.2285800005045075
iteration : 6248
train acc:  0.84375
train loss:  0.3089796304702759
train gradient:  0.19046490981748576
iteration : 6249
train acc:  0.84375
train loss:  0.34536951780319214
train gradient:  0.22034849186190678
iteration : 6250
train acc:  0.828125
train loss:  0.37730303406715393
train gradient:  0.223485359771531
iteration : 6251
train acc:  0.84375
train loss:  0.3124154210090637
train gradient:  0.17698164061416705
iteration : 6252
train acc:  0.828125
train loss:  0.4342135787010193
train gradient:  0.2800243681243055
iteration : 6253
train acc:  0.8828125
train loss:  0.2671612501144409
train gradient:  0.13964680008850672
iteration : 6254
train acc:  0.890625
train loss:  0.28620633482933044
train gradient:  0.1741003752028841
iteration : 6255
train acc:  0.890625
train loss:  0.2812632918357849
train gradient:  0.21249930196129774
iteration : 6256
train acc:  0.8359375
train loss:  0.33914247155189514
train gradient:  0.1619954252158035
iteration : 6257
train acc:  0.8515625
train loss:  0.32180142402648926
train gradient:  0.180373698412297
iteration : 6258
train acc:  0.875
train loss:  0.30564069747924805
train gradient:  0.1484893900541162
iteration : 6259
train acc:  0.8046875
train loss:  0.4051293432712555
train gradient:  0.3181657886510364
iteration : 6260
train acc:  0.8828125
train loss:  0.2733856737613678
train gradient:  0.14378877050704933
iteration : 6261
train acc:  0.8828125
train loss:  0.33432111144065857
train gradient:  0.15679880465220247
iteration : 6262
train acc:  0.8203125
train loss:  0.34919607639312744
train gradient:  0.20373586934401516
iteration : 6263
train acc:  0.84375
train loss:  0.2789424657821655
train gradient:  0.18981589532829884
iteration : 6264
train acc:  0.78125
train loss:  0.44115597009658813
train gradient:  0.3559493999031652
iteration : 6265
train acc:  0.8671875
train loss:  0.34514734148979187
train gradient:  0.39134593545260044
iteration : 6266
train acc:  0.8203125
train loss:  0.3207034766674042
train gradient:  0.1679914585417471
iteration : 6267
train acc:  0.8515625
train loss:  0.3256315588951111
train gradient:  0.17366754364983897
iteration : 6268
train acc:  0.8671875
train loss:  0.3316565752029419
train gradient:  0.17962756421905626
iteration : 6269
train acc:  0.84375
train loss:  0.3386327028274536
train gradient:  0.2502124605336462
iteration : 6270
train acc:  0.8125
train loss:  0.39861977100372314
train gradient:  0.3167521063275785
iteration : 6271
train acc:  0.828125
train loss:  0.3702946603298187
train gradient:  0.1993034957210839
iteration : 6272
train acc:  0.8828125
train loss:  0.29017359018325806
train gradient:  0.25201915093893484
iteration : 6273
train acc:  0.7734375
train loss:  0.45102041959762573
train gradient:  0.31244154450290273
iteration : 6274
train acc:  0.8515625
train loss:  0.3506431579589844
train gradient:  0.27366211054160205
iteration : 6275
train acc:  0.8359375
train loss:  0.3844190835952759
train gradient:  0.31881414983215667
iteration : 6276
train acc:  0.7578125
train loss:  0.5410279035568237
train gradient:  0.4909797113403731
iteration : 6277
train acc:  0.875
train loss:  0.29829537868499756
train gradient:  0.19935655217573148
iteration : 6278
train acc:  0.8515625
train loss:  0.3618353009223938
train gradient:  0.21181430132433926
iteration : 6279
train acc:  0.828125
train loss:  0.36421066522598267
train gradient:  0.2129335656391702
iteration : 6280
train acc:  0.8203125
train loss:  0.43496328592300415
train gradient:  0.42045932853570583
iteration : 6281
train acc:  0.8828125
train loss:  0.34201309084892273
train gradient:  0.13827887870125105
iteration : 6282
train acc:  0.8359375
train loss:  0.39967119693756104
train gradient:  0.36416991706594104
iteration : 6283
train acc:  0.7734375
train loss:  0.4001067876815796
train gradient:  0.2987798765030771
iteration : 6284
train acc:  0.8515625
train loss:  0.3573952615261078
train gradient:  0.2369407074164046
iteration : 6285
train acc:  0.859375
train loss:  0.35965800285339355
train gradient:  0.28057372319314294
iteration : 6286
train acc:  0.8203125
train loss:  0.38754239678382874
train gradient:  0.3170262191348205
iteration : 6287
train acc:  0.8828125
train loss:  0.26177021861076355
train gradient:  0.16916485668869907
iteration : 6288
train acc:  0.8515625
train loss:  0.33419615030288696
train gradient:  0.19968142237850928
iteration : 6289
train acc:  0.8671875
train loss:  0.38101404905319214
train gradient:  0.32137343005787833
iteration : 6290
train acc:  0.8125
train loss:  0.39129215478897095
train gradient:  0.2742828365873955
iteration : 6291
train acc:  0.84375
train loss:  0.3368822932243347
train gradient:  0.1890863949947511
iteration : 6292
train acc:  0.859375
train loss:  0.3023550808429718
train gradient:  0.16634648047922934
iteration : 6293
train acc:  0.828125
train loss:  0.4210643768310547
train gradient:  0.2610483505544325
iteration : 6294
train acc:  0.8203125
train loss:  0.3688061535358429
train gradient:  0.19538718569342117
iteration : 6295
train acc:  0.8515625
train loss:  0.2836715281009674
train gradient:  0.13615508236097013
iteration : 6296
train acc:  0.8203125
train loss:  0.33960795402526855
train gradient:  0.19431539432951972
iteration : 6297
train acc:  0.8828125
train loss:  0.29903894662857056
train gradient:  0.26145211261845247
iteration : 6298
train acc:  0.8125
train loss:  0.38468167185783386
train gradient:  0.26817030638011646
iteration : 6299
train acc:  0.859375
train loss:  0.3424120843410492
train gradient:  0.15792369273564233
iteration : 6300
train acc:  0.84375
train loss:  0.39863985776901245
train gradient:  0.21043311440362567
iteration : 6301
train acc:  0.8203125
train loss:  0.34001800417900085
train gradient:  0.17266096428181743
iteration : 6302
train acc:  0.8671875
train loss:  0.3094690442085266
train gradient:  0.14812060192354215
iteration : 6303
train acc:  0.8203125
train loss:  0.38245880603790283
train gradient:  0.2774452157977051
iteration : 6304
train acc:  0.7734375
train loss:  0.4532667100429535
train gradient:  0.3150147954498417
iteration : 6305
train acc:  0.875
train loss:  0.33045363426208496
train gradient:  0.22948176598105002
iteration : 6306
train acc:  0.875
train loss:  0.31451594829559326
train gradient:  0.1574527822465734
iteration : 6307
train acc:  0.8203125
train loss:  0.39687347412109375
train gradient:  0.2168560577492901
iteration : 6308
train acc:  0.84375
train loss:  0.3475392758846283
train gradient:  0.1906765669638062
iteration : 6309
train acc:  0.875
train loss:  0.3672155737876892
train gradient:  0.2895600377844343
iteration : 6310
train acc:  0.828125
train loss:  0.35588425397872925
train gradient:  0.28284666949705783
iteration : 6311
train acc:  0.859375
train loss:  0.3285536766052246
train gradient:  0.1999602588718491
iteration : 6312
train acc:  0.8515625
train loss:  0.3317834138870239
train gradient:  0.24117990867663813
iteration : 6313
train acc:  0.8359375
train loss:  0.3229696750640869
train gradient:  0.16694399447931332
iteration : 6314
train acc:  0.890625
train loss:  0.3828359544277191
train gradient:  0.20449408517000606
iteration : 6315
train acc:  0.890625
train loss:  0.31189894676208496
train gradient:  0.22780416070249807
iteration : 6316
train acc:  0.84375
train loss:  0.3657839298248291
train gradient:  0.21542092457498535
iteration : 6317
train acc:  0.8359375
train loss:  0.46970948576927185
train gradient:  0.294342268471385
iteration : 6318
train acc:  0.875
train loss:  0.29017990827560425
train gradient:  0.15538174610101593
iteration : 6319
train acc:  0.8359375
train loss:  0.3466128706932068
train gradient:  0.19841295707362905
iteration : 6320
train acc:  0.828125
train loss:  0.3446575999259949
train gradient:  0.17361028843176765
iteration : 6321
train acc:  0.796875
train loss:  0.44905319809913635
train gradient:  0.3642534698007546
iteration : 6322
train acc:  0.875
train loss:  0.3100091814994812
train gradient:  0.19655648701322168
iteration : 6323
train acc:  0.8671875
train loss:  0.3285898268222809
train gradient:  0.23708088613381728
iteration : 6324
train acc:  0.859375
train loss:  0.36876946687698364
train gradient:  0.2483553237688165
iteration : 6325
train acc:  0.84375
train loss:  0.33337992429733276
train gradient:  0.1889467223877464
iteration : 6326
train acc:  0.8515625
train loss:  0.3378244638442993
train gradient:  0.2016459483465921
iteration : 6327
train acc:  0.8359375
train loss:  0.3555249571800232
train gradient:  0.16569840953091192
iteration : 6328
train acc:  0.8046875
train loss:  0.35677799582481384
train gradient:  0.4100904928771234
iteration : 6329
train acc:  0.8828125
train loss:  0.2778379023075104
train gradient:  0.19562692597790582
iteration : 6330
train acc:  0.8359375
train loss:  0.3880584239959717
train gradient:  0.22498083250349205
iteration : 6331
train acc:  0.8125
train loss:  0.3609657287597656
train gradient:  0.26161142427201955
iteration : 6332
train acc:  0.8828125
train loss:  0.32914960384368896
train gradient:  0.1742523583923153
iteration : 6333
train acc:  0.84375
train loss:  0.33796313405036926
train gradient:  0.18771703993299554
iteration : 6334
train acc:  0.8515625
train loss:  0.32321810722351074
train gradient:  0.1911322297191438
iteration : 6335
train acc:  0.8359375
train loss:  0.3430665135383606
train gradient:  0.21959346149631292
iteration : 6336
train acc:  0.84375
train loss:  0.28852221369743347
train gradient:  0.11922319249368436
iteration : 6337
train acc:  0.90625
train loss:  0.272544801235199
train gradient:  0.11885323880691012
iteration : 6338
train acc:  0.890625
train loss:  0.2841923236846924
train gradient:  0.18318930575430092
iteration : 6339
train acc:  0.859375
train loss:  0.34335416555404663
train gradient:  0.14797825971919668
iteration : 6340
train acc:  0.875
train loss:  0.3103157877922058
train gradient:  0.24408524448090918
iteration : 6341
train acc:  0.8671875
train loss:  0.3187806010246277
train gradient:  0.28767492950325446
iteration : 6342
train acc:  0.8203125
train loss:  0.35976532101631165
train gradient:  0.24160215260967466
iteration : 6343
train acc:  0.8671875
train loss:  0.2724322974681854
train gradient:  0.17556315245483006
iteration : 6344
train acc:  0.7734375
train loss:  0.4851629137992859
train gradient:  0.38757342189558946
iteration : 6345
train acc:  0.8359375
train loss:  0.33909961581230164
train gradient:  0.18700390144163426
iteration : 6346
train acc:  0.84375
train loss:  0.32766175270080566
train gradient:  0.233353937612373
iteration : 6347
train acc:  0.875
train loss:  0.27901458740234375
train gradient:  0.1435505576432222
iteration : 6348
train acc:  0.8125
train loss:  0.4197925925254822
train gradient:  0.31738808792272993
iteration : 6349
train acc:  0.84375
train loss:  0.36753591895103455
train gradient:  0.23169638069523546
iteration : 6350
train acc:  0.875
train loss:  0.30386245250701904
train gradient:  0.17167928307910207
iteration : 6351
train acc:  0.875
train loss:  0.30786752700805664
train gradient:  0.2052678071548202
iteration : 6352
train acc:  0.84375
train loss:  0.3716239929199219
train gradient:  0.2050661539871016
iteration : 6353
train acc:  0.84375
train loss:  0.3760729432106018
train gradient:  0.31396931060674776
iteration : 6354
train acc:  0.8203125
train loss:  0.3641509413719177
train gradient:  0.2543535369064478
iteration : 6355
train acc:  0.8515625
train loss:  0.28705763816833496
train gradient:  0.16853656248769652
iteration : 6356
train acc:  0.8984375
train loss:  0.2517837584018707
train gradient:  0.14731644848653397
iteration : 6357
train acc:  0.8046875
train loss:  0.36396312713623047
train gradient:  0.278408850580972
iteration : 6358
train acc:  0.84375
train loss:  0.3988618850708008
train gradient:  0.25428521534760806
iteration : 6359
train acc:  0.875
train loss:  0.32851552963256836
train gradient:  0.15544590748211778
iteration : 6360
train acc:  0.921875
train loss:  0.2658371925354004
train gradient:  0.11056364771120879
iteration : 6361
train acc:  0.875
train loss:  0.2880380153656006
train gradient:  0.29693589493955846
iteration : 6362
train acc:  0.8671875
train loss:  0.35497671365737915
train gradient:  0.2218560028275916
iteration : 6363
train acc:  0.8671875
train loss:  0.33942416310310364
train gradient:  0.22404185819121755
iteration : 6364
train acc:  0.84375
train loss:  0.381412148475647
train gradient:  0.2342645749386283
iteration : 6365
train acc:  0.9140625
train loss:  0.2905988097190857
train gradient:  0.12880090601635052
iteration : 6366
train acc:  0.859375
train loss:  0.3399465084075928
train gradient:  0.3001962554838795
iteration : 6367
train acc:  0.796875
train loss:  0.39118692278862
train gradient:  0.42919764121699844
iteration : 6368
train acc:  0.8203125
train loss:  0.4413120746612549
train gradient:  0.3816908997953981
iteration : 6369
train acc:  0.90625
train loss:  0.2678714394569397
train gradient:  0.17268713385812934
iteration : 6370
train acc:  0.8828125
train loss:  0.28056102991104126
train gradient:  0.20749133790922192
iteration : 6371
train acc:  0.84375
train loss:  0.3205741345882416
train gradient:  0.2520426059328464
iteration : 6372
train acc:  0.8125
train loss:  0.41518187522888184
train gradient:  0.33403804946718324
iteration : 6373
train acc:  0.875
train loss:  0.2974760830402374
train gradient:  0.1742042022925425
iteration : 6374
train acc:  0.859375
train loss:  0.3697444796562195
train gradient:  0.1948850013585321
iteration : 6375
train acc:  0.8359375
train loss:  0.37178361415863037
train gradient:  0.2033531738446068
iteration : 6376
train acc:  0.90625
train loss:  0.2587912380695343
train gradient:  0.17859607241293732
iteration : 6377
train acc:  0.8515625
train loss:  0.3813891112804413
train gradient:  0.328017897517993
iteration : 6378
train acc:  0.8671875
train loss:  0.29085126519203186
train gradient:  0.19930855161766992
iteration : 6379
train acc:  0.8671875
train loss:  0.2908492684364319
train gradient:  0.15312018355514412
iteration : 6380
train acc:  0.8515625
train loss:  0.2996481657028198
train gradient:  0.1410678350852438
iteration : 6381
train acc:  0.828125
train loss:  0.3240049481391907
train gradient:  0.24317191847293557
iteration : 6382
train acc:  0.828125
train loss:  0.40730175375938416
train gradient:  0.2686781324389959
iteration : 6383
train acc:  0.8046875
train loss:  0.36152857542037964
train gradient:  0.19244509887405797
iteration : 6384
train acc:  0.8671875
train loss:  0.352398157119751
train gradient:  0.2264704583488476
iteration : 6385
train acc:  0.8515625
train loss:  0.37117820978164673
train gradient:  0.2751148859474087
iteration : 6386
train acc:  0.8515625
train loss:  0.3767544627189636
train gradient:  0.3210864524733458
iteration : 6387
train acc:  0.84375
train loss:  0.3307937681674957
train gradient:  0.23833559178558866
iteration : 6388
train acc:  0.875
train loss:  0.36846697330474854
train gradient:  0.22640004427342003
iteration : 6389
train acc:  0.84375
train loss:  0.3234005570411682
train gradient:  0.21444109637573608
iteration : 6390
train acc:  0.84375
train loss:  0.3353253901004791
train gradient:  0.24423571517722342
iteration : 6391
train acc:  0.875
train loss:  0.3238738179206848
train gradient:  0.18532047113035047
iteration : 6392
train acc:  0.765625
train loss:  0.4385247528553009
train gradient:  0.2832313660305132
iteration : 6393
train acc:  0.8515625
train loss:  0.32986700534820557
train gradient:  0.17694264903183868
iteration : 6394
train acc:  0.8125
train loss:  0.41299816966056824
train gradient:  0.3287922245916734
iteration : 6395
train acc:  0.859375
train loss:  0.3267284035682678
train gradient:  0.19852089383251809
iteration : 6396
train acc:  0.890625
train loss:  0.29847967624664307
train gradient:  0.19397173080017643
iteration : 6397
train acc:  0.84375
train loss:  0.360260546207428
train gradient:  0.24184381634739144
iteration : 6398
train acc:  0.8515625
train loss:  0.34400802850723267
train gradient:  0.20412822239298475
iteration : 6399
train acc:  0.859375
train loss:  0.3154400885105133
train gradient:  0.16962571997591425
iteration : 6400
train acc:  0.8671875
train loss:  0.28244051337242126
train gradient:  0.27828025687052504
iteration : 6401
train acc:  0.8046875
train loss:  0.42899027466773987
train gradient:  0.29008035255268144
iteration : 6402
train acc:  0.8359375
train loss:  0.33887314796447754
train gradient:  0.17915958349781486
iteration : 6403
train acc:  0.8671875
train loss:  0.3685319721698761
train gradient:  0.24142949407394163
iteration : 6404
train acc:  0.8203125
train loss:  0.3797016739845276
train gradient:  0.19884451114084242
iteration : 6405
train acc:  0.859375
train loss:  0.32636356353759766
train gradient:  0.28628679078782654
iteration : 6406
train acc:  0.796875
train loss:  0.4037393629550934
train gradient:  0.29979878730712667
iteration : 6407
train acc:  0.7890625
train loss:  0.3476491868495941
train gradient:  0.2782120167734555
iteration : 6408
train acc:  0.875
train loss:  0.3057238459587097
train gradient:  0.17912099829652672
iteration : 6409
train acc:  0.8203125
train loss:  0.40790000557899475
train gradient:  0.3012643272534373
iteration : 6410
train acc:  0.84375
train loss:  0.38545000553131104
train gradient:  0.24524303577009565
iteration : 6411
train acc:  0.859375
train loss:  0.35784631967544556
train gradient:  0.2366737208703124
iteration : 6412
train acc:  0.796875
train loss:  0.39332887530326843
train gradient:  0.21875747571383014
iteration : 6413
train acc:  0.8125
train loss:  0.38922709226608276
train gradient:  0.2154827017304634
iteration : 6414
train acc:  0.8359375
train loss:  0.3688352108001709
train gradient:  0.2518080773676461
iteration : 6415
train acc:  0.828125
train loss:  0.3144740164279938
train gradient:  0.23667515090872626
iteration : 6416
train acc:  0.8125
train loss:  0.4012814164161682
train gradient:  0.31815346516770837
iteration : 6417
train acc:  0.859375
train loss:  0.3581323027610779
train gradient:  0.20825404115604548
iteration : 6418
train acc:  0.8359375
train loss:  0.35895830392837524
train gradient:  0.1698802447596742
iteration : 6419
train acc:  0.8671875
train loss:  0.32689368724823
train gradient:  0.34667226924436273
iteration : 6420
train acc:  0.875
train loss:  0.29518646001815796
train gradient:  0.15761549899190677
iteration : 6421
train acc:  0.7890625
train loss:  0.45253318548202515
train gradient:  0.2995232736693892
iteration : 6422
train acc:  0.8515625
train loss:  0.3572845458984375
train gradient:  0.23415378101294038
iteration : 6423
train acc:  0.8671875
train loss:  0.31955140829086304
train gradient:  0.134307354137212
iteration : 6424
train acc:  0.84375
train loss:  0.38741767406463623
train gradient:  0.46857816113674855
iteration : 6425
train acc:  0.875
train loss:  0.3274625539779663
train gradient:  0.20568983999869792
iteration : 6426
train acc:  0.84375
train loss:  0.34624212980270386
train gradient:  0.22292789335104995
iteration : 6427
train acc:  0.8671875
train loss:  0.35996145009994507
train gradient:  0.21819216965697102
iteration : 6428
train acc:  0.828125
train loss:  0.4097976088523865
train gradient:  0.19489596284932378
iteration : 6429
train acc:  0.8515625
train loss:  0.3055054545402527
train gradient:  0.13705959153520142
iteration : 6430
train acc:  0.8828125
train loss:  0.27259841561317444
train gradient:  0.13255869779163149
iteration : 6431
train acc:  0.875
train loss:  0.30657458305358887
train gradient:  0.14469801105832036
iteration : 6432
train acc:  0.875
train loss:  0.2837677001953125
train gradient:  0.16980161973807423
iteration : 6433
train acc:  0.890625
train loss:  0.33292239904403687
train gradient:  0.14824000205663235
iteration : 6434
train acc:  0.84375
train loss:  0.3591420650482178
train gradient:  0.2235318693160588
iteration : 6435
train acc:  0.78125
train loss:  0.41052576899528503
train gradient:  0.2114797561701821
iteration : 6436
train acc:  0.9140625
train loss:  0.26340121030807495
train gradient:  0.1382541704173033
iteration : 6437
train acc:  0.7734375
train loss:  0.35083234310150146
train gradient:  0.2526221982629429
iteration : 6438
train acc:  0.859375
train loss:  0.31655067205429077
train gradient:  0.1707104860405637
iteration : 6439
train acc:  0.84375
train loss:  0.4033370614051819
train gradient:  0.26256428760383954
iteration : 6440
train acc:  0.859375
train loss:  0.3329329490661621
train gradient:  0.18870181064851255
iteration : 6441
train acc:  0.8203125
train loss:  0.3766183853149414
train gradient:  0.18215570791560626
iteration : 6442
train acc:  0.84375
train loss:  0.3696517050266266
train gradient:  0.17585168482849955
iteration : 6443
train acc:  0.8671875
train loss:  0.349057137966156
train gradient:  0.2987207425115151
iteration : 6444
train acc:  0.8203125
train loss:  0.35172390937805176
train gradient:  0.1688560600624266
iteration : 6445
train acc:  0.859375
train loss:  0.3212650418281555
train gradient:  0.15695909007199466
iteration : 6446
train acc:  0.7890625
train loss:  0.49542558193206787
train gradient:  0.3951116527144301
iteration : 6447
train acc:  0.8359375
train loss:  0.3996107280254364
train gradient:  0.24713591250939415
iteration : 6448
train acc:  0.828125
train loss:  0.34595298767089844
train gradient:  0.26983390901337595
iteration : 6449
train acc:  0.765625
train loss:  0.4063340425491333
train gradient:  0.21727873561408523
iteration : 6450
train acc:  0.8203125
train loss:  0.373771607875824
train gradient:  0.21075145713046606
iteration : 6451
train acc:  0.8828125
train loss:  0.32107532024383545
train gradient:  0.19267613734716338
iteration : 6452
train acc:  0.8515625
train loss:  0.34952521324157715
train gradient:  0.20703464323144677
iteration : 6453
train acc:  0.859375
train loss:  0.346286416053772
train gradient:  0.16756387999250638
iteration : 6454
train acc:  0.8046875
train loss:  0.4070990979671478
train gradient:  0.29396334963895693
iteration : 6455
train acc:  0.875
train loss:  0.32038313150405884
train gradient:  0.18110582343205434
iteration : 6456
train acc:  0.8515625
train loss:  0.30838221311569214
train gradient:  0.17248194392756183
iteration : 6457
train acc:  0.859375
train loss:  0.33577966690063477
train gradient:  0.16062136801688764
iteration : 6458
train acc:  0.8671875
train loss:  0.2744625508785248
train gradient:  0.14146295836192824
iteration : 6459
train acc:  0.8359375
train loss:  0.3132551908493042
train gradient:  0.1568732821825913
iteration : 6460
train acc:  0.875
train loss:  0.3129003047943115
train gradient:  0.12146402274870202
iteration : 6461
train acc:  0.8203125
train loss:  0.3971821963787079
train gradient:  0.211700199254316
iteration : 6462
train acc:  0.875
train loss:  0.348582923412323
train gradient:  0.22294814121099504
iteration : 6463
train acc:  0.8359375
train loss:  0.3451136350631714
train gradient:  0.15795855816382787
iteration : 6464
train acc:  0.8359375
train loss:  0.3652794063091278
train gradient:  0.2708349268654224
iteration : 6465
train acc:  0.8359375
train loss:  0.4177873134613037
train gradient:  0.2873159674900083
iteration : 6466
train acc:  0.8671875
train loss:  0.3124814033508301
train gradient:  0.1602080988282016
iteration : 6467
train acc:  0.828125
train loss:  0.4112253189086914
train gradient:  0.21887768550661954
iteration : 6468
train acc:  0.8515625
train loss:  0.31726551055908203
train gradient:  0.2750609792901459
iteration : 6469
train acc:  0.859375
train loss:  0.3487491011619568
train gradient:  0.18899271960690736
iteration : 6470
train acc:  0.875
train loss:  0.3190575838088989
train gradient:  0.15972625189244358
iteration : 6471
train acc:  0.859375
train loss:  0.3162827789783478
train gradient:  0.16253878364640723
iteration : 6472
train acc:  0.8984375
train loss:  0.24803292751312256
train gradient:  0.10703951413594005
iteration : 6473
train acc:  0.8515625
train loss:  0.33814623951911926
train gradient:  0.18536679278906534
iteration : 6474
train acc:  0.8515625
train loss:  0.37007516622543335
train gradient:  0.24567730727646309
iteration : 6475
train acc:  0.8125
train loss:  0.42356616258621216
train gradient:  0.22495210093451662
iteration : 6476
train acc:  0.859375
train loss:  0.31689444184303284
train gradient:  0.14140495884451726
iteration : 6477
train acc:  0.84375
train loss:  0.31270360946655273
train gradient:  0.14268417103929304
iteration : 6478
train acc:  0.9296875
train loss:  0.24633142352104187
train gradient:  0.09829582889972709
iteration : 6479
train acc:  0.8515625
train loss:  0.3180820643901825
train gradient:  0.18660400627553525
iteration : 6480
train acc:  0.84375
train loss:  0.3881387710571289
train gradient:  0.23959599918285945
iteration : 6481
train acc:  0.8359375
train loss:  0.42544978857040405
train gradient:  0.3265729522964323
iteration : 6482
train acc:  0.8359375
train loss:  0.3236367106437683
train gradient:  0.22608026373611675
iteration : 6483
train acc:  0.8125
train loss:  0.36210858821868896
train gradient:  0.22367522067165765
iteration : 6484
train acc:  0.84375
train loss:  0.3300303816795349
train gradient:  0.18534466906672376
iteration : 6485
train acc:  0.8125
train loss:  0.40433788299560547
train gradient:  0.23085703565676363
iteration : 6486
train acc:  0.8359375
train loss:  0.36787915229797363
train gradient:  0.1619082482684192
iteration : 6487
train acc:  0.875
train loss:  0.37296998500823975
train gradient:  0.255829922759093
iteration : 6488
train acc:  0.859375
train loss:  0.3478773832321167
train gradient:  0.17637879725358624
iteration : 6489
train acc:  0.8359375
train loss:  0.3153083622455597
train gradient:  0.15287959772233217
iteration : 6490
train acc:  0.84375
train loss:  0.3969895541667938
train gradient:  0.23850930285105162
iteration : 6491
train acc:  0.8359375
train loss:  0.3175583481788635
train gradient:  0.1841823176312114
iteration : 6492
train acc:  0.8984375
train loss:  0.25633853673934937
train gradient:  0.16070387403086245
iteration : 6493
train acc:  0.828125
train loss:  0.3984896242618561
train gradient:  0.24579332388454117
iteration : 6494
train acc:  0.875
train loss:  0.24570512771606445
train gradient:  0.10602645215797295
iteration : 6495
train acc:  0.84375
train loss:  0.39728832244873047
train gradient:  0.24442319107652433
iteration : 6496
train acc:  0.8671875
train loss:  0.292680561542511
train gradient:  0.20913904862424093
iteration : 6497
train acc:  0.8203125
train loss:  0.34961700439453125
train gradient:  0.18772808541803365
iteration : 6498
train acc:  0.8671875
train loss:  0.3450644016265869
train gradient:  0.16457526346290152
iteration : 6499
train acc:  0.8671875
train loss:  0.3310975432395935
train gradient:  0.19531592387922292
iteration : 6500
train acc:  0.9296875
train loss:  0.2713419198989868
train gradient:  0.17406932312066148
iteration : 6501
train acc:  0.8515625
train loss:  0.29987096786499023
train gradient:  0.15635286360461506
iteration : 6502
train acc:  0.7578125
train loss:  0.4074932336807251
train gradient:  0.21786216499684483
iteration : 6503
train acc:  0.875
train loss:  0.3441425859928131
train gradient:  0.19642893971604694
iteration : 6504
train acc:  0.8515625
train loss:  0.31695035099983215
train gradient:  0.14880830027438857
iteration : 6505
train acc:  0.734375
train loss:  0.5394045114517212
train gradient:  0.3689155486608599
iteration : 6506
train acc:  0.8046875
train loss:  0.3654690086841583
train gradient:  0.18184927514932833
iteration : 6507
train acc:  0.796875
train loss:  0.38515061140060425
train gradient:  0.28692502715942564
iteration : 6508
train acc:  0.8671875
train loss:  0.3258320391178131
train gradient:  0.21544890789437487
iteration : 6509
train acc:  0.828125
train loss:  0.33246731758117676
train gradient:  0.14144208204733325
iteration : 6510
train acc:  0.890625
train loss:  0.3461458683013916
train gradient:  0.2447249109327968
iteration : 6511
train acc:  0.859375
train loss:  0.3146146535873413
train gradient:  0.176956012427115
iteration : 6512
train acc:  0.828125
train loss:  0.4015060365200043
train gradient:  0.26174646357511017
iteration : 6513
train acc:  0.875
train loss:  0.3176323175430298
train gradient:  0.16551475132924842
iteration : 6514
train acc:  0.8828125
train loss:  0.3306472897529602
train gradient:  0.13352944019115254
iteration : 6515
train acc:  0.84375
train loss:  0.34393465518951416
train gradient:  0.17897602366505483
iteration : 6516
train acc:  0.8203125
train loss:  0.41608452796936035
train gradient:  0.2665190445971531
iteration : 6517
train acc:  0.7421875
train loss:  0.4358275532722473
train gradient:  0.27556081021237394
iteration : 6518
train acc:  0.859375
train loss:  0.3169148564338684
train gradient:  0.18905067879997423
iteration : 6519
train acc:  0.875
train loss:  0.25407326221466064
train gradient:  0.1438232969432212
iteration : 6520
train acc:  0.84375
train loss:  0.3688267469406128
train gradient:  0.194723521164571
iteration : 6521
train acc:  0.8671875
train loss:  0.3285987377166748
train gradient:  0.15860394835359432
iteration : 6522
train acc:  0.84375
train loss:  0.3410722017288208
train gradient:  0.1492464263750235
iteration : 6523
train acc:  0.859375
train loss:  0.339018851518631
train gradient:  0.21281864270779785
iteration : 6524
train acc:  0.90625
train loss:  0.27244967222213745
train gradient:  0.14782718382359478
iteration : 6525
train acc:  0.796875
train loss:  0.37470272183418274
train gradient:  0.17217128773574178
iteration : 6526
train acc:  0.8828125
train loss:  0.29940176010131836
train gradient:  0.17412322566239782
iteration : 6527
train acc:  0.796875
train loss:  0.4138706624507904
train gradient:  0.2787164038416966
iteration : 6528
train acc:  0.859375
train loss:  0.3279882073402405
train gradient:  0.2692149959558958
iteration : 6529
train acc:  0.9296875
train loss:  0.2199706733226776
train gradient:  0.12391527911715121
iteration : 6530
train acc:  0.84375
train loss:  0.3751989006996155
train gradient:  0.30390856693875196
iteration : 6531
train acc:  0.8359375
train loss:  0.3708364963531494
train gradient:  0.23816193117078027
iteration : 6532
train acc:  0.828125
train loss:  0.34981226921081543
train gradient:  0.21980281230963256
iteration : 6533
train acc:  0.8671875
train loss:  0.338489830493927
train gradient:  0.14084870559134338
iteration : 6534
train acc:  0.84375
train loss:  0.384247750043869
train gradient:  0.23263638963663963
iteration : 6535
train acc:  0.8671875
train loss:  0.3269886374473572
train gradient:  0.20268750687225018
iteration : 6536
train acc:  0.8359375
train loss:  0.3875645399093628
train gradient:  0.2864399324045446
iteration : 6537
train acc:  0.8671875
train loss:  0.35211312770843506
train gradient:  0.22396134506977058
iteration : 6538
train acc:  0.890625
train loss:  0.33046361804008484
train gradient:  0.20128501308481586
iteration : 6539
train acc:  0.8046875
train loss:  0.3692467212677002
train gradient:  0.18929684151655735
iteration : 6540
train acc:  0.8984375
train loss:  0.2797148823738098
train gradient:  0.12334682246034243
iteration : 6541
train acc:  0.8515625
train loss:  0.3506110906600952
train gradient:  0.15691626034136588
iteration : 6542
train acc:  0.875
train loss:  0.2963268756866455
train gradient:  0.2125459058939108
iteration : 6543
train acc:  0.7578125
train loss:  0.49473461508750916
train gradient:  0.42443178513709556
iteration : 6544
train acc:  0.8359375
train loss:  0.32145753502845764
train gradient:  0.22718326265464028
iteration : 6545
train acc:  0.828125
train loss:  0.3885043263435364
train gradient:  0.24799793595195324
iteration : 6546
train acc:  0.890625
train loss:  0.26770517230033875
train gradient:  0.12997230222031972
iteration : 6547
train acc:  0.890625
train loss:  0.2677573561668396
train gradient:  0.11415594084728019
iteration : 6548
train acc:  0.875
train loss:  0.27745914459228516
train gradient:  0.1562385911272784
iteration : 6549
train acc:  0.8671875
train loss:  0.27131417393684387
train gradient:  0.13024241728295616
iteration : 6550
train acc:  0.8046875
train loss:  0.4004192352294922
train gradient:  0.3521181375934739
iteration : 6551
train acc:  0.8515625
train loss:  0.34505435824394226
train gradient:  0.17898337880212825
iteration : 6552
train acc:  0.8828125
train loss:  0.34203749895095825
train gradient:  0.15514388559041423
iteration : 6553
train acc:  0.8828125
train loss:  0.2341768741607666
train gradient:  0.11052963026354952
iteration : 6554
train acc:  0.84375
train loss:  0.36585262417793274
train gradient:  0.1896556639082297
iteration : 6555
train acc:  0.8203125
train loss:  0.40083616971969604
train gradient:  0.329466295816801
iteration : 6556
train acc:  0.8828125
train loss:  0.2550048232078552
train gradient:  0.11512170441016464
iteration : 6557
train acc:  0.8828125
train loss:  0.2714289128780365
train gradient:  0.1120708804267231
iteration : 6558
train acc:  0.8515625
train loss:  0.2979321777820587
train gradient:  0.15995463117448847
iteration : 6559
train acc:  0.859375
train loss:  0.3429145812988281
train gradient:  0.2212566729235872
iteration : 6560
train acc:  0.84375
train loss:  0.4043138027191162
train gradient:  0.3229196484111713
iteration : 6561
train acc:  0.8125
train loss:  0.3602428138256073
train gradient:  0.2948600572172347
iteration : 6562
train acc:  0.8671875
train loss:  0.2841864824295044
train gradient:  0.13355275383487716
iteration : 6563
train acc:  0.8828125
train loss:  0.31928586959838867
train gradient:  0.1590249283389505
iteration : 6564
train acc:  0.875
train loss:  0.2949986159801483
train gradient:  0.1251853579563103
iteration : 6565
train acc:  0.8203125
train loss:  0.3903386890888214
train gradient:  0.17329709415752065
iteration : 6566
train acc:  0.8984375
train loss:  0.3097377121448517
train gradient:  0.12547609009647104
iteration : 6567
train acc:  0.890625
train loss:  0.3331039547920227
train gradient:  0.18327059207811897
iteration : 6568
train acc:  0.8359375
train loss:  0.3624914586544037
train gradient:  0.16929759616998824
iteration : 6569
train acc:  0.828125
train loss:  0.36977243423461914
train gradient:  0.19071785791237744
iteration : 6570
train acc:  0.84375
train loss:  0.4420672655105591
train gradient:  0.3090026225829295
iteration : 6571
train acc:  0.8125
train loss:  0.4086095094680786
train gradient:  0.26828726758980453
iteration : 6572
train acc:  0.828125
train loss:  0.344326376914978
train gradient:  0.2205161218142654
iteration : 6573
train acc:  0.8359375
train loss:  0.3401910066604614
train gradient:  0.16701434765221335
iteration : 6574
train acc:  0.8359375
train loss:  0.352073073387146
train gradient:  0.2210358610128776
iteration : 6575
train acc:  0.8359375
train loss:  0.3184583783149719
train gradient:  0.2017289908414679
iteration : 6576
train acc:  0.8671875
train loss:  0.32882511615753174
train gradient:  0.19113471342447877
iteration : 6577
train acc:  0.828125
train loss:  0.37624022364616394
train gradient:  0.21543936140689457
iteration : 6578
train acc:  0.8125
train loss:  0.46821606159210205
train gradient:  0.36059307895337556
iteration : 6579
train acc:  0.84375
train loss:  0.35865518450737
train gradient:  0.16038395597858732
iteration : 6580
train acc:  0.8203125
train loss:  0.4035613238811493
train gradient:  0.1953679777708447
iteration : 6581
train acc:  0.8671875
train loss:  0.3742537498474121
train gradient:  0.28051405535363166
iteration : 6582
train acc:  0.8203125
train loss:  0.3441343903541565
train gradient:  0.20089479311238842
iteration : 6583
train acc:  0.8515625
train loss:  0.29615557193756104
train gradient:  0.19021045626272645
iteration : 6584
train acc:  0.875
train loss:  0.31020498275756836
train gradient:  0.14072493239664408
iteration : 6585
train acc:  0.8515625
train loss:  0.34672361612319946
train gradient:  0.15853768576831712
iteration : 6586
train acc:  0.859375
train loss:  0.3263629078865051
train gradient:  0.18960068766412252
iteration : 6587
train acc:  0.8203125
train loss:  0.41703861951828003
train gradient:  0.3066766481638619
iteration : 6588
train acc:  0.8125
train loss:  0.3341899514198303
train gradient:  0.24785307839756865
iteration : 6589
train acc:  0.8828125
train loss:  0.30581095814704895
train gradient:  0.16115774596116345
iteration : 6590
train acc:  0.84375
train loss:  0.33966875076293945
train gradient:  0.24196622454104982
iteration : 6591
train acc:  0.859375
train loss:  0.32668328285217285
train gradient:  0.17779209693917472
iteration : 6592
train acc:  0.796875
train loss:  0.35457396507263184
train gradient:  0.1914012248711964
iteration : 6593
train acc:  0.78125
train loss:  0.5138973593711853
train gradient:  0.31428120796486314
iteration : 6594
train acc:  0.90625
train loss:  0.27331921458244324
train gradient:  0.14483989351283857
iteration : 6595
train acc:  0.8359375
train loss:  0.35085317492485046
train gradient:  0.20177804716520692
iteration : 6596
train acc:  0.828125
train loss:  0.3969219923019409
train gradient:  0.20410139179824544
iteration : 6597
train acc:  0.8671875
train loss:  0.3134622573852539
train gradient:  0.1702614713206449
iteration : 6598
train acc:  0.8671875
train loss:  0.30189698934555054
train gradient:  0.13042338324850095
iteration : 6599
train acc:  0.859375
train loss:  0.3897441029548645
train gradient:  0.18182416258956352
iteration : 6600
train acc:  0.8828125
train loss:  0.3082367181777954
train gradient:  0.19606813383557875
iteration : 6601
train acc:  0.8515625
train loss:  0.32381540536880493
train gradient:  0.20301112016387282
iteration : 6602
train acc:  0.8671875
train loss:  0.343043714761734
train gradient:  0.20968654792580127
iteration : 6603
train acc:  0.90625
train loss:  0.244551420211792
train gradient:  0.11907271203951637
iteration : 6604
train acc:  0.8046875
train loss:  0.4401922821998596
train gradient:  0.305434755389348
iteration : 6605
train acc:  0.8203125
train loss:  0.3414265215396881
train gradient:  0.18026668490211378
iteration : 6606
train acc:  0.8515625
train loss:  0.36492806673049927
train gradient:  0.2799703660304392
iteration : 6607
train acc:  0.875
train loss:  0.3737024664878845
train gradient:  0.2516513608496086
iteration : 6608
train acc:  0.875
train loss:  0.3526013493537903
train gradient:  0.21348769272664198
iteration : 6609
train acc:  0.875
train loss:  0.34808990359306335
train gradient:  0.23484745007034183
iteration : 6610
train acc:  0.875
train loss:  0.2955954372882843
train gradient:  0.13865823338720482
iteration : 6611
train acc:  0.875
train loss:  0.28507599234580994
train gradient:  0.1726990291778671
iteration : 6612
train acc:  0.8125
train loss:  0.356626033782959
train gradient:  0.2242989953577116
iteration : 6613
train acc:  0.8046875
train loss:  0.417034775018692
train gradient:  0.26379193022365444
iteration : 6614
train acc:  0.875
train loss:  0.333107590675354
train gradient:  0.20747696760389453
iteration : 6615
train acc:  0.890625
train loss:  0.2877674102783203
train gradient:  0.2048452536373607
iteration : 6616
train acc:  0.84375
train loss:  0.3130863308906555
train gradient:  0.15803638494168867
iteration : 6617
train acc:  0.8671875
train loss:  0.2919301986694336
train gradient:  0.1862807674828004
iteration : 6618
train acc:  0.890625
train loss:  0.2735099196434021
train gradient:  0.23979894503229066
iteration : 6619
train acc:  0.828125
train loss:  0.35817015171051025
train gradient:  0.1866968923401681
iteration : 6620
train acc:  0.875
train loss:  0.29880186915397644
train gradient:  0.14980490545984843
iteration : 6621
train acc:  0.8828125
train loss:  0.33278048038482666
train gradient:  0.20233085301668124
iteration : 6622
train acc:  0.859375
train loss:  0.3558793067932129
train gradient:  0.2564774542625415
iteration : 6623
train acc:  0.8203125
train loss:  0.36675646901130676
train gradient:  0.28491921716727386
iteration : 6624
train acc:  0.84375
train loss:  0.3566502034664154
train gradient:  0.22635262680659174
iteration : 6625
train acc:  0.8671875
train loss:  0.35955142974853516
train gradient:  0.2406057619516811
iteration : 6626
train acc:  0.84375
train loss:  0.33778393268585205
train gradient:  0.17416158109692104
iteration : 6627
train acc:  0.8984375
train loss:  0.26246732473373413
train gradient:  0.16271495633319255
iteration : 6628
train acc:  0.8515625
train loss:  0.33115383982658386
train gradient:  0.19601285684244688
iteration : 6629
train acc:  0.875
train loss:  0.3340032696723938
train gradient:  0.21924550484341704
iteration : 6630
train acc:  0.8046875
train loss:  0.36679840087890625
train gradient:  0.23339810712531506
iteration : 6631
train acc:  0.84375
train loss:  0.3543989658355713
train gradient:  0.2929260623700765
iteration : 6632
train acc:  0.859375
train loss:  0.2903003692626953
train gradient:  0.14041524495756413
iteration : 6633
train acc:  0.8359375
train loss:  0.2873656153678894
train gradient:  0.14898809253369968
iteration : 6634
train acc:  0.8828125
train loss:  0.3070927858352661
train gradient:  0.1786342239994509
iteration : 6635
train acc:  0.8515625
train loss:  0.377116858959198
train gradient:  0.33640055014337067
iteration : 6636
train acc:  0.828125
train loss:  0.35475242137908936
train gradient:  0.20451015346919002
iteration : 6637
train acc:  0.859375
train loss:  0.32803958654403687
train gradient:  0.24658327721062123
iteration : 6638
train acc:  0.8828125
train loss:  0.33302968740463257
train gradient:  0.20539773893209248
iteration : 6639
train acc:  0.890625
train loss:  0.283363938331604
train gradient:  0.12304252604926655
iteration : 6640
train acc:  0.859375
train loss:  0.3193145990371704
train gradient:  0.17936499385962407
iteration : 6641
train acc:  0.859375
train loss:  0.32384222745895386
train gradient:  0.2676166872373941
iteration : 6642
train acc:  0.8671875
train loss:  0.34452128410339355
train gradient:  0.202388261154338
iteration : 6643
train acc:  0.875
train loss:  0.30338865518569946
train gradient:  0.17380219440307224
iteration : 6644
train acc:  0.828125
train loss:  0.37247759103775024
train gradient:  0.38990361083216885
iteration : 6645
train acc:  0.875
train loss:  0.27166634798049927
train gradient:  0.17250170031303746
iteration : 6646
train acc:  0.8046875
train loss:  0.3683823347091675
train gradient:  0.3354673134712245
iteration : 6647
train acc:  0.8671875
train loss:  0.295396625995636
train gradient:  0.20571741170534233
iteration : 6648
train acc:  0.8515625
train loss:  0.3589198589324951
train gradient:  0.2128927681026715
iteration : 6649
train acc:  0.859375
train loss:  0.36024194955825806
train gradient:  0.23825948765922292
iteration : 6650
train acc:  0.84375
train loss:  0.3416280746459961
train gradient:  0.2673814449167798
iteration : 6651
train acc:  0.890625
train loss:  0.2693295478820801
train gradient:  0.18064817626598056
iteration : 6652
train acc:  0.828125
train loss:  0.34745174646377563
train gradient:  0.2518523733111739
iteration : 6653
train acc:  0.8515625
train loss:  0.2852764129638672
train gradient:  0.22995768137346573
iteration : 6654
train acc:  0.8203125
train loss:  0.3305546045303345
train gradient:  0.15430974608397335
iteration : 6655
train acc:  0.8515625
train loss:  0.3507000803947449
train gradient:  0.18914690325932593
iteration : 6656
train acc:  0.8203125
train loss:  0.3960122764110565
train gradient:  0.3147233198038248
iteration : 6657
train acc:  0.859375
train loss:  0.3857177495956421
train gradient:  0.3250155003460051
iteration : 6658
train acc:  0.8671875
train loss:  0.32843995094299316
train gradient:  0.224546161088918
iteration : 6659
train acc:  0.875
train loss:  0.3427084684371948
train gradient:  0.1874046495017481
iteration : 6660
train acc:  0.8359375
train loss:  0.39748793840408325
train gradient:  0.2677318452463566
iteration : 6661
train acc:  0.9140625
train loss:  0.2614840567111969
train gradient:  0.16417214107987063
iteration : 6662
train acc:  0.84375
train loss:  0.33803775906562805
train gradient:  0.20028786617057137
iteration : 6663
train acc:  0.8828125
train loss:  0.3059873580932617
train gradient:  0.12678851919241813
iteration : 6664
train acc:  0.8828125
train loss:  0.3123489022254944
train gradient:  0.22049543943851538
iteration : 6665
train acc:  0.796875
train loss:  0.408048152923584
train gradient:  0.28850327753847216
iteration : 6666
train acc:  0.8671875
train loss:  0.31509512662887573
train gradient:  0.17161367315785594
iteration : 6667
train acc:  0.8203125
train loss:  0.4550913870334625
train gradient:  0.4165237943585809
iteration : 6668
train acc:  0.8046875
train loss:  0.394589364528656
train gradient:  0.264041020150447
iteration : 6669
train acc:  0.828125
train loss:  0.4094778895378113
train gradient:  0.39343096926617904
iteration : 6670
train acc:  0.875
train loss:  0.27653032541275024
train gradient:  0.16072944280831444
iteration : 6671
train acc:  0.8828125
train loss:  0.2417774200439453
train gradient:  0.16597409668800922
iteration : 6672
train acc:  0.828125
train loss:  0.38364237546920776
train gradient:  0.365873225930402
iteration : 6673
train acc:  0.8671875
train loss:  0.30721187591552734
train gradient:  0.16479860255536338
iteration : 6674
train acc:  0.8671875
train loss:  0.2889043390750885
train gradient:  0.20727743237607593
iteration : 6675
train acc:  0.8359375
train loss:  0.32436251640319824
train gradient:  0.1492658971385697
iteration : 6676
train acc:  0.875
train loss:  0.301388680934906
train gradient:  0.17418274823779545
iteration : 6677
train acc:  0.84375
train loss:  0.36599642038345337
train gradient:  0.1633424637889146
iteration : 6678
train acc:  0.859375
train loss:  0.2938774824142456
train gradient:  0.24611214510393847
iteration : 6679
train acc:  0.8203125
train loss:  0.3322300314903259
train gradient:  0.1609400695464096
iteration : 6680
train acc:  0.890625
train loss:  0.2737312614917755
train gradient:  0.12824472519011876
iteration : 6681
train acc:  0.8515625
train loss:  0.36416250467300415
train gradient:  0.20232667396714213
iteration : 6682
train acc:  0.859375
train loss:  0.2977178394794464
train gradient:  0.1504338926038961
iteration : 6683
train acc:  0.875
train loss:  0.35926613211631775
train gradient:  0.2405133032310397
iteration : 6684
train acc:  0.890625
train loss:  0.23941347002983093
train gradient:  0.14019303523991233
iteration : 6685
train acc:  0.8046875
train loss:  0.43288981914520264
train gradient:  0.36183588831107744
iteration : 6686
train acc:  0.859375
train loss:  0.3567650318145752
train gradient:  0.22652045463562406
iteration : 6687
train acc:  0.875
train loss:  0.28971391916275024
train gradient:  0.19662588053198554
iteration : 6688
train acc:  0.84375
train loss:  0.4046591520309448
train gradient:  0.26873248946027317
iteration : 6689
train acc:  0.8359375
train loss:  0.3511556088924408
train gradient:  0.2549058677235792
iteration : 6690
train acc:  0.90625
train loss:  0.2892884910106659
train gradient:  0.21661914069713903
iteration : 6691
train acc:  0.8203125
train loss:  0.42298534512519836
train gradient:  0.29117127116059416
iteration : 6692
train acc:  0.84375
train loss:  0.3264845609664917
train gradient:  0.23185206774634987
iteration : 6693
train acc:  0.828125
train loss:  0.3605516850948334
train gradient:  0.20632534950199602
iteration : 6694
train acc:  0.859375
train loss:  0.33768177032470703
train gradient:  0.14921243033149295
iteration : 6695
train acc:  0.84375
train loss:  0.34172379970550537
train gradient:  0.19617210348371322
iteration : 6696
train acc:  0.8125
train loss:  0.3992999196052551
train gradient:  0.34192772034399777
iteration : 6697
train acc:  0.796875
train loss:  0.40182816982269287
train gradient:  0.24201230617034247
iteration : 6698
train acc:  0.8203125
train loss:  0.4021977186203003
train gradient:  0.19793633228468507
iteration : 6699
train acc:  0.84375
train loss:  0.2968999147415161
train gradient:  0.1275240993623862
iteration : 6700
train acc:  0.796875
train loss:  0.46058958768844604
train gradient:  0.3236469817091199
iteration : 6701
train acc:  0.8984375
train loss:  0.27701106667518616
train gradient:  0.14207020629482897
iteration : 6702
train acc:  0.8203125
train loss:  0.4043574333190918
train gradient:  0.2196437192117401
iteration : 6703
train acc:  0.8203125
train loss:  0.41133052110671997
train gradient:  0.25904104785883086
iteration : 6704
train acc:  0.8125
train loss:  0.459225594997406
train gradient:  0.31405338967654806
iteration : 6705
train acc:  0.8515625
train loss:  0.3416959047317505
train gradient:  0.16338643214688992
iteration : 6706
train acc:  0.8828125
train loss:  0.30513137578964233
train gradient:  0.22409005356071982
iteration : 6707
train acc:  0.890625
train loss:  0.3476209044456482
train gradient:  0.16347634675124006
iteration : 6708
train acc:  0.8046875
train loss:  0.4323309063911438
train gradient:  0.22133382702011467
iteration : 6709
train acc:  0.8828125
train loss:  0.31184208393096924
train gradient:  0.1428160916055904
iteration : 6710
train acc:  0.84375
train loss:  0.3379884362220764
train gradient:  0.20391155563589275
iteration : 6711
train acc:  0.8671875
train loss:  0.2979157865047455
train gradient:  0.1825360678104942
iteration : 6712
train acc:  0.796875
train loss:  0.3418360948562622
train gradient:  0.21296698291446703
iteration : 6713
train acc:  0.8671875
train loss:  0.31460142135620117
train gradient:  0.238010766063778
iteration : 6714
train acc:  0.90625
train loss:  0.28627049922943115
train gradient:  0.13247308251514936
iteration : 6715
train acc:  0.90625
train loss:  0.2636319398880005
train gradient:  0.11700753233173987
iteration : 6716
train acc:  0.875
train loss:  0.37141096591949463
train gradient:  0.2332672483581945
iteration : 6717
train acc:  0.859375
train loss:  0.3353482186794281
train gradient:  0.2329756626856286
iteration : 6718
train acc:  0.875
train loss:  0.3557468056678772
train gradient:  0.24103520573210735
iteration : 6719
train acc:  0.9140625
train loss:  0.24024146795272827
train gradient:  0.1363315505812212
iteration : 6720
train acc:  0.8046875
train loss:  0.40152212977409363
train gradient:  0.19049369106479486
iteration : 6721
train acc:  0.859375
train loss:  0.37718716263771057
train gradient:  0.19532482989649852
iteration : 6722
train acc:  0.890625
train loss:  0.3009476065635681
train gradient:  0.16182373935504535
iteration : 6723
train acc:  0.890625
train loss:  0.27579641342163086
train gradient:  0.18802518391750067
iteration : 6724
train acc:  0.8671875
train loss:  0.3221115171909332
train gradient:  0.22038522698094565
iteration : 6725
train acc:  0.8828125
train loss:  0.26856181025505066
train gradient:  0.16207094076538836
iteration : 6726
train acc:  0.859375
train loss:  0.2944062054157257
train gradient:  0.19851638546870956
iteration : 6727
train acc:  0.828125
train loss:  0.40260523557662964
train gradient:  0.2381196023540199
iteration : 6728
train acc:  0.8203125
train loss:  0.4290223717689514
train gradient:  0.24103725771816106
iteration : 6729
train acc:  0.875
train loss:  0.27881044149398804
train gradient:  0.18799671035589993
iteration : 6730
train acc:  0.8671875
train loss:  0.3255564570426941
train gradient:  0.17726032378418843
iteration : 6731
train acc:  0.875
train loss:  0.2839897572994232
train gradient:  0.13308039459197157
iteration : 6732
train acc:  0.84375
train loss:  0.43566811084747314
train gradient:  0.26876497913055575
iteration : 6733
train acc:  0.8671875
train loss:  0.34574300050735474
train gradient:  0.2554307056283379
iteration : 6734
train acc:  0.8203125
train loss:  0.3975025415420532
train gradient:  0.26202837927527606
iteration : 6735
train acc:  0.8671875
train loss:  0.28455179929733276
train gradient:  0.17588776767057684
iteration : 6736
train acc:  0.8984375
train loss:  0.3184352517127991
train gradient:  0.19024708824799558
iteration : 6737
train acc:  0.8671875
train loss:  0.36815741658210754
train gradient:  0.2714599325958183
iteration : 6738
train acc:  0.9140625
train loss:  0.2546865940093994
train gradient:  0.12516030938289466
iteration : 6739
train acc:  0.8203125
train loss:  0.3399554491043091
train gradient:  0.20961089347339915
iteration : 6740
train acc:  0.8671875
train loss:  0.3029630780220032
train gradient:  0.2549639160901057
iteration : 6741
train acc:  0.90625
train loss:  0.2858641445636749
train gradient:  0.23388197682998013
iteration : 6742
train acc:  0.8984375
train loss:  0.23879724740982056
train gradient:  0.17485411511288607
iteration : 6743
train acc:  0.8671875
train loss:  0.29272618889808655
train gradient:  0.13618775349163098
iteration : 6744
train acc:  0.9140625
train loss:  0.27397435903549194
train gradient:  0.11495610825567835
iteration : 6745
train acc:  0.875
train loss:  0.25493884086608887
train gradient:  0.15543698882149742
iteration : 6746
train acc:  0.84375
train loss:  0.36600038409233093
train gradient:  0.18614988081624587
iteration : 6747
train acc:  0.8125
train loss:  0.40771031379699707
train gradient:  0.29892268796306753
iteration : 6748
train acc:  0.8359375
train loss:  0.3350246846675873
train gradient:  0.22176725642220635
iteration : 6749
train acc:  0.8515625
train loss:  0.34859007596969604
train gradient:  0.175910719428697
iteration : 6750
train acc:  0.90625
train loss:  0.2689640522003174
train gradient:  0.13714903776815424
iteration : 6751
train acc:  0.8828125
train loss:  0.29933443665504456
train gradient:  0.23080122114862772
iteration : 6752
train acc:  0.8515625
train loss:  0.3607671558856964
train gradient:  0.2741969700743243
iteration : 6753
train acc:  0.8359375
train loss:  0.376315712928772
train gradient:  0.28785973168869045
iteration : 6754
train acc:  0.8125
train loss:  0.4004793167114258
train gradient:  0.2768586959535617
iteration : 6755
train acc:  0.9140625
train loss:  0.24193406105041504
train gradient:  0.12164887752551225
iteration : 6756
train acc:  0.7734375
train loss:  0.4186471402645111
train gradient:  0.32281761440355866
iteration : 6757
train acc:  0.84375
train loss:  0.31705546379089355
train gradient:  0.316104782221206
iteration : 6758
train acc:  0.921875
train loss:  0.22982744872570038
train gradient:  0.11351166645698084
iteration : 6759
train acc:  0.828125
train loss:  0.3817684054374695
train gradient:  0.3404237836866822
iteration : 6760
train acc:  0.859375
train loss:  0.3094983696937561
train gradient:  0.214686384450477
iteration : 6761
train acc:  0.859375
train loss:  0.336747944355011
train gradient:  0.25480553071214773
iteration : 6762
train acc:  0.8828125
train loss:  0.28013551235198975
train gradient:  0.16026301135031376
iteration : 6763
train acc:  0.8515625
train loss:  0.2908928394317627
train gradient:  0.1345673190440933
iteration : 6764
train acc:  0.828125
train loss:  0.4019966721534729
train gradient:  0.2939156319119089
iteration : 6765
train acc:  0.8671875
train loss:  0.29449257254600525
train gradient:  0.21050766138550725
iteration : 6766
train acc:  0.8984375
train loss:  0.2861751616001129
train gradient:  0.209939574960345
iteration : 6767
train acc:  0.84375
train loss:  0.3129369616508484
train gradient:  0.19678926659400936
iteration : 6768
train acc:  0.8828125
train loss:  0.27715563774108887
train gradient:  0.19170698855505663
iteration : 6769
train acc:  0.890625
train loss:  0.28589972853660583
train gradient:  0.19964216178687405
iteration : 6770
train acc:  0.8515625
train loss:  0.27951738238334656
train gradient:  0.25362037453921116
iteration : 6771
train acc:  0.7890625
train loss:  0.439206600189209
train gradient:  0.29861972567679407
iteration : 6772
train acc:  0.84375
train loss:  0.3907618522644043
train gradient:  0.24512810757570597
iteration : 6773
train acc:  0.890625
train loss:  0.28425753116607666
train gradient:  0.4196358755322165
iteration : 6774
train acc:  0.890625
train loss:  0.2807811498641968
train gradient:  0.12702939009890957
iteration : 6775
train acc:  0.90625
train loss:  0.2815362215042114
train gradient:  0.15292607639595385
iteration : 6776
train acc:  0.84375
train loss:  0.31319648027420044
train gradient:  0.1802889011379059
iteration : 6777
train acc:  0.859375
train loss:  0.2629115879535675
train gradient:  0.2186125707285349
iteration : 6778
train acc:  0.890625
train loss:  0.3241567611694336
train gradient:  0.17600514948089524
iteration : 6779
train acc:  0.8515625
train loss:  0.3965204656124115
train gradient:  0.3079387914840229
iteration : 6780
train acc:  0.8515625
train loss:  0.31790658831596375
train gradient:  0.19683578758808365
iteration : 6781
train acc:  0.875
train loss:  0.34083622694015503
train gradient:  0.23939012602524432
iteration : 6782
train acc:  0.8828125
train loss:  0.29380613565444946
train gradient:  0.2900917810737496
iteration : 6783
train acc:  0.78125
train loss:  0.3872176706790924
train gradient:  0.24608772742162938
iteration : 6784
train acc:  0.8828125
train loss:  0.30389466881752014
train gradient:  0.18241287411088253
iteration : 6785
train acc:  0.8359375
train loss:  0.3996641933917999
train gradient:  0.5161006822207088
iteration : 6786
train acc:  0.859375
train loss:  0.31505388021469116
train gradient:  0.2004254742232216
iteration : 6787
train acc:  0.8671875
train loss:  0.3292529582977295
train gradient:  0.21455912294303775
iteration : 6788
train acc:  0.921875
train loss:  0.248246967792511
train gradient:  0.17930877118732802
iteration : 6789
train acc:  0.84375
train loss:  0.40874436497688293
train gradient:  0.2664461209076772
iteration : 6790
train acc:  0.828125
train loss:  0.40322816371917725
train gradient:  0.29827726821592787
iteration : 6791
train acc:  0.875
train loss:  0.34516650438308716
train gradient:  0.2521786120905981
iteration : 6792
train acc:  0.8359375
train loss:  0.4011363685131073
train gradient:  0.22244837451635216
iteration : 6793
train acc:  0.859375
train loss:  0.3083548843860626
train gradient:  0.21792947152823944
iteration : 6794
train acc:  0.8828125
train loss:  0.279556006193161
train gradient:  0.15061990410522685
iteration : 6795
train acc:  0.8671875
train loss:  0.33460894227027893
train gradient:  0.20329020076511145
iteration : 6796
train acc:  0.828125
train loss:  0.40017879009246826
train gradient:  0.2659759735706301
iteration : 6797
train acc:  0.8203125
train loss:  0.3325621485710144
train gradient:  0.18899005970108482
iteration : 6798
train acc:  0.8515625
train loss:  0.36336106061935425
train gradient:  0.27660919820468227
iteration : 6799
train acc:  0.7890625
train loss:  0.4244540333747864
train gradient:  0.2898369007587017
iteration : 6800
train acc:  0.8359375
train loss:  0.41128671169281006
train gradient:  0.2777079145462029
iteration : 6801
train acc:  0.8125
train loss:  0.41215354204177856
train gradient:  0.3023527734910475
iteration : 6802
train acc:  0.875
train loss:  0.329595148563385
train gradient:  0.16704382044869198
iteration : 6803
train acc:  0.8671875
train loss:  0.3557237982749939
train gradient:  0.1936759767249786
iteration : 6804
train acc:  0.8125
train loss:  0.4220123291015625
train gradient:  0.2713362958168572
iteration : 6805
train acc:  0.859375
train loss:  0.35854238271713257
train gradient:  0.26282395306790457
iteration : 6806
train acc:  0.8359375
train loss:  0.4033619165420532
train gradient:  0.24040047199516126
iteration : 6807
train acc:  0.8359375
train loss:  0.33730190992355347
train gradient:  0.2346539964475986
iteration : 6808
train acc:  0.8125
train loss:  0.4222213327884674
train gradient:  0.2679313662200517
iteration : 6809
train acc:  0.859375
train loss:  0.3969956636428833
train gradient:  0.2613762575006621
iteration : 6810
train acc:  0.8359375
train loss:  0.3683193325996399
train gradient:  0.23598673484796195
iteration : 6811
train acc:  0.828125
train loss:  0.3078257441520691
train gradient:  0.18859658173824723
iteration : 6812
train acc:  0.8515625
train loss:  0.3631706237792969
train gradient:  0.2524706466131706
iteration : 6813
train acc:  0.796875
train loss:  0.4307969808578491
train gradient:  0.24048759734889938
iteration : 6814
train acc:  0.859375
train loss:  0.38307714462280273
train gradient:  0.21313703373915982
iteration : 6815
train acc:  0.78125
train loss:  0.42609769105911255
train gradient:  0.33742601869219335
iteration : 6816
train acc:  0.84375
train loss:  0.3748869299888611
train gradient:  0.2718714375727476
iteration : 6817
train acc:  0.84375
train loss:  0.3612273037433624
train gradient:  0.17989864087230462
iteration : 6818
train acc:  0.8125
train loss:  0.3468742370605469
train gradient:  0.1979715151743554
iteration : 6819
train acc:  0.859375
train loss:  0.2943081855773926
train gradient:  0.1610468199315939
iteration : 6820
train acc:  0.84375
train loss:  0.3775217831134796
train gradient:  0.16249321176524234
iteration : 6821
train acc:  0.828125
train loss:  0.37829136848449707
train gradient:  0.18496092288964333
iteration : 6822
train acc:  0.875
train loss:  0.28427207469940186
train gradient:  0.12497725731109738
iteration : 6823
train acc:  0.8125
train loss:  0.40201765298843384
train gradient:  0.25520529016260585
iteration : 6824
train acc:  0.8671875
train loss:  0.3039775490760803
train gradient:  0.1998402401981928
iteration : 6825
train acc:  0.84375
train loss:  0.30263620615005493
train gradient:  0.13375489308799976
iteration : 6826
train acc:  0.8515625
train loss:  0.35277944803237915
train gradient:  0.40568991724917935
iteration : 6827
train acc:  0.84375
train loss:  0.34916460514068604
train gradient:  0.25371445392891184
iteration : 6828
train acc:  0.7890625
train loss:  0.4614408612251282
train gradient:  0.30238841878354356
iteration : 6829
train acc:  0.7890625
train loss:  0.42022350430488586
train gradient:  0.2503614785138215
iteration : 6830
train acc:  0.78125
train loss:  0.37894147634506226
train gradient:  0.22949013746192876
iteration : 6831
train acc:  0.8984375
train loss:  0.30771604180336
train gradient:  0.1823631384429339
iteration : 6832
train acc:  0.859375
train loss:  0.31028443574905396
train gradient:  0.15046974117884215
iteration : 6833
train acc:  0.8125
train loss:  0.41799163818359375
train gradient:  0.1635889071924747
iteration : 6834
train acc:  0.796875
train loss:  0.39483192563056946
train gradient:  0.21040037256484345
iteration : 6835
train acc:  0.84375
train loss:  0.32663464546203613
train gradient:  0.19390473645071488
iteration : 6836
train acc:  0.8359375
train loss:  0.40985995531082153
train gradient:  0.2284903943677738
iteration : 6837
train acc:  0.8671875
train loss:  0.31962206959724426
train gradient:  0.1365032442172433
iteration : 6838
train acc:  0.8046875
train loss:  0.3225567936897278
train gradient:  0.15827334017739614
iteration : 6839
train acc:  0.8203125
train loss:  0.3669428825378418
train gradient:  0.16357295814674205
iteration : 6840
train acc:  0.875
train loss:  0.2946351170539856
train gradient:  0.21373981154281424
iteration : 6841
train acc:  0.8671875
train loss:  0.3260813355445862
train gradient:  0.18806985172736135
iteration : 6842
train acc:  0.9140625
train loss:  0.27858978509902954
train gradient:  0.15384280891009777
iteration : 6843
train acc:  0.8515625
train loss:  0.3159790337085724
train gradient:  0.15456627496577577
iteration : 6844
train acc:  0.859375
train loss:  0.34005242586135864
train gradient:  0.19626511107158182
iteration : 6845
train acc:  0.859375
train loss:  0.2958596348762512
train gradient:  0.12500071783055233
iteration : 6846
train acc:  0.8984375
train loss:  0.2581980526447296
train gradient:  0.1392751986217448
iteration : 6847
train acc:  0.859375
train loss:  0.28786584734916687
train gradient:  0.14923867332132446
iteration : 6848
train acc:  0.8828125
train loss:  0.2915527820587158
train gradient:  0.18696338819751612
iteration : 6849
train acc:  0.8671875
train loss:  0.31633150577545166
train gradient:  0.16975294851743672
iteration : 6850
train acc:  0.8203125
train loss:  0.34111183881759644
train gradient:  0.137862892280009
iteration : 6851
train acc:  0.8359375
train loss:  0.39812472462654114
train gradient:  0.2933856626152315
iteration : 6852
train acc:  0.859375
train loss:  0.3840384781360626
train gradient:  0.22491033588261425
iteration : 6853
train acc:  0.8125
train loss:  0.38112959265708923
train gradient:  0.20199987422011068
iteration : 6854
train acc:  0.8671875
train loss:  0.3476915955543518
train gradient:  0.20292513191482217
iteration : 6855
train acc:  0.78125
train loss:  0.42720770835876465
train gradient:  0.2673884793768532
iteration : 6856
train acc:  0.90625
train loss:  0.2606290578842163
train gradient:  0.12730205496367072
iteration : 6857
train acc:  0.875
train loss:  0.30169251561164856
train gradient:  0.16222015596573455
iteration : 6858
train acc:  0.8671875
train loss:  0.28842082619667053
train gradient:  0.15945275813731502
iteration : 6859
train acc:  0.859375
train loss:  0.33463358879089355
train gradient:  0.20630003190341523
iteration : 6860
train acc:  0.8125
train loss:  0.3430948555469513
train gradient:  0.2028041615961531
iteration : 6861
train acc:  0.84375
train loss:  0.3451823592185974
train gradient:  0.18462340799521088
iteration : 6862
train acc:  0.8046875
train loss:  0.4112691283226013
train gradient:  0.2730470819357136
iteration : 6863
train acc:  0.8671875
train loss:  0.32712990045547485
train gradient:  0.13485231725413344
iteration : 6864
train acc:  0.859375
train loss:  0.3445824682712555
train gradient:  0.20835358566594916
iteration : 6865
train acc:  0.7734375
train loss:  0.4368966221809387
train gradient:  0.32471017632866755
iteration : 6866
train acc:  0.8515625
train loss:  0.29113391041755676
train gradient:  0.15840768345154033
iteration : 6867
train acc:  0.8515625
train loss:  0.3384743928909302
train gradient:  0.1803288268922244
iteration : 6868
train acc:  0.8515625
train loss:  0.3399565815925598
train gradient:  0.17880934726431194
iteration : 6869
train acc:  0.875
train loss:  0.33102354407310486
train gradient:  0.22724831869813303
iteration : 6870
train acc:  0.890625
train loss:  0.28712087869644165
train gradient:  0.14273552377226234
iteration : 6871
train acc:  0.8359375
train loss:  0.30243006348609924
train gradient:  0.16077852090468706
iteration : 6872
train acc:  0.890625
train loss:  0.2462807595729828
train gradient:  0.14093194626000605
iteration : 6873
train acc:  0.8671875
train loss:  0.33680060505867004
train gradient:  0.20796136199308396
iteration : 6874
train acc:  0.8671875
train loss:  0.2862420380115509
train gradient:  0.13658965400420348
iteration : 6875
train acc:  0.890625
train loss:  0.2956453859806061
train gradient:  0.315621420272442
iteration : 6876
train acc:  0.8515625
train loss:  0.36060604453086853
train gradient:  0.21458087532352488
iteration : 6877
train acc:  0.828125
train loss:  0.3223839998245239
train gradient:  0.21796654551130015
iteration : 6878
train acc:  0.796875
train loss:  0.43652933835983276
train gradient:  0.3233049555653166
iteration : 6879
train acc:  0.8515625
train loss:  0.3165769577026367
train gradient:  0.19818098807681522
iteration : 6880
train acc:  0.8046875
train loss:  0.3994646668434143
train gradient:  0.314763406222188
iteration : 6881
train acc:  0.84375
train loss:  0.33814355731010437
train gradient:  0.1588077310173468
iteration : 6882
train acc:  0.9296875
train loss:  0.27081209421157837
train gradient:  0.10856365533404033
iteration : 6883
train acc:  0.828125
train loss:  0.3791179656982422
train gradient:  0.2637871249799421
iteration : 6884
train acc:  0.8984375
train loss:  0.3339918851852417
train gradient:  0.2139783201282764
iteration : 6885
train acc:  0.8515625
train loss:  0.36476022005081177
train gradient:  0.18957000170297877
iteration : 6886
train acc:  0.8515625
train loss:  0.3756093978881836
train gradient:  0.23500916770776364
iteration : 6887
train acc:  0.875
train loss:  0.2897133529186249
train gradient:  0.1694971511059289
iteration : 6888
train acc:  0.8203125
train loss:  0.3823402225971222
train gradient:  0.25192992401852793
iteration : 6889
train acc:  0.796875
train loss:  0.37466195225715637
train gradient:  0.23645980114953016
iteration : 6890
train acc:  0.8046875
train loss:  0.36429956555366516
train gradient:  0.21660426574861213
iteration : 6891
train acc:  0.8984375
train loss:  0.3014364242553711
train gradient:  0.21686706991419413
iteration : 6892
train acc:  0.8671875
train loss:  0.33650749921798706
train gradient:  0.13566919435237376
iteration : 6893
train acc:  0.84375
train loss:  0.37847334146499634
train gradient:  0.22502287945426155
iteration : 6894
train acc:  0.8671875
train loss:  0.32757529616355896
train gradient:  0.2071839855781515
iteration : 6895
train acc:  0.828125
train loss:  0.3869095742702484
train gradient:  0.23022411162007572
iteration : 6896
train acc:  0.8828125
train loss:  0.2898457646369934
train gradient:  0.15127232778350908
iteration : 6897
train acc:  0.859375
train loss:  0.3258151710033417
train gradient:  0.2091730771898563
iteration : 6898
train acc:  0.84375
train loss:  0.3843080997467041
train gradient:  0.2746135900346136
iteration : 6899
train acc:  0.8515625
train loss:  0.34289616346359253
train gradient:  0.23984926053618527
iteration : 6900
train acc:  0.8359375
train loss:  0.30598539113998413
train gradient:  0.15581344365275357
iteration : 6901
train acc:  0.90625
train loss:  0.321835458278656
train gradient:  0.17884759884796012
iteration : 6902
train acc:  0.8515625
train loss:  0.3389858603477478
train gradient:  0.15502765456684836
iteration : 6903
train acc:  0.890625
train loss:  0.2710625231266022
train gradient:  0.1167992514609984
iteration : 6904
train acc:  0.875
train loss:  0.33115214109420776
train gradient:  0.23826311102602826
iteration : 6905
train acc:  0.828125
train loss:  0.3782990276813507
train gradient:  0.3075970712065847
iteration : 6906
train acc:  0.890625
train loss:  0.2992396950721741
train gradient:  0.1612100677234677
iteration : 6907
train acc:  0.8203125
train loss:  0.36753973364830017
train gradient:  0.2814968416034631
iteration : 6908
train acc:  0.859375
train loss:  0.35928472876548767
train gradient:  0.2833635627994891
iteration : 6909
train acc:  0.828125
train loss:  0.34765928983688354
train gradient:  0.20886580778112535
iteration : 6910
train acc:  0.8125
train loss:  0.41262340545654297
train gradient:  0.2748090528626434
iteration : 6911
train acc:  0.8671875
train loss:  0.32939422130584717
train gradient:  0.21285007361386765
iteration : 6912
train acc:  0.8515625
train loss:  0.34006941318511963
train gradient:  0.1903052266370679
iteration : 6913
train acc:  0.84375
train loss:  0.36194777488708496
train gradient:  0.19070410197683052
iteration : 6914
train acc:  0.8515625
train loss:  0.32841789722442627
train gradient:  0.22009628425002903
iteration : 6915
train acc:  0.8828125
train loss:  0.3164268732070923
train gradient:  0.2171657193981334
iteration : 6916
train acc:  0.8046875
train loss:  0.3682195544242859
train gradient:  0.30333167828516155
iteration : 6917
train acc:  0.828125
train loss:  0.3957076668739319
train gradient:  0.31987190797273385
iteration : 6918
train acc:  0.84375
train loss:  0.295978844165802
train gradient:  0.15035440219229437
iteration : 6919
train acc:  0.8828125
train loss:  0.28749191761016846
train gradient:  0.1490881262353982
iteration : 6920
train acc:  0.8359375
train loss:  0.34325963258743286
train gradient:  0.19684430547585532
iteration : 6921
train acc:  0.8828125
train loss:  0.26797980070114136
train gradient:  0.15461665909710182
iteration : 6922
train acc:  0.8828125
train loss:  0.30524742603302
train gradient:  0.14399843934372686
iteration : 6923
train acc:  0.953125
train loss:  0.21808762848377228
train gradient:  0.10587154678503984
iteration : 6924
train acc:  0.84375
train loss:  0.30895835161209106
train gradient:  0.16069219985534938
iteration : 6925
train acc:  0.8828125
train loss:  0.3722570538520813
train gradient:  0.18247144173606986
iteration : 6926
train acc:  0.875
train loss:  0.2783489227294922
train gradient:  0.17821483735272603
iteration : 6927
train acc:  0.875
train loss:  0.3044850826263428
train gradient:  0.13077743224223431
iteration : 6928
train acc:  0.8671875
train loss:  0.2961975932121277
train gradient:  0.0987161735724921
iteration : 6929
train acc:  0.8125
train loss:  0.37348437309265137
train gradient:  0.20154903435267854
iteration : 6930
train acc:  0.8515625
train loss:  0.3721206784248352
train gradient:  0.21163742820196207
iteration : 6931
train acc:  0.8671875
train loss:  0.25971555709838867
train gradient:  0.1736231685165341
iteration : 6932
train acc:  0.875
train loss:  0.3048064112663269
train gradient:  0.18055776578344335
iteration : 6933
train acc:  0.8046875
train loss:  0.3953087329864502
train gradient:  0.2277942215217963
iteration : 6934
train acc:  0.8515625
train loss:  0.34951162338256836
train gradient:  0.2501240124100692
iteration : 6935
train acc:  0.8515625
train loss:  0.34841781854629517
train gradient:  0.2592638845079426
iteration : 6936
train acc:  0.84375
train loss:  0.34233149886131287
train gradient:  0.17380705547699432
iteration : 6937
train acc:  0.8125
train loss:  0.4036906957626343
train gradient:  0.22615832592020185
iteration : 6938
train acc:  0.8828125
train loss:  0.3076666593551636
train gradient:  0.2267610417681904
iteration : 6939
train acc:  0.875
train loss:  0.29252690076828003
train gradient:  0.11499790758802364
iteration : 6940
train acc:  0.8046875
train loss:  0.3761255443096161
train gradient:  0.24482547450494502
iteration : 6941
train acc:  0.8125
train loss:  0.3519655466079712
train gradient:  0.19534585586119654
iteration : 6942
train acc:  0.8203125
train loss:  0.44009682536125183
train gradient:  0.27577256413738094
iteration : 6943
train acc:  0.8671875
train loss:  0.38644009828567505
train gradient:  0.2782801621256274
iteration : 6944
train acc:  0.8828125
train loss:  0.32522937655448914
train gradient:  0.15940329684561927
iteration : 6945
train acc:  0.859375
train loss:  0.42426908016204834
train gradient:  0.3863620482264393
iteration : 6946
train acc:  0.8671875
train loss:  0.31760308146476746
train gradient:  0.16445200182978645
iteration : 6947
train acc:  0.8203125
train loss:  0.352322518825531
train gradient:  0.17646777867123514
iteration : 6948
train acc:  0.8671875
train loss:  0.31358036398887634
train gradient:  0.21789922998054403
iteration : 6949
train acc:  0.890625
train loss:  0.3011779487133026
train gradient:  0.21345960044405204
iteration : 6950
train acc:  0.8828125
train loss:  0.25623440742492676
train gradient:  0.24668554583720326
iteration : 6951
train acc:  0.8515625
train loss:  0.34075337648391724
train gradient:  0.14564595529248942
iteration : 6952
train acc:  0.8125
train loss:  0.4023837745189667
train gradient:  0.2633204393416687
iteration : 6953
train acc:  0.875
train loss:  0.31339606642723083
train gradient:  0.22563709041798005
iteration : 6954
train acc:  0.859375
train loss:  0.3020108938217163
train gradient:  0.21146919435041645
iteration : 6955
train acc:  0.859375
train loss:  0.3297891616821289
train gradient:  0.1683356671474834
iteration : 6956
train acc:  0.7578125
train loss:  0.40145745873451233
train gradient:  0.21470221502640705
iteration : 6957
train acc:  0.8515625
train loss:  0.29992592334747314
train gradient:  0.15408667863628503
iteration : 6958
train acc:  0.859375
train loss:  0.27798059582710266
train gradient:  0.19692281922755028
iteration : 6959
train acc:  0.8671875
train loss:  0.33413106203079224
train gradient:  0.19556780859064204
iteration : 6960
train acc:  0.859375
train loss:  0.31290340423583984
train gradient:  0.26147300200488066
iteration : 6961
train acc:  0.8046875
train loss:  0.3833049237728119
train gradient:  0.27528206637591224
iteration : 6962
train acc:  0.8515625
train loss:  0.31832778453826904
train gradient:  0.21878749367480316
iteration : 6963
train acc:  0.8671875
train loss:  0.33984094858169556
train gradient:  0.2899744400346451
iteration : 6964
train acc:  0.84375
train loss:  0.3544544279575348
train gradient:  0.17623831849579027
iteration : 6965
train acc:  0.8359375
train loss:  0.33876997232437134
train gradient:  0.21902459924177126
iteration : 6966
train acc:  0.8984375
train loss:  0.2713157832622528
train gradient:  0.10717413466582415
iteration : 6967
train acc:  0.84375
train loss:  0.30583006143569946
train gradient:  0.18201025876993615
iteration : 6968
train acc:  0.84375
train loss:  0.316514253616333
train gradient:  0.1486132524970701
iteration : 6969
train acc:  0.875
train loss:  0.27660953998565674
train gradient:  0.18094043147494665
iteration : 6970
train acc:  0.8515625
train loss:  0.32695358991622925
train gradient:  0.15618683478178647
iteration : 6971
train acc:  0.7890625
train loss:  0.4946098029613495
train gradient:  0.43704011908174506
iteration : 6972
train acc:  0.8359375
train loss:  0.3536292314529419
train gradient:  0.1901012347769435
iteration : 6973
train acc:  0.859375
train loss:  0.3243212401866913
train gradient:  0.1696624163947152
iteration : 6974
train acc:  0.875
train loss:  0.32540178298950195
train gradient:  0.23083138453565838
iteration : 6975
train acc:  0.8359375
train loss:  0.3821863830089569
train gradient:  0.17985957918172096
iteration : 6976
train acc:  0.8125
train loss:  0.3832533359527588
train gradient:  0.26850871267060006
iteration : 6977
train acc:  0.8828125
train loss:  0.2999219596385956
train gradient:  0.15031090249971574
iteration : 6978
train acc:  0.8203125
train loss:  0.37479692697525024
train gradient:  0.2900748454945734
iteration : 6979
train acc:  0.90625
train loss:  0.25702667236328125
train gradient:  0.1502146399148354
iteration : 6980
train acc:  0.90625
train loss:  0.23858433961868286
train gradient:  0.14446683141362687
iteration : 6981
train acc:  0.84375
train loss:  0.40245240926742554
train gradient:  0.24290990110024968
iteration : 6982
train acc:  0.8359375
train loss:  0.34417369961738586
train gradient:  0.22266673936917863
iteration : 6983
train acc:  0.859375
train loss:  0.2974787950515747
train gradient:  0.19473756063462794
iteration : 6984
train acc:  0.84375
train loss:  0.3097325563430786
train gradient:  0.18031491137341898
iteration : 6985
train acc:  0.8359375
train loss:  0.4022219777107239
train gradient:  0.2512138835064106
iteration : 6986
train acc:  0.8671875
train loss:  0.2950516641139984
train gradient:  0.16441292001110996
iteration : 6987
train acc:  0.875
train loss:  0.32769328355789185
train gradient:  0.1589871295925325
iteration : 6988
train acc:  0.84375
train loss:  0.3959653973579407
train gradient:  0.23937655215238884
iteration : 6989
train acc:  0.8828125
train loss:  0.32049769163131714
train gradient:  0.12403583920498883
iteration : 6990
train acc:  0.828125
train loss:  0.3906211256980896
train gradient:  0.2759522337138979
iteration : 6991
train acc:  0.8671875
train loss:  0.35133087635040283
train gradient:  0.33911394965292446
iteration : 6992
train acc:  0.84375
train loss:  0.31087827682495117
train gradient:  0.18724165637832454
iteration : 6993
train acc:  0.8671875
train loss:  0.33742785453796387
train gradient:  0.1763142173374655
iteration : 6994
train acc:  0.875
train loss:  0.3115977644920349
train gradient:  0.12936405282210237
iteration : 6995
train acc:  0.890625
train loss:  0.25901031494140625
train gradient:  0.15390607436536413
iteration : 6996
train acc:  0.828125
train loss:  0.36257797479629517
train gradient:  0.25421011526961357
iteration : 6997
train acc:  0.8515625
train loss:  0.35006994009017944
train gradient:  0.228408144537632
iteration : 6998
train acc:  0.890625
train loss:  0.28574037551879883
train gradient:  0.14181363347549636
iteration : 6999
train acc:  0.8984375
train loss:  0.260657399892807
train gradient:  0.11818873957846711
iteration : 7000
train acc:  0.890625
train loss:  0.31222379207611084
train gradient:  0.20595518735535265
iteration : 7001
train acc:  0.8984375
train loss:  0.3166045844554901
train gradient:  0.1798918573589619
iteration : 7002
train acc:  0.78125
train loss:  0.4714122414588928
train gradient:  0.33102037792214517
iteration : 7003
train acc:  0.8359375
train loss:  0.3305808901786804
train gradient:  0.22870214852510162
iteration : 7004
train acc:  0.9140625
train loss:  0.25282371044158936
train gradient:  0.1649858463237588
iteration : 7005
train acc:  0.8125
train loss:  0.38228368759155273
train gradient:  0.17275070844980528
iteration : 7006
train acc:  0.875
train loss:  0.306974321603775
train gradient:  0.20834144509199748
iteration : 7007
train acc:  0.875
train loss:  0.36569517850875854
train gradient:  0.21810827011541867
iteration : 7008
train acc:  0.8203125
train loss:  0.37303054332733154
train gradient:  0.2410673971592256
iteration : 7009
train acc:  0.8515625
train loss:  0.3214968740940094
train gradient:  0.15232767788055174
iteration : 7010
train acc:  0.8671875
train loss:  0.32060620188713074
train gradient:  0.1589701921173089
iteration : 7011
train acc:  0.875
train loss:  0.3139103651046753
train gradient:  0.1815846999711558
iteration : 7012
train acc:  0.796875
train loss:  0.3734692931175232
train gradient:  0.23875055379642318
iteration : 7013
train acc:  0.859375
train loss:  0.3722045421600342
train gradient:  0.2051938476641097
iteration : 7014
train acc:  0.8671875
train loss:  0.2983364462852478
train gradient:  0.15027659089517348
iteration : 7015
train acc:  0.8359375
train loss:  0.4086511433124542
train gradient:  0.24080417777847896
iteration : 7016
train acc:  0.859375
train loss:  0.34628745913505554
train gradient:  0.16546243573240388
iteration : 7017
train acc:  0.8984375
train loss:  0.24141472578048706
train gradient:  0.11034012026679624
iteration : 7018
train acc:  0.90625
train loss:  0.3176628351211548
train gradient:  0.21990817110997662
iteration : 7019
train acc:  0.8203125
train loss:  0.38227367401123047
train gradient:  0.265850246228842
iteration : 7020
train acc:  0.828125
train loss:  0.39193594455718994
train gradient:  0.17674819466469185
iteration : 7021
train acc:  0.8125
train loss:  0.39429059624671936
train gradient:  0.21359729116227352
iteration : 7022
train acc:  0.828125
train loss:  0.35698071122169495
train gradient:  0.19056499549493777
iteration : 7023
train acc:  0.7890625
train loss:  0.3699740767478943
train gradient:  0.22871515383265323
iteration : 7024
train acc:  0.8671875
train loss:  0.2917380928993225
train gradient:  0.15256610588740882
iteration : 7025
train acc:  0.84375
train loss:  0.3077077865600586
train gradient:  0.2003466147143821
iteration : 7026
train acc:  0.875
train loss:  0.27958911657333374
train gradient:  0.15998501428390033
iteration : 7027
train acc:  0.7890625
train loss:  0.418972909450531
train gradient:  0.21532383856981183
iteration : 7028
train acc:  0.921875
train loss:  0.2503249943256378
train gradient:  0.12156380430461368
iteration : 7029
train acc:  0.859375
train loss:  0.3612201511859894
train gradient:  0.20151461579932986
iteration : 7030
train acc:  0.90625
train loss:  0.2599020004272461
train gradient:  0.12311435734511907
iteration : 7031
train acc:  0.921875
train loss:  0.27699655294418335
train gradient:  0.11582114617957978
iteration : 7032
train acc:  0.828125
train loss:  0.4283917546272278
train gradient:  0.35469716053255235
iteration : 7033
train acc:  0.8671875
train loss:  0.2712298631668091
train gradient:  0.14402212817113014
iteration : 7034
train acc:  0.875
train loss:  0.3032325506210327
train gradient:  0.17634352212517168
iteration : 7035
train acc:  0.8515625
train loss:  0.33507513999938965
train gradient:  0.19274417495277751
iteration : 7036
train acc:  0.8203125
train loss:  0.3580780327320099
train gradient:  0.27506625049685535
iteration : 7037
train acc:  0.8515625
train loss:  0.3856073021888733
train gradient:  0.19074623368838634
iteration : 7038
train acc:  0.8046875
train loss:  0.3493676781654358
train gradient:  0.2463885506948808
iteration : 7039
train acc:  0.8203125
train loss:  0.32679522037506104
train gradient:  0.16377333100042876
iteration : 7040
train acc:  0.859375
train loss:  0.32785364985466003
train gradient:  0.1521467835008073
iteration : 7041
train acc:  0.84375
train loss:  0.3365105390548706
train gradient:  0.2237572984754111
iteration : 7042
train acc:  0.828125
train loss:  0.3888993263244629
train gradient:  0.2602375702134275
iteration : 7043
train acc:  0.890625
train loss:  0.30390501022338867
train gradient:  0.28391729971588914
iteration : 7044
train acc:  0.8203125
train loss:  0.3175782561302185
train gradient:  0.17851007538560942
iteration : 7045
train acc:  0.9296875
train loss:  0.2589282691478729
train gradient:  0.11339190670107707
iteration : 7046
train acc:  0.8203125
train loss:  0.3802483081817627
train gradient:  0.2068600614738533
iteration : 7047
train acc:  0.8359375
train loss:  0.3526095449924469
train gradient:  0.23337361462670203
iteration : 7048
train acc:  0.8515625
train loss:  0.3264205753803253
train gradient:  0.20022338180781385
iteration : 7049
train acc:  0.8671875
train loss:  0.32731908559799194
train gradient:  0.24951444784025928
iteration : 7050
train acc:  0.859375
train loss:  0.286465048789978
train gradient:  0.1734817023214409
iteration : 7051
train acc:  0.8359375
train loss:  0.40921521186828613
train gradient:  0.25831692753075347
iteration : 7052
train acc:  0.8671875
train loss:  0.3460380434989929
train gradient:  0.12354919584693101
iteration : 7053
train acc:  0.90625
train loss:  0.25603199005126953
train gradient:  0.11695396735479362
iteration : 7054
train acc:  0.84375
train loss:  0.3806282877922058
train gradient:  0.28081081628256466
iteration : 7055
train acc:  0.84375
train loss:  0.3976525068283081
train gradient:  0.26578597307029583
iteration : 7056
train acc:  0.8671875
train loss:  0.27209362387657166
train gradient:  0.1894977173589828
iteration : 7057
train acc:  0.8671875
train loss:  0.3694100081920624
train gradient:  0.22640753560532392
iteration : 7058
train acc:  0.828125
train loss:  0.3383147418498993
train gradient:  0.2057476722682503
iteration : 7059
train acc:  0.875
train loss:  0.2915644645690918
train gradient:  0.16796408833518495
iteration : 7060
train acc:  0.8515625
train loss:  0.32241302728652954
train gradient:  0.21944889080678723
iteration : 7061
train acc:  0.84375
train loss:  0.34596171975135803
train gradient:  0.20720177577453514
iteration : 7062
train acc:  0.859375
train loss:  0.3635724186897278
train gradient:  0.2342171991536109
iteration : 7063
train acc:  0.875
train loss:  0.26602524518966675
train gradient:  0.15616109476360668
iteration : 7064
train acc:  0.859375
train loss:  0.3313087224960327
train gradient:  0.1932055620460879
iteration : 7065
train acc:  0.8125
train loss:  0.43010854721069336
train gradient:  0.35386056285302714
iteration : 7066
train acc:  0.84375
train loss:  0.3609328866004944
train gradient:  0.19832160954268285
iteration : 7067
train acc:  0.84375
train loss:  0.3363098204135895
train gradient:  0.1576711078161706
iteration : 7068
train acc:  0.859375
train loss:  0.33480188250541687
train gradient:  0.2286651536019411
iteration : 7069
train acc:  0.8984375
train loss:  0.2670498490333557
train gradient:  0.19969734230448982
iteration : 7070
train acc:  0.859375
train loss:  0.35336601734161377
train gradient:  0.1589550854881632
iteration : 7071
train acc:  0.859375
train loss:  0.33877983689308167
train gradient:  0.22155301546019873
iteration : 7072
train acc:  0.7578125
train loss:  0.48527538776397705
train gradient:  0.3583061180440442
iteration : 7073
train acc:  0.90625
train loss:  0.29613548517227173
train gradient:  0.1747994960069439
iteration : 7074
train acc:  0.84375
train loss:  0.30448347330093384
train gradient:  0.15584818190143757
iteration : 7075
train acc:  0.890625
train loss:  0.3103538155555725
train gradient:  0.27177761844727816
iteration : 7076
train acc:  0.8828125
train loss:  0.30252817273139954
train gradient:  0.19567243977354631
iteration : 7077
train acc:  0.859375
train loss:  0.35508859157562256
train gradient:  0.20840399324648512
iteration : 7078
train acc:  0.875
train loss:  0.33698299527168274
train gradient:  0.2105867715423468
iteration : 7079
train acc:  0.84375
train loss:  0.4028029441833496
train gradient:  0.346911718981325
iteration : 7080
train acc:  0.875
train loss:  0.23912584781646729
train gradient:  0.13378542087154585
iteration : 7081
train acc:  0.875
train loss:  0.2974713146686554
train gradient:  0.15222313843351443
iteration : 7082
train acc:  0.8359375
train loss:  0.37704920768737793
train gradient:  0.19059943151949754
iteration : 7083
train acc:  0.8359375
train loss:  0.3016851246356964
train gradient:  0.15321913502290196
iteration : 7084
train acc:  0.84375
train loss:  0.36954259872436523
train gradient:  0.16993667770996368
iteration : 7085
train acc:  0.8515625
train loss:  0.30375561118125916
train gradient:  0.1399932033283388
iteration : 7086
train acc:  0.875
train loss:  0.32642829418182373
train gradient:  0.17338272151137837
iteration : 7087
train acc:  0.8515625
train loss:  0.36381176114082336
train gradient:  0.20401417864935092
iteration : 7088
train acc:  0.8515625
train loss:  0.35980796813964844
train gradient:  0.2788035422467291
iteration : 7089
train acc:  0.8046875
train loss:  0.4677591621875763
train gradient:  0.321983458840075
iteration : 7090
train acc:  0.859375
train loss:  0.3363429605960846
train gradient:  0.20336681410835059
iteration : 7091
train acc:  0.875
train loss:  0.3080989122390747
train gradient:  0.12136031620016359
iteration : 7092
train acc:  0.7890625
train loss:  0.4057575464248657
train gradient:  0.2954466447131553
iteration : 7093
train acc:  0.859375
train loss:  0.3587268590927124
train gradient:  0.1886831424819319
iteration : 7094
train acc:  0.859375
train loss:  0.29161694645881653
train gradient:  0.17798372241579857
iteration : 7095
train acc:  0.8828125
train loss:  0.3164949417114258
train gradient:  0.1605425581468058
iteration : 7096
train acc:  0.828125
train loss:  0.4266842007637024
train gradient:  0.2630838516918275
iteration : 7097
train acc:  0.8671875
train loss:  0.3051154613494873
train gradient:  0.13306256072169953
iteration : 7098
train acc:  0.8203125
train loss:  0.3615986406803131
train gradient:  0.1749751507296596
iteration : 7099
train acc:  0.8125
train loss:  0.39272645115852356
train gradient:  0.29469640493847804
iteration : 7100
train acc:  0.84375
train loss:  0.3418681025505066
train gradient:  0.16356972015119764
iteration : 7101
train acc:  0.8671875
train loss:  0.2943936586380005
train gradient:  0.12107068418341545
iteration : 7102
train acc:  0.8515625
train loss:  0.32801318168640137
train gradient:  0.1768170862032826
iteration : 7103
train acc:  0.8828125
train loss:  0.2856599688529968
train gradient:  0.13609387463449557
iteration : 7104
train acc:  0.859375
train loss:  0.33735010027885437
train gradient:  0.18712236988277417
iteration : 7105
train acc:  0.8359375
train loss:  0.38670939207077026
train gradient:  0.21311769985677012
iteration : 7106
train acc:  0.8359375
train loss:  0.3552666902542114
train gradient:  0.21925250018811965
iteration : 7107
train acc:  0.875
train loss:  0.3476784825325012
train gradient:  0.21885065328139058
iteration : 7108
train acc:  0.921875
train loss:  0.26736873388290405
train gradient:  0.21617154843164615
iteration : 7109
train acc:  0.8984375
train loss:  0.2954198122024536
train gradient:  0.10273394234164064
iteration : 7110
train acc:  0.8046875
train loss:  0.4236167073249817
train gradient:  0.20762877908520144
iteration : 7111
train acc:  0.859375
train loss:  0.3370969891548157
train gradient:  0.14469246730750676
iteration : 7112
train acc:  0.8125
train loss:  0.49995172023773193
train gradient:  0.632356599770675
iteration : 7113
train acc:  0.8046875
train loss:  0.4249853193759918
train gradient:  0.29446176598920376
iteration : 7114
train acc:  0.796875
train loss:  0.40704405307769775
train gradient:  0.24132407479774867
iteration : 7115
train acc:  0.8984375
train loss:  0.2679122984409332
train gradient:  0.10995986244967637
iteration : 7116
train acc:  0.8828125
train loss:  0.3277371823787689
train gradient:  0.1647532618507383
iteration : 7117
train acc:  0.8203125
train loss:  0.37483301758766174
train gradient:  0.20623075922256504
iteration : 7118
train acc:  0.8203125
train loss:  0.3832330107688904
train gradient:  0.1983036448326428
iteration : 7119
train acc:  0.8046875
train loss:  0.42222845554351807
train gradient:  0.26305623434529396
iteration : 7120
train acc:  0.890625
train loss:  0.28767669200897217
train gradient:  0.15887479948267452
iteration : 7121
train acc:  0.890625
train loss:  0.3058817684650421
train gradient:  0.20007592831654875
iteration : 7122
train acc:  0.8515625
train loss:  0.31779229640960693
train gradient:  0.13588469004821604
iteration : 7123
train acc:  0.8046875
train loss:  0.3594866991043091
train gradient:  0.3886508813713633
iteration : 7124
train acc:  0.8984375
train loss:  0.27225956320762634
train gradient:  0.13843126917535506
iteration : 7125
train acc:  0.8828125
train loss:  0.2956428527832031
train gradient:  0.1685644330202482
iteration : 7126
train acc:  0.8359375
train loss:  0.3400225043296814
train gradient:  0.18973638405960153
iteration : 7127
train acc:  0.8984375
train loss:  0.31602567434310913
train gradient:  0.2164966808615471
iteration : 7128
train acc:  0.84375
train loss:  0.3580092191696167
train gradient:  0.17660706329121906
iteration : 7129
train acc:  0.90625
train loss:  0.23438391089439392
train gradient:  0.12103592556676518
iteration : 7130
train acc:  0.8203125
train loss:  0.35354083776474
train gradient:  0.21037496843134962
iteration : 7131
train acc:  0.8359375
train loss:  0.37697941064834595
train gradient:  0.21026124023144122
iteration : 7132
train acc:  0.8359375
train loss:  0.323479562997818
train gradient:  0.15062806084471267
iteration : 7133
train acc:  0.890625
train loss:  0.3064015805721283
train gradient:  0.1972873819453016
iteration : 7134
train acc:  0.78125
train loss:  0.3903543949127197
train gradient:  0.24810316894412204
iteration : 7135
train acc:  0.8828125
train loss:  0.331104576587677
train gradient:  0.23633226922686873
iteration : 7136
train acc:  0.8359375
train loss:  0.40365004539489746
train gradient:  0.26196814045136785
iteration : 7137
train acc:  0.8984375
train loss:  0.2889670133590698
train gradient:  0.1412691539283638
iteration : 7138
train acc:  0.90625
train loss:  0.2855530381202698
train gradient:  0.13838572788588294
iteration : 7139
train acc:  0.8203125
train loss:  0.3859304189682007
train gradient:  0.193926620551647
iteration : 7140
train acc:  0.8046875
train loss:  0.3784424662590027
train gradient:  0.24872510045336596
iteration : 7141
train acc:  0.90625
train loss:  0.2832280099391937
train gradient:  0.1659992285014758
iteration : 7142
train acc:  0.875
train loss:  0.3581768274307251
train gradient:  0.20706228507771876
iteration : 7143
train acc:  0.8203125
train loss:  0.3461952209472656
train gradient:  0.17164734639115764
iteration : 7144
train acc:  0.859375
train loss:  0.3017458915710449
train gradient:  0.1802023864727971
iteration : 7145
train acc:  0.78125
train loss:  0.4421018362045288
train gradient:  0.3571434024516993
iteration : 7146
train acc:  0.8671875
train loss:  0.2905813455581665
train gradient:  0.16114324519775675
iteration : 7147
train acc:  0.8984375
train loss:  0.30446016788482666
train gradient:  0.15810621054222537
iteration : 7148
train acc:  0.796875
train loss:  0.4034028649330139
train gradient:  0.26618394131117235
iteration : 7149
train acc:  0.8359375
train loss:  0.3455243706703186
train gradient:  0.1621828890475741
iteration : 7150
train acc:  0.875
train loss:  0.30178308486938477
train gradient:  0.19085097601734902
iteration : 7151
train acc:  0.828125
train loss:  0.3530205488204956
train gradient:  0.19206838705668383
iteration : 7152
train acc:  0.828125
train loss:  0.3609679937362671
train gradient:  0.2182400851083733
iteration : 7153
train acc:  0.875
train loss:  0.3012944459915161
train gradient:  0.17373185404889704
iteration : 7154
train acc:  0.8671875
train loss:  0.274191677570343
train gradient:  0.11052449863987951
iteration : 7155
train acc:  0.8046875
train loss:  0.37434709072113037
train gradient:  0.31927445847048486
iteration : 7156
train acc:  0.828125
train loss:  0.35760611295700073
train gradient:  0.23527230581774194
iteration : 7157
train acc:  0.8125
train loss:  0.41873103380203247
train gradient:  0.2970856933638876
iteration : 7158
train acc:  0.828125
train loss:  0.4067912697792053
train gradient:  0.2114356339667669
iteration : 7159
train acc:  0.8515625
train loss:  0.41856223344802856
train gradient:  0.26523917873137515
iteration : 7160
train acc:  0.84375
train loss:  0.3450474739074707
train gradient:  0.31418020771353056
iteration : 7161
train acc:  0.8046875
train loss:  0.3832566738128662
train gradient:  0.24388787755534208
iteration : 7162
train acc:  0.8828125
train loss:  0.30049818754196167
train gradient:  0.12978054365255956
iteration : 7163
train acc:  0.8359375
train loss:  0.3566725552082062
train gradient:  0.20477322131102366
iteration : 7164
train acc:  0.8203125
train loss:  0.3547092378139496
train gradient:  0.18109705017038663
iteration : 7165
train acc:  0.875
train loss:  0.2808382511138916
train gradient:  0.1554067441317482
iteration : 7166
train acc:  0.8515625
train loss:  0.36634135246276855
train gradient:  0.22198456648252918
iteration : 7167
train acc:  0.859375
train loss:  0.39369702339172363
train gradient:  0.26140722723384896
iteration : 7168
train acc:  0.8359375
train loss:  0.4092785120010376
train gradient:  0.25915011900748763
iteration : 7169
train acc:  0.8359375
train loss:  0.3740743398666382
train gradient:  0.1972934354094847
iteration : 7170
train acc:  0.890625
train loss:  0.33432066440582275
train gradient:  0.14761442595609792
iteration : 7171
train acc:  0.8671875
train loss:  0.27737605571746826
train gradient:  0.16184893858980953
iteration : 7172
train acc:  0.8515625
train loss:  0.37865227460861206
train gradient:  0.21915158441399418
iteration : 7173
train acc:  0.8359375
train loss:  0.370583713054657
train gradient:  0.18234094154069463
iteration : 7174
train acc:  0.8359375
train loss:  0.3615839183330536
train gradient:  0.18966093471613565
iteration : 7175
train acc:  0.90625
train loss:  0.2674604058265686
train gradient:  0.1317455844140183
iteration : 7176
train acc:  0.859375
train loss:  0.3120381534099579
train gradient:  0.16502654904641
iteration : 7177
train acc:  0.8828125
train loss:  0.3113177418708801
train gradient:  0.2181471066475816
iteration : 7178
train acc:  0.84375
train loss:  0.3504297733306885
train gradient:  0.16189061742630584
iteration : 7179
train acc:  0.8515625
train loss:  0.3374807834625244
train gradient:  0.2128946488699467
iteration : 7180
train acc:  0.90625
train loss:  0.2730247676372528
train gradient:  0.11886454286468237
iteration : 7181
train acc:  0.8046875
train loss:  0.40680819749832153
train gradient:  0.25889064183456556
iteration : 7182
train acc:  0.8125
train loss:  0.4336756467819214
train gradient:  0.27041807644828103
iteration : 7183
train acc:  0.8671875
train loss:  0.3727390170097351
train gradient:  0.24427441720148652
iteration : 7184
train acc:  0.8125
train loss:  0.36450764536857605
train gradient:  0.3220654466888014
iteration : 7185
train acc:  0.8203125
train loss:  0.39543086290359497
train gradient:  0.24250456622663086
iteration : 7186
train acc:  0.8359375
train loss:  0.36420679092407227
train gradient:  0.24066271650022528
iteration : 7187
train acc:  0.84375
train loss:  0.33449113368988037
train gradient:  0.16031539129484912
iteration : 7188
train acc:  0.8671875
train loss:  0.31567174196243286
train gradient:  0.12473418398071077
iteration : 7189
train acc:  0.8359375
train loss:  0.4142998158931732
train gradient:  0.21890664570043528
iteration : 7190
train acc:  0.828125
train loss:  0.36874091625213623
train gradient:  0.18382567182415438
iteration : 7191
train acc:  0.8046875
train loss:  0.42117437720298767
train gradient:  0.24433571149241473
iteration : 7192
train acc:  0.8828125
train loss:  0.2720527648925781
train gradient:  0.1231247846205892
iteration : 7193
train acc:  0.8359375
train loss:  0.390677809715271
train gradient:  0.19603528569818718
iteration : 7194
train acc:  0.875
train loss:  0.32898131012916565
train gradient:  0.1440740390274005
iteration : 7195
train acc:  0.875
train loss:  0.3121338188648224
train gradient:  0.20022706362143494
iteration : 7196
train acc:  0.8671875
train loss:  0.30836933851242065
train gradient:  0.15596979315270257
iteration : 7197
train acc:  0.859375
train loss:  0.3248400092124939
train gradient:  0.128180025174029
iteration : 7198
train acc:  0.8046875
train loss:  0.3829447329044342
train gradient:  0.28043046430981067
iteration : 7199
train acc:  0.828125
train loss:  0.3725118041038513
train gradient:  0.1537244193264239
iteration : 7200
train acc:  0.859375
train loss:  0.3227281868457794
train gradient:  0.13625813777071125
iteration : 7201
train acc:  0.875
train loss:  0.3297274708747864
train gradient:  0.1721329916549414
iteration : 7202
train acc:  0.8828125
train loss:  0.3120764493942261
train gradient:  0.1356682078344542
iteration : 7203
train acc:  0.8359375
train loss:  0.38438716530799866
train gradient:  0.2524023219157797
iteration : 7204
train acc:  0.9140625
train loss:  0.2543729543685913
train gradient:  0.09353044180781017
iteration : 7205
train acc:  0.8515625
train loss:  0.3735769987106323
train gradient:  0.3014310318341952
iteration : 7206
train acc:  0.8828125
train loss:  0.3228217363357544
train gradient:  0.1387685468257471
iteration : 7207
train acc:  0.8203125
train loss:  0.3484705686569214
train gradient:  0.26081065151240096
iteration : 7208
train acc:  0.8671875
train loss:  0.3000532388687134
train gradient:  0.15842437226412714
iteration : 7209
train acc:  0.828125
train loss:  0.32862138748168945
train gradient:  0.19385187308896634
iteration : 7210
train acc:  0.890625
train loss:  0.2522941827774048
train gradient:  0.08452917597610844
iteration : 7211
train acc:  0.8046875
train loss:  0.41066890954971313
train gradient:  0.2509350422244141
iteration : 7212
train acc:  0.8359375
train loss:  0.3484848439693451
train gradient:  0.20855878866025607
iteration : 7213
train acc:  0.8984375
train loss:  0.3058309257030487
train gradient:  0.21139906967664515
iteration : 7214
train acc:  0.8359375
train loss:  0.3500749170780182
train gradient:  0.18325677934924195
iteration : 7215
train acc:  0.859375
train loss:  0.30086541175842285
train gradient:  0.12233441597643874
iteration : 7216
train acc:  0.78125
train loss:  0.40014180541038513
train gradient:  0.1950829972378713
iteration : 7217
train acc:  0.8203125
train loss:  0.38645267486572266
train gradient:  0.5251824607052026
iteration : 7218
train acc:  0.8671875
train loss:  0.3255511522293091
train gradient:  0.18443439255298127
iteration : 7219
train acc:  0.8203125
train loss:  0.36805474758148193
train gradient:  0.239536226158733
iteration : 7220
train acc:  0.875
train loss:  0.3101174831390381
train gradient:  0.11937011823836521
iteration : 7221
train acc:  0.8515625
train loss:  0.3636190593242645
train gradient:  0.21883024094300102
iteration : 7222
train acc:  0.859375
train loss:  0.34405142068862915
train gradient:  0.26099927676891344
iteration : 7223
train acc:  0.859375
train loss:  0.32947593927383423
train gradient:  0.20137766268966933
iteration : 7224
train acc:  0.828125
train loss:  0.3868125081062317
train gradient:  0.18210125705601038
iteration : 7225
train acc:  0.828125
train loss:  0.3286202549934387
train gradient:  0.18053255420133624
iteration : 7226
train acc:  0.8515625
train loss:  0.3248170018196106
train gradient:  0.18675406445944254
iteration : 7227
train acc:  0.828125
train loss:  0.359517902135849
train gradient:  0.19957125598723402
iteration : 7228
train acc:  0.84375
train loss:  0.39411360025405884
train gradient:  0.3070000367204665
iteration : 7229
train acc:  0.84375
train loss:  0.3768700361251831
train gradient:  0.26310707625460916
iteration : 7230
train acc:  0.8984375
train loss:  0.2826346457004547
train gradient:  0.19887956229453696
iteration : 7231
train acc:  0.875
train loss:  0.2948575019836426
train gradient:  0.163780876006076
iteration : 7232
train acc:  0.828125
train loss:  0.36656448245048523
train gradient:  0.23845757196828898
iteration : 7233
train acc:  0.828125
train loss:  0.4492107629776001
train gradient:  0.23992679776705853
iteration : 7234
train acc:  0.84375
train loss:  0.3507184386253357
train gradient:  0.17622151538056896
iteration : 7235
train acc:  0.796875
train loss:  0.4242725372314453
train gradient:  0.24930752195018754
iteration : 7236
train acc:  0.859375
train loss:  0.3209015727043152
train gradient:  0.1852552308561416
iteration : 7237
train acc:  0.875
train loss:  0.33058446645736694
train gradient:  0.19962893510287094
iteration : 7238
train acc:  0.796875
train loss:  0.36945411562919617
train gradient:  0.20864839540446783
iteration : 7239
train acc:  0.875
train loss:  0.2824636399745941
train gradient:  0.09250044429401069
iteration : 7240
train acc:  0.859375
train loss:  0.2937052845954895
train gradient:  0.18400353056006985
iteration : 7241
train acc:  0.8828125
train loss:  0.3189563751220703
train gradient:  0.1844192748082774
iteration : 7242
train acc:  0.8359375
train loss:  0.3299153447151184
train gradient:  0.20757747370099355
iteration : 7243
train acc:  0.875
train loss:  0.3155377507209778
train gradient:  0.15867708638156536
iteration : 7244
train acc:  0.765625
train loss:  0.39845019578933716
train gradient:  0.277012858017851
iteration : 7245
train acc:  0.84375
train loss:  0.35328707098960876
train gradient:  0.2190916674385057
iteration : 7246
train acc:  0.90625
train loss:  0.2594677805900574
train gradient:  0.16162260938446052
iteration : 7247
train acc:  0.8984375
train loss:  0.2522975206375122
train gradient:  0.16622959257168154
iteration : 7248
train acc:  0.8359375
train loss:  0.3340117037296295
train gradient:  0.22115470012774502
iteration : 7249
train acc:  0.84375
train loss:  0.36246612668037415
train gradient:  0.20438623360575203
iteration : 7250
train acc:  0.8125
train loss:  0.4147149920463562
train gradient:  0.3463124196266107
iteration : 7251
train acc:  0.8359375
train loss:  0.3670770227909088
train gradient:  0.19065748159802876
iteration : 7252
train acc:  0.84375
train loss:  0.36138367652893066
train gradient:  0.20677506929196807
iteration : 7253
train acc:  0.84375
train loss:  0.33816879987716675
train gradient:  0.1792364821147976
iteration : 7254
train acc:  0.84375
train loss:  0.39205190539360046
train gradient:  0.2741761468390854
iteration : 7255
train acc:  0.875
train loss:  0.30092543363571167
train gradient:  0.20804744462641256
iteration : 7256
train acc:  0.8203125
train loss:  0.41585206985473633
train gradient:  0.3706480469638976
iteration : 7257
train acc:  0.8359375
train loss:  0.3205798864364624
train gradient:  0.2156142151747646
iteration : 7258
train acc:  0.8203125
train loss:  0.4352990984916687
train gradient:  0.22460985214504695
iteration : 7259
train acc:  0.8828125
train loss:  0.2511138319969177
train gradient:  0.10407389866428043
iteration : 7260
train acc:  0.84375
train loss:  0.3029094338417053
train gradient:  0.18752071926619757
iteration : 7261
train acc:  0.828125
train loss:  0.3930378556251526
train gradient:  0.193583123247199
iteration : 7262
train acc:  0.875
train loss:  0.268202006816864
train gradient:  0.10624631239325989
iteration : 7263
train acc:  0.8515625
train loss:  0.33385562896728516
train gradient:  0.17169559919933003
iteration : 7264
train acc:  0.8046875
train loss:  0.32851898670196533
train gradient:  0.1757897102970548
iteration : 7265
train acc:  0.875
train loss:  0.333418607711792
train gradient:  0.2623465656366539
iteration : 7266
train acc:  0.875
train loss:  0.34892943501472473
train gradient:  0.16808899830673565
iteration : 7267
train acc:  0.8671875
train loss:  0.3663528859615326
train gradient:  0.287023232334919
iteration : 7268
train acc:  0.8671875
train loss:  0.40797197818756104
train gradient:  0.2478371301805694
iteration : 7269
train acc:  0.8671875
train loss:  0.3338976204395294
train gradient:  0.1872029799052904
iteration : 7270
train acc:  0.8671875
train loss:  0.3809865713119507
train gradient:  0.18392156048035135
iteration : 7271
train acc:  0.859375
train loss:  0.362467497587204
train gradient:  0.2345271359052971
iteration : 7272
train acc:  0.8671875
train loss:  0.3116568922996521
train gradient:  0.17861490939396368
iteration : 7273
train acc:  0.875
train loss:  0.3169163465499878
train gradient:  0.18274730766656885
iteration : 7274
train acc:  0.8359375
train loss:  0.3314861059188843
train gradient:  0.209078438615593
iteration : 7275
train acc:  0.8046875
train loss:  0.4464317560195923
train gradient:  0.31928126516561667
iteration : 7276
train acc:  0.8828125
train loss:  0.3021479845046997
train gradient:  0.12276296525660525
iteration : 7277
train acc:  0.8203125
train loss:  0.3930041790008545
train gradient:  0.26616333476603565
iteration : 7278
train acc:  0.8515625
train loss:  0.3747197985649109
train gradient:  0.23458940584154955
iteration : 7279
train acc:  0.796875
train loss:  0.35775041580200195
train gradient:  0.20604423237404168
iteration : 7280
train acc:  0.8515625
train loss:  0.34113526344299316
train gradient:  0.23643286568267624
iteration : 7281
train acc:  0.828125
train loss:  0.4498201012611389
train gradient:  0.2787738747261473
iteration : 7282
train acc:  0.8828125
train loss:  0.3010174632072449
train gradient:  0.19149813385154385
iteration : 7283
train acc:  0.828125
train loss:  0.34735316038131714
train gradient:  0.20876242515823917
iteration : 7284
train acc:  0.859375
train loss:  0.34912484884262085
train gradient:  0.17798699497350368
iteration : 7285
train acc:  0.8515625
train loss:  0.3338543474674225
train gradient:  0.18349110377263708
iteration : 7286
train acc:  0.8671875
train loss:  0.3690537214279175
train gradient:  0.2240766211764893
iteration : 7287
train acc:  0.8828125
train loss:  0.31086796522140503
train gradient:  0.13516601650507254
iteration : 7288
train acc:  0.8515625
train loss:  0.35451316833496094
train gradient:  0.2677632642737449
iteration : 7289
train acc:  0.890625
train loss:  0.29845982789993286
train gradient:  0.12414605996277316
iteration : 7290
train acc:  0.8203125
train loss:  0.46072912216186523
train gradient:  0.29347076768712177
iteration : 7291
train acc:  0.875
train loss:  0.34356364607810974
train gradient:  0.24866392192657966
iteration : 7292
train acc:  0.9140625
train loss:  0.2356923669576645
train gradient:  0.1210690278970516
iteration : 7293
train acc:  0.84375
train loss:  0.30566591024398804
train gradient:  0.18626429198875954
iteration : 7294
train acc:  0.8359375
train loss:  0.35064369440078735
train gradient:  0.20506966113464065
iteration : 7295
train acc:  0.8515625
train loss:  0.37410032749176025
train gradient:  0.23081462028140307
iteration : 7296
train acc:  0.8359375
train loss:  0.35086238384246826
train gradient:  0.20560902746554693
iteration : 7297
train acc:  0.8984375
train loss:  0.3097056746482849
train gradient:  0.1957204699675724
iteration : 7298
train acc:  0.8671875
train loss:  0.32097500562667847
train gradient:  0.1747027332319865
iteration : 7299
train acc:  0.875
train loss:  0.32091060280799866
train gradient:  0.19388611327865596
iteration : 7300
train acc:  0.8203125
train loss:  0.3760164976119995
train gradient:  0.24687720606640529
iteration : 7301
train acc:  0.8359375
train loss:  0.384233295917511
train gradient:  0.2153295164583212
iteration : 7302
train acc:  0.9140625
train loss:  0.2784550189971924
train gradient:  0.1921929440525693
iteration : 7303
train acc:  0.796875
train loss:  0.3838008642196655
train gradient:  0.22544507800602856
iteration : 7304
train acc:  0.8203125
train loss:  0.3527596592903137
train gradient:  0.13887389092191735
iteration : 7305
train acc:  0.8984375
train loss:  0.3347724676132202
train gradient:  0.26373581434026133
iteration : 7306
train acc:  0.859375
train loss:  0.36289507150650024
train gradient:  0.22767807430138903
iteration : 7307
train acc:  0.8828125
train loss:  0.28851035237312317
train gradient:  0.20222274031386417
iteration : 7308
train acc:  0.8359375
train loss:  0.34864115715026855
train gradient:  0.15221367016325044
iteration : 7309
train acc:  0.828125
train loss:  0.35508331656455994
train gradient:  0.2340746759640392
iteration : 7310
train acc:  0.859375
train loss:  0.3145292401313782
train gradient:  0.22556893692711313
iteration : 7311
train acc:  0.859375
train loss:  0.3490349054336548
train gradient:  0.2753340456379907
iteration : 7312
train acc:  0.8984375
train loss:  0.2819628417491913
train gradient:  0.1059211143565562
iteration : 7313
train acc:  0.859375
train loss:  0.29466766119003296
train gradient:  0.15075655326780021
iteration : 7314
train acc:  0.90625
train loss:  0.25439250469207764
train gradient:  0.09036954344584723
iteration : 7315
train acc:  0.8515625
train loss:  0.3880441188812256
train gradient:  0.2427395253903623
iteration : 7316
train acc:  0.828125
train loss:  0.3390064537525177
train gradient:  0.1621840609283327
iteration : 7317
train acc:  0.8515625
train loss:  0.3727962374687195
train gradient:  0.24067874080409368
iteration : 7318
train acc:  0.8359375
train loss:  0.33479341864585876
train gradient:  0.19392524122212104
iteration : 7319
train acc:  0.8359375
train loss:  0.3315434455871582
train gradient:  0.23236582526356714
iteration : 7320
train acc:  0.90625
train loss:  0.269051730632782
train gradient:  0.15034770987593477
iteration : 7321
train acc:  0.8671875
train loss:  0.3399706482887268
train gradient:  0.28095815768142046
iteration : 7322
train acc:  0.7890625
train loss:  0.3889741897583008
train gradient:  0.2823501419562682
iteration : 7323
train acc:  0.828125
train loss:  0.38064539432525635
train gradient:  0.20149412387009252
iteration : 7324
train acc:  0.8515625
train loss:  0.3715693950653076
train gradient:  0.2563148849489575
iteration : 7325
train acc:  0.8515625
train loss:  0.3327580690383911
train gradient:  0.18606276232159885
iteration : 7326
train acc:  0.859375
train loss:  0.3666622042655945
train gradient:  0.31377242294489116
iteration : 7327
train acc:  0.8828125
train loss:  0.2777990698814392
train gradient:  0.11256796278621826
iteration : 7328
train acc:  0.859375
train loss:  0.3373141586780548
train gradient:  0.1438767545945439
iteration : 7329
train acc:  0.8515625
train loss:  0.32956722378730774
train gradient:  0.1810988951600812
iteration : 7330
train acc:  0.828125
train loss:  0.3191922903060913
train gradient:  0.19560443384690454
iteration : 7331
train acc:  0.828125
train loss:  0.3581393361091614
train gradient:  0.21674665133315943
iteration : 7332
train acc:  0.859375
train loss:  0.2774476706981659
train gradient:  0.1494212641588262
iteration : 7333
train acc:  0.859375
train loss:  0.31638824939727783
train gradient:  0.3451320360230827
iteration : 7334
train acc:  0.8359375
train loss:  0.34164464473724365
train gradient:  0.19104586431617718
iteration : 7335
train acc:  0.8359375
train loss:  0.3818349242210388
train gradient:  0.25230063233585065
iteration : 7336
train acc:  0.890625
train loss:  0.254599392414093
train gradient:  0.20088765624272786
iteration : 7337
train acc:  0.875
train loss:  0.30468088388442993
train gradient:  0.12200824830350415
iteration : 7338
train acc:  0.8671875
train loss:  0.2859136760234833
train gradient:  0.10597680464439907
iteration : 7339
train acc:  0.8125
train loss:  0.3748231530189514
train gradient:  0.28961909934253977
iteration : 7340
train acc:  0.8203125
train loss:  0.3781552314758301
train gradient:  0.28056813339147957
iteration : 7341
train acc:  0.90625
train loss:  0.2635703980922699
train gradient:  0.13284479987017844
iteration : 7342
train acc:  0.84375
train loss:  0.34410303831100464
train gradient:  0.17575653849877787
iteration : 7343
train acc:  0.84375
train loss:  0.32627806067466736
train gradient:  0.18958551369932972
iteration : 7344
train acc:  0.8515625
train loss:  0.3236495852470398
train gradient:  0.19326182134004632
iteration : 7345
train acc:  0.8515625
train loss:  0.35419562458992004
train gradient:  0.20189714809234377
iteration : 7346
train acc:  0.875
train loss:  0.27102893590927124
train gradient:  0.13020902003278784
iteration : 7347
train acc:  0.7734375
train loss:  0.4484080374240875
train gradient:  0.2469194226405601
iteration : 7348
train acc:  0.890625
train loss:  0.2788007855415344
train gradient:  0.1633065942528636
iteration : 7349
train acc:  0.8359375
train loss:  0.3714976906776428
train gradient:  0.1789869344540587
iteration : 7350
train acc:  0.8203125
train loss:  0.3435289263725281
train gradient:  0.2251001875322467
iteration : 7351
train acc:  0.890625
train loss:  0.27711400389671326
train gradient:  0.12883394526718306
iteration : 7352
train acc:  0.8515625
train loss:  0.37058359384536743
train gradient:  0.2722371529756853
iteration : 7353
train acc:  0.8046875
train loss:  0.36561742424964905
train gradient:  0.21382385183289748
iteration : 7354
train acc:  0.8515625
train loss:  0.3361774682998657
train gradient:  0.19041472437115253
iteration : 7355
train acc:  0.8828125
train loss:  0.273232102394104
train gradient:  0.14707268049660033
iteration : 7356
train acc:  0.8359375
train loss:  0.3735036551952362
train gradient:  0.30173052673426926
iteration : 7357
train acc:  0.796875
train loss:  0.4363971948623657
train gradient:  0.24834731957519043
iteration : 7358
train acc:  0.8046875
train loss:  0.4314696788787842
train gradient:  0.23523630975050386
iteration : 7359
train acc:  0.828125
train loss:  0.3799685835838318
train gradient:  0.3889343214938987
iteration : 7360
train acc:  0.84375
train loss:  0.35907068848609924
train gradient:  0.1818953667259709
iteration : 7361
train acc:  0.8515625
train loss:  0.3122747540473938
train gradient:  0.1706492654094896
iteration : 7362
train acc:  0.875
train loss:  0.3306238353252411
train gradient:  0.19835927898324973
iteration : 7363
train acc:  0.8359375
train loss:  0.35083654522895813
train gradient:  0.19037823967431944
iteration : 7364
train acc:  0.859375
train loss:  0.41170763969421387
train gradient:  0.42768836138119837
iteration : 7365
train acc:  0.8359375
train loss:  0.37824559211730957
train gradient:  0.22211093435417795
iteration : 7366
train acc:  0.828125
train loss:  0.401177316904068
train gradient:  0.26608490039920185
iteration : 7367
train acc:  0.796875
train loss:  0.40232932567596436
train gradient:  0.2075010550905984
iteration : 7368
train acc:  0.84375
train loss:  0.3793233036994934
train gradient:  0.2269335146801944
iteration : 7369
train acc:  0.8984375
train loss:  0.3037908375263214
train gradient:  0.13994941991910087
iteration : 7370
train acc:  0.8515625
train loss:  0.33055028319358826
train gradient:  0.15155124902292938
iteration : 7371
train acc:  0.8125
train loss:  0.3690984845161438
train gradient:  0.48477527458496295
iteration : 7372
train acc:  0.78125
train loss:  0.4261835515499115
train gradient:  0.9459157666550202
iteration : 7373
train acc:  0.875
train loss:  0.3356073498725891
train gradient:  0.17334429809526772
iteration : 7374
train acc:  0.8828125
train loss:  0.2847995162010193
train gradient:  0.14021131726822972
iteration : 7375
train acc:  0.8515625
train loss:  0.31199297308921814
train gradient:  0.1440652395059553
iteration : 7376
train acc:  0.859375
train loss:  0.32206493616104126
train gradient:  0.2698177657841503
iteration : 7377
train acc:  0.8046875
train loss:  0.3841157555580139
train gradient:  0.24518411621054398
iteration : 7378
train acc:  0.8203125
train loss:  0.37651827931404114
train gradient:  0.19128536957924025
iteration : 7379
train acc:  0.8359375
train loss:  0.3307073712348938
train gradient:  0.20551111201816463
iteration : 7380
train acc:  0.8359375
train loss:  0.34950774908065796
train gradient:  0.1620441139244689
iteration : 7381
train acc:  0.8515625
train loss:  0.3119184672832489
train gradient:  0.20675966958037825
iteration : 7382
train acc:  0.8203125
train loss:  0.39803600311279297
train gradient:  0.22747655908364317
iteration : 7383
train acc:  0.859375
train loss:  0.35809624195098877
train gradient:  0.2347241492349797
iteration : 7384
train acc:  0.828125
train loss:  0.36233532428741455
train gradient:  0.259031026171017
iteration : 7385
train acc:  0.859375
train loss:  0.30488866567611694
train gradient:  0.1176487127096214
iteration : 7386
train acc:  0.859375
train loss:  0.34704825282096863
train gradient:  0.18060064509661533
iteration : 7387
train acc:  0.84375
train loss:  0.4102190136909485
train gradient:  0.24328355194923923
iteration : 7388
train acc:  0.8984375
train loss:  0.2986212372779846
train gradient:  0.29938761028265937
iteration : 7389
train acc:  0.84375
train loss:  0.3230423033237457
train gradient:  0.15460004188281617
iteration : 7390
train acc:  0.8515625
train loss:  0.2858523726463318
train gradient:  0.12867193917803085
iteration : 7391
train acc:  0.8203125
train loss:  0.38033798336982727
train gradient:  0.20203663549634635
iteration : 7392
train acc:  0.859375
train loss:  0.3544687330722809
train gradient:  0.1765197667772497
iteration : 7393
train acc:  0.8671875
train loss:  0.35417622327804565
train gradient:  0.20004305314408996
iteration : 7394
train acc:  0.8125
train loss:  0.36068016290664673
train gradient:  0.21690030626950396
iteration : 7395
train acc:  0.8359375
train loss:  0.36027783155441284
train gradient:  0.16153270697953548
iteration : 7396
train acc:  0.8203125
train loss:  0.32383355498313904
train gradient:  0.16673689015235102
iteration : 7397
train acc:  0.875
train loss:  0.3074759840965271
train gradient:  0.16489831659268162
iteration : 7398
train acc:  0.828125
train loss:  0.4098226726055145
train gradient:  0.23395851747846558
iteration : 7399
train acc:  0.875
train loss:  0.38329896330833435
train gradient:  0.15796552449226192
iteration : 7400
train acc:  0.875
train loss:  0.33464255928993225
train gradient:  0.14164835018313293
iteration : 7401
train acc:  0.8515625
train loss:  0.31513461470603943
train gradient:  0.19092479310221036
iteration : 7402
train acc:  0.8515625
train loss:  0.3448330760002136
train gradient:  0.17521840445050918
iteration : 7403
train acc:  0.84375
train loss:  0.323684424161911
train gradient:  0.15011970412905223
iteration : 7404
train acc:  0.8828125
train loss:  0.3264688551425934
train gradient:  0.21260768526435242
iteration : 7405
train acc:  0.875
train loss:  0.28487539291381836
train gradient:  0.17556909204822185
iteration : 7406
train acc:  0.875
train loss:  0.28629159927368164
train gradient:  0.1507295991413955
iteration : 7407
train acc:  0.84375
train loss:  0.30195990204811096
train gradient:  0.1452475485335894
iteration : 7408
train acc:  0.828125
train loss:  0.38113486766815186
train gradient:  0.2107011276675885
iteration : 7409
train acc:  0.859375
train loss:  0.32474541664123535
train gradient:  0.16452181578868813
iteration : 7410
train acc:  0.796875
train loss:  0.4387553334236145
train gradient:  0.28491504620342845
iteration : 7411
train acc:  0.8515625
train loss:  0.321983277797699
train gradient:  0.16230673772545662
iteration : 7412
train acc:  0.859375
train loss:  0.31413155794143677
train gradient:  0.14865370180608653
iteration : 7413
train acc:  0.8203125
train loss:  0.369668185710907
train gradient:  0.2195742678112759
iteration : 7414
train acc:  0.875
train loss:  0.3523727059364319
train gradient:  0.22572177027010248
iteration : 7415
train acc:  0.8359375
train loss:  0.36847737431526184
train gradient:  0.23007771962674356
iteration : 7416
train acc:  0.8515625
train loss:  0.3341625928878784
train gradient:  0.1586794059510286
iteration : 7417
train acc:  0.8515625
train loss:  0.4178791046142578
train gradient:  0.2087076693682277
iteration : 7418
train acc:  0.8515625
train loss:  0.3312515914440155
train gradient:  0.16843599821589805
iteration : 7419
train acc:  0.8515625
train loss:  0.30485326051712036
train gradient:  0.25001835831405284
iteration : 7420
train acc:  0.859375
train loss:  0.35850226879119873
train gradient:  0.1991141537370693
iteration : 7421
train acc:  0.8984375
train loss:  0.25929898023605347
train gradient:  0.1314508652432536
iteration : 7422
train acc:  0.8515625
train loss:  0.35218745470046997
train gradient:  0.16555271514252676
iteration : 7423
train acc:  0.8046875
train loss:  0.3751779794692993
train gradient:  0.17356220573785602
iteration : 7424
train acc:  0.8671875
train loss:  0.3449406027793884
train gradient:  0.3343470055737538
iteration : 7425
train acc:  0.8515625
train loss:  0.2845648527145386
train gradient:  0.20756466358712752
iteration : 7426
train acc:  0.8671875
train loss:  0.341189444065094
train gradient:  0.21714746500677995
iteration : 7427
train acc:  0.859375
train loss:  0.2982368767261505
train gradient:  0.14016305224163456
iteration : 7428
train acc:  0.8828125
train loss:  0.366479754447937
train gradient:  0.29533865852168667
iteration : 7429
train acc:  0.84375
train loss:  0.4422610402107239
train gradient:  0.33207946847239006
iteration : 7430
train acc:  0.859375
train loss:  0.33109501004219055
train gradient:  0.34708086764880136
iteration : 7431
train acc:  0.859375
train loss:  0.35493791103363037
train gradient:  0.17740322573806086
iteration : 7432
train acc:  0.8828125
train loss:  0.30010342597961426
train gradient:  0.1156565193496843
iteration : 7433
train acc:  0.875
train loss:  0.326018750667572
train gradient:  0.19220056034409277
iteration : 7434
train acc:  0.8125
train loss:  0.38403981924057007
train gradient:  0.18128431099977865
iteration : 7435
train acc:  0.78125
train loss:  0.38828954100608826
train gradient:  0.19533361925248804
iteration : 7436
train acc:  0.875
train loss:  0.3212139308452606
train gradient:  0.1617314584796916
iteration : 7437
train acc:  0.8515625
train loss:  0.30168431997299194
train gradient:  0.16352931663223785
iteration : 7438
train acc:  0.875
train loss:  0.321547269821167
train gradient:  0.1641067032216106
iteration : 7439
train acc:  0.875
train loss:  0.3039082884788513
train gradient:  0.2226170572723588
iteration : 7440
train acc:  0.8359375
train loss:  0.4199669063091278
train gradient:  0.25866633951124574
iteration : 7441
train acc:  0.765625
train loss:  0.4265340268611908
train gradient:  0.2822241840759767
iteration : 7442
train acc:  0.796875
train loss:  0.4094083309173584
train gradient:  0.2692456930709452
iteration : 7443
train acc:  0.859375
train loss:  0.35839372873306274
train gradient:  0.18895259316341048
iteration : 7444
train acc:  0.84375
train loss:  0.39746177196502686
train gradient:  0.2426945662078755
iteration : 7445
train acc:  0.8359375
train loss:  0.38652828335762024
train gradient:  0.19653243930268965
iteration : 7446
train acc:  0.8671875
train loss:  0.31682756543159485
train gradient:  0.19510434866591952
iteration : 7447
train acc:  0.8359375
train loss:  0.28959470987319946
train gradient:  0.12503248971777847
iteration : 7448
train acc:  0.8515625
train loss:  0.34115707874298096
train gradient:  0.18322559204499572
iteration : 7449
train acc:  0.875
train loss:  0.3105277121067047
train gradient:  0.2030556040157086
iteration : 7450
train acc:  0.8515625
train loss:  0.39566922187805176
train gradient:  0.2682691297834606
iteration : 7451
train acc:  0.8671875
train loss:  0.288303941488266
train gradient:  0.1269206860036292
iteration : 7452
train acc:  0.8203125
train loss:  0.3733946681022644
train gradient:  0.2122505030206135
iteration : 7453
train acc:  0.828125
train loss:  0.35823750495910645
train gradient:  0.16549459425254498
iteration : 7454
train acc:  0.859375
train loss:  0.3091357946395874
train gradient:  0.18870646349014447
iteration : 7455
train acc:  0.84375
train loss:  0.35108691453933716
train gradient:  0.1681244187042883
iteration : 7456
train acc:  0.859375
train loss:  0.3466537296772003
train gradient:  0.1699707585207162
iteration : 7457
train acc:  0.859375
train loss:  0.33760637044906616
train gradient:  0.13915308888753103
iteration : 7458
train acc:  0.84375
train loss:  0.3489399552345276
train gradient:  0.1969564945553068
iteration : 7459
train acc:  0.8046875
train loss:  0.3952332139015198
train gradient:  0.25967899124211935
iteration : 7460
train acc:  0.8984375
train loss:  0.24810975790023804
train gradient:  0.19229432150734846
iteration : 7461
train acc:  0.890625
train loss:  0.3097255527973175
train gradient:  0.1594138619927775
iteration : 7462
train acc:  0.8203125
train loss:  0.38627076148986816
train gradient:  0.24094834897755324
iteration : 7463
train acc:  0.8203125
train loss:  0.3639160394668579
train gradient:  0.18963032595867219
iteration : 7464
train acc:  0.78125
train loss:  0.4204135537147522
train gradient:  0.3121206162701839
iteration : 7465
train acc:  0.8984375
train loss:  0.30080875754356384
train gradient:  0.15642763590494746
iteration : 7466
train acc:  0.8125
train loss:  0.3986067771911621
train gradient:  0.2888694176556215
iteration : 7467
train acc:  0.84375
train loss:  0.31092673540115356
train gradient:  0.15208259829767767
iteration : 7468
train acc:  0.90625
train loss:  0.32413893938064575
train gradient:  0.1525972228041575
iteration : 7469
train acc:  0.875
train loss:  0.2948898673057556
train gradient:  0.1880789704376618
iteration : 7470
train acc:  0.8203125
train loss:  0.328402042388916
train gradient:  0.17515000780836576
iteration : 7471
train acc:  0.8125
train loss:  0.33922454714775085
train gradient:  0.1702347882457323
iteration : 7472
train acc:  0.8125
train loss:  0.3990063965320587
train gradient:  0.32597739000162596
iteration : 7473
train acc:  0.84375
train loss:  0.34245574474334717
train gradient:  0.17799043466011275
iteration : 7474
train acc:  0.8671875
train loss:  0.3208726942539215
train gradient:  0.17148832480627801
iteration : 7475
train acc:  0.875
train loss:  0.35375484824180603
train gradient:  0.18383642380706106
iteration : 7476
train acc:  0.8359375
train loss:  0.35298317670822144
train gradient:  0.3283033833524067
iteration : 7477
train acc:  0.8359375
train loss:  0.3830854296684265
train gradient:  0.212797850556865
iteration : 7478
train acc:  0.84375
train loss:  0.40740224719047546
train gradient:  0.28153129104836677
iteration : 7479
train acc:  0.828125
train loss:  0.40556561946868896
train gradient:  0.20648275896625456
iteration : 7480
train acc:  0.921875
train loss:  0.24222609400749207
train gradient:  0.1254426525763948
iteration : 7481
train acc:  0.859375
train loss:  0.2802686095237732
train gradient:  0.14605734151555322
iteration : 7482
train acc:  0.8359375
train loss:  0.3425106704235077
train gradient:  0.14355505619888392
iteration : 7483
train acc:  0.8515625
train loss:  0.3342480957508087
train gradient:  0.16543586270210914
iteration : 7484
train acc:  0.8828125
train loss:  0.33471959829330444
train gradient:  0.1360636817462583
iteration : 7485
train acc:  0.8828125
train loss:  0.3097647726535797
train gradient:  0.17932202375674683
iteration : 7486
train acc:  0.875
train loss:  0.2963074743747711
train gradient:  0.13676602390110293
iteration : 7487
train acc:  0.890625
train loss:  0.2681722640991211
train gradient:  0.12373759061222085
iteration : 7488
train acc:  0.875
train loss:  0.29754817485809326
train gradient:  0.12309987953428772
iteration : 7489
train acc:  0.859375
train loss:  0.3091573715209961
train gradient:  0.14963544909239077
iteration : 7490
train acc:  0.859375
train loss:  0.33210819959640503
train gradient:  0.17945368474590662
iteration : 7491
train acc:  0.8359375
train loss:  0.3781771659851074
train gradient:  0.21560092257966332
iteration : 7492
train acc:  0.8046875
train loss:  0.4097066819667816
train gradient:  0.21620865057528738
iteration : 7493
train acc:  0.8984375
train loss:  0.31383371353149414
train gradient:  0.30668928156053976
iteration : 7494
train acc:  0.84375
train loss:  0.31684964895248413
train gradient:  0.1795661388042808
iteration : 7495
train acc:  0.765625
train loss:  0.4513108730316162
train gradient:  0.22922488820698064
iteration : 7496
train acc:  0.8203125
train loss:  0.35838401317596436
train gradient:  0.22931805950019346
iteration : 7497
train acc:  0.8359375
train loss:  0.37746661901474
train gradient:  0.21611598591896528
iteration : 7498
train acc:  0.84375
train loss:  0.38591450452804565
train gradient:  0.21746816030220095
iteration : 7499
train acc:  0.8359375
train loss:  0.361507773399353
train gradient:  0.20647805135035446
iteration : 7500
train acc:  0.8828125
train loss:  0.2808755040168762
train gradient:  0.13944852915555303
iteration : 7501
train acc:  0.84375
train loss:  0.32665932178497314
train gradient:  0.1517937903637071
iteration : 7502
train acc:  0.8671875
train loss:  0.3097746968269348
train gradient:  0.11295312709617338
iteration : 7503
train acc:  0.90625
train loss:  0.22671160101890564
train gradient:  0.08766317448217698
iteration : 7504
train acc:  0.8515625
train loss:  0.37366360425949097
train gradient:  0.20448245973662427
iteration : 7505
train acc:  0.8515625
train loss:  0.36148446798324585
train gradient:  0.30142516085984966
iteration : 7506
train acc:  0.8671875
train loss:  0.2891809940338135
train gradient:  0.1591088705482485
iteration : 7507
train acc:  0.8515625
train loss:  0.3102129101753235
train gradient:  0.1303105482433311
iteration : 7508
train acc:  0.890625
train loss:  0.28727105259895325
train gradient:  0.15684479970984394
iteration : 7509
train acc:  0.84375
train loss:  0.3140774071216583
train gradient:  0.22035049243972232
iteration : 7510
train acc:  0.859375
train loss:  0.2890549302101135
train gradient:  0.2993759252326194
iteration : 7511
train acc:  0.828125
train loss:  0.39695751667022705
train gradient:  0.1973610113693427
iteration : 7512
train acc:  0.8671875
train loss:  0.3530905842781067
train gradient:  0.21792838881464427
iteration : 7513
train acc:  0.8828125
train loss:  0.3203508257865906
train gradient:  0.16155914824643322
iteration : 7514
train acc:  0.8984375
train loss:  0.2923694849014282
train gradient:  0.16177321672846992
iteration : 7515
train acc:  0.890625
train loss:  0.2660340666770935
train gradient:  0.13330403877711805
iteration : 7516
train acc:  0.8828125
train loss:  0.2699005603790283
train gradient:  0.13654855825079698
iteration : 7517
train acc:  0.8203125
train loss:  0.34036698937416077
train gradient:  0.1863935444369954
iteration : 7518
train acc:  0.875
train loss:  0.33402055501937866
train gradient:  0.14407598108544006
iteration : 7519
train acc:  0.84375
train loss:  0.3117210865020752
train gradient:  0.17314697716655966
iteration : 7520
train acc:  0.8828125
train loss:  0.2826170325279236
train gradient:  0.1337956318833933
iteration : 7521
train acc:  0.8671875
train loss:  0.39662009477615356
train gradient:  0.21720938561590364
iteration : 7522
train acc:  0.828125
train loss:  0.36566904187202454
train gradient:  0.2210023565658098
iteration : 7523
train acc:  0.8359375
train loss:  0.38151389360427856
train gradient:  0.2091484387011655
iteration : 7524
train acc:  0.890625
train loss:  0.2754228413105011
train gradient:  0.12188896200283121
iteration : 7525
train acc:  0.859375
train loss:  0.29765352606773376
train gradient:  0.17201721732590472
iteration : 7526
train acc:  0.8984375
train loss:  0.2924867272377014
train gradient:  0.14936607487696094
iteration : 7527
train acc:  0.8671875
train loss:  0.3048977553844452
train gradient:  0.13668713463710255
iteration : 7528
train acc:  0.8984375
train loss:  0.2978382408618927
train gradient:  0.15911959245093638
iteration : 7529
train acc:  0.8515625
train loss:  0.30938050150871277
train gradient:  0.15672122480946138
iteration : 7530
train acc:  0.84375
train loss:  0.30502474308013916
train gradient:  0.1838319088755775
iteration : 7531
train acc:  0.9140625
train loss:  0.24886244535446167
train gradient:  0.10762007510735518
iteration : 7532
train acc:  0.8359375
train loss:  0.38762861490249634
train gradient:  0.22337732930128135
iteration : 7533
train acc:  0.8515625
train loss:  0.3830205798149109
train gradient:  0.17289710269497313
iteration : 7534
train acc:  0.8203125
train loss:  0.335265189409256
train gradient:  0.2853681762221162
iteration : 7535
train acc:  0.8515625
train loss:  0.4306408762931824
train gradient:  0.352199515594666
iteration : 7536
train acc:  0.8046875
train loss:  0.44591474533081055
train gradient:  0.30180206758453637
iteration : 7537
train acc:  0.8828125
train loss:  0.2732973098754883
train gradient:  0.1700864035477232
iteration : 7538
train acc:  0.8359375
train loss:  0.3265599012374878
train gradient:  0.18291251812105583
iteration : 7539
train acc:  0.890625
train loss:  0.2986726760864258
train gradient:  0.2435239439339732
iteration : 7540
train acc:  0.890625
train loss:  0.32096028327941895
train gradient:  0.1435632515419898
iteration : 7541
train acc:  0.8515625
train loss:  0.3359540104866028
train gradient:  0.2364271406738756
iteration : 7542
train acc:  0.859375
train loss:  0.2942142188549042
train gradient:  0.15550048808193298
iteration : 7543
train acc:  0.9140625
train loss:  0.31146374344825745
train gradient:  0.11353996014606532
iteration : 7544
train acc:  0.8046875
train loss:  0.41548508405685425
train gradient:  0.24793208657382557
iteration : 7545
train acc:  0.8203125
train loss:  0.437502920627594
train gradient:  0.3760908645784377
iteration : 7546
train acc:  0.7890625
train loss:  0.3767513036727905
train gradient:  0.26362486729143864
iteration : 7547
train acc:  0.84375
train loss:  0.3426976799964905
train gradient:  0.19644784441043156
iteration : 7548
train acc:  0.8671875
train loss:  0.3926011919975281
train gradient:  0.2878765885066194
iteration : 7549
train acc:  0.8203125
train loss:  0.37826985120773315
train gradient:  0.24612417484711085
iteration : 7550
train acc:  0.9140625
train loss:  0.23020422458648682
train gradient:  0.10058787961506205
iteration : 7551
train acc:  0.8984375
train loss:  0.25346171855926514
train gradient:  0.10978022086487049
iteration : 7552
train acc:  0.875
train loss:  0.3248046040534973
train gradient:  0.2007843874502816
iteration : 7553
train acc:  0.859375
train loss:  0.40600162744522095
train gradient:  0.22106847231570728
iteration : 7554
train acc:  0.84375
train loss:  0.3542759120464325
train gradient:  0.19477311344233905
iteration : 7555
train acc:  0.8671875
train loss:  0.33357855677604675
train gradient:  0.1744879025839794
iteration : 7556
train acc:  0.8203125
train loss:  0.4201144576072693
train gradient:  0.27050913736523424
iteration : 7557
train acc:  0.828125
train loss:  0.36858925223350525
train gradient:  0.1995654789901118
iteration : 7558
train acc:  0.8125
train loss:  0.4017479419708252
train gradient:  0.37735685977466726
iteration : 7559
train acc:  0.8359375
train loss:  0.34280088543891907
train gradient:  0.24183989238480846
iteration : 7560
train acc:  0.8359375
train loss:  0.3784133493900299
train gradient:  0.2373684800421958
iteration : 7561
train acc:  0.890625
train loss:  0.3277015686035156
train gradient:  0.16339990932134718
iteration : 7562
train acc:  0.828125
train loss:  0.383108913898468
train gradient:  0.22010902378366592
iteration : 7563
train acc:  0.859375
train loss:  0.3142276406288147
train gradient:  0.15232737573874094
iteration : 7564
train acc:  0.8671875
train loss:  0.310253769159317
train gradient:  0.13286414665964696
iteration : 7565
train acc:  0.8046875
train loss:  0.41694799065589905
train gradient:  0.2400191989686337
iteration : 7566
train acc:  0.8125
train loss:  0.42965978384017944
train gradient:  0.2513137047850992
iteration : 7567
train acc:  0.84375
train loss:  0.36684972047805786
train gradient:  0.20481772664868045
iteration : 7568
train acc:  0.875
train loss:  0.29367077350616455
train gradient:  0.1229387223007359
iteration : 7569
train acc:  0.8515625
train loss:  0.3791600465774536
train gradient:  0.19978780404746732
iteration : 7570
train acc:  0.84375
train loss:  0.33812570571899414
train gradient:  0.1626737595564147
iteration : 7571
train acc:  0.8203125
train loss:  0.36456799507141113
train gradient:  0.21018864349267272
iteration : 7572
train acc:  0.8671875
train loss:  0.33845773339271545
train gradient:  0.2767691878212664
iteration : 7573
train acc:  0.78125
train loss:  0.449837327003479
train gradient:  0.37863446746545737
iteration : 7574
train acc:  0.8515625
train loss:  0.33759912848472595
train gradient:  0.16135744662335677
iteration : 7575
train acc:  0.875
train loss:  0.2980663776397705
train gradient:  0.1631111759861109
iteration : 7576
train acc:  0.859375
train loss:  0.3517940640449524
train gradient:  0.18333339265956433
iteration : 7577
train acc:  0.7734375
train loss:  0.4180530905723572
train gradient:  0.2085341611174612
iteration : 7578
train acc:  0.8203125
train loss:  0.34317484498023987
train gradient:  0.13135376869550505
iteration : 7579
train acc:  0.8671875
train loss:  0.3085256814956665
train gradient:  0.17281289585731116
iteration : 7580
train acc:  0.859375
train loss:  0.30714213848114014
train gradient:  0.19538728406081207
iteration : 7581
train acc:  0.84375
train loss:  0.3597627282142639
train gradient:  0.2155552208378602
iteration : 7582
train acc:  0.890625
train loss:  0.3310844898223877
train gradient:  0.24850641896596143
iteration : 7583
train acc:  0.8359375
train loss:  0.3700474202632904
train gradient:  0.23456467678869808
iteration : 7584
train acc:  0.828125
train loss:  0.33635997772216797
train gradient:  0.21783493142927002
iteration : 7585
train acc:  0.8125
train loss:  0.41150420904159546
train gradient:  0.22593291572321966
iteration : 7586
train acc:  0.8671875
train loss:  0.2858979403972626
train gradient:  0.15182352553713319
iteration : 7587
train acc:  0.8515625
train loss:  0.3060954809188843
train gradient:  0.15422418983581007
iteration : 7588
train acc:  0.859375
train loss:  0.3063368797302246
train gradient:  0.13631159012196042
iteration : 7589
train acc:  0.8359375
train loss:  0.3483155369758606
train gradient:  0.17211351980717876
iteration : 7590
train acc:  0.859375
train loss:  0.3619934320449829
train gradient:  0.25639284996300116
iteration : 7591
train acc:  0.84375
train loss:  0.36178362369537354
train gradient:  0.35079916060594213
iteration : 7592
train acc:  0.7890625
train loss:  0.3899129629135132
train gradient:  0.21083529835699005
iteration : 7593
train acc:  0.84375
train loss:  0.3284280002117157
train gradient:  0.16253821997577436
iteration : 7594
train acc:  0.8984375
train loss:  0.2890952229499817
train gradient:  0.15291370571323912
iteration : 7595
train acc:  0.8515625
train loss:  0.32742056250572205
train gradient:  0.1725772961202869
iteration : 7596
train acc:  0.8203125
train loss:  0.4105948805809021
train gradient:  0.2498139728117884
iteration : 7597
train acc:  0.8359375
train loss:  0.38879019021987915
train gradient:  0.24280210375440464
iteration : 7598
train acc:  0.8671875
train loss:  0.3130449652671814
train gradient:  0.131849101565063
iteration : 7599
train acc:  0.828125
train loss:  0.3762848973274231
train gradient:  0.24566001900237971
iteration : 7600
train acc:  0.859375
train loss:  0.32872575521469116
train gradient:  0.1713269671702684
iteration : 7601
train acc:  0.8359375
train loss:  0.29721158742904663
train gradient:  0.1243508919216271
iteration : 7602
train acc:  0.859375
train loss:  0.32589191198349
train gradient:  0.2165252156105229
iteration : 7603
train acc:  0.890625
train loss:  0.31132829189300537
train gradient:  0.13569147944281004
iteration : 7604
train acc:  0.8515625
train loss:  0.31809332966804504
train gradient:  0.1765994393238446
iteration : 7605
train acc:  0.859375
train loss:  0.33680588006973267
train gradient:  0.16453343816115926
iteration : 7606
train acc:  0.78125
train loss:  0.3912317454814911
train gradient:  0.3054717647685721
iteration : 7607
train acc:  0.8359375
train loss:  0.371819406747818
train gradient:  0.20215443503759517
iteration : 7608
train acc:  0.859375
train loss:  0.3388432264328003
train gradient:  0.2158531094916351
iteration : 7609
train acc:  0.84375
train loss:  0.4099183678627014
train gradient:  0.20303022060758608
iteration : 7610
train acc:  0.859375
train loss:  0.30760663747787476
train gradient:  0.18102682249023533
iteration : 7611
train acc:  0.8828125
train loss:  0.29257550835609436
train gradient:  0.16204936922868257
iteration : 7612
train acc:  0.8359375
train loss:  0.30547213554382324
train gradient:  0.11677436434800172
iteration : 7613
train acc:  0.8671875
train loss:  0.335151731967926
train gradient:  0.21555170970178394
iteration : 7614
train acc:  0.8671875
train loss:  0.2663801610469818
train gradient:  0.14754958138744428
iteration : 7615
train acc:  0.859375
train loss:  0.33178961277008057
train gradient:  0.22381497818370172
iteration : 7616
train acc:  0.859375
train loss:  0.29231029748916626
train gradient:  0.2495290735466199
iteration : 7617
train acc:  0.8203125
train loss:  0.39320892095565796
train gradient:  0.3074106551931874
iteration : 7618
train acc:  0.875
train loss:  0.27066487073898315
train gradient:  0.18974242916668982
iteration : 7619
train acc:  0.8671875
train loss:  0.2944420576095581
train gradient:  0.12911324864761797
iteration : 7620
train acc:  0.8359375
train loss:  0.3638620972633362
train gradient:  0.3369392210273783
iteration : 7621
train acc:  0.8828125
train loss:  0.3060431480407715
train gradient:  0.13415523344205005
iteration : 7622
train acc:  0.875
train loss:  0.3009014427661896
train gradient:  0.1260356061181847
iteration : 7623
train acc:  0.875
train loss:  0.32707133889198303
train gradient:  0.15439086918187556
iteration : 7624
train acc:  0.8828125
train loss:  0.24204030632972717
train gradient:  0.10627137033427712
iteration : 7625
train acc:  0.8671875
train loss:  0.3204641342163086
train gradient:  0.15563230161356406
iteration : 7626
train acc:  0.890625
train loss:  0.3071938753128052
train gradient:  0.18878503502673036
iteration : 7627
train acc:  0.875
train loss:  0.32326996326446533
train gradient:  0.1488982559327383
iteration : 7628
train acc:  0.8359375
train loss:  0.3318098783493042
train gradient:  0.21555051475616185
iteration : 7629
train acc:  0.859375
train loss:  0.3301367163658142
train gradient:  0.20841895130597787
iteration : 7630
train acc:  0.8359375
train loss:  0.3240913450717926
train gradient:  0.25111569633864317
iteration : 7631
train acc:  0.859375
train loss:  0.3378310799598694
train gradient:  0.18985411579404557
iteration : 7632
train acc:  0.859375
train loss:  0.31538891792297363
train gradient:  0.15828615411895974
iteration : 7633
train acc:  0.8359375
train loss:  0.3610191345214844
train gradient:  0.2503761176638509
iteration : 7634
train acc:  0.8125
train loss:  0.42806220054626465
train gradient:  0.28098384795214504
iteration : 7635
train acc:  0.828125
train loss:  0.4056556224822998
train gradient:  0.36450657418694016
iteration : 7636
train acc:  0.8515625
train loss:  0.3292035460472107
train gradient:  0.19343281665057782
iteration : 7637
train acc:  0.8984375
train loss:  0.2500994801521301
train gradient:  0.12550991897326327
iteration : 7638
train acc:  0.8515625
train loss:  0.30484941601753235
train gradient:  0.12607606070124322
iteration : 7639
train acc:  0.8515625
train loss:  0.3458635210990906
train gradient:  0.25836237846671234
iteration : 7640
train acc:  0.84375
train loss:  0.3412477374076843
train gradient:  0.24687220206137134
iteration : 7641
train acc:  0.859375
train loss:  0.3390631079673767
train gradient:  0.1861576293941552
iteration : 7642
train acc:  0.8671875
train loss:  0.33333319425582886
train gradient:  0.16428337632193912
iteration : 7643
train acc:  0.84375
train loss:  0.32664692401885986
train gradient:  0.15017275214397863
iteration : 7644
train acc:  0.859375
train loss:  0.31010305881500244
train gradient:  0.12836732361686654
iteration : 7645
train acc:  0.859375
train loss:  0.35508978366851807
train gradient:  0.2177486612427346
iteration : 7646
train acc:  0.859375
train loss:  0.3500942289829254
train gradient:  0.19238949711993153
iteration : 7647
train acc:  0.8671875
train loss:  0.32471099495887756
train gradient:  0.15845822784091942
iteration : 7648
train acc:  0.8125
train loss:  0.3905428647994995
train gradient:  0.291775602303126
iteration : 7649
train acc:  0.8125
train loss:  0.39166560769081116
train gradient:  0.2529588693824593
iteration : 7650
train acc:  0.8515625
train loss:  0.2844271957874298
train gradient:  0.17370054811147667
iteration : 7651
train acc:  0.8828125
train loss:  0.2723410725593567
train gradient:  0.13759907123179804
iteration : 7652
train acc:  0.890625
train loss:  0.2840103507041931
train gradient:  0.15564599627082784
iteration : 7653
train acc:  0.8671875
train loss:  0.29578545689582825
train gradient:  0.10970435498100067
iteration : 7654
train acc:  0.84375
train loss:  0.323890745639801
train gradient:  0.18322927834576522
iteration : 7655
train acc:  0.84375
train loss:  0.35152000188827515
train gradient:  0.18883085110150824
iteration : 7656
train acc:  0.84375
train loss:  0.2970239520072937
train gradient:  0.14583988817777677
iteration : 7657
train acc:  0.828125
train loss:  0.32741808891296387
train gradient:  0.24286504121740593
iteration : 7658
train acc:  0.84375
train loss:  0.3269820213317871
train gradient:  0.2647193732334488
iteration : 7659
train acc:  0.8359375
train loss:  0.3563064634799957
train gradient:  0.19791729760812193
iteration : 7660
train acc:  0.8671875
train loss:  0.2916968762874603
train gradient:  0.1993790433123343
iteration : 7661
train acc:  0.859375
train loss:  0.3323034942150116
train gradient:  0.23157246130446452
iteration : 7662
train acc:  0.921875
train loss:  0.22942861914634705
train gradient:  0.14235999821649759
iteration : 7663
train acc:  0.8359375
train loss:  0.31292855739593506
train gradient:  0.19432288529858155
iteration : 7664
train acc:  0.8203125
train loss:  0.3279137909412384
train gradient:  0.20384023570969612
iteration : 7665
train acc:  0.875
train loss:  0.3023797273635864
train gradient:  0.15155340212827872
iteration : 7666
train acc:  0.859375
train loss:  0.40280240774154663
train gradient:  0.24054435787475514
iteration : 7667
train acc:  0.84375
train loss:  0.3188781440258026
train gradient:  0.1691679627920349
iteration : 7668
train acc:  0.8671875
train loss:  0.2964620590209961
train gradient:  0.2041177903281516
iteration : 7669
train acc:  0.859375
train loss:  0.37349992990493774
train gradient:  0.21740468135941776
iteration : 7670
train acc:  0.7734375
train loss:  0.42855557799339294
train gradient:  0.2801878103247399
iteration : 7671
train acc:  0.8984375
train loss:  0.2376367747783661
train gradient:  0.18122862550092095
iteration : 7672
train acc:  0.828125
train loss:  0.34357476234436035
train gradient:  0.2766911865140717
iteration : 7673
train acc:  0.8203125
train loss:  0.3490685224533081
train gradient:  0.20363950356662452
iteration : 7674
train acc:  0.9296875
train loss:  0.25130695104599
train gradient:  0.15880429425146603
iteration : 7675
train acc:  0.8515625
train loss:  0.2691439986228943
train gradient:  0.1896335414237776
iteration : 7676
train acc:  0.8515625
train loss:  0.39441579580307007
train gradient:  0.25859807316256717
iteration : 7677
train acc:  0.84375
train loss:  0.35481226444244385
train gradient:  0.3071409238154346
iteration : 7678
train acc:  0.8359375
train loss:  0.41950905323028564
train gradient:  0.27397278943888903
iteration : 7679
train acc:  0.875
train loss:  0.28472834825515747
train gradient:  0.21109085216784546
iteration : 7680
train acc:  0.84375
train loss:  0.2998805046081543
train gradient:  0.2434162850836742
iteration : 7681
train acc:  0.8515625
train loss:  0.30021148920059204
train gradient:  0.18740013704636926
iteration : 7682
train acc:  0.8125
train loss:  0.37833648920059204
train gradient:  0.2995696802680369
iteration : 7683
train acc:  0.796875
train loss:  0.4202919602394104
train gradient:  0.3152624781800057
iteration : 7684
train acc:  0.8828125
train loss:  0.32671281695365906
train gradient:  0.2312725774704833
iteration : 7685
train acc:  0.890625
train loss:  0.26203519105911255
train gradient:  0.15848797315852692
iteration : 7686
train acc:  0.8359375
train loss:  0.3643765449523926
train gradient:  0.2601677185044722
iteration : 7687
train acc:  0.828125
train loss:  0.39015549421310425
train gradient:  0.40710979368631267
iteration : 7688
train acc:  0.859375
train loss:  0.34855639934539795
train gradient:  0.18478479884227764
iteration : 7689
train acc:  0.8359375
train loss:  0.3383556008338928
train gradient:  0.23176684717405505
iteration : 7690
train acc:  0.8828125
train loss:  0.2921411991119385
train gradient:  0.15898877768862746
iteration : 7691
train acc:  0.859375
train loss:  0.3035046458244324
train gradient:  0.23249139290639909
iteration : 7692
train acc:  0.84375
train loss:  0.3526741862297058
train gradient:  0.3853444560666015
iteration : 7693
train acc:  0.875
train loss:  0.2881278395652771
train gradient:  0.14604895936997608
iteration : 7694
train acc:  0.84375
train loss:  0.31081661581993103
train gradient:  0.27112925779528896
iteration : 7695
train acc:  0.875
train loss:  0.25367918610572815
train gradient:  0.12249990349721461
iteration : 7696
train acc:  0.8828125
train loss:  0.3089691698551178
train gradient:  0.2702191976778511
iteration : 7697
train acc:  0.859375
train loss:  0.3541255295276642
train gradient:  0.25436576989889575
iteration : 7698
train acc:  0.8125
train loss:  0.39533287286758423
train gradient:  0.34997831634702214
iteration : 7699
train acc:  0.796875
train loss:  0.4030671715736389
train gradient:  0.3816451760496221
iteration : 7700
train acc:  0.8359375
train loss:  0.3434186577796936
train gradient:  0.268038587889326
iteration : 7701
train acc:  0.8671875
train loss:  0.3268817663192749
train gradient:  0.19804739153590464
iteration : 7702
train acc:  0.8671875
train loss:  0.27666404843330383
train gradient:  0.23060419993405024
iteration : 7703
train acc:  0.90625
train loss:  0.250432550907135
train gradient:  0.17024828064576414
iteration : 7704
train acc:  0.8671875
train loss:  0.29291975498199463
train gradient:  0.1342415261301563
iteration : 7705
train acc:  0.859375
train loss:  0.3099479079246521
train gradient:  0.24191678032695668
iteration : 7706
train acc:  0.875
train loss:  0.2944280505180359
train gradient:  0.19900431713266012
iteration : 7707
train acc:  0.84375
train loss:  0.2850677967071533
train gradient:  0.1337824654657267
iteration : 7708
train acc:  0.8046875
train loss:  0.4015101194381714
train gradient:  0.3357559933031635
iteration : 7709
train acc:  0.828125
train loss:  0.43804410099983215
train gradient:  0.408879080652496
iteration : 7710
train acc:  0.875
train loss:  0.3360830545425415
train gradient:  0.23218255852150801
iteration : 7711
train acc:  0.8046875
train loss:  0.41371259093284607
train gradient:  0.24360889956783627
iteration : 7712
train acc:  0.84375
train loss:  0.351498007774353
train gradient:  0.198851398748975
iteration : 7713
train acc:  0.890625
train loss:  0.2821166515350342
train gradient:  0.15005945908045334
iteration : 7714
train acc:  0.8203125
train loss:  0.3290818929672241
train gradient:  0.18012906615771768
iteration : 7715
train acc:  0.8515625
train loss:  0.29442235827445984
train gradient:  0.15150682997297332
iteration : 7716
train acc:  0.8671875
train loss:  0.3338753581047058
train gradient:  0.2123674732604633
iteration : 7717
train acc:  0.90625
train loss:  0.22988305985927582
train gradient:  0.12155161783510156
iteration : 7718
train acc:  0.90625
train loss:  0.21476274728775024
train gradient:  0.19220622852104188
iteration : 7719
train acc:  0.8515625
train loss:  0.36813682317733765
train gradient:  0.2699639452806234
iteration : 7720
train acc:  0.8515625
train loss:  0.30420899391174316
train gradient:  0.25130212603724783
iteration : 7721
train acc:  0.8671875
train loss:  0.3739478886127472
train gradient:  0.18928953923640968
iteration : 7722
train acc:  0.859375
train loss:  0.29102057218551636
train gradient:  0.1723507241182286
iteration : 7723
train acc:  0.859375
train loss:  0.31739819049835205
train gradient:  0.20470322361146048
iteration : 7724
train acc:  0.859375
train loss:  0.30483266711235046
train gradient:  0.17995943786300533
iteration : 7725
train acc:  0.875
train loss:  0.28384870290756226
train gradient:  0.1704070504742467
iteration : 7726
train acc:  0.8515625
train loss:  0.3356887698173523
train gradient:  0.15365011190457945
iteration : 7727
train acc:  0.8515625
train loss:  0.3562633991241455
train gradient:  0.21916952434694037
iteration : 7728
train acc:  0.890625
train loss:  0.25886979699134827
train gradient:  0.16347415941973484
iteration : 7729
train acc:  0.8515625
train loss:  0.3561550974845886
train gradient:  0.2664361897351474
iteration : 7730
train acc:  0.875
train loss:  0.2643510699272156
train gradient:  0.15083378052251178
iteration : 7731
train acc:  0.875
train loss:  0.3446025848388672
train gradient:  0.27953278176707147
iteration : 7732
train acc:  0.8515625
train loss:  0.34930458664894104
train gradient:  0.20668573268972765
iteration : 7733
train acc:  0.8984375
train loss:  0.315295934677124
train gradient:  0.1569974252055199
iteration : 7734
train acc:  0.890625
train loss:  0.3994235694408417
train gradient:  0.5314958994844315
iteration : 7735
train acc:  0.8515625
train loss:  0.34692591428756714
train gradient:  0.25058137466571573
iteration : 7736
train acc:  0.8125
train loss:  0.3470720052719116
train gradient:  0.20878332002698335
iteration : 7737
train acc:  0.859375
train loss:  0.2946373224258423
train gradient:  0.13558625388399068
iteration : 7738
train acc:  0.8359375
train loss:  0.3301178514957428
train gradient:  0.2582136449783246
iteration : 7739
train acc:  0.828125
train loss:  0.39440587162971497
train gradient:  0.25939502608476517
iteration : 7740
train acc:  0.875
train loss:  0.32410794496536255
train gradient:  0.2316324284044471
iteration : 7741
train acc:  0.8125
train loss:  0.38607263565063477
train gradient:  0.25747075930402474
iteration : 7742
train acc:  0.8203125
train loss:  0.3509674072265625
train gradient:  0.19909574070236388
iteration : 7743
train acc:  0.8203125
train loss:  0.4591634273529053
train gradient:  0.36301642338182055
iteration : 7744
train acc:  0.8671875
train loss:  0.32304954528808594
train gradient:  0.19479483197766506
iteration : 7745
train acc:  0.8515625
train loss:  0.31680428981781006
train gradient:  0.13920005250470419
iteration : 7746
train acc:  0.84375
train loss:  0.2678478956222534
train gradient:  0.13984582669101686
iteration : 7747
train acc:  0.859375
train loss:  0.3386639356613159
train gradient:  0.19183283524807065
iteration : 7748
train acc:  0.859375
train loss:  0.3050331175327301
train gradient:  0.2386508923835293
iteration : 7749
train acc:  0.8828125
train loss:  0.25519734621047974
train gradient:  0.10480429377769292
iteration : 7750
train acc:  0.8828125
train loss:  0.23023384809494019
train gradient:  0.10902514245502486
iteration : 7751
train acc:  0.75
train loss:  0.49986734986305237
train gradient:  0.49223839207585723
iteration : 7752
train acc:  0.8828125
train loss:  0.31292837858200073
train gradient:  0.19512413540228557
iteration : 7753
train acc:  0.8671875
train loss:  0.3271447718143463
train gradient:  0.18847960000139438
iteration : 7754
train acc:  0.8984375
train loss:  0.3157821297645569
train gradient:  0.22076437361082796
iteration : 7755
train acc:  0.890625
train loss:  0.30947673320770264
train gradient:  0.4677316866384977
iteration : 7756
train acc:  0.84375
train loss:  0.34445875883102417
train gradient:  0.14770875456862104
iteration : 7757
train acc:  0.875
train loss:  0.2962305247783661
train gradient:  0.24076014364452844
iteration : 7758
train acc:  0.859375
train loss:  0.3501698970794678
train gradient:  0.22375120177513919
iteration : 7759
train acc:  0.921875
train loss:  0.2694889307022095
train gradient:  0.17185574955005237
iteration : 7760
train acc:  0.84375
train loss:  0.33725422620773315
train gradient:  0.17760257782314876
iteration : 7761
train acc:  0.8828125
train loss:  0.27072975039482117
train gradient:  0.15793847274306091
iteration : 7762
train acc:  0.8359375
train loss:  0.41189324855804443
train gradient:  0.2518084549207286
iteration : 7763
train acc:  0.7890625
train loss:  0.3981465697288513
train gradient:  0.2980662688022183
iteration : 7764
train acc:  0.7578125
train loss:  0.47550657391548157
train gradient:  0.2790736076577849
iteration : 7765
train acc:  0.8828125
train loss:  0.3316463828086853
train gradient:  0.18180388351136184
iteration : 7766
train acc:  0.890625
train loss:  0.29500794410705566
train gradient:  0.1639115291830384
iteration : 7767
train acc:  0.8359375
train loss:  0.3566429615020752
train gradient:  0.21628475547694914
iteration : 7768
train acc:  0.90625
train loss:  0.25565260648727417
train gradient:  0.1207768345352628
iteration : 7769
train acc:  0.890625
train loss:  0.280479371547699
train gradient:  0.11864288974385702
iteration : 7770
train acc:  0.8046875
train loss:  0.39430683851242065
train gradient:  0.20697248257355833
iteration : 7771
train acc:  0.7734375
train loss:  0.39276325702667236
train gradient:  0.29241074513865817
iteration : 7772
train acc:  0.8671875
train loss:  0.3690517246723175
train gradient:  0.21350412857289466
iteration : 7773
train acc:  0.875
train loss:  0.3209383487701416
train gradient:  0.23015419213420413
iteration : 7774
train acc:  0.859375
train loss:  0.28216201066970825
train gradient:  0.21152016237980373
iteration : 7775
train acc:  0.828125
train loss:  0.3856041431427002
train gradient:  0.230374438433314
iteration : 7776
train acc:  0.8828125
train loss:  0.2607836127281189
train gradient:  0.1185141221732756
iteration : 7777
train acc:  0.8203125
train loss:  0.4214879870414734
train gradient:  0.31155140028616973
iteration : 7778
train acc:  0.890625
train loss:  0.3364681601524353
train gradient:  0.23558938451425
iteration : 7779
train acc:  0.828125
train loss:  0.36632344126701355
train gradient:  0.15244348682111125
iteration : 7780
train acc:  0.84375
train loss:  0.3048599660396576
train gradient:  0.1842775825811344
iteration : 7781
train acc:  0.8671875
train loss:  0.3106781244277954
train gradient:  0.16854200497194627
iteration : 7782
train acc:  0.890625
train loss:  0.28421202301979065
train gradient:  0.14942551090707987
iteration : 7783
train acc:  0.8515625
train loss:  0.3865821361541748
train gradient:  0.21950753948364615
iteration : 7784
train acc:  0.84375
train loss:  0.3320029377937317
train gradient:  0.18292525991287426
iteration : 7785
train acc:  0.875
train loss:  0.28429585695266724
train gradient:  0.15738351815994361
iteration : 7786
train acc:  0.859375
train loss:  0.291151225566864
train gradient:  0.16776759895832305
iteration : 7787
train acc:  0.84375
train loss:  0.36608636379241943
train gradient:  0.2027935352098251
iteration : 7788
train acc:  0.84375
train loss:  0.33121252059936523
train gradient:  0.15680700279497067
iteration : 7789
train acc:  0.859375
train loss:  0.32515180110931396
train gradient:  0.19481702021006628
iteration : 7790
train acc:  0.9375
train loss:  0.20240332186222076
train gradient:  0.09115965802158865
iteration : 7791
train acc:  0.859375
train loss:  0.30498677492141724
train gradient:  0.1380630159765453
iteration : 7792
train acc:  0.8203125
train loss:  0.3579656183719635
train gradient:  0.18333942544387624
iteration : 7793
train acc:  0.8046875
train loss:  0.3438740670681
train gradient:  0.20721976902224415
iteration : 7794
train acc:  0.8671875
train loss:  0.291256308555603
train gradient:  0.14979065342360945
iteration : 7795
train acc:  0.8359375
train loss:  0.3656153678894043
train gradient:  0.16558680198923265
iteration : 7796
train acc:  0.875
train loss:  0.25883764028549194
train gradient:  0.13880923659181715
iteration : 7797
train acc:  0.84375
train loss:  0.36398783326148987
train gradient:  0.1566346070540779
iteration : 7798
train acc:  0.8671875
train loss:  0.32872796058654785
train gradient:  0.2346661989854978
iteration : 7799
train acc:  0.8515625
train loss:  0.36981120705604553
train gradient:  0.18566948128092026
iteration : 7800
train acc:  0.8984375
train loss:  0.3071519732475281
train gradient:  0.11675085486593795
iteration : 7801
train acc:  0.859375
train loss:  0.2900652587413788
train gradient:  0.1546443181063612
iteration : 7802
train acc:  0.875
train loss:  0.303104043006897
train gradient:  0.20027581331927496
iteration : 7803
train acc:  0.90625
train loss:  0.30328577756881714
train gradient:  0.19104289905787247
iteration : 7804
train acc:  0.8359375
train loss:  0.3134947419166565
train gradient:  0.15263051303675737
iteration : 7805
train acc:  0.890625
train loss:  0.2869546413421631
train gradient:  0.1820988331806791
iteration : 7806
train acc:  0.8515625
train loss:  0.3818536400794983
train gradient:  0.2161841753416258
iteration : 7807
train acc:  0.875
train loss:  0.3008739948272705
train gradient:  0.17783913513792993
iteration : 7808
train acc:  0.8203125
train loss:  0.409770131111145
train gradient:  0.24959912279165955
iteration : 7809
train acc:  0.8671875
train loss:  0.35388898849487305
train gradient:  0.20871415150493566
iteration : 7810
train acc:  0.8515625
train loss:  0.3289557695388794
train gradient:  0.110717512787913
iteration : 7811
train acc:  0.8984375
train loss:  0.3135807514190674
train gradient:  0.1593632249031593
iteration : 7812
train acc:  0.8671875
train loss:  0.2560081481933594
train gradient:  0.15629384709950184
iteration : 7813
train acc:  0.84375
train loss:  0.3128530979156494
train gradient:  0.1950660926298646
iteration : 7814
train acc:  0.8515625
train loss:  0.36447346210479736
train gradient:  0.28173553880008345
iteration : 7815
train acc:  0.8515625
train loss:  0.3917001783847809
train gradient:  0.22728818955771135
iteration : 7816
train acc:  0.8515625
train loss:  0.37132930755615234
train gradient:  0.2599448925916764
iteration : 7817
train acc:  0.828125
train loss:  0.3605811893939972
train gradient:  0.2272032470464232
iteration : 7818
train acc:  0.8515625
train loss:  0.32579827308654785
train gradient:  0.1845220164768846
iteration : 7819
train acc:  0.8203125
train loss:  0.43680715560913086
train gradient:  0.3267269962989029
iteration : 7820
train acc:  0.8359375
train loss:  0.35367798805236816
train gradient:  0.20950150074769833
iteration : 7821
train acc:  0.8046875
train loss:  0.48137903213500977
train gradient:  0.4879384172924544
iteration : 7822
train acc:  0.828125
train loss:  0.38667893409729004
train gradient:  0.2089398958171614
iteration : 7823
train acc:  0.8515625
train loss:  0.3525107502937317
train gradient:  0.18251420840491908
iteration : 7824
train acc:  0.8203125
train loss:  0.3336404263973236
train gradient:  0.17629573721707376
iteration : 7825
train acc:  0.7890625
train loss:  0.4210442900657654
train gradient:  0.46220735421162945
iteration : 7826
train acc:  0.8125
train loss:  0.39639657735824585
train gradient:  0.2157895732947298
iteration : 7827
train acc:  0.8828125
train loss:  0.26356589794158936
train gradient:  0.11315421526890872
iteration : 7828
train acc:  0.8671875
train loss:  0.28476065397262573
train gradient:  0.17175767224400013
iteration : 7829
train acc:  0.828125
train loss:  0.3743929862976074
train gradient:  0.2902891455760066
iteration : 7830
train acc:  0.8515625
train loss:  0.3526318073272705
train gradient:  0.23222024381772288
iteration : 7831
train acc:  0.8828125
train loss:  0.2983006238937378
train gradient:  0.1541017773048004
iteration : 7832
train acc:  0.828125
train loss:  0.44187259674072266
train gradient:  0.2897716273980127
iteration : 7833
train acc:  0.8515625
train loss:  0.3567919135093689
train gradient:  0.23218477806963844
iteration : 7834
train acc:  0.875
train loss:  0.3260219097137451
train gradient:  0.15762442628957024
iteration : 7835
train acc:  0.8359375
train loss:  0.3828129470348358
train gradient:  0.22186339787911014
iteration : 7836
train acc:  0.8515625
train loss:  0.3526567220687866
train gradient:  0.23851707447229997
iteration : 7837
train acc:  0.8359375
train loss:  0.3606194257736206
train gradient:  0.17942919879798386
iteration : 7838
train acc:  0.8671875
train loss:  0.3081156015396118
train gradient:  0.1527285698929088
iteration : 7839
train acc:  0.8515625
train loss:  0.31080225110054016
train gradient:  0.2110541513823329
iteration : 7840
train acc:  0.8828125
train loss:  0.29844731092453003
train gradient:  0.12940854483560804
iteration : 7841
train acc:  0.8203125
train loss:  0.34707656502723694
train gradient:  0.1999032476346437
iteration : 7842
train acc:  0.8046875
train loss:  0.40839600563049316
train gradient:  0.2394246530368444
iteration : 7843
train acc:  0.875
train loss:  0.3132258653640747
train gradient:  0.1896250703328074
iteration : 7844
train acc:  0.828125
train loss:  0.3875110447406769
train gradient:  0.18680020171509548
iteration : 7845
train acc:  0.84375
train loss:  0.3523710370063782
train gradient:  0.1577372757204873
iteration : 7846
train acc:  0.84375
train loss:  0.2928384244441986
train gradient:  0.16461949764849398
iteration : 7847
train acc:  0.8359375
train loss:  0.3418957591056824
train gradient:  0.17351913558769205
iteration : 7848
train acc:  0.859375
train loss:  0.33742234110832214
train gradient:  0.2525330437137621
iteration : 7849
train acc:  0.8671875
train loss:  0.284201979637146
train gradient:  0.16904032984696574
iteration : 7850
train acc:  0.859375
train loss:  0.31973573565483093
train gradient:  0.20767683233619158
iteration : 7851
train acc:  0.8515625
train loss:  0.3784492611885071
train gradient:  0.1785742944850963
iteration : 7852
train acc:  0.875
train loss:  0.32810887694358826
train gradient:  0.15037125077990335
iteration : 7853
train acc:  0.8203125
train loss:  0.42995864152908325
train gradient:  0.27841630943377677
iteration : 7854
train acc:  0.8203125
train loss:  0.3951924741268158
train gradient:  0.2901264986730197
iteration : 7855
train acc:  0.8359375
train loss:  0.37562376260757446
train gradient:  0.3262898793035073
iteration : 7856
train acc:  0.9140625
train loss:  0.2949066162109375
train gradient:  0.19237623538890328
iteration : 7857
train acc:  0.8984375
train loss:  0.2549939751625061
train gradient:  0.08442884679008886
iteration : 7858
train acc:  0.84375
train loss:  0.3614692687988281
train gradient:  0.26049015565675965
iteration : 7859
train acc:  0.8515625
train loss:  0.31958529353141785
train gradient:  0.15957569844295888
iteration : 7860
train acc:  0.8515625
train loss:  0.3713113069534302
train gradient:  0.31773329845198134
iteration : 7861
train acc:  0.7890625
train loss:  0.36458954215049744
train gradient:  0.17756317141294908
iteration : 7862
train acc:  0.8671875
train loss:  0.36082199215888977
train gradient:  0.1641365884870642
iteration : 7863
train acc:  0.8671875
train loss:  0.3282378911972046
train gradient:  0.14087973189841138
iteration : 7864
train acc:  0.796875
train loss:  0.3648838698863983
train gradient:  0.38769287477900377
iteration : 7865
train acc:  0.8203125
train loss:  0.3573310375213623
train gradient:  0.20065986155300267
iteration : 7866
train acc:  0.84375
train loss:  0.34479838609695435
train gradient:  0.1806797374865204
iteration : 7867
train acc:  0.796875
train loss:  0.44358405470848083
train gradient:  0.26368473091947864
iteration : 7868
train acc:  0.828125
train loss:  0.37728577852249146
train gradient:  0.18574244599276718
iteration : 7869
train acc:  0.859375
train loss:  0.33719879388809204
train gradient:  0.17156679712362827
iteration : 7870
train acc:  0.84375
train loss:  0.3475400507450104
train gradient:  0.2487797611521606
iteration : 7871
train acc:  0.859375
train loss:  0.3671211898326874
train gradient:  0.2594457327360578
iteration : 7872
train acc:  0.8203125
train loss:  0.38231074810028076
train gradient:  0.17860946022846044
iteration : 7873
train acc:  0.78125
train loss:  0.47138768434524536
train gradient:  0.2873727675784524
iteration : 7874
train acc:  0.8203125
train loss:  0.35840967297554016
train gradient:  0.14607270829919122
iteration : 7875
train acc:  0.828125
train loss:  0.36929959058761597
train gradient:  0.32669780891836236
iteration : 7876
train acc:  0.8515625
train loss:  0.33795106410980225
train gradient:  0.19302115937683878
iteration : 7877
train acc:  0.875
train loss:  0.2942442297935486
train gradient:  0.1624555905855965
iteration : 7878
train acc:  0.890625
train loss:  0.26931965351104736
train gradient:  0.09295565891185378
iteration : 7879
train acc:  0.8515625
train loss:  0.3014962375164032
train gradient:  0.23109489748924555
iteration : 7880
train acc:  0.8984375
train loss:  0.2339319884777069
train gradient:  0.07631713495728289
iteration : 7881
train acc:  0.8515625
train loss:  0.3733469545841217
train gradient:  0.15865726077748726
iteration : 7882
train acc:  0.8125
train loss:  0.45751139521598816
train gradient:  0.3118407210186241
iteration : 7883
train acc:  0.8828125
train loss:  0.31061428785324097
train gradient:  0.1579771139553423
iteration : 7884
train acc:  0.8515625
train loss:  0.3578694462776184
train gradient:  0.2781887861918896
iteration : 7885
train acc:  0.8515625
train loss:  0.358542263507843
train gradient:  0.17868413130928473
iteration : 7886
train acc:  0.8671875
train loss:  0.29831796884536743
train gradient:  0.11360922344401837
iteration : 7887
train acc:  0.765625
train loss:  0.41341742873191833
train gradient:  0.2998522783611139
iteration : 7888
train acc:  0.90625
train loss:  0.2879639267921448
train gradient:  0.13479672859866387
iteration : 7889
train acc:  0.8203125
train loss:  0.38553085923194885
train gradient:  0.20986201795348974
iteration : 7890
train acc:  0.84375
train loss:  0.3326757848262787
train gradient:  0.18007194473461685
iteration : 7891
train acc:  0.84375
train loss:  0.34944862127304077
train gradient:  0.21248771133622735
iteration : 7892
train acc:  0.796875
train loss:  0.3705357313156128
train gradient:  0.2501783323987867
iteration : 7893
train acc:  0.8984375
train loss:  0.28423386812210083
train gradient:  0.12844670311936823
iteration : 7894
train acc:  0.8515625
train loss:  0.3360891342163086
train gradient:  0.18783836741072146
iteration : 7895
train acc:  0.890625
train loss:  0.24697443842887878
train gradient:  0.12152012326387863
iteration : 7896
train acc:  0.875
train loss:  0.35888540744781494
train gradient:  0.14855665552527741
iteration : 7897
train acc:  0.8203125
train loss:  0.35417038202285767
train gradient:  0.17571900835267973
iteration : 7898
train acc:  0.8359375
train loss:  0.39825838804244995
train gradient:  0.23815519538228863
iteration : 7899
train acc:  0.8828125
train loss:  0.3093011975288391
train gradient:  0.14614814395727146
iteration : 7900
train acc:  0.8125
train loss:  0.383461594581604
train gradient:  0.2737628437448764
iteration : 7901
train acc:  0.8359375
train loss:  0.3425993323326111
train gradient:  0.26619246995206214
iteration : 7902
train acc:  0.8515625
train loss:  0.3083600103855133
train gradient:  0.18472321181679574
iteration : 7903
train acc:  0.8671875
train loss:  0.3184105157852173
train gradient:  0.1680547139438478
iteration : 7904
train acc:  0.859375
train loss:  0.3400186002254486
train gradient:  0.23276938966884897
iteration : 7905
train acc:  0.8046875
train loss:  0.3729490637779236
train gradient:  0.23702962068636674
iteration : 7906
train acc:  0.7890625
train loss:  0.3711468577384949
train gradient:  0.28615935225963607
iteration : 7907
train acc:  0.8359375
train loss:  0.34819838404655457
train gradient:  0.2499863707931455
iteration : 7908
train acc:  0.859375
train loss:  0.31335657835006714
train gradient:  0.15755923278505654
iteration : 7909
train acc:  0.8046875
train loss:  0.40124744176864624
train gradient:  0.19624308570119994
iteration : 7910
train acc:  0.875
train loss:  0.3646516799926758
train gradient:  0.14952577715306914
iteration : 7911
train acc:  0.8515625
train loss:  0.33492210507392883
train gradient:  0.19654549351256034
iteration : 7912
train acc:  0.7890625
train loss:  0.4134948253631592
train gradient:  0.2630622466635726
iteration : 7913
train acc:  0.78125
train loss:  0.41373157501220703
train gradient:  0.23494946797976735
iteration : 7914
train acc:  0.8671875
train loss:  0.32962626218795776
train gradient:  0.16877617687567778
iteration : 7915
train acc:  0.8671875
train loss:  0.31769824028015137
train gradient:  0.13987404233284573
iteration : 7916
train acc:  0.8359375
train loss:  0.4012468159198761
train gradient:  0.17762149594145477
iteration : 7917
train acc:  0.8671875
train loss:  0.29165375232696533
train gradient:  0.17136562253467885
iteration : 7918
train acc:  0.859375
train loss:  0.3254534602165222
train gradient:  0.181993272731399
iteration : 7919
train acc:  0.875
train loss:  0.3195953667163849
train gradient:  0.10448437782833368
iteration : 7920
train acc:  0.84375
train loss:  0.30170735716819763
train gradient:  0.13693460425780707
iteration : 7921
train acc:  0.8671875
train loss:  0.355820894241333
train gradient:  0.228091640427107
iteration : 7922
train acc:  0.8046875
train loss:  0.4238322079181671
train gradient:  0.33558332659283485
iteration : 7923
train acc:  0.8203125
train loss:  0.3988756537437439
train gradient:  0.24678151402081192
iteration : 7924
train acc:  0.875
train loss:  0.3166627287864685
train gradient:  0.20477893158658966
iteration : 7925
train acc:  0.8515625
train loss:  0.3259335160255432
train gradient:  0.10697969610171866
iteration : 7926
train acc:  0.875
train loss:  0.3288184404373169
train gradient:  0.18833049914292116
iteration : 7927
train acc:  0.8828125
train loss:  0.30293798446655273
train gradient:  0.15580257822944377
iteration : 7928
train acc:  0.9140625
train loss:  0.24552485346794128
train gradient:  0.11034376174212172
iteration : 7929
train acc:  0.8515625
train loss:  0.31255263090133667
train gradient:  0.12137877515968529
iteration : 7930
train acc:  0.890625
train loss:  0.33106017112731934
train gradient:  0.17462573502029305
iteration : 7931
train acc:  0.875
train loss:  0.28796151280403137
train gradient:  0.1135805913307286
iteration : 7932
train acc:  0.78125
train loss:  0.45367902517318726
train gradient:  0.28917421426228107
iteration : 7933
train acc:  0.8359375
train loss:  0.36360234022140503
train gradient:  0.159950493085473
iteration : 7934
train acc:  0.859375
train loss:  0.3339964747428894
train gradient:  0.308455166079485
iteration : 7935
train acc:  0.90625
train loss:  0.3078511357307434
train gradient:  0.16286724644632408
iteration : 7936
train acc:  0.8828125
train loss:  0.3265346586704254
train gradient:  0.19737434230943007
iteration : 7937
train acc:  0.875
train loss:  0.3067779839038849
train gradient:  0.11847793612983529
iteration : 7938
train acc:  0.875
train loss:  0.33092156052589417
train gradient:  0.1650734377674703
iteration : 7939
train acc:  0.828125
train loss:  0.3948501944541931
train gradient:  0.22275475729610145
iteration : 7940
train acc:  0.8359375
train loss:  0.38253360986709595
train gradient:  0.24477813887222283
iteration : 7941
train acc:  0.796875
train loss:  0.3635178804397583
train gradient:  0.19664844287067756
iteration : 7942
train acc:  0.84375
train loss:  0.3723788261413574
train gradient:  0.2539004411497302
iteration : 7943
train acc:  0.84375
train loss:  0.35241732001304626
train gradient:  0.1286488533392199
iteration : 7944
train acc:  0.8671875
train loss:  0.296989381313324
train gradient:  0.12078710689811964
iteration : 7945
train acc:  0.828125
train loss:  0.4220121502876282
train gradient:  0.22861465372316847
iteration : 7946
train acc:  0.8046875
train loss:  0.36795830726623535
train gradient:  0.20027818153236185
iteration : 7947
train acc:  0.8515625
train loss:  0.3195728063583374
train gradient:  0.12098030953441945
iteration : 7948
train acc:  0.859375
train loss:  0.34878236055374146
train gradient:  0.14310986262294345
iteration : 7949
train acc:  0.8359375
train loss:  0.33999279141426086
train gradient:  0.14048657010872395
iteration : 7950
train acc:  0.8125
train loss:  0.4423535466194153
train gradient:  0.3181356132343216
iteration : 7951
train acc:  0.84375
train loss:  0.37755662202835083
train gradient:  0.18930061081326055
iteration : 7952
train acc:  0.875
train loss:  0.27510660886764526
train gradient:  0.14384025547838322
iteration : 7953
train acc:  0.8984375
train loss:  0.30741533637046814
train gradient:  0.15507214675261552
iteration : 7954
train acc:  0.890625
train loss:  0.2549123764038086
train gradient:  0.12576097002920367
iteration : 7955
train acc:  0.828125
train loss:  0.3634638488292694
train gradient:  0.1711229707602569
iteration : 7956
train acc:  0.84375
train loss:  0.34334099292755127
train gradient:  0.191497281317434
iteration : 7957
train acc:  0.8515625
train loss:  0.3706933856010437
train gradient:  0.1869524417694881
iteration : 7958
train acc:  0.890625
train loss:  0.2977777123451233
train gradient:  0.11147472172251256
iteration : 7959
train acc:  0.8203125
train loss:  0.3709332346916199
train gradient:  0.22633642348958402
iteration : 7960
train acc:  0.84375
train loss:  0.3141053318977356
train gradient:  0.18428443067282205
iteration : 7961
train acc:  0.90625
train loss:  0.3920186758041382
train gradient:  0.21983594927733588
iteration : 7962
train acc:  0.8984375
train loss:  0.2784883379936218
train gradient:  0.1372870677458941
iteration : 7963
train acc:  0.8828125
train loss:  0.2725197672843933
train gradient:  0.13389442173051003
iteration : 7964
train acc:  0.8046875
train loss:  0.36838793754577637
train gradient:  0.2345417028083533
iteration : 7965
train acc:  0.859375
train loss:  0.2764360308647156
train gradient:  0.15280666875260895
iteration : 7966
train acc:  0.8046875
train loss:  0.3974296450614929
train gradient:  0.2093668623711895
iteration : 7967
train acc:  0.84375
train loss:  0.30304697155952454
train gradient:  0.2980790437750839
iteration : 7968
train acc:  0.859375
train loss:  0.3197576701641083
train gradient:  0.12194717934270904
iteration : 7969
train acc:  0.828125
train loss:  0.33391159772872925
train gradient:  0.20757557501723006
iteration : 7970
train acc:  0.8203125
train loss:  0.38483843207359314
train gradient:  0.28763392320442016
iteration : 7971
train acc:  0.859375
train loss:  0.35626935958862305
train gradient:  0.19515577803691775
iteration : 7972
train acc:  0.84375
train loss:  0.33900266885757446
train gradient:  0.13327720564889278
iteration : 7973
train acc:  0.8671875
train loss:  0.3077993392944336
train gradient:  0.14354489933196385
iteration : 7974
train acc:  0.8828125
train loss:  0.3233413100242615
train gradient:  0.15168931039320943
iteration : 7975
train acc:  0.875
train loss:  0.3008705675601959
train gradient:  0.14814591732382051
iteration : 7976
train acc:  0.859375
train loss:  0.3447319269180298
train gradient:  0.19381304389270057
iteration : 7977
train acc:  0.8203125
train loss:  0.3995763659477234
train gradient:  0.2515706722382298
iteration : 7978
train acc:  0.8515625
train loss:  0.34600237011909485
train gradient:  0.22156533833214936
iteration : 7979
train acc:  0.8984375
train loss:  0.26796990633010864
train gradient:  0.14480974784775258
iteration : 7980
train acc:  0.859375
train loss:  0.3181859850883484
train gradient:  0.14388650063205383
iteration : 7981
train acc:  0.8828125
train loss:  0.2786649763584137
train gradient:  0.11352871435864946
iteration : 7982
train acc:  0.8359375
train loss:  0.3752792477607727
train gradient:  0.2306465142646606
iteration : 7983
train acc:  0.8515625
train loss:  0.43124428391456604
train gradient:  0.29722371921325386
iteration : 7984
train acc:  0.8984375
train loss:  0.2561686635017395
train gradient:  0.09689588510323992
iteration : 7985
train acc:  0.8203125
train loss:  0.3758994936943054
train gradient:  0.24132695193867465
iteration : 7986
train acc:  0.84375
train loss:  0.28768882155418396
train gradient:  0.16412388922988644
iteration : 7987
train acc:  0.8125
train loss:  0.34611040353775024
train gradient:  0.2252446798731994
iteration : 7988
train acc:  0.8515625
train loss:  0.36493203043937683
train gradient:  0.2288715610672421
iteration : 7989
train acc:  0.8515625
train loss:  0.32495105266571045
train gradient:  0.17104722517548082
iteration : 7990
train acc:  0.859375
train loss:  0.29708993434906006
train gradient:  0.3343989459782449
iteration : 7991
train acc:  0.8671875
train loss:  0.3660673499107361
train gradient:  0.16617345194057692
iteration : 7992
train acc:  0.8515625
train loss:  0.3216267228126526
train gradient:  0.14389080188958472
iteration : 7993
train acc:  0.8203125
train loss:  0.3807811737060547
train gradient:  0.2100883254667251
iteration : 7994
train acc:  0.875
train loss:  0.28035545349121094
train gradient:  0.13438972041298214
iteration : 7995
train acc:  0.875
train loss:  0.269357293844223
train gradient:  0.12710055654411495
iteration : 7996
train acc:  0.8359375
train loss:  0.3198097348213196
train gradient:  0.1867594182161258
iteration : 7997
train acc:  0.8671875
train loss:  0.30810022354125977
train gradient:  0.19600385495334668
iteration : 7998
train acc:  0.8515625
train loss:  0.3320120573043823
train gradient:  0.15051343270716333
iteration : 7999
train acc:  0.890625
train loss:  0.28830578923225403
train gradient:  0.15525400778670168
iteration : 8000
train acc:  0.8359375
train loss:  0.38223183155059814
train gradient:  0.1955129672977446
iteration : 8001
train acc:  0.875
train loss:  0.33062541484832764
train gradient:  0.1521494232183782
iteration : 8002
train acc:  0.828125
train loss:  0.42846450209617615
train gradient:  0.2379023046051103
iteration : 8003
train acc:  0.7890625
train loss:  0.41946595907211304
train gradient:  0.3275526098224217
iteration : 8004
train acc:  0.828125
train loss:  0.3948579728603363
train gradient:  0.18567899139220012
iteration : 8005
train acc:  0.84375
train loss:  0.29812929034233093
train gradient:  0.12377768162805512
iteration : 8006
train acc:  0.8046875
train loss:  0.4670443832874298
train gradient:  0.27208851694819236
iteration : 8007
train acc:  0.8828125
train loss:  0.27541583776474
train gradient:  0.14096440212391384
iteration : 8008
train acc:  0.8203125
train loss:  0.3853801488876343
train gradient:  0.287692725743575
iteration : 8009
train acc:  0.8125
train loss:  0.41511327028274536
train gradient:  0.20471241571607104
iteration : 8010
train acc:  0.8984375
train loss:  0.2544303238391876
train gradient:  0.09016299942892564
iteration : 8011
train acc:  0.8046875
train loss:  0.4898452162742615
train gradient:  0.2839986080100201
iteration : 8012
train acc:  0.8359375
train loss:  0.33280467987060547
train gradient:  0.2141348484956141
iteration : 8013
train acc:  0.84375
train loss:  0.30673283338546753
train gradient:  0.25025018156356554
iteration : 8014
train acc:  0.7890625
train loss:  0.351290762424469
train gradient:  0.20827639864606026
iteration : 8015
train acc:  0.828125
train loss:  0.33289891481399536
train gradient:  0.1830224659186228
iteration : 8016
train acc:  0.875
train loss:  0.3373194932937622
train gradient:  0.20549097102931002
iteration : 8017
train acc:  0.84375
train loss:  0.35339173674583435
train gradient:  0.231774986246296
iteration : 8018
train acc:  0.8984375
train loss:  0.2671360969543457
train gradient:  0.10617262530888483
iteration : 8019
train acc:  0.8828125
train loss:  0.2876827120780945
train gradient:  0.126306659216545
iteration : 8020
train acc:  0.875
train loss:  0.3121570348739624
train gradient:  0.16538514249436043
iteration : 8021
train acc:  0.875
train loss:  0.33946049213409424
train gradient:  0.17108668532135002
iteration : 8022
train acc:  0.84375
train loss:  0.3290225863456726
train gradient:  0.17326425654940916
iteration : 8023
train acc:  0.84375
train loss:  0.336331307888031
train gradient:  0.20110598695039472
iteration : 8024
train acc:  0.890625
train loss:  0.2649289071559906
train gradient:  0.1063024769165996
iteration : 8025
train acc:  0.8984375
train loss:  0.22186286747455597
train gradient:  0.08318509938832529
iteration : 8026
train acc:  0.8125
train loss:  0.36372441053390503
train gradient:  0.23850508343775284
iteration : 8027
train acc:  0.84375
train loss:  0.3947974145412445
train gradient:  0.23418180939678424
iteration : 8028
train acc:  0.9140625
train loss:  0.29163888096809387
train gradient:  0.1508494058907492
iteration : 8029
train acc:  0.84375
train loss:  0.31112802028656006
train gradient:  0.1327179139433861
iteration : 8030
train acc:  0.7578125
train loss:  0.43427008390426636
train gradient:  0.25035250699856926
iteration : 8031
train acc:  0.875
train loss:  0.30706754326820374
train gradient:  0.2199437593238533
iteration : 8032
train acc:  0.84375
train loss:  0.36638200283050537
train gradient:  0.18341392891842545
iteration : 8033
train acc:  0.796875
train loss:  0.3909018039703369
train gradient:  0.22508391388485177
iteration : 8034
train acc:  0.828125
train loss:  0.39605477452278137
train gradient:  0.23364420306982459
iteration : 8035
train acc:  0.8515625
train loss:  0.3010844588279724
train gradient:  0.15087846285712986
iteration : 8036
train acc:  0.8828125
train loss:  0.274917870759964
train gradient:  0.2561703794859295
iteration : 8037
train acc:  0.8515625
train loss:  0.3167991042137146
train gradient:  0.146849172932273
iteration : 8038
train acc:  0.875
train loss:  0.3200903534889221
train gradient:  0.11938391481114116
iteration : 8039
train acc:  0.828125
train loss:  0.36506128311157227
train gradient:  0.17988559005952523
iteration : 8040
train acc:  0.8046875
train loss:  0.38459688425064087
train gradient:  0.33777910269649764
iteration : 8041
train acc:  0.796875
train loss:  0.4187377691268921
train gradient:  0.22069105663062832
iteration : 8042
train acc:  0.828125
train loss:  0.4035486578941345
train gradient:  0.2564439370657884
iteration : 8043
train acc:  0.8671875
train loss:  0.31506550312042236
train gradient:  0.13832083699004272
iteration : 8044
train acc:  0.7734375
train loss:  0.4679104685783386
train gradient:  0.3169095465833925
iteration : 8045
train acc:  0.8515625
train loss:  0.3697984218597412
train gradient:  0.22393514458423786
iteration : 8046
train acc:  0.8671875
train loss:  0.3177785277366638
train gradient:  0.22094262418779537
iteration : 8047
train acc:  0.875
train loss:  0.3645078241825104
train gradient:  0.12946519170552792
iteration : 8048
train acc:  0.8125
train loss:  0.3640814423561096
train gradient:  0.21431383194771875
iteration : 8049
train acc:  0.7890625
train loss:  0.3834652304649353
train gradient:  0.19461710218247036
iteration : 8050
train acc:  0.828125
train loss:  0.37384122610092163
train gradient:  0.20509552836505285
iteration : 8051
train acc:  0.8828125
train loss:  0.2871319353580475
train gradient:  0.1341939551990215
iteration : 8052
train acc:  0.859375
train loss:  0.3530420660972595
train gradient:  0.14809174977378955
iteration : 8053
train acc:  0.84375
train loss:  0.3058379292488098
train gradient:  0.10193986443066713
iteration : 8054
train acc:  0.859375
train loss:  0.3893885910511017
train gradient:  0.15881276260577917
iteration : 8055
train acc:  0.859375
train loss:  0.29795897006988525
train gradient:  0.15557149969103756
iteration : 8056
train acc:  0.8828125
train loss:  0.31217557191848755
train gradient:  0.14806316194252359
iteration : 8057
train acc:  0.8125
train loss:  0.3821030855178833
train gradient:  0.19988931115805486
iteration : 8058
train acc:  0.8046875
train loss:  0.3817049264907837
train gradient:  0.14607241485639463
iteration : 8059
train acc:  0.8671875
train loss:  0.30297940969467163
train gradient:  0.12025081066088651
iteration : 8060
train acc:  0.8046875
train loss:  0.37856531143188477
train gradient:  0.20200399153580514
iteration : 8061
train acc:  0.8515625
train loss:  0.3913298547267914
train gradient:  0.20131402368934403
iteration : 8062
train acc:  0.8671875
train loss:  0.32309871912002563
train gradient:  0.15722708967329932
iteration : 8063
train acc:  0.9296875
train loss:  0.26308658719062805
train gradient:  0.09515632191215029
iteration : 8064
train acc:  0.8828125
train loss:  0.29663756489753723
train gradient:  0.12518343790577907
iteration : 8065
train acc:  0.8828125
train loss:  0.30716776847839355
train gradient:  0.14762356616900021
iteration : 8066
train acc:  0.8671875
train loss:  0.3228151202201843
train gradient:  0.17701298177181093
iteration : 8067
train acc:  0.8828125
train loss:  0.30344340205192566
train gradient:  0.23124766971871796
iteration : 8068
train acc:  0.8515625
train loss:  0.30915558338165283
train gradient:  0.13602171429902138
iteration : 8069
train acc:  0.859375
train loss:  0.3304610848426819
train gradient:  0.14982331516760639
iteration : 8070
train acc:  0.7890625
train loss:  0.4077223539352417
train gradient:  0.3030532843645878
iteration : 8071
train acc:  0.84375
train loss:  0.34320735931396484
train gradient:  0.1325507239312494
iteration : 8072
train acc:  0.8984375
train loss:  0.2993529140949249
train gradient:  0.14817187652264996
iteration : 8073
train acc:  0.84375
train loss:  0.3300215005874634
train gradient:  0.18756627753768038
iteration : 8074
train acc:  0.8359375
train loss:  0.35116297006607056
train gradient:  0.18829393577015235
iteration : 8075
train acc:  0.7890625
train loss:  0.419303834438324
train gradient:  0.24617994953469347
iteration : 8076
train acc:  0.890625
train loss:  0.33558210730552673
train gradient:  0.14387880500229458
iteration : 8077
train acc:  0.828125
train loss:  0.35928431153297424
train gradient:  0.24423771811959455
iteration : 8078
train acc:  0.84375
train loss:  0.33491653203964233
train gradient:  0.1526157013539196
iteration : 8079
train acc:  0.8984375
train loss:  0.2601321339607239
train gradient:  0.10327442559542803
iteration : 8080
train acc:  0.84375
train loss:  0.31673890352249146
train gradient:  0.15606563424566328
iteration : 8081
train acc:  0.8515625
train loss:  0.326917827129364
train gradient:  0.21165703859538942
iteration : 8082
train acc:  0.859375
train loss:  0.3353714942932129
train gradient:  0.20915040031862653
iteration : 8083
train acc:  0.8203125
train loss:  0.34451404213905334
train gradient:  0.1574232130004091
iteration : 8084
train acc:  0.8125
train loss:  0.33331117033958435
train gradient:  0.19609986097989204
iteration : 8085
train acc:  0.828125
train loss:  0.35194116830825806
train gradient:  0.19094964182336416
iteration : 8086
train acc:  0.8671875
train loss:  0.2967210114002228
train gradient:  0.12115535560048114
iteration : 8087
train acc:  0.8046875
train loss:  0.4481360912322998
train gradient:  0.323468438492208
iteration : 8088
train acc:  0.875
train loss:  0.26944321393966675
train gradient:  0.11504166305964732
iteration : 8089
train acc:  0.84375
train loss:  0.32538604736328125
train gradient:  0.13616297489236878
iteration : 8090
train acc:  0.859375
train loss:  0.32255300879478455
train gradient:  0.10675478419709489
iteration : 8091
train acc:  0.8984375
train loss:  0.2840806245803833
train gradient:  0.1387598725085259
iteration : 8092
train acc:  0.890625
train loss:  0.330139696598053
train gradient:  0.1910586961800726
iteration : 8093
train acc:  0.890625
train loss:  0.2920432984828949
train gradient:  0.17730801355064046
iteration : 8094
train acc:  0.875
train loss:  0.31692513823509216
train gradient:  0.2296272115713604
iteration : 8095
train acc:  0.8671875
train loss:  0.3766709268093109
train gradient:  0.2811942620693864
iteration : 8096
train acc:  0.8671875
train loss:  0.3156352639198303
train gradient:  0.10828668635401768
iteration : 8097
train acc:  0.890625
train loss:  0.23750081658363342
train gradient:  0.08501552581686453
iteration : 8098
train acc:  0.8203125
train loss:  0.37078338861465454
train gradient:  0.1486593231099247
iteration : 8099
train acc:  0.859375
train loss:  0.33484959602355957
train gradient:  0.13051011178936703
iteration : 8100
train acc:  0.8515625
train loss:  0.38552871346473694
train gradient:  0.17720533820786677
iteration : 8101
train acc:  0.859375
train loss:  0.3495655357837677
train gradient:  0.22307383591457508
iteration : 8102
train acc:  0.7734375
train loss:  0.3782053589820862
train gradient:  0.2149748518812284
iteration : 8103
train acc:  0.859375
train loss:  0.2573387920856476
train gradient:  0.12765024957085264
iteration : 8104
train acc:  0.859375
train loss:  0.3529350161552429
train gradient:  0.22935108636468526
iteration : 8105
train acc:  0.859375
train loss:  0.3874332010746002
train gradient:  0.18120169867881444
iteration : 8106
train acc:  0.8828125
train loss:  0.26901304721832275
train gradient:  0.1199313710741212
iteration : 8107
train acc:  0.859375
train loss:  0.3263069987297058
train gradient:  0.13577152015957883
iteration : 8108
train acc:  0.8515625
train loss:  0.39958125352859497
train gradient:  0.15741278003139697
iteration : 8109
train acc:  0.796875
train loss:  0.42673546075820923
train gradient:  0.27618934579708715
iteration : 8110
train acc:  0.859375
train loss:  0.3676352798938751
train gradient:  0.1811014038949494
iteration : 8111
train acc:  0.84375
train loss:  0.33609190583229065
train gradient:  0.16484931636563344
iteration : 8112
train acc:  0.8671875
train loss:  0.36801767349243164
train gradient:  0.18605158960654572
iteration : 8113
train acc:  0.921875
train loss:  0.2543765604496002
train gradient:  0.12366648342687732
iteration : 8114
train acc:  0.875
train loss:  0.3284374475479126
train gradient:  0.17637688094459514
iteration : 8115
train acc:  0.8671875
train loss:  0.2947654724121094
train gradient:  0.14389356228202052
iteration : 8116
train acc:  0.859375
train loss:  0.3352115750312805
train gradient:  0.14390730201879542
iteration : 8117
train acc:  0.828125
train loss:  0.37600457668304443
train gradient:  0.2032871100072357
iteration : 8118
train acc:  0.859375
train loss:  0.3577987551689148
train gradient:  0.21649095087161402
iteration : 8119
train acc:  0.8203125
train loss:  0.35020750761032104
train gradient:  0.1902171340040723
iteration : 8120
train acc:  0.875
train loss:  0.3588540554046631
train gradient:  0.2144485888924187
iteration : 8121
train acc:  0.8828125
train loss:  0.3055133819580078
train gradient:  0.18385611210101777
iteration : 8122
train acc:  0.84375
train loss:  0.3591945171356201
train gradient:  0.1798312682482258
iteration : 8123
train acc:  0.796875
train loss:  0.3950434923171997
train gradient:  0.28321097030712417
iteration : 8124
train acc:  0.8046875
train loss:  0.40189993381500244
train gradient:  0.2499277597266833
iteration : 8125
train acc:  0.828125
train loss:  0.3432214558124542
train gradient:  0.1975097580552168
iteration : 8126
train acc:  0.875
train loss:  0.29751354455947876
train gradient:  0.1847494682183743
iteration : 8127
train acc:  0.90625
train loss:  0.2608903646469116
train gradient:  0.17301008434060255
iteration : 8128
train acc:  0.8359375
train loss:  0.3866578936576843
train gradient:  0.27295128844191224
iteration : 8129
train acc:  0.828125
train loss:  0.4378302991390228
train gradient:  0.1989478541660849
iteration : 8130
train acc:  0.859375
train loss:  0.38212400674819946
train gradient:  0.1679170621951693
iteration : 8131
train acc:  0.8671875
train loss:  0.3385069966316223
train gradient:  0.19115628149588104
iteration : 8132
train acc:  0.84375
train loss:  0.36218518018722534
train gradient:  0.20042526994692378
iteration : 8133
train acc:  0.8515625
train loss:  0.3789660632610321
train gradient:  0.21643083064841873
iteration : 8134
train acc:  0.875
train loss:  0.30766767263412476
train gradient:  0.13508835288576015
iteration : 8135
train acc:  0.8359375
train loss:  0.4102657437324524
train gradient:  0.32254355106728333
iteration : 8136
train acc:  0.8359375
train loss:  0.36075446009635925
train gradient:  0.34414615692117534
iteration : 8137
train acc:  0.890625
train loss:  0.2893140912055969
train gradient:  0.13041998281517847
iteration : 8138
train acc:  0.8046875
train loss:  0.33992424607276917
train gradient:  0.14714505255008642
iteration : 8139
train acc:  0.8671875
train loss:  0.3059981167316437
train gradient:  0.1524646864970876
iteration : 8140
train acc:  0.8203125
train loss:  0.37304431200027466
train gradient:  0.17239293671140837
iteration : 8141
train acc:  0.828125
train loss:  0.34619051218032837
train gradient:  0.19669892380047216
iteration : 8142
train acc:  0.8828125
train loss:  0.24498878419399261
train gradient:  0.14753032834517665
iteration : 8143
train acc:  0.8515625
train loss:  0.33046114444732666
train gradient:  0.17277105410115517
iteration : 8144
train acc:  0.90625
train loss:  0.2968049645423889
train gradient:  0.1328753061498067
iteration : 8145
train acc:  0.875
train loss:  0.37871867418289185
train gradient:  0.21040989473247845
iteration : 8146
train acc:  0.828125
train loss:  0.34795892238616943
train gradient:  0.1837994990202812
iteration : 8147
train acc:  0.890625
train loss:  0.24567753076553345
train gradient:  0.11631847733805904
iteration : 8148
train acc:  0.84375
train loss:  0.36790895462036133
train gradient:  0.17653439452032288
iteration : 8149
train acc:  0.8984375
train loss:  0.2647336423397064
train gradient:  0.11835895814191148
iteration : 8150
train acc:  0.8828125
train loss:  0.26529812812805176
train gradient:  0.13156073909433005
iteration : 8151
train acc:  0.84375
train loss:  0.3256765305995941
train gradient:  0.17788014466088514
iteration : 8152
train acc:  0.84375
train loss:  0.36666685342788696
train gradient:  0.14869014151428409
iteration : 8153
train acc:  0.8671875
train loss:  0.34105780720710754
train gradient:  0.24629225503268593
iteration : 8154
train acc:  0.8515625
train loss:  0.3824847340583801
train gradient:  0.18192763860964364
iteration : 8155
train acc:  0.875
train loss:  0.2557443380355835
train gradient:  0.1397173396734313
iteration : 8156
train acc:  0.859375
train loss:  0.3055051565170288
train gradient:  0.14462886104081035
iteration : 8157
train acc:  0.8671875
train loss:  0.3504634201526642
train gradient:  0.20645912118133802
iteration : 8158
train acc:  0.828125
train loss:  0.4224863350391388
train gradient:  0.23665562591123562
iteration : 8159
train acc:  0.828125
train loss:  0.37862628698349
train gradient:  0.18374337378425543
iteration : 8160
train acc:  0.8515625
train loss:  0.34986576437950134
train gradient:  0.22989723881458507
iteration : 8161
train acc:  0.84375
train loss:  0.35493767261505127
train gradient:  0.15318092440923065
iteration : 8162
train acc:  0.890625
train loss:  0.25501349568367004
train gradient:  0.10958555158926349
iteration : 8163
train acc:  0.875
train loss:  0.2679353952407837
train gradient:  0.11598666467042151
iteration : 8164
train acc:  0.8125
train loss:  0.4060770273208618
train gradient:  0.24545334955591785
iteration : 8165
train acc:  0.875
train loss:  0.31079453229904175
train gradient:  0.13372721595998194
iteration : 8166
train acc:  0.859375
train loss:  0.3604012131690979
train gradient:  0.160969268205279
iteration : 8167
train acc:  0.90625
train loss:  0.2754655182361603
train gradient:  0.11482455723126235
iteration : 8168
train acc:  0.828125
train loss:  0.44052302837371826
train gradient:  0.22831200120939274
iteration : 8169
train acc:  0.84375
train loss:  0.42704567313194275
train gradient:  0.24538725524968988
iteration : 8170
train acc:  0.8984375
train loss:  0.2863033413887024
train gradient:  0.1666260809568991
iteration : 8171
train acc:  0.765625
train loss:  0.44935232400894165
train gradient:  0.3259576434220948
iteration : 8172
train acc:  0.8203125
train loss:  0.38911235332489014
train gradient:  0.28432901400470756
iteration : 8173
train acc:  0.859375
train loss:  0.3011201322078705
train gradient:  0.13427653743654985
iteration : 8174
train acc:  0.8671875
train loss:  0.28031623363494873
train gradient:  0.14170987614891423
iteration : 8175
train acc:  0.84375
train loss:  0.36793336272239685
train gradient:  0.1732433284292518
iteration : 8176
train acc:  0.8828125
train loss:  0.26068103313446045
train gradient:  0.12405527752099257
iteration : 8177
train acc:  0.8125
train loss:  0.3962244391441345
train gradient:  0.21127520864049176
iteration : 8178
train acc:  0.875
train loss:  0.32317841053009033
train gradient:  0.1823673585545591
iteration : 8179
train acc:  0.8203125
train loss:  0.4101826548576355
train gradient:  0.24043182723501955
iteration : 8180
train acc:  0.84375
train loss:  0.35402944684028625
train gradient:  0.19321746337567564
iteration : 8181
train acc:  0.8671875
train loss:  0.3449821472167969
train gradient:  0.1748497447415528
iteration : 8182
train acc:  0.875
train loss:  0.2989956736564636
train gradient:  0.12529347416979875
iteration : 8183
train acc:  0.765625
train loss:  0.46273088455200195
train gradient:  0.32150436937271526
iteration : 8184
train acc:  0.859375
train loss:  0.3238224387168884
train gradient:  0.16990939028628985
iteration : 8185
train acc:  0.828125
train loss:  0.35599827766418457
train gradient:  0.20410994174045285
iteration : 8186
train acc:  0.7734375
train loss:  0.3923293650150299
train gradient:  0.22529537039749825
iteration : 8187
train acc:  0.8046875
train loss:  0.36685001850128174
train gradient:  0.1331480226273179
iteration : 8188
train acc:  0.84375
train loss:  0.38197940587997437
train gradient:  0.2473163345296568
iteration : 8189
train acc:  0.828125
train loss:  0.3607531487941742
train gradient:  0.15217388186806682
iteration : 8190
train acc:  0.875
train loss:  0.34517863392829895
train gradient:  0.11286244672271016
iteration : 8191
train acc:  0.84375
train loss:  0.3384983539581299
train gradient:  0.16372595263134987
iteration : 8192
train acc:  0.8671875
train loss:  0.31374213099479675
train gradient:  0.15546843575347438
iteration : 8193
train acc:  0.84375
train loss:  0.34343376755714417
train gradient:  0.14550029898402372
iteration : 8194
train acc:  0.875
train loss:  0.27764469385147095
train gradient:  0.12814498403375124
iteration : 8195
train acc:  0.84375
train loss:  0.3390243351459503
train gradient:  0.14632306071327822
iteration : 8196
train acc:  0.84375
train loss:  0.34084552526474
train gradient:  0.21836792656131856
iteration : 8197
train acc:  0.8515625
train loss:  0.32720428705215454
train gradient:  0.18109884391022527
iteration : 8198
train acc:  0.8359375
train loss:  0.33459439873695374
train gradient:  0.14729704427011067
iteration : 8199
train acc:  0.8203125
train loss:  0.4358065724372864
train gradient:  0.2719237938597521
iteration : 8200
train acc:  0.8046875
train loss:  0.38703930377960205
train gradient:  0.21223304989939387
iteration : 8201
train acc:  0.8125
train loss:  0.3728390634059906
train gradient:  0.23760769212720922
iteration : 8202
train acc:  0.8671875
train loss:  0.2821403741836548
train gradient:  0.11449454012291436
iteration : 8203
train acc:  0.859375
train loss:  0.30821678042411804
train gradient:  0.12707774940012276
iteration : 8204
train acc:  0.890625
train loss:  0.2887974679470062
train gradient:  0.13345304679772158
iteration : 8205
train acc:  0.8671875
train loss:  0.26016032695770264
train gradient:  0.11133146569355365
iteration : 8206
train acc:  0.84375
train loss:  0.32521018385887146
train gradient:  0.17415814684329736
iteration : 8207
train acc:  0.8828125
train loss:  0.33186960220336914
train gradient:  0.13714823424117914
iteration : 8208
train acc:  0.859375
train loss:  0.3170645236968994
train gradient:  0.16251291386749112
iteration : 8209
train acc:  0.859375
train loss:  0.33144932985305786
train gradient:  0.20416325055294476
iteration : 8210
train acc:  0.828125
train loss:  0.3738973140716553
train gradient:  0.18105242837548838
iteration : 8211
train acc:  0.859375
train loss:  0.3478218615055084
train gradient:  0.20780473129747423
iteration : 8212
train acc:  0.796875
train loss:  0.3728865385055542
train gradient:  0.1855108719050501
iteration : 8213
train acc:  0.8828125
train loss:  0.3008536100387573
train gradient:  0.11842369786480346
iteration : 8214
train acc:  0.90625
train loss:  0.23160725831985474
train gradient:  0.08419146892215254
iteration : 8215
train acc:  0.859375
train loss:  0.3289203643798828
train gradient:  0.14280262451224168
iteration : 8216
train acc:  0.8359375
train loss:  0.3597189784049988
train gradient:  0.18216873012431573
iteration : 8217
train acc:  0.890625
train loss:  0.3254851698875427
train gradient:  0.2125883597626504
iteration : 8218
train acc:  0.8203125
train loss:  0.39558351039886475
train gradient:  0.34080556446905197
iteration : 8219
train acc:  0.8515625
train loss:  0.30611300468444824
train gradient:  0.18953173021886804
iteration : 8220
train acc:  0.8671875
train loss:  0.28600266575813293
train gradient:  0.21545325165251542
iteration : 8221
train acc:  0.828125
train loss:  0.3477054238319397
train gradient:  0.2323350534535814
iteration : 8222
train acc:  0.8671875
train loss:  0.31124812364578247
train gradient:  0.1943551607604262
iteration : 8223
train acc:  0.8515625
train loss:  0.335499107837677
train gradient:  0.18470934934802416
iteration : 8224
train acc:  0.8671875
train loss:  0.2909797430038452
train gradient:  0.11710474486653062
iteration : 8225
train acc:  0.828125
train loss:  0.3936806917190552
train gradient:  0.2209987106624215
iteration : 8226
train acc:  0.859375
train loss:  0.3294484615325928
train gradient:  0.18206047582915877
iteration : 8227
train acc:  0.84375
train loss:  0.33331239223480225
train gradient:  0.1697413916171837
iteration : 8228
train acc:  0.9140625
train loss:  0.25168460607528687
train gradient:  0.11386838708415531
iteration : 8229
train acc:  0.875
train loss:  0.2899308502674103
train gradient:  0.1479784260971713
iteration : 8230
train acc:  0.8515625
train loss:  0.3982418179512024
train gradient:  0.27043021371478043
iteration : 8231
train acc:  0.859375
train loss:  0.322359561920166
train gradient:  0.1906914284524958
iteration : 8232
train acc:  0.859375
train loss:  0.3280174434185028
train gradient:  0.19642740225213484
iteration : 8233
train acc:  0.8515625
train loss:  0.2914333939552307
train gradient:  0.16096288224179545
iteration : 8234
train acc:  0.7890625
train loss:  0.4086155593395233
train gradient:  0.2424279901645472
iteration : 8235
train acc:  0.8203125
train loss:  0.3842625021934509
train gradient:  0.18735412046027033
iteration : 8236
train acc:  0.875
train loss:  0.2812921702861786
train gradient:  0.09535625959818737
iteration : 8237
train acc:  0.7734375
train loss:  0.49468082189559937
train gradient:  0.2724314085203149
iteration : 8238
train acc:  0.890625
train loss:  0.2951875329017639
train gradient:  0.23893447833509057
iteration : 8239
train acc:  0.828125
train loss:  0.3453235924243927
train gradient:  0.15729639994703315
iteration : 8240
train acc:  0.8203125
train loss:  0.4090336561203003
train gradient:  0.2308715818199007
iteration : 8241
train acc:  0.8828125
train loss:  0.2750661075115204
train gradient:  0.13567754037912
iteration : 8242
train acc:  0.8828125
train loss:  0.2673915922641754
train gradient:  0.12438232114033475
iteration : 8243
train acc:  0.8515625
train loss:  0.3080233931541443
train gradient:  0.2664579126706646
iteration : 8244
train acc:  0.8515625
train loss:  0.3471483290195465
train gradient:  0.22193049131045506
iteration : 8245
train acc:  0.8828125
train loss:  0.3083718419075012
train gradient:  0.1352013770906128
iteration : 8246
train acc:  0.7734375
train loss:  0.3770465850830078
train gradient:  0.2860191828017974
iteration : 8247
train acc:  0.890625
train loss:  0.2869188189506531
train gradient:  0.09978439754970082
iteration : 8248
train acc:  0.8515625
train loss:  0.3236270546913147
train gradient:  0.18677347598506538
iteration : 8249
train acc:  0.8671875
train loss:  0.35339081287384033
train gradient:  0.17006373010338338
iteration : 8250
train acc:  0.8515625
train loss:  0.3319053053855896
train gradient:  0.16019906774253126
iteration : 8251
train acc:  0.875
train loss:  0.3241094946861267
train gradient:  0.16470423979470059
iteration : 8252
train acc:  0.84375
train loss:  0.3770620822906494
train gradient:  0.21126416358517325
iteration : 8253
train acc:  0.828125
train loss:  0.3385862708091736
train gradient:  0.22999666166776153
iteration : 8254
train acc:  0.8203125
train loss:  0.42175787687301636
train gradient:  0.2539833665311787
iteration : 8255
train acc:  0.8125
train loss:  0.3863343298435211
train gradient:  0.22044151835983708
iteration : 8256
train acc:  0.875
train loss:  0.332816481590271
train gradient:  0.23779716579062896
iteration : 8257
train acc:  0.875
train loss:  0.3528963327407837
train gradient:  0.16318141911104217
iteration : 8258
train acc:  0.8125
train loss:  0.3290911912918091
train gradient:  0.21412464952919757
iteration : 8259
train acc:  0.84375
train loss:  0.3133518099784851
train gradient:  0.1386159391150911
iteration : 8260
train acc:  0.84375
train loss:  0.3359682559967041
train gradient:  0.19382075244331448
iteration : 8261
train acc:  0.8984375
train loss:  0.2765910029411316
train gradient:  0.16834962234123035
iteration : 8262
train acc:  0.8359375
train loss:  0.3384248614311218
train gradient:  0.1631028315226004
iteration : 8263
train acc:  0.78125
train loss:  0.3620501756668091
train gradient:  0.19394929914642883
iteration : 8264
train acc:  0.90625
train loss:  0.2513315677642822
train gradient:  0.12921559498614563
iteration : 8265
train acc:  0.8828125
train loss:  0.28378430008888245
train gradient:  0.10383597664454278
iteration : 8266
train acc:  0.90625
train loss:  0.2805231213569641
train gradient:  0.1476151731336666
iteration : 8267
train acc:  0.8828125
train loss:  0.2817033529281616
train gradient:  0.13730538059253825
iteration : 8268
train acc:  0.8515625
train loss:  0.3463408648967743
train gradient:  0.17555776024501502
iteration : 8269
train acc:  0.828125
train loss:  0.3353464603424072
train gradient:  0.21783686484744264
iteration : 8270
train acc:  0.8046875
train loss:  0.38570263981819153
train gradient:  0.18109074485104165
iteration : 8271
train acc:  0.84375
train loss:  0.3558041453361511
train gradient:  0.22397184807081608
iteration : 8272
train acc:  0.8515625
train loss:  0.34079593420028687
train gradient:  0.2818093793572172
iteration : 8273
train acc:  0.828125
train loss:  0.3911019563674927
train gradient:  0.22899833702742933
iteration : 8274
train acc:  0.859375
train loss:  0.3482435643672943
train gradient:  0.22648987258728104
iteration : 8275
train acc:  0.8125
train loss:  0.38494008779525757
train gradient:  0.16494962343919906
iteration : 8276
train acc:  0.8359375
train loss:  0.3703695833683014
train gradient:  0.26581694400476574
iteration : 8277
train acc:  0.8828125
train loss:  0.2910284996032715
train gradient:  0.19619225525585934
iteration : 8278
train acc:  0.8828125
train loss:  0.287201464176178
train gradient:  0.10300222808381558
iteration : 8279
train acc:  0.8671875
train loss:  0.31392624974250793
train gradient:  0.2516579150069225
iteration : 8280
train acc:  0.8359375
train loss:  0.35888731479644775
train gradient:  0.211654441414145
iteration : 8281
train acc:  0.8203125
train loss:  0.3718825578689575
train gradient:  0.30387668997203565
iteration : 8282
train acc:  0.890625
train loss:  0.2802315354347229
train gradient:  0.1486358975732173
iteration : 8283
train acc:  0.859375
train loss:  0.3867109417915344
train gradient:  0.2625626128692814
iteration : 8284
train acc:  0.875
train loss:  0.39702096581459045
train gradient:  0.17733395472256835
iteration : 8285
train acc:  0.8515625
train loss:  0.3428304195404053
train gradient:  0.21976884674713193
iteration : 8286
train acc:  0.8984375
train loss:  0.2820112705230713
train gradient:  0.11538831481979464
iteration : 8287
train acc:  0.859375
train loss:  0.32202938199043274
train gradient:  0.20494677966775182
iteration : 8288
train acc:  0.796875
train loss:  0.4843675494194031
train gradient:  0.3061297721792467
iteration : 8289
train acc:  0.875
train loss:  0.25925981998443604
train gradient:  0.102469038089258
iteration : 8290
train acc:  0.8828125
train loss:  0.31182101368904114
train gradient:  0.14958567749629584
iteration : 8291
train acc:  0.8671875
train loss:  0.36146634817123413
train gradient:  0.25070578163544954
iteration : 8292
train acc:  0.8515625
train loss:  0.36082756519317627
train gradient:  0.14473010204677098
iteration : 8293
train acc:  0.828125
train loss:  0.3815179467201233
train gradient:  0.271373619427999
iteration : 8294
train acc:  0.8515625
train loss:  0.35722485184669495
train gradient:  0.1864639418303151
iteration : 8295
train acc:  0.859375
train loss:  0.2946945130825043
train gradient:  0.1862082451000687
iteration : 8296
train acc:  0.859375
train loss:  0.34638455510139465
train gradient:  0.17876960202212672
iteration : 8297
train acc:  0.859375
train loss:  0.3238217830657959
train gradient:  0.13899181184546322
iteration : 8298
train acc:  0.84375
train loss:  0.3392246663570404
train gradient:  0.16347758967064288
iteration : 8299
train acc:  0.8671875
train loss:  0.3762621283531189
train gradient:  0.2690505727321133
iteration : 8300
train acc:  0.875
train loss:  0.34030455350875854
train gradient:  0.20196073724806152
iteration : 8301
train acc:  0.8671875
train loss:  0.3251136839389801
train gradient:  0.209114364429457
iteration : 8302
train acc:  0.796875
train loss:  0.43209218978881836
train gradient:  0.18514382160365728
iteration : 8303
train acc:  0.921875
train loss:  0.25576022267341614
train gradient:  0.11061325043831112
iteration : 8304
train acc:  0.828125
train loss:  0.3804149329662323
train gradient:  0.2943397225283326
iteration : 8305
train acc:  0.859375
train loss:  0.32995954155921936
train gradient:  0.19658606854812333
iteration : 8306
train acc:  0.796875
train loss:  0.42584937810897827
train gradient:  0.24960322030604642
iteration : 8307
train acc:  0.859375
train loss:  0.29972177743911743
train gradient:  0.18034494738106485
iteration : 8308
train acc:  0.8359375
train loss:  0.4140301048755646
train gradient:  0.244719044712911
iteration : 8309
train acc:  0.859375
train loss:  0.29827630519866943
train gradient:  0.1414455263395573
iteration : 8310
train acc:  0.8984375
train loss:  0.29400187730789185
train gradient:  0.16707722179785184
iteration : 8311
train acc:  0.859375
train loss:  0.3393222987651825
train gradient:  0.18364439693567747
iteration : 8312
train acc:  0.8203125
train loss:  0.3972037434577942
train gradient:  0.29480611071369783
iteration : 8313
train acc:  0.8125
train loss:  0.3930109739303589
train gradient:  0.21864775773807268
iteration : 8314
train acc:  0.8828125
train loss:  0.26911461353302
train gradient:  0.10518390135261518
iteration : 8315
train acc:  0.8359375
train loss:  0.41486263275146484
train gradient:  0.29083384619739366
iteration : 8316
train acc:  0.8671875
train loss:  0.36925655603408813
train gradient:  0.16652038862783747
iteration : 8317
train acc:  0.8046875
train loss:  0.3560154438018799
train gradient:  0.1446693442044943
iteration : 8318
train acc:  0.8125
train loss:  0.4473496377468109
train gradient:  0.3043226977899948
iteration : 8319
train acc:  0.8828125
train loss:  0.27732911705970764
train gradient:  0.1100216376301809
iteration : 8320
train acc:  0.84375
train loss:  0.41474154591560364
train gradient:  0.1735397488889951
iteration : 8321
train acc:  0.890625
train loss:  0.30093899369239807
train gradient:  0.1549672561314974
iteration : 8322
train acc:  0.8359375
train loss:  0.3960791230201721
train gradient:  0.26167830636447925
iteration : 8323
train acc:  0.8671875
train loss:  0.3463086485862732
train gradient:  0.16528380221158329
iteration : 8324
train acc:  0.8359375
train loss:  0.3887668251991272
train gradient:  0.2547724994762912
iteration : 8325
train acc:  0.875
train loss:  0.2976064682006836
train gradient:  0.25869023426157667
iteration : 8326
train acc:  0.7890625
train loss:  0.4537334740161896
train gradient:  0.28423127085063665
iteration : 8327
train acc:  0.8359375
train loss:  0.34918391704559326
train gradient:  0.24799261255304703
iteration : 8328
train acc:  0.8828125
train loss:  0.3456689417362213
train gradient:  0.2337056076253784
iteration : 8329
train acc:  0.828125
train loss:  0.3984079360961914
train gradient:  0.2170056516938527
iteration : 8330
train acc:  0.8125
train loss:  0.34757813811302185
train gradient:  0.1511756194421668
iteration : 8331
train acc:  0.90625
train loss:  0.28611916303634644
train gradient:  0.09736192514291085
iteration : 8332
train acc:  0.8203125
train loss:  0.38176196813583374
train gradient:  0.19742205938258028
iteration : 8333
train acc:  0.875
train loss:  0.30474579334259033
train gradient:  0.18754609678052364
iteration : 8334
train acc:  0.8203125
train loss:  0.36734241247177124
train gradient:  0.17812076703688193
iteration : 8335
train acc:  0.78125
train loss:  0.3831247091293335
train gradient:  0.17236767419815074
iteration : 8336
train acc:  0.8515625
train loss:  0.356361448764801
train gradient:  0.18827529004598392
iteration : 8337
train acc:  0.890625
train loss:  0.29612961411476135
train gradient:  0.16894226794666822
iteration : 8338
train acc:  0.796875
train loss:  0.4552973806858063
train gradient:  0.2353392801599491
iteration : 8339
train acc:  0.890625
train loss:  0.36376953125
train gradient:  0.2689673122148713
iteration : 8340
train acc:  0.8828125
train loss:  0.2884191870689392
train gradient:  0.13080923626307606
iteration : 8341
train acc:  0.8203125
train loss:  0.44004470109939575
train gradient:  0.24474651855416826
iteration : 8342
train acc:  0.921875
train loss:  0.2535938322544098
train gradient:  0.09244003842471545
iteration : 8343
train acc:  0.8828125
train loss:  0.3721926510334015
train gradient:  0.148329826430799
iteration : 8344
train acc:  0.890625
train loss:  0.3007102608680725
train gradient:  0.11666440509081259
iteration : 8345
train acc:  0.8671875
train loss:  0.3665022850036621
train gradient:  0.16432972301138393
iteration : 8346
train acc:  0.828125
train loss:  0.3555562496185303
train gradient:  0.14384669641386544
iteration : 8347
train acc:  0.859375
train loss:  0.34232980012893677
train gradient:  0.13843353108910972
iteration : 8348
train acc:  0.875
train loss:  0.2865599989891052
train gradient:  0.13862676429271015
iteration : 8349
train acc:  0.8671875
train loss:  0.31509268283843994
train gradient:  0.10589585935454134
iteration : 8350
train acc:  0.8671875
train loss:  0.39466872811317444
train gradient:  0.1766332118252108
iteration : 8351
train acc:  0.859375
train loss:  0.32453441619873047
train gradient:  0.13322447620611128
iteration : 8352
train acc:  0.8359375
train loss:  0.42483967542648315
train gradient:  0.312045035423408
iteration : 8353
train acc:  0.8671875
train loss:  0.3658851683139801
train gradient:  0.22052476836671642
iteration : 8354
train acc:  0.890625
train loss:  0.28245872259140015
train gradient:  0.09991776957515888
iteration : 8355
train acc:  0.8125
train loss:  0.34391388297080994
train gradient:  0.17244934668110423
iteration : 8356
train acc:  0.8671875
train loss:  0.2866252064704895
train gradient:  0.22712033594793585
iteration : 8357
train acc:  0.8828125
train loss:  0.289059042930603
train gradient:  0.11588754135176796
iteration : 8358
train acc:  0.8515625
train loss:  0.34002357721328735
train gradient:  0.1419375369602178
iteration : 8359
train acc:  0.84375
train loss:  0.356793612241745
train gradient:  0.14410826605984917
iteration : 8360
train acc:  0.8203125
train loss:  0.3810867667198181
train gradient:  0.1615748384182143
iteration : 8361
train acc:  0.8515625
train loss:  0.3113383948802948
train gradient:  0.11375427652916312
iteration : 8362
train acc:  0.8359375
train loss:  0.3812515139579773
train gradient:  0.14720962963945378
iteration : 8363
train acc:  0.8359375
train loss:  0.3363335132598877
train gradient:  0.19485789536890408
iteration : 8364
train acc:  0.8046875
train loss:  0.3874951899051666
train gradient:  0.16143374022414553
iteration : 8365
train acc:  0.890625
train loss:  0.2752182185649872
train gradient:  0.18090391821201537
iteration : 8366
train acc:  0.8203125
train loss:  0.37818655371665955
train gradient:  0.15352722071719596
iteration : 8367
train acc:  0.921875
train loss:  0.27428096532821655
train gradient:  0.10869914723622066
iteration : 8368
train acc:  0.828125
train loss:  0.36506620049476624
train gradient:  0.21761897174318012
iteration : 8369
train acc:  0.8671875
train loss:  0.3175237476825714
train gradient:  0.10053464491532012
iteration : 8370
train acc:  0.84375
train loss:  0.32934775948524475
train gradient:  0.14874671087999103
iteration : 8371
train acc:  0.890625
train loss:  0.3233983516693115
train gradient:  0.12520835588688878
iteration : 8372
train acc:  0.8828125
train loss:  0.2521003186702728
train gradient:  0.12779154829297468
iteration : 8373
train acc:  0.8515625
train loss:  0.3813672661781311
train gradient:  0.16592600543224392
iteration : 8374
train acc:  0.9296875
train loss:  0.2629600763320923
train gradient:  0.1298404091124954
iteration : 8375
train acc:  0.8203125
train loss:  0.4744223356246948
train gradient:  0.32168551605708634
iteration : 8376
train acc:  0.8828125
train loss:  0.2951008975505829
train gradient:  0.11440911649432063
iteration : 8377
train acc:  0.8515625
train loss:  0.3182200789451599
train gradient:  0.12933090423043397
iteration : 8378
train acc:  0.859375
train loss:  0.34007197618484497
train gradient:  0.1937693341028786
iteration : 8379
train acc:  0.8515625
train loss:  0.3450753688812256
train gradient:  0.15056090175258685
iteration : 8380
train acc:  0.859375
train loss:  0.3282260298728943
train gradient:  0.18574296796638942
iteration : 8381
train acc:  0.875
train loss:  0.3221942186355591
train gradient:  0.12359947565379305
iteration : 8382
train acc:  0.8359375
train loss:  0.3644995391368866
train gradient:  0.1990683880281548
iteration : 8383
train acc:  0.8828125
train loss:  0.27570950984954834
train gradient:  0.12696165545300436
iteration : 8384
train acc:  0.875
train loss:  0.29853153228759766
train gradient:  0.14302782053553864
iteration : 8385
train acc:  0.9375
train loss:  0.22294940054416656
train gradient:  0.12416147297989943
iteration : 8386
train acc:  0.8828125
train loss:  0.3109622299671173
train gradient:  0.15275681098563137
iteration : 8387
train acc:  0.8515625
train loss:  0.356465607881546
train gradient:  0.2312028607108959
iteration : 8388
train acc:  0.8515625
train loss:  0.3783704936504364
train gradient:  0.25983748415023483
iteration : 8389
train acc:  0.8359375
train loss:  0.35913974046707153
train gradient:  0.17335853056446027
iteration : 8390
train acc:  0.8046875
train loss:  0.377244770526886
train gradient:  0.227744209431265
iteration : 8391
train acc:  0.921875
train loss:  0.22771936655044556
train gradient:  0.11800854363654618
iteration : 8392
train acc:  0.8046875
train loss:  0.36493706703186035
train gradient:  0.20859226019908084
iteration : 8393
train acc:  0.890625
train loss:  0.26631617546081543
train gradient:  0.12168366446877257
iteration : 8394
train acc:  0.8046875
train loss:  0.39910900592803955
train gradient:  0.19319178865852532
iteration : 8395
train acc:  0.8203125
train loss:  0.3942826986312866
train gradient:  0.21312858458842826
iteration : 8396
train acc:  0.90625
train loss:  0.2847161889076233
train gradient:  0.12374899064237525
iteration : 8397
train acc:  0.8515625
train loss:  0.3269384503364563
train gradient:  0.16284883548174828
iteration : 8398
train acc:  0.8203125
train loss:  0.36275383830070496
train gradient:  0.1653677761082284
iteration : 8399
train acc:  0.78125
train loss:  0.39859285950660706
train gradient:  0.2200305472050363
iteration : 8400
train acc:  0.875
train loss:  0.2635834515094757
train gradient:  0.12463466365187534
iteration : 8401
train acc:  0.8203125
train loss:  0.4193100035190582
train gradient:  0.2633171388138321
iteration : 8402
train acc:  0.875
train loss:  0.27095144987106323
train gradient:  0.12396821197257903
iteration : 8403
train acc:  0.8671875
train loss:  0.2708277106285095
train gradient:  0.11256615951695216
iteration : 8404
train acc:  0.8359375
train loss:  0.3836050033569336
train gradient:  0.29835422547635615
iteration : 8405
train acc:  0.8515625
train loss:  0.30138325691223145
train gradient:  0.15175998353876322
iteration : 8406
train acc:  0.8203125
train loss:  0.41129356622695923
train gradient:  0.2102965215652074
iteration : 8407
train acc:  0.828125
train loss:  0.3438066840171814
train gradient:  0.18255478775193218
iteration : 8408
train acc:  0.8671875
train loss:  0.3603026866912842
train gradient:  0.21418562935262997
iteration : 8409
train acc:  0.7890625
train loss:  0.40583860874176025
train gradient:  0.29511360940203885
iteration : 8410
train acc:  0.8984375
train loss:  0.2459278404712677
train gradient:  0.12018645601337308
iteration : 8411
train acc:  0.9140625
train loss:  0.31106117367744446
train gradient:  0.1651880999296989
iteration : 8412
train acc:  0.8515625
train loss:  0.3076496720314026
train gradient:  0.23932983670951102
iteration : 8413
train acc:  0.796875
train loss:  0.39132213592529297
train gradient:  0.18855269809304792
iteration : 8414
train acc:  0.8046875
train loss:  0.3965917229652405
train gradient:  0.20541432744937901
iteration : 8415
train acc:  0.875
train loss:  0.29845312237739563
train gradient:  0.21709271358063326
iteration : 8416
train acc:  0.8671875
train loss:  0.28751808404922485
train gradient:  0.1398762903519139
iteration : 8417
train acc:  0.8671875
train loss:  0.32204341888427734
train gradient:  0.12898615993389603
iteration : 8418
train acc:  0.8125
train loss:  0.3635414242744446
train gradient:  0.26327018291808607
iteration : 8419
train acc:  0.7890625
train loss:  0.4391954839229584
train gradient:  0.27610479814363703
iteration : 8420
train acc:  0.8359375
train loss:  0.3800318241119385
train gradient:  0.2444727877494412
iteration : 8421
train acc:  0.8125
train loss:  0.4184540808200836
train gradient:  0.26165828624987775
iteration : 8422
train acc:  0.8671875
train loss:  0.31508132815361023
train gradient:  0.1657786046277213
iteration : 8423
train acc:  0.8671875
train loss:  0.3211209177970886
train gradient:  0.17796024532185684
iteration : 8424
train acc:  0.8828125
train loss:  0.31124407052993774
train gradient:  0.15163802200973414
iteration : 8425
train acc:  0.8515625
train loss:  0.35501790046691895
train gradient:  0.12672596040973133
iteration : 8426
train acc:  0.8828125
train loss:  0.29690268635749817
train gradient:  0.15065260212448872
iteration : 8427
train acc:  0.8984375
train loss:  0.2507033050060272
train gradient:  0.09877885158811595
iteration : 8428
train acc:  0.8515625
train loss:  0.34367257356643677
train gradient:  0.1510372709231322
iteration : 8429
train acc:  0.90625
train loss:  0.25661078095436096
train gradient:  0.10495634867068418
iteration : 8430
train acc:  0.875
train loss:  0.29011306166648865
train gradient:  0.15909577637185615
iteration : 8431
train acc:  0.859375
train loss:  0.37452930212020874
train gradient:  0.17855409130768762
iteration : 8432
train acc:  0.8359375
train loss:  0.40307891368865967
train gradient:  0.29328085959126254
iteration : 8433
train acc:  0.8828125
train loss:  0.3025268316268921
train gradient:  0.19857237022859855
iteration : 8434
train acc:  0.859375
train loss:  0.31225061416625977
train gradient:  0.16179691181873412
iteration : 8435
train acc:  0.8671875
train loss:  0.3187752664089203
train gradient:  0.16550686341878088
iteration : 8436
train acc:  0.8359375
train loss:  0.3796755075454712
train gradient:  0.23859922662004485
iteration : 8437
train acc:  0.875
train loss:  0.27649569511413574
train gradient:  0.1309078895193656
iteration : 8438
train acc:  0.875
train loss:  0.30841436982154846
train gradient:  0.11335471679500214
iteration : 8439
train acc:  0.8359375
train loss:  0.36100131273269653
train gradient:  0.1918590985876238
iteration : 8440
train acc:  0.84375
train loss:  0.3401933014392853
train gradient:  0.16626762594988878
iteration : 8441
train acc:  0.8125
train loss:  0.3874381184577942
train gradient:  0.2205914004320031
iteration : 8442
train acc:  0.8515625
train loss:  0.39821815490722656
train gradient:  0.21286818545366007
iteration : 8443
train acc:  0.828125
train loss:  0.3887141942977905
train gradient:  0.23837866348700823
iteration : 8444
train acc:  0.8359375
train loss:  0.35797154903411865
train gradient:  0.15846843393644577
iteration : 8445
train acc:  0.8984375
train loss:  0.2688648998737335
train gradient:  0.12804503848306803
iteration : 8446
train acc:  0.796875
train loss:  0.4942860007286072
train gradient:  0.3372009435237034
iteration : 8447
train acc:  0.859375
train loss:  0.30668777227401733
train gradient:  0.18616255650437644
iteration : 8448
train acc:  0.8046875
train loss:  0.3981340527534485
train gradient:  0.24428119399931614
iteration : 8449
train acc:  0.84375
train loss:  0.3958032727241516
train gradient:  0.28922589455428516
iteration : 8450
train acc:  0.84375
train loss:  0.32905611395835876
train gradient:  0.15150397891473694
iteration : 8451
train acc:  0.8828125
train loss:  0.30698251724243164
train gradient:  0.15472493849585245
iteration : 8452
train acc:  0.828125
train loss:  0.37963804602622986
train gradient:  0.19697621909287039
iteration : 8453
train acc:  0.859375
train loss:  0.301975816488266
train gradient:  0.15491618291630319
iteration : 8454
train acc:  0.8671875
train loss:  0.30032336711883545
train gradient:  0.16252061618125907
iteration : 8455
train acc:  0.8359375
train loss:  0.3509988784790039
train gradient:  0.158508962460443
iteration : 8456
train acc:  0.8203125
train loss:  0.3341975212097168
train gradient:  0.14428317912943628
iteration : 8457
train acc:  0.796875
train loss:  0.4005530774593353
train gradient:  0.19138635273781018
iteration : 8458
train acc:  0.859375
train loss:  0.30914002656936646
train gradient:  0.18313415694462334
iteration : 8459
train acc:  0.8828125
train loss:  0.29235339164733887
train gradient:  0.11230483799721659
iteration : 8460
train acc:  0.8125
train loss:  0.3563067317008972
train gradient:  0.22192807848640073
iteration : 8461
train acc:  0.859375
train loss:  0.32950282096862793
train gradient:  0.15736950575129172
iteration : 8462
train acc:  0.8359375
train loss:  0.3150043487548828
train gradient:  0.1851094127830623
iteration : 8463
train acc:  0.8671875
train loss:  0.3082950711250305
train gradient:  0.24301683104348792
iteration : 8464
train acc:  0.8828125
train loss:  0.3005906343460083
train gradient:  0.13243037805815736
iteration : 8465
train acc:  0.8671875
train loss:  0.2831406593322754
train gradient:  0.10313744832073199
iteration : 8466
train acc:  0.8671875
train loss:  0.2835335433483124
train gradient:  0.15784653462497364
iteration : 8467
train acc:  0.859375
train loss:  0.3095361590385437
train gradient:  0.22123501741694784
iteration : 8468
train acc:  0.828125
train loss:  0.38759100437164307
train gradient:  0.272812828201577
iteration : 8469
train acc:  0.859375
train loss:  0.33330821990966797
train gradient:  0.15428677513045921
iteration : 8470
train acc:  0.8828125
train loss:  0.3120766878128052
train gradient:  0.1652366212150012
iteration : 8471
train acc:  0.8125
train loss:  0.4063452184200287
train gradient:  0.2271588209243051
iteration : 8472
train acc:  0.8515625
train loss:  0.3369143605232239
train gradient:  0.18607170970007875
iteration : 8473
train acc:  0.8671875
train loss:  0.286016047000885
train gradient:  0.18169194331332414
iteration : 8474
train acc:  0.796875
train loss:  0.38629570603370667
train gradient:  0.207189466126215
iteration : 8475
train acc:  0.828125
train loss:  0.3303079605102539
train gradient:  0.18574344162970763
iteration : 8476
train acc:  0.8984375
train loss:  0.2578720152378082
train gradient:  0.14193865581354417
iteration : 8477
train acc:  0.8671875
train loss:  0.3176012933254242
train gradient:  0.1716050655426503
iteration : 8478
train acc:  0.8671875
train loss:  0.32359325885772705
train gradient:  0.1524754289590274
iteration : 8479
train acc:  0.828125
train loss:  0.3977941870689392
train gradient:  0.2546858505978311
iteration : 8480
train acc:  0.859375
train loss:  0.25814250111579895
train gradient:  0.165881031350022
iteration : 8481
train acc:  0.84375
train loss:  0.3596020042896271
train gradient:  0.1646205102670925
iteration : 8482
train acc:  0.796875
train loss:  0.4633449614048004
train gradient:  0.3303533591148948
iteration : 8483
train acc:  0.8828125
train loss:  0.2891434133052826
train gradient:  0.16889495515827208
iteration : 8484
train acc:  0.828125
train loss:  0.35501235723495483
train gradient:  0.14972316654279744
iteration : 8485
train acc:  0.84375
train loss:  0.334265798330307
train gradient:  0.22887010465392668
iteration : 8486
train acc:  0.828125
train loss:  0.30716031789779663
train gradient:  0.16211457838627177
iteration : 8487
train acc:  0.828125
train loss:  0.3951646685600281
train gradient:  0.21070193615799743
iteration : 8488
train acc:  0.8359375
train loss:  0.3312433660030365
train gradient:  0.19872309516229336
iteration : 8489
train acc:  0.8671875
train loss:  0.327264666557312
train gradient:  0.23494763484952014
iteration : 8490
train acc:  0.875
train loss:  0.3149852156639099
train gradient:  0.13237141055706028
iteration : 8491
train acc:  0.859375
train loss:  0.2922743558883667
train gradient:  0.11820642469267366
iteration : 8492
train acc:  0.8671875
train loss:  0.3552810549736023
train gradient:  0.15686361758614437
iteration : 8493
train acc:  0.90625
train loss:  0.27166885137557983
train gradient:  0.09949685152220135
iteration : 8494
train acc:  0.8828125
train loss:  0.308249294757843
train gradient:  0.21989106296561917
iteration : 8495
train acc:  0.8125
train loss:  0.3944540321826935
train gradient:  0.1904843078619094
iteration : 8496
train acc:  0.8828125
train loss:  0.28922659158706665
train gradient:  0.17109921036568843
iteration : 8497
train acc:  0.8828125
train loss:  0.3008238971233368
train gradient:  0.15513362517354296
iteration : 8498
train acc:  0.890625
train loss:  0.2503693699836731
train gradient:  0.10127200491983078
iteration : 8499
train acc:  0.890625
train loss:  0.2530122995376587
train gradient:  0.11098157190483275
iteration : 8500
train acc:  0.84375
train loss:  0.34843093156814575
train gradient:  0.18809145672043592
iteration : 8501
train acc:  0.84375
train loss:  0.34333351254463196
train gradient:  0.16649218216630562
iteration : 8502
train acc:  0.9140625
train loss:  0.2787034511566162
train gradient:  0.15274732281395215
iteration : 8503
train acc:  0.90625
train loss:  0.254366934299469
train gradient:  0.11771382586190565
iteration : 8504
train acc:  0.84375
train loss:  0.37187856435775757
train gradient:  0.18934718065646822
iteration : 8505
train acc:  0.875
train loss:  0.3397015929222107
train gradient:  0.2100825129786028
iteration : 8506
train acc:  0.8515625
train loss:  0.3013817071914673
train gradient:  0.2219219927951784
iteration : 8507
train acc:  0.78125
train loss:  0.38984525203704834
train gradient:  0.26263557519967917
iteration : 8508
train acc:  0.8359375
train loss:  0.38629257678985596
train gradient:  0.19576969821584533
iteration : 8509
train acc:  0.8828125
train loss:  0.33995962142944336
train gradient:  0.16901772662117565
iteration : 8510
train acc:  0.9140625
train loss:  0.269340455532074
train gradient:  0.1073596812350493
iteration : 8511
train acc:  0.84375
train loss:  0.3720685839653015
train gradient:  0.1744126165163965
iteration : 8512
train acc:  0.8203125
train loss:  0.4001094102859497
train gradient:  0.22326205317198816
iteration : 8513
train acc:  0.8828125
train loss:  0.30450311303138733
train gradient:  0.1711991474715913
iteration : 8514
train acc:  0.859375
train loss:  0.3015982508659363
train gradient:  0.16686658028528195
iteration : 8515
train acc:  0.859375
train loss:  0.28384512662887573
train gradient:  0.13616914843764372
iteration : 8516
train acc:  0.890625
train loss:  0.2648317813873291
train gradient:  0.12612601419504038
iteration : 8517
train acc:  0.9140625
train loss:  0.24602557718753815
train gradient:  0.09662285728870548
iteration : 8518
train acc:  0.8515625
train loss:  0.3663424849510193
train gradient:  0.1894742593126972
iteration : 8519
train acc:  0.8671875
train loss:  0.3118414580821991
train gradient:  0.13167738628408954
iteration : 8520
train acc:  0.84375
train loss:  0.28291797637939453
train gradient:  0.13818022721098466
iteration : 8521
train acc:  0.8828125
train loss:  0.3905026614665985
train gradient:  0.23482234961606388
iteration : 8522
train acc:  0.859375
train loss:  0.3070497512817383
train gradient:  0.17244701989359473
iteration : 8523
train acc:  0.8203125
train loss:  0.4115558862686157
train gradient:  0.22577595490128236
iteration : 8524
train acc:  0.875
train loss:  0.319526731967926
train gradient:  0.17294115906696875
iteration : 8525
train acc:  0.8203125
train loss:  0.43888092041015625
train gradient:  0.25666924962573506
iteration : 8526
train acc:  0.8515625
train loss:  0.36006200313568115
train gradient:  0.16629313514516703
iteration : 8527
train acc:  0.8828125
train loss:  0.3458045721054077
train gradient:  0.21178217482250875
iteration : 8528
train acc:  0.8671875
train loss:  0.285156786441803
train gradient:  0.1292920186264403
iteration : 8529
train acc:  0.8125
train loss:  0.44225218892097473
train gradient:  0.21182426183801795
iteration : 8530
train acc:  0.875
train loss:  0.327922523021698
train gradient:  0.1368810670184771
iteration : 8531
train acc:  0.765625
train loss:  0.4593527615070343
train gradient:  0.30621642914881203
iteration : 8532
train acc:  0.8359375
train loss:  0.3734223544597626
train gradient:  0.22161653377527868
iteration : 8533
train acc:  0.8203125
train loss:  0.42582499980926514
train gradient:  0.20620289293161886
iteration : 8534
train acc:  0.875
train loss:  0.30972999334335327
train gradient:  0.14468589882720628
iteration : 8535
train acc:  0.8515625
train loss:  0.38900870084762573
train gradient:  0.22806604812334602
iteration : 8536
train acc:  0.8046875
train loss:  0.3982296586036682
train gradient:  0.1985361494555225
iteration : 8537
train acc:  0.8125
train loss:  0.3928470015525818
train gradient:  0.2589540819158918
iteration : 8538
train acc:  0.8984375
train loss:  0.27445441484451294
train gradient:  0.12210229314478761
iteration : 8539
train acc:  0.875
train loss:  0.351093053817749
train gradient:  0.1987059714841696
iteration : 8540
train acc:  0.84375
train loss:  0.3974248468875885
train gradient:  0.20643397347554135
iteration : 8541
train acc:  0.8046875
train loss:  0.40125399827957153
train gradient:  0.19444255945863237
iteration : 8542
train acc:  0.8359375
train loss:  0.37147295475006104
train gradient:  0.22957570773837507
iteration : 8543
train acc:  0.8125
train loss:  0.3362067639827728
train gradient:  0.13606259496678724
iteration : 8544
train acc:  0.8125
train loss:  0.35773420333862305
train gradient:  0.2289616255562159
iteration : 8545
train acc:  0.8671875
train loss:  0.2945699095726013
train gradient:  0.3280284947022924
iteration : 8546
train acc:  0.875
train loss:  0.3144919276237488
train gradient:  0.1276645629563215
iteration : 8547
train acc:  0.84375
train loss:  0.33606988191604614
train gradient:  0.18463792966252984
iteration : 8548
train acc:  0.8515625
train loss:  0.324828565120697
train gradient:  0.1121064458975028
iteration : 8549
train acc:  0.875
train loss:  0.30564016103744507
train gradient:  0.12599146425183633
iteration : 8550
train acc:  0.8984375
train loss:  0.2688392400741577
train gradient:  0.15235904417998686
iteration : 8551
train acc:  0.859375
train loss:  0.3196149468421936
train gradient:  0.16708568792572695
iteration : 8552
train acc:  0.84375
train loss:  0.3581945300102234
train gradient:  0.14485251187480586
iteration : 8553
train acc:  0.7890625
train loss:  0.43569353222846985
train gradient:  0.24937283526953158
iteration : 8554
train acc:  0.7890625
train loss:  0.38823971152305603
train gradient:  0.17234149217689954
iteration : 8555
train acc:  0.8984375
train loss:  0.2788255214691162
train gradient:  0.12490204909299048
iteration : 8556
train acc:  0.8671875
train loss:  0.3566122055053711
train gradient:  0.14986738508438704
iteration : 8557
train acc:  0.8515625
train loss:  0.35788536071777344
train gradient:  0.21849247028765645
iteration : 8558
train acc:  0.8828125
train loss:  0.2623891234397888
train gradient:  0.12372672604283477
iteration : 8559
train acc:  0.859375
train loss:  0.3198530077934265
train gradient:  0.16623935381692345
iteration : 8560
train acc:  0.828125
train loss:  0.40936732292175293
train gradient:  0.24050170314742542
iteration : 8561
train acc:  0.9140625
train loss:  0.2321871668100357
train gradient:  0.08994104071943455
iteration : 8562
train acc:  0.8671875
train loss:  0.2924184799194336
train gradient:  0.1724230174110265
iteration : 8563
train acc:  0.8515625
train loss:  0.305200457572937
train gradient:  0.16929544744346925
iteration : 8564
train acc:  0.8828125
train loss:  0.2940199077129364
train gradient:  0.1558817172067345
iteration : 8565
train acc:  0.828125
train loss:  0.35760819911956787
train gradient:  0.21184402405338265
iteration : 8566
train acc:  0.84375
train loss:  0.3391677439212799
train gradient:  0.16856323793216477
iteration : 8567
train acc:  0.8046875
train loss:  0.4543740749359131
train gradient:  0.36967234269645394
iteration : 8568
train acc:  0.84375
train loss:  0.3316801190376282
train gradient:  0.15036504903784786
iteration : 8569
train acc:  0.796875
train loss:  0.42889413237571716
train gradient:  0.1891275730456599
iteration : 8570
train acc:  0.859375
train loss:  0.30615538358688354
train gradient:  0.15299946793075375
iteration : 8571
train acc:  0.796875
train loss:  0.4490300714969635
train gradient:  0.24751213441682432
iteration : 8572
train acc:  0.84375
train loss:  0.3008004426956177
train gradient:  0.11723151409707497
iteration : 8573
train acc:  0.8515625
train loss:  0.3551967740058899
train gradient:  0.17907001423295313
iteration : 8574
train acc:  0.859375
train loss:  0.4164268672466278
train gradient:  0.3083276531582661
iteration : 8575
train acc:  0.8125
train loss:  0.35212525725364685
train gradient:  0.23123015796755292
iteration : 8576
train acc:  0.8671875
train loss:  0.2975967526435852
train gradient:  0.16029028446084598
iteration : 8577
train acc:  0.828125
train loss:  0.3849368691444397
train gradient:  0.20909091655365503
iteration : 8578
train acc:  0.890625
train loss:  0.3410075902938843
train gradient:  0.2084066702272575
iteration : 8579
train acc:  0.8828125
train loss:  0.2884712815284729
train gradient:  0.09206910107749919
iteration : 8580
train acc:  0.84375
train loss:  0.32855749130249023
train gradient:  0.15804594931754773
iteration : 8581
train acc:  0.84375
train loss:  0.297909140586853
train gradient:  0.15997275804705924
iteration : 8582
train acc:  0.8671875
train loss:  0.3728259801864624
train gradient:  0.2225594935978764
iteration : 8583
train acc:  0.8515625
train loss:  0.38096654415130615
train gradient:  0.16153393930470986
iteration : 8584
train acc:  0.828125
train loss:  0.34927624464035034
train gradient:  0.15419122607135793
iteration : 8585
train acc:  0.828125
train loss:  0.3584110140800476
train gradient:  0.16019823821858048
iteration : 8586
train acc:  0.8671875
train loss:  0.3064115047454834
train gradient:  0.12729316886052522
iteration : 8587
train acc:  0.875
train loss:  0.28023794293403625
train gradient:  0.11100682369537174
iteration : 8588
train acc:  0.8671875
train loss:  0.2957669496536255
train gradient:  0.10926723029882038
iteration : 8589
train acc:  0.8125
train loss:  0.35847410559654236
train gradient:  0.28190810373411895
iteration : 8590
train acc:  0.8359375
train loss:  0.3697589039802551
train gradient:  0.18596882167886505
iteration : 8591
train acc:  0.796875
train loss:  0.46046099066734314
train gradient:  0.31944549398916483
iteration : 8592
train acc:  0.8828125
train loss:  0.2593134641647339
train gradient:  0.13778395800062598
iteration : 8593
train acc:  0.828125
train loss:  0.37905681133270264
train gradient:  0.1428654040730703
iteration : 8594
train acc:  0.8515625
train loss:  0.3250870406627655
train gradient:  0.1355968232443362
iteration : 8595
train acc:  0.7890625
train loss:  0.43858206272125244
train gradient:  0.30336041097995736
iteration : 8596
train acc:  0.890625
train loss:  0.3158760964870453
train gradient:  0.17111515593931598
iteration : 8597
train acc:  0.8515625
train loss:  0.31082209944725037
train gradient:  0.18839017557519916
iteration : 8598
train acc:  0.90625
train loss:  0.27539321780204773
train gradient:  0.09605910896856895
iteration : 8599
train acc:  0.875
train loss:  0.2976309061050415
train gradient:  0.15678295131987713
iteration : 8600
train acc:  0.890625
train loss:  0.25993114709854126
train gradient:  0.10664924783342454
iteration : 8601
train acc:  0.828125
train loss:  0.3503117561340332
train gradient:  0.15754682172236986
iteration : 8602
train acc:  0.8828125
train loss:  0.33339056372642517
train gradient:  0.12172172069322384
iteration : 8603
train acc:  0.875
train loss:  0.3449183404445648
train gradient:  0.15048093915223512
iteration : 8604
train acc:  0.875
train loss:  0.3453768491744995
train gradient:  0.14391501065461204
iteration : 8605
train acc:  0.875
train loss:  0.34411779046058655
train gradient:  0.2589234706216572
iteration : 8606
train acc:  0.796875
train loss:  0.3975690007209778
train gradient:  0.1793427039723518
iteration : 8607
train acc:  0.875
train loss:  0.3007843494415283
train gradient:  0.15742971288297852
iteration : 8608
train acc:  0.890625
train loss:  0.29383349418640137
train gradient:  0.19139289621528227
iteration : 8609
train acc:  0.796875
train loss:  0.3946591019630432
train gradient:  0.27606689057553346
iteration : 8610
train acc:  0.8828125
train loss:  0.3104704022407532
train gradient:  0.1428756958225785
iteration : 8611
train acc:  0.84375
train loss:  0.3501568138599396
train gradient:  0.20841001364966566
iteration : 8612
train acc:  0.8515625
train loss:  0.36594951152801514
train gradient:  0.20595888093157527
iteration : 8613
train acc:  0.8671875
train loss:  0.2920992374420166
train gradient:  0.1371130545767928
iteration : 8614
train acc:  0.8203125
train loss:  0.3704614043235779
train gradient:  0.18206223374035835
iteration : 8615
train acc:  0.8828125
train loss:  0.25713685154914856
train gradient:  0.11988580626711828
iteration : 8616
train acc:  0.859375
train loss:  0.29509037733078003
train gradient:  0.12056719524333687
iteration : 8617
train acc:  0.859375
train loss:  0.34079739451408386
train gradient:  0.17679318607038014
iteration : 8618
train acc:  0.8515625
train loss:  0.3239789605140686
train gradient:  0.19796054774848865
iteration : 8619
train acc:  0.875
train loss:  0.2881494164466858
train gradient:  0.1614274629277346
iteration : 8620
train acc:  0.9296875
train loss:  0.21789683401584625
train gradient:  0.1289803862182302
iteration : 8621
train acc:  0.9375
train loss:  0.2400951385498047
train gradient:  0.09552542867480117
iteration : 8622
train acc:  0.8671875
train loss:  0.3235308527946472
train gradient:  0.21676813433994357
iteration : 8623
train acc:  0.8359375
train loss:  0.3921988606452942
train gradient:  0.18772332243357331
iteration : 8624
train acc:  0.90625
train loss:  0.28036922216415405
train gradient:  0.1177056304387075
iteration : 8625
train acc:  0.8515625
train loss:  0.33503836393356323
train gradient:  0.21942860046383797
iteration : 8626
train acc:  0.8203125
train loss:  0.3641165792942047
train gradient:  0.21956855858941934
iteration : 8627
train acc:  0.8828125
train loss:  0.29783838987350464
train gradient:  0.11879668244407204
iteration : 8628
train acc:  0.8359375
train loss:  0.36023199558258057
train gradient:  0.19347211669946385
iteration : 8629
train acc:  0.8125
train loss:  0.42254823446273804
train gradient:  0.41524850366627314
iteration : 8630
train acc:  0.8046875
train loss:  0.36181190609931946
train gradient:  0.21693285884961688
iteration : 8631
train acc:  0.84375
train loss:  0.38329288363456726
train gradient:  0.23418791246524917
iteration : 8632
train acc:  0.8046875
train loss:  0.4321661591529846
train gradient:  0.23534060503923143
iteration : 8633
train acc:  0.9296875
train loss:  0.24955391883850098
train gradient:  0.12885428157067189
iteration : 8634
train acc:  0.7734375
train loss:  0.44518113136291504
train gradient:  0.29345990421162266
iteration : 8635
train acc:  0.8671875
train loss:  0.31611528992652893
train gradient:  0.11301852340318588
iteration : 8636
train acc:  0.890625
train loss:  0.2789030969142914
train gradient:  0.09927510815672841
iteration : 8637
train acc:  0.8046875
train loss:  0.3520454168319702
train gradient:  0.1649719665829008
iteration : 8638
train acc:  0.8984375
train loss:  0.2717112898826599
train gradient:  0.1465895928874333
iteration : 8639
train acc:  0.890625
train loss:  0.28883096575737
train gradient:  0.12598242492225953
iteration : 8640
train acc:  0.78125
train loss:  0.44405895471572876
train gradient:  0.32082133529483675
iteration : 8641
train acc:  0.859375
train loss:  0.29613763093948364
train gradient:  0.13360922624211685
iteration : 8642
train acc:  0.828125
train loss:  0.34939056634902954
train gradient:  0.22825229988909218
iteration : 8643
train acc:  0.8671875
train loss:  0.3869013786315918
train gradient:  0.2745191062011865
iteration : 8644
train acc:  0.953125
train loss:  0.1935615837574005
train gradient:  0.11437053693467875
iteration : 8645
train acc:  0.7578125
train loss:  0.4376257359981537
train gradient:  0.24039697656722583
iteration : 8646
train acc:  0.828125
train loss:  0.34461352229118347
train gradient:  0.1851295133385008
iteration : 8647
train acc:  0.8125
train loss:  0.3422250747680664
train gradient:  0.19869273280123825
iteration : 8648
train acc:  0.8671875
train loss:  0.2926212549209595
train gradient:  0.13506577543247067
iteration : 8649
train acc:  0.875
train loss:  0.2968742847442627
train gradient:  0.13409921486246112
iteration : 8650
train acc:  0.8984375
train loss:  0.24348792433738708
train gradient:  0.1088689023721218
iteration : 8651
train acc:  0.8828125
train loss:  0.2668607831001282
train gradient:  0.11734350824491857
iteration : 8652
train acc:  0.859375
train loss:  0.344880610704422
train gradient:  0.22164400578916943
iteration : 8653
train acc:  0.875
train loss:  0.3601752817630768
train gradient:  0.2928294812765941
iteration : 8654
train acc:  0.84375
train loss:  0.33954930305480957
train gradient:  0.2074347160573663
iteration : 8655
train acc:  0.90625
train loss:  0.24452243745326996
train gradient:  0.1443439460380484
iteration : 8656
train acc:  0.921875
train loss:  0.23110751807689667
train gradient:  0.199481501684162
iteration : 8657
train acc:  0.8671875
train loss:  0.30407288670539856
train gradient:  0.11610641436227205
iteration : 8658
train acc:  0.8515625
train loss:  0.3539348244667053
train gradient:  0.19051766677797666
iteration : 8659
train acc:  0.8671875
train loss:  0.3150220513343811
train gradient:  0.17406619387282266
iteration : 8660
train acc:  0.8828125
train loss:  0.33527684211730957
train gradient:  0.1791015329482608
iteration : 8661
train acc:  0.859375
train loss:  0.3327518403530121
train gradient:  0.19832632433001485
iteration : 8662
train acc:  0.890625
train loss:  0.28944751620292664
train gradient:  0.1491845827705307
iteration : 8663
train acc:  0.8515625
train loss:  0.3078867793083191
train gradient:  0.2227876455708595
iteration : 8664
train acc:  0.859375
train loss:  0.3151669502258301
train gradient:  0.1492779026315275
iteration : 8665
train acc:  0.859375
train loss:  0.352615088224411
train gradient:  0.13866111979710738
iteration : 8666
train acc:  0.8671875
train loss:  0.31517910957336426
train gradient:  0.1428469207215447
iteration : 8667
train acc:  0.8671875
train loss:  0.28036820888519287
train gradient:  0.13234595621634296
iteration : 8668
train acc:  0.8203125
train loss:  0.35692062973976135
train gradient:  0.2025758624474728
iteration : 8669
train acc:  0.828125
train loss:  0.37749943137168884
train gradient:  0.24086740382408026
iteration : 8670
train acc:  0.84375
train loss:  0.3264196515083313
train gradient:  0.23321338519083484
iteration : 8671
train acc:  0.8984375
train loss:  0.2703564763069153
train gradient:  0.12812142727019676
iteration : 8672
train acc:  0.828125
train loss:  0.32011109590530396
train gradient:  0.18268036521613423
iteration : 8673
train acc:  0.8671875
train loss:  0.39646872878074646
train gradient:  0.2901249198675868
iteration : 8674
train acc:  0.8125
train loss:  0.42290741205215454
train gradient:  0.2980292203326508
iteration : 8675
train acc:  0.8203125
train loss:  0.3991837501525879
train gradient:  0.1833443556451282
iteration : 8676
train acc:  0.890625
train loss:  0.29453855752944946
train gradient:  0.1450681706394026
iteration : 8677
train acc:  0.859375
train loss:  0.3080531358718872
train gradient:  0.14655339644246224
iteration : 8678
train acc:  0.8359375
train loss:  0.3905147314071655
train gradient:  0.18996061448111884
iteration : 8679
train acc:  0.8515625
train loss:  0.28901731967926025
train gradient:  0.1471335765137037
iteration : 8680
train acc:  0.8203125
train loss:  0.3620698153972626
train gradient:  0.21694992651038195
iteration : 8681
train acc:  0.8359375
train loss:  0.36475494503974915
train gradient:  0.26081235570658606
iteration : 8682
train acc:  0.796875
train loss:  0.395000159740448
train gradient:  0.25866917384131793
iteration : 8683
train acc:  0.921875
train loss:  0.2374172955751419
train gradient:  0.09026278385795355
iteration : 8684
train acc:  0.9453125
train loss:  0.2195151448249817
train gradient:  0.12869940420485504
iteration : 8685
train acc:  0.8203125
train loss:  0.3632313013076782
train gradient:  0.17086704686369303
iteration : 8686
train acc:  0.828125
train loss:  0.32814720273017883
train gradient:  0.19739815544757827
iteration : 8687
train acc:  0.8984375
train loss:  0.28858208656311035
train gradient:  0.14533827336215757
iteration : 8688
train acc:  0.859375
train loss:  0.32449162006378174
train gradient:  0.17072012839405717
iteration : 8689
train acc:  0.8515625
train loss:  0.3263699412345886
train gradient:  0.16083460564530816
iteration : 8690
train acc:  0.7890625
train loss:  0.44363197684288025
train gradient:  0.2682399717125397
iteration : 8691
train acc:  0.8515625
train loss:  0.3112959861755371
train gradient:  0.1411886517240147
iteration : 8692
train acc:  0.8125
train loss:  0.3997983932495117
train gradient:  0.24327323579338647
iteration : 8693
train acc:  0.8828125
train loss:  0.3374311923980713
train gradient:  0.16159748416444536
iteration : 8694
train acc:  0.84375
train loss:  0.3582379221916199
train gradient:  0.15078740380186306
iteration : 8695
train acc:  0.8359375
train loss:  0.3222056031227112
train gradient:  0.13255549335873668
iteration : 8696
train acc:  0.84375
train loss:  0.3853076696395874
train gradient:  0.22980845987680507
iteration : 8697
train acc:  0.84375
train loss:  0.3027747571468353
train gradient:  0.1064933246863166
iteration : 8698
train acc:  0.8671875
train loss:  0.2806614637374878
train gradient:  0.21173212348349646
iteration : 8699
train acc:  0.8125
train loss:  0.36575204133987427
train gradient:  0.2935089666132591
iteration : 8700
train acc:  0.859375
train loss:  0.37280896306037903
train gradient:  0.1795539376190738
iteration : 8701
train acc:  0.828125
train loss:  0.3890702724456787
train gradient:  0.23228921406152858
iteration : 8702
train acc:  0.859375
train loss:  0.3831493854522705
train gradient:  0.2325878908212941
iteration : 8703
train acc:  0.8359375
train loss:  0.40319642424583435
train gradient:  0.23113614999083618
iteration : 8704
train acc:  0.84375
train loss:  0.35715407133102417
train gradient:  0.17661789199237488
iteration : 8705
train acc:  0.8046875
train loss:  0.36239659786224365
train gradient:  0.17911305217477697
iteration : 8706
train acc:  0.8203125
train loss:  0.4673588275909424
train gradient:  0.30900307940794236
iteration : 8707
train acc:  0.8671875
train loss:  0.2627246081829071
train gradient:  0.12522557157084363
iteration : 8708
train acc:  0.8984375
train loss:  0.28271135687828064
train gradient:  0.11630209415300916
iteration : 8709
train acc:  0.859375
train loss:  0.31644880771636963
train gradient:  0.1314878199827525
iteration : 8710
train acc:  0.859375
train loss:  0.35655397176742554
train gradient:  0.17847383111608248
iteration : 8711
train acc:  0.8828125
train loss:  0.2843499779701233
train gradient:  0.13986777749169052
iteration : 8712
train acc:  0.8828125
train loss:  0.3059266209602356
train gradient:  0.1745209062647613
iteration : 8713
train acc:  0.8984375
train loss:  0.25388675928115845
train gradient:  0.11561008474143751
iteration : 8714
train acc:  0.875
train loss:  0.33309125900268555
train gradient:  0.1671320184983634
iteration : 8715
train acc:  0.9296875
train loss:  0.22070831060409546
train gradient:  0.09771503523211025
iteration : 8716
train acc:  0.8359375
train loss:  0.41463518142700195
train gradient:  0.2356147357877647
iteration : 8717
train acc:  0.890625
train loss:  0.3280703127384186
train gradient:  0.11403223212862172
iteration : 8718
train acc:  0.875
train loss:  0.3025785982608795
train gradient:  0.1671061079174749
iteration : 8719
train acc:  0.8046875
train loss:  0.43993133306503296
train gradient:  0.3397056677574795
iteration : 8720
train acc:  0.8125
train loss:  0.34602946043014526
train gradient:  0.20722175456436925
iteration : 8721
train acc:  0.8359375
train loss:  0.362795889377594
train gradient:  0.13675042424522088
iteration : 8722
train acc:  0.84375
train loss:  0.31695300340652466
train gradient:  0.1400830767257851
iteration : 8723
train acc:  0.8671875
train loss:  0.33805471658706665
train gradient:  0.14495063973185202
iteration : 8724
train acc:  0.890625
train loss:  0.2774919271469116
train gradient:  0.13870940693123
iteration : 8725
train acc:  0.8671875
train loss:  0.32088136672973633
train gradient:  0.17224326568722462
iteration : 8726
train acc:  0.8359375
train loss:  0.32063528895378113
train gradient:  0.16261800418045275
iteration : 8727
train acc:  0.796875
train loss:  0.4120957851409912
train gradient:  0.27710124570225897
iteration : 8728
train acc:  0.890625
train loss:  0.2828913927078247
train gradient:  0.1202890387309455
iteration : 8729
train acc:  0.8125
train loss:  0.43449655175209045
train gradient:  0.23339553918960132
iteration : 8730
train acc:  0.84375
train loss:  0.35309648513793945
train gradient:  0.15628260456019166
iteration : 8731
train acc:  0.828125
train loss:  0.3836069405078888
train gradient:  0.27675676826345935
iteration : 8732
train acc:  0.890625
train loss:  0.240411639213562
train gradient:  0.16046307601255932
iteration : 8733
train acc:  0.8984375
train loss:  0.2900847792625427
train gradient:  0.1056096290548775
iteration : 8734
train acc:  0.84375
train loss:  0.36037206649780273
train gradient:  0.20364794765188032
iteration : 8735
train acc:  0.8203125
train loss:  0.3739885091781616
train gradient:  0.19303080679872217
iteration : 8736
train acc:  0.890625
train loss:  0.2799615263938904
train gradient:  0.1411943533428443
iteration : 8737
train acc:  0.8671875
train loss:  0.31600436568260193
train gradient:  0.14939915388246072
iteration : 8738
train acc:  0.828125
train loss:  0.40469449758529663
train gradient:  0.21591344406048033
iteration : 8739
train acc:  0.9140625
train loss:  0.2744870185852051
train gradient:  0.18888410315850906
iteration : 8740
train acc:  0.9140625
train loss:  0.25273066759109497
train gradient:  0.12812901811210708
iteration : 8741
train acc:  0.84375
train loss:  0.37309208512306213
train gradient:  0.27072735606869774
iteration : 8742
train acc:  0.8203125
train loss:  0.36013609170913696
train gradient:  0.21318167918767403
iteration : 8743
train acc:  0.8671875
train loss:  0.3001970648765564
train gradient:  0.19294513723382195
iteration : 8744
train acc:  0.8125
train loss:  0.383681058883667
train gradient:  0.37930025909057496
iteration : 8745
train acc:  0.875
train loss:  0.3136358857154846
train gradient:  0.16869559869220582
iteration : 8746
train acc:  0.84375
train loss:  0.38814041018486023
train gradient:  0.18153466332655463
iteration : 8747
train acc:  0.890625
train loss:  0.2706855833530426
train gradient:  0.1604464492702015
iteration : 8748
train acc:  0.8515625
train loss:  0.3620988130569458
train gradient:  0.2528703759798151
iteration : 8749
train acc:  0.8359375
train loss:  0.3322402238845825
train gradient:  0.2456174052291496
iteration : 8750
train acc:  0.859375
train loss:  0.35632574558258057
train gradient:  0.1834417251081419
iteration : 8751
train acc:  0.8828125
train loss:  0.23656725883483887
train gradient:  0.12906724938596387
iteration : 8752
train acc:  0.8515625
train loss:  0.34599560499191284
train gradient:  0.15636974091463107
iteration : 8753
train acc:  0.84375
train loss:  0.3649439811706543
train gradient:  0.1806938107528946
iteration : 8754
train acc:  0.8359375
train loss:  0.29458087682724
train gradient:  0.1269477173682249
iteration : 8755
train acc:  0.859375
train loss:  0.3676756024360657
train gradient:  0.15991156189989794
iteration : 8756
train acc:  0.828125
train loss:  0.3690512776374817
train gradient:  0.19445635306511633
iteration : 8757
train acc:  0.8359375
train loss:  0.3330956697463989
train gradient:  0.22463345092561082
iteration : 8758
train acc:  0.8671875
train loss:  0.34576618671417236
train gradient:  0.17664444529585724
iteration : 8759
train acc:  0.8046875
train loss:  0.3730478584766388
train gradient:  0.271835916570075
iteration : 8760
train acc:  0.8828125
train loss:  0.3360051214694977
train gradient:  0.1935842780262212
iteration : 8761
train acc:  0.8515625
train loss:  0.37333396077156067
train gradient:  0.18431841877810856
iteration : 8762
train acc:  0.8515625
train loss:  0.3717593550682068
train gradient:  0.23307297079666617
iteration : 8763
train acc:  0.875
train loss:  0.2855997085571289
train gradient:  0.13523810648704082
iteration : 8764
train acc:  0.890625
train loss:  0.28888994455337524
train gradient:  0.1271865068676466
iteration : 8765
train acc:  0.8203125
train loss:  0.4031370282173157
train gradient:  0.24653759009180878
iteration : 8766
train acc:  0.84375
train loss:  0.40681564807891846
train gradient:  0.2094960844494441
iteration : 8767
train acc:  0.828125
train loss:  0.38930580019950867
train gradient:  0.24012542553794208
iteration : 8768
train acc:  0.84375
train loss:  0.35482853651046753
train gradient:  0.14883212086423148
iteration : 8769
train acc:  0.828125
train loss:  0.4464396834373474
train gradient:  0.21046612131003836
iteration : 8770
train acc:  0.84375
train loss:  0.4243989884853363
train gradient:  0.23249736509188582
iteration : 8771
train acc:  0.859375
train loss:  0.31109368801116943
train gradient:  0.1856697907451087
iteration : 8772
train acc:  0.8515625
train loss:  0.3366113603115082
train gradient:  0.1524712771831418
iteration : 8773
train acc:  0.875
train loss:  0.30474624037742615
train gradient:  0.1158118415002428
iteration : 8774
train acc:  0.890625
train loss:  0.3521476686000824
train gradient:  0.18083454505725777
iteration : 8775
train acc:  0.8984375
train loss:  0.2741367816925049
train gradient:  0.09156309118706134
iteration : 8776
train acc:  0.8125
train loss:  0.39864665269851685
train gradient:  0.1732945929735883
iteration : 8777
train acc:  0.828125
train loss:  0.4100933372974396
train gradient:  0.3051739994082022
iteration : 8778
train acc:  0.8359375
train loss:  0.34938716888427734
train gradient:  0.18005508834570685
iteration : 8779
train acc:  0.875
train loss:  0.29265403747558594
train gradient:  0.16245657122133833
iteration : 8780
train acc:  0.9140625
train loss:  0.29872623085975647
train gradient:  0.16703897711865273
iteration : 8781
train acc:  0.8984375
train loss:  0.2733265459537506
train gradient:  0.1256086419650004
iteration : 8782
train acc:  0.890625
train loss:  0.2443104386329651
train gradient:  0.10120747317972607
iteration : 8783
train acc:  0.8359375
train loss:  0.3388243317604065
train gradient:  0.19137415738754102
iteration : 8784
train acc:  0.8359375
train loss:  0.3663143515586853
train gradient:  0.14716517562285913
iteration : 8785
train acc:  0.859375
train loss:  0.3406175374984741
train gradient:  0.16042334965175803
iteration : 8786
train acc:  0.8359375
train loss:  0.36245661973953247
train gradient:  0.19287024471964226
iteration : 8787
train acc:  0.890625
train loss:  0.3191090226173401
train gradient:  0.14220449589574918
iteration : 8788
train acc:  0.875
train loss:  0.305768221616745
train gradient:  0.14108658694110193
iteration : 8789
train acc:  0.84375
train loss:  0.3606995940208435
train gradient:  0.3116475933847004
iteration : 8790
train acc:  0.828125
train loss:  0.37287357449531555
train gradient:  0.2639700591872302
iteration : 8791
train acc:  0.8515625
train loss:  0.290554940700531
train gradient:  0.17596674203917756
iteration : 8792
train acc:  0.8828125
train loss:  0.2825763523578644
train gradient:  0.10491773457066086
iteration : 8793
train acc:  0.8671875
train loss:  0.29831212759017944
train gradient:  0.13015012372508472
iteration : 8794
train acc:  0.8828125
train loss:  0.3350558876991272
train gradient:  0.25676752370869116
iteration : 8795
train acc:  0.7890625
train loss:  0.44765275716781616
train gradient:  0.41061190060528246
iteration : 8796
train acc:  0.890625
train loss:  0.2967431843280792
train gradient:  0.1980500633685312
iteration : 8797
train acc:  0.84375
train loss:  0.3675919473171234
train gradient:  0.42031739999509765
iteration : 8798
train acc:  0.875
train loss:  0.3254888951778412
train gradient:  0.21069637058062263
iteration : 8799
train acc:  0.828125
train loss:  0.3396686315536499
train gradient:  0.18165588058109924
iteration : 8800
train acc:  0.921875
train loss:  0.25498154759407043
train gradient:  0.15688321263788355
iteration : 8801
train acc:  0.8359375
train loss:  0.3155129849910736
train gradient:  0.21639848790678967
iteration : 8802
train acc:  0.8203125
train loss:  0.3657551407814026
train gradient:  0.25971322502071026
iteration : 8803
train acc:  0.8515625
train loss:  0.3286229372024536
train gradient:  0.19350268447759866
iteration : 8804
train acc:  0.8125
train loss:  0.4240342974662781
train gradient:  0.25606420438171695
iteration : 8805
train acc:  0.8671875
train loss:  0.30307620763778687
train gradient:  0.17564929136064958
iteration : 8806
train acc:  0.8125
train loss:  0.43836259841918945
train gradient:  0.20095084577919695
iteration : 8807
train acc:  0.875
train loss:  0.2693171501159668
train gradient:  0.11758753733686526
iteration : 8808
train acc:  0.84375
train loss:  0.3198947310447693
train gradient:  0.16598469657632992
iteration : 8809
train acc:  0.8203125
train loss:  0.3272966146469116
train gradient:  0.15443360373795706
iteration : 8810
train acc:  0.828125
train loss:  0.31733596324920654
train gradient:  0.18061969049462578
iteration : 8811
train acc:  0.859375
train loss:  0.306975394487381
train gradient:  0.1945607939787515
iteration : 8812
train acc:  0.875
train loss:  0.2658233642578125
train gradient:  0.14793028399173283
iteration : 8813
train acc:  0.8359375
train loss:  0.3341714143753052
train gradient:  0.19805342110608104
iteration : 8814
train acc:  0.859375
train loss:  0.3226155638694763
train gradient:  0.18620078500804824
iteration : 8815
train acc:  0.8671875
train loss:  0.3657692074775696
train gradient:  0.18807097393534558
iteration : 8816
train acc:  0.8828125
train loss:  0.28154754638671875
train gradient:  0.11218229867557908
iteration : 8817
train acc:  0.8203125
train loss:  0.3719005882740021
train gradient:  0.17645646862636205
iteration : 8818
train acc:  0.859375
train loss:  0.35041582584381104
train gradient:  0.18973534400032796
iteration : 8819
train acc:  0.7734375
train loss:  0.42010533809661865
train gradient:  0.26188163278896504
iteration : 8820
train acc:  0.828125
train loss:  0.3627225160598755
train gradient:  0.19360811700474162
iteration : 8821
train acc:  0.890625
train loss:  0.25512874126434326
train gradient:  0.09805767004291965
iteration : 8822
train acc:  0.8828125
train loss:  0.29315507411956787
train gradient:  0.14191384382163524
iteration : 8823
train acc:  0.859375
train loss:  0.30589914321899414
train gradient:  0.13503659317489233
iteration : 8824
train acc:  0.8984375
train loss:  0.2909003496170044
train gradient:  0.19402813182138096
iteration : 8825
train acc:  0.8203125
train loss:  0.3662220537662506
train gradient:  0.22971266924146988
iteration : 8826
train acc:  0.875
train loss:  0.2579765021800995
train gradient:  0.1945124313420962
iteration : 8827
train acc:  0.8515625
train loss:  0.31350553035736084
train gradient:  0.20277250855439397
iteration : 8828
train acc:  0.90625
train loss:  0.30872875452041626
train gradient:  0.17719808640323637
iteration : 8829
train acc:  0.84375
train loss:  0.38159728050231934
train gradient:  0.18809688289513055
iteration : 8830
train acc:  0.8984375
train loss:  0.30852243304252625
train gradient:  0.12032632341499447
iteration : 8831
train acc:  0.859375
train loss:  0.357098251581192
train gradient:  0.20173011416296294
iteration : 8832
train acc:  0.875
train loss:  0.3137657046318054
train gradient:  0.16333669340358148
iteration : 8833
train acc:  0.859375
train loss:  0.3508549630641937
train gradient:  0.1840444894918451
iteration : 8834
train acc:  0.9140625
train loss:  0.2659040689468384
train gradient:  0.10389484855541499
iteration : 8835
train acc:  0.8671875
train loss:  0.3265964388847351
train gradient:  0.16095071414753442
iteration : 8836
train acc:  0.8828125
train loss:  0.29618504643440247
train gradient:  0.1845911878882407
iteration : 8837
train acc:  0.8046875
train loss:  0.40893101692199707
train gradient:  0.28774864472281536
iteration : 8838
train acc:  0.8515625
train loss:  0.3341829478740692
train gradient:  0.16216518029615243
iteration : 8839
train acc:  0.890625
train loss:  0.33692312240600586
train gradient:  0.19668358431233585
iteration : 8840
train acc:  0.8671875
train loss:  0.3291449546813965
train gradient:  0.18101983644471203
iteration : 8841
train acc:  0.890625
train loss:  0.29765135049819946
train gradient:  0.15395030507598317
iteration : 8842
train acc:  0.8046875
train loss:  0.4102100431919098
train gradient:  0.2691557436787275
iteration : 8843
train acc:  0.8671875
train loss:  0.34295234084129333
train gradient:  0.1757657684276268
iteration : 8844
train acc:  0.8671875
train loss:  0.31938621401786804
train gradient:  0.2607955782417114
iteration : 8845
train acc:  0.8515625
train loss:  0.3165287375450134
train gradient:  0.160778833838651
iteration : 8846
train acc:  0.8984375
train loss:  0.2958207130432129
train gradient:  0.1337569500615297
iteration : 8847
train acc:  0.8515625
train loss:  0.41532665491104126
train gradient:  0.30676142600267065
iteration : 8848
train acc:  0.8359375
train loss:  0.4113568067550659
train gradient:  0.2758244596840837
iteration : 8849
train acc:  0.8671875
train loss:  0.3304285407066345
train gradient:  0.18641780369858676
iteration : 8850
train acc:  0.859375
train loss:  0.37316614389419556
train gradient:  0.18638431746766904
iteration : 8851
train acc:  0.8203125
train loss:  0.40677186846733093
train gradient:  0.199661413408299
iteration : 8852
train acc:  0.8828125
train loss:  0.316223680973053
train gradient:  0.256516949536937
iteration : 8853
train acc:  0.875
train loss:  0.29195311665534973
train gradient:  0.1458769658580601
iteration : 8854
train acc:  0.8359375
train loss:  0.34137576818466187
train gradient:  0.15617911046715924
iteration : 8855
train acc:  0.8984375
train loss:  0.2621980607509613
train gradient:  0.2038725377472323
iteration : 8856
train acc:  0.9140625
train loss:  0.2754618525505066
train gradient:  0.10227626243134987
iteration : 8857
train acc:  0.875
train loss:  0.2580707371234894
train gradient:  0.14762584209781898
iteration : 8858
train acc:  0.8359375
train loss:  0.35015082359313965
train gradient:  0.19825633889304972
iteration : 8859
train acc:  0.8515625
train loss:  0.31454625725746155
train gradient:  0.16991887247689808
iteration : 8860
train acc:  0.8828125
train loss:  0.29849088191986084
train gradient:  0.16549948330868175
iteration : 8861
train acc:  0.875
train loss:  0.2750557065010071
train gradient:  0.13173859788154835
iteration : 8862
train acc:  0.84375
train loss:  0.3619476556777954
train gradient:  0.20980869509059116
iteration : 8863
train acc:  0.8359375
train loss:  0.3663368225097656
train gradient:  0.2779392356478082
iteration : 8864
train acc:  0.875
train loss:  0.3283213675022125
train gradient:  0.30534958897069786
iteration : 8865
train acc:  0.8671875
train loss:  0.3697237968444824
train gradient:  0.17859221005128678
iteration : 8866
train acc:  0.921875
train loss:  0.20487593114376068
train gradient:  0.09415671338455298
iteration : 8867
train acc:  0.8515625
train loss:  0.36080119013786316
train gradient:  0.20809349933835386
iteration : 8868
train acc:  0.7734375
train loss:  0.4526744782924652
train gradient:  0.28017241633384105
iteration : 8869
train acc:  0.796875
train loss:  0.36920830607414246
train gradient:  0.22028758947917793
iteration : 8870
train acc:  0.859375
train loss:  0.3802299499511719
train gradient:  0.2738989514729991
iteration : 8871
train acc:  0.84375
train loss:  0.3571203351020813
train gradient:  0.1532234063103817
iteration : 8872
train acc:  0.8671875
train loss:  0.26193463802337646
train gradient:  0.12893143789394795
iteration : 8873
train acc:  0.8828125
train loss:  0.2802412509918213
train gradient:  0.14594978773513917
iteration : 8874
train acc:  0.875
train loss:  0.325479656457901
train gradient:  0.22505686960101212
iteration : 8875
train acc:  0.859375
train loss:  0.3348101079463959
train gradient:  0.19775728849830562
iteration : 8876
train acc:  0.828125
train loss:  0.33124515414237976
train gradient:  0.19614927127342557
iteration : 8877
train acc:  0.875
train loss:  0.31749290227890015
train gradient:  0.18333376087238376
iteration : 8878
train acc:  0.8515625
train loss:  0.32674479484558105
train gradient:  0.1661031809902423
iteration : 8879
train acc:  0.8515625
train loss:  0.3154754042625427
train gradient:  0.23309069683511424
iteration : 8880
train acc:  0.84375
train loss:  0.3575436472892761
train gradient:  0.21771762775626305
iteration : 8881
train acc:  0.8359375
train loss:  0.35683292150497437
train gradient:  0.18506680589751878
iteration : 8882
train acc:  0.8125
train loss:  0.36366012692451477
train gradient:  0.18310398650661436
iteration : 8883
train acc:  0.8359375
train loss:  0.36448389291763306
train gradient:  0.1776440310853192
iteration : 8884
train acc:  0.84375
train loss:  0.37531283497810364
train gradient:  0.17981320068612236
iteration : 8885
train acc:  0.8828125
train loss:  0.323594331741333
train gradient:  0.16904254328063506
iteration : 8886
train acc:  0.8359375
train loss:  0.4133795201778412
train gradient:  0.2188091995039274
iteration : 8887
train acc:  0.8671875
train loss:  0.3181241750717163
train gradient:  0.20411857578172468
iteration : 8888
train acc:  0.8203125
train loss:  0.40637534856796265
train gradient:  0.2336141306107053
iteration : 8889
train acc:  0.8828125
train loss:  0.27559900283813477
train gradient:  0.15301725389404747
iteration : 8890
train acc:  0.8515625
train loss:  0.32221877574920654
train gradient:  0.1529953457651307
iteration : 8891
train acc:  0.8203125
train loss:  0.3756534457206726
train gradient:  0.16508832212253738
iteration : 8892
train acc:  0.8671875
train loss:  0.33904528617858887
train gradient:  0.12811238416020904
iteration : 8893
train acc:  0.890625
train loss:  0.28705090284347534
train gradient:  0.15268499728030502
iteration : 8894
train acc:  0.7890625
train loss:  0.3863324820995331
train gradient:  0.16936052527950432
iteration : 8895
train acc:  0.875
train loss:  0.3230022192001343
train gradient:  0.18660031538252256
iteration : 8896
train acc:  0.9296875
train loss:  0.2603108286857605
train gradient:  0.1073308687540217
iteration : 8897
train acc:  0.8515625
train loss:  0.3215857744216919
train gradient:  0.14659957865504236
iteration : 8898
train acc:  0.8515625
train loss:  0.3381802439689636
train gradient:  0.13326847223194077
iteration : 8899
train acc:  0.8515625
train loss:  0.29898184537887573
train gradient:  0.17255031635957221
iteration : 8900
train acc:  0.8515625
train loss:  0.35951703786849976
train gradient:  0.16808316845052051
iteration : 8901
train acc:  0.8515625
train loss:  0.27569249272346497
train gradient:  0.12757331557274398
iteration : 8902
train acc:  0.8828125
train loss:  0.30018749833106995
train gradient:  0.13452687853947096
iteration : 8903
train acc:  0.859375
train loss:  0.31916379928588867
train gradient:  0.16690003745490034
iteration : 8904
train acc:  0.84375
train loss:  0.3759087324142456
train gradient:  0.18644373835849412
iteration : 8905
train acc:  0.8515625
train loss:  0.3038164973258972
train gradient:  0.1609360698917353
iteration : 8906
train acc:  0.875
train loss:  0.29184502363204956
train gradient:  0.11404937287579707
iteration : 8907
train acc:  0.828125
train loss:  0.3473600745201111
train gradient:  0.23524292718823675
iteration : 8908
train acc:  0.875
train loss:  0.34118953347206116
train gradient:  0.1630532881669492
iteration : 8909
train acc:  0.8984375
train loss:  0.25941622257232666
train gradient:  0.09633698009973939
iteration : 8910
train acc:  0.8359375
train loss:  0.36921101808547974
train gradient:  0.20170189475096378
iteration : 8911
train acc:  0.8671875
train loss:  0.35282742977142334
train gradient:  0.16636446325819418
iteration : 8912
train acc:  0.8515625
train loss:  0.31101763248443604
train gradient:  0.1504170579388812
iteration : 8913
train acc:  0.8515625
train loss:  0.30584773421287537
train gradient:  0.13554023628267492
iteration : 8914
train acc:  0.8984375
train loss:  0.2860984802246094
train gradient:  0.13526243686906056
iteration : 8915
train acc:  0.921875
train loss:  0.2590813636779785
train gradient:  0.179663237693449
iteration : 8916
train acc:  0.890625
train loss:  0.2559053897857666
train gradient:  0.10334771880836592
iteration : 8917
train acc:  0.8359375
train loss:  0.3667733371257782
train gradient:  0.28729938967949414
iteration : 8918
train acc:  0.8046875
train loss:  0.41390275955200195
train gradient:  0.2760513083233989
iteration : 8919
train acc:  0.921875
train loss:  0.23532186448574066
train gradient:  0.0900263898500603
iteration : 8920
train acc:  0.859375
train loss:  0.3681613802909851
train gradient:  0.2517770637036364
iteration : 8921
train acc:  0.875
train loss:  0.33879151940345764
train gradient:  0.26250307221413216
iteration : 8922
train acc:  0.8828125
train loss:  0.31710079312324524
train gradient:  0.1463107833621335
iteration : 8923
train acc:  0.84375
train loss:  0.33363670110702515
train gradient:  0.21085518981869755
iteration : 8924
train acc:  0.8828125
train loss:  0.24863819777965546
train gradient:  0.09126211472269002
iteration : 8925
train acc:  0.8359375
train loss:  0.3742757737636566
train gradient:  0.21949350709202722
iteration : 8926
train acc:  0.8671875
train loss:  0.3472750186920166
train gradient:  0.20790976231563035
iteration : 8927
train acc:  0.875
train loss:  0.36788976192474365
train gradient:  0.19512310284469636
iteration : 8928
train acc:  0.8046875
train loss:  0.42498302459716797
train gradient:  0.34998591574081633
iteration : 8929
train acc:  0.8671875
train loss:  0.28562161326408386
train gradient:  0.18944841859406267
iteration : 8930
train acc:  0.8828125
train loss:  0.3187039792537689
train gradient:  0.14413832263494059
iteration : 8931
train acc:  0.859375
train loss:  0.3360416889190674
train gradient:  0.16224740656313036
iteration : 8932
train acc:  0.7890625
train loss:  0.43588483333587646
train gradient:  0.2901890206114452
iteration : 8933
train acc:  0.875
train loss:  0.28070294857025146
train gradient:  0.13787697538492383
iteration : 8934
train acc:  0.8125
train loss:  0.4361022710800171
train gradient:  0.2771344252195619
iteration : 8935
train acc:  0.828125
train loss:  0.3697826564311981
train gradient:  0.253784824270389
iteration : 8936
train acc:  0.8671875
train loss:  0.32111144065856934
train gradient:  0.15338233180728889
iteration : 8937
train acc:  0.8515625
train loss:  0.3602175712585449
train gradient:  0.18248522622299107
iteration : 8938
train acc:  0.8671875
train loss:  0.34509795904159546
train gradient:  0.16823888751218355
iteration : 8939
train acc:  0.8828125
train loss:  0.2811419367790222
train gradient:  0.11339816062272515
iteration : 8940
train acc:  0.84375
train loss:  0.3487113118171692
train gradient:  0.1734051048063107
iteration : 8941
train acc:  0.8359375
train loss:  0.34892910718917847
train gradient:  0.17072081404791514
iteration : 8942
train acc:  0.8203125
train loss:  0.39086055755615234
train gradient:  0.1890634259882509
iteration : 8943
train acc:  0.890625
train loss:  0.31822875142097473
train gradient:  0.1357742315163189
iteration : 8944
train acc:  0.875
train loss:  0.30047309398651123
train gradient:  0.11458633545952039
iteration : 8945
train acc:  0.8203125
train loss:  0.3719022870063782
train gradient:  0.16742129189727195
iteration : 8946
train acc:  0.84375
train loss:  0.31456053256988525
train gradient:  0.17318350557639223
iteration : 8947
train acc:  0.8515625
train loss:  0.3232156038284302
train gradient:  0.1933790249592942
iteration : 8948
train acc:  0.8203125
train loss:  0.3928206264972687
train gradient:  0.2953179116650674
iteration : 8949
train acc:  0.8828125
train loss:  0.34546852111816406
train gradient:  0.18344304701744413
iteration : 8950
train acc:  0.8671875
train loss:  0.28939753770828247
train gradient:  0.11889710365570619
iteration : 8951
train acc:  0.8828125
train loss:  0.26911661028862
train gradient:  0.19200546698460924
iteration : 8952
train acc:  0.890625
train loss:  0.32563912868499756
train gradient:  0.14888149828290775
iteration : 8953
train acc:  0.875
train loss:  0.311397910118103
train gradient:  0.16865033079586272
iteration : 8954
train acc:  0.875
train loss:  0.29839253425598145
train gradient:  0.15734188206529082
iteration : 8955
train acc:  0.8359375
train loss:  0.30289608240127563
train gradient:  0.22887654209769748
iteration : 8956
train acc:  0.8359375
train loss:  0.3828261196613312
train gradient:  0.22953059432172776
iteration : 8957
train acc:  0.8671875
train loss:  0.37337225675582886
train gradient:  0.2043091090617615
iteration : 8958
train acc:  0.875
train loss:  0.29193171858787537
train gradient:  0.15289515743176058
iteration : 8959
train acc:  0.8125
train loss:  0.386574923992157
train gradient:  0.17132698835097235
iteration : 8960
train acc:  0.828125
train loss:  0.30203425884246826
train gradient:  0.1397305826180577
iteration : 8961
train acc:  0.90625
train loss:  0.2594301104545593
train gradient:  0.09093894947378597
iteration : 8962
train acc:  0.78125
train loss:  0.47386863827705383
train gradient:  0.2720833990281009
iteration : 8963
train acc:  0.84375
train loss:  0.3789534270763397
train gradient:  0.42145459677203817
iteration : 8964
train acc:  0.8515625
train loss:  0.3355328142642975
train gradient:  0.15942617932756165
iteration : 8965
train acc:  0.859375
train loss:  0.3301178216934204
train gradient:  0.15123795875693702
iteration : 8966
train acc:  0.859375
train loss:  0.3327990770339966
train gradient:  0.13354500601087932
iteration : 8967
train acc:  0.796875
train loss:  0.49366113543510437
train gradient:  0.3031485169136489
iteration : 8968
train acc:  0.8359375
train loss:  0.3626598119735718
train gradient:  0.15718066412717055
iteration : 8969
train acc:  0.8515625
train loss:  0.32935652136802673
train gradient:  0.14119018420703294
iteration : 8970
train acc:  0.84375
train loss:  0.33715885877609253
train gradient:  0.14721408774229597
iteration : 8971
train acc:  0.875
train loss:  0.2841333746910095
train gradient:  0.10989430454995938
iteration : 8972
train acc:  0.828125
train loss:  0.45146387815475464
train gradient:  0.21393106118652305
iteration : 8973
train acc:  0.8671875
train loss:  0.31992536783218384
train gradient:  0.21020078811478757
iteration : 8974
train acc:  0.8359375
train loss:  0.3836507797241211
train gradient:  0.2162876050853629
iteration : 8975
train acc:  0.8671875
train loss:  0.28442996740341187
train gradient:  0.10973087015756244
iteration : 8976
train acc:  0.8359375
train loss:  0.3446977138519287
train gradient:  0.14618460434017155
iteration : 8977
train acc:  0.890625
train loss:  0.24940578639507294
train gradient:  0.07983860027746993
iteration : 8978
train acc:  0.8671875
train loss:  0.28221386671066284
train gradient:  0.09798531051607097
iteration : 8979
train acc:  0.84375
train loss:  0.352441668510437
train gradient:  0.1647059558992095
iteration : 8980
train acc:  0.8671875
train loss:  0.37789618968963623
train gradient:  0.18504035453916648
iteration : 8981
train acc:  0.921875
train loss:  0.24153104424476624
train gradient:  0.12784974816020106
iteration : 8982
train acc:  0.8359375
train loss:  0.4163055419921875
train gradient:  0.26258586134690265
iteration : 8983
train acc:  0.8671875
train loss:  0.29115334153175354
train gradient:  0.1292467576465623
iteration : 8984
train acc:  0.8203125
train loss:  0.3273102045059204
train gradient:  0.15523688830922466
iteration : 8985
train acc:  0.8828125
train loss:  0.2765839099884033
train gradient:  0.11802423626395779
iteration : 8986
train acc:  0.875
train loss:  0.2724648714065552
train gradient:  0.11742393288876102
iteration : 8987
train acc:  0.875
train loss:  0.27338048815727234
train gradient:  0.12054640534751009
iteration : 8988
train acc:  0.8828125
train loss:  0.3288395404815674
train gradient:  0.18906531837780083
iteration : 8989
train acc:  0.8984375
train loss:  0.26216942071914673
train gradient:  0.11023862046535662
iteration : 8990
train acc:  0.8984375
train loss:  0.2574407160282135
train gradient:  0.11936524476173849
iteration : 8991
train acc:  0.84375
train loss:  0.3529494106769562
train gradient:  0.20309547977159745
iteration : 8992
train acc:  0.8671875
train loss:  0.323507159948349
train gradient:  0.14206870117435122
iteration : 8993
train acc:  0.8046875
train loss:  0.3907031714916229
train gradient:  0.22803147523725298
iteration : 8994
train acc:  0.90625
train loss:  0.3448839485645294
train gradient:  0.12326467734364492
iteration : 8995
train acc:  0.84375
train loss:  0.316785991191864
train gradient:  0.17501504268323242
iteration : 8996
train acc:  0.859375
train loss:  0.3769051432609558
train gradient:  0.23313932084380878
iteration : 8997
train acc:  0.859375
train loss:  0.2911979556083679
train gradient:  0.10362350635085606
iteration : 8998
train acc:  0.859375
train loss:  0.3046640157699585
train gradient:  0.12319133197833616
iteration : 8999
train acc:  0.8515625
train loss:  0.33254459500312805
train gradient:  0.15110772801124156
iteration : 9000
train acc:  0.796875
train loss:  0.429689884185791
train gradient:  0.31456115036588156
iteration : 9001
train acc:  0.84375
train loss:  0.35639119148254395
train gradient:  0.1976480956513017
iteration : 9002
train acc:  0.9140625
train loss:  0.25151270627975464
train gradient:  0.19024930863494072
iteration : 9003
train acc:  0.84375
train loss:  0.36555731296539307
train gradient:  0.19980372099205324
iteration : 9004
train acc:  0.8515625
train loss:  0.37033528089523315
train gradient:  0.16311240448008274
iteration : 9005
train acc:  0.8984375
train loss:  0.2715633511543274
train gradient:  0.12425010404793674
iteration : 9006
train acc:  0.875
train loss:  0.278720498085022
train gradient:  0.13651079576456743
iteration : 9007
train acc:  0.7890625
train loss:  0.3554176688194275
train gradient:  0.18754653694681667
iteration : 9008
train acc:  0.90625
train loss:  0.2625717520713806
train gradient:  0.1121617028436963
iteration : 9009
train acc:  0.8515625
train loss:  0.31192877888679504
train gradient:  0.11816588073985394
iteration : 9010
train acc:  0.84375
train loss:  0.3966473340988159
train gradient:  0.21604554626583716
iteration : 9011
train acc:  0.84375
train loss:  0.3216188848018646
train gradient:  0.15957599890739388
iteration : 9012
train acc:  0.8671875
train loss:  0.3253587484359741
train gradient:  0.18688261416306431
iteration : 9013
train acc:  0.84375
train loss:  0.35245126485824585
train gradient:  0.163930686120805
iteration : 9014
train acc:  0.890625
train loss:  0.2650189995765686
train gradient:  0.10547109402934614
iteration : 9015
train acc:  0.890625
train loss:  0.2862839102745056
train gradient:  0.18667454109568074
iteration : 9016
train acc:  0.828125
train loss:  0.4465838670730591
train gradient:  0.3094632006674546
iteration : 9017
train acc:  0.875
train loss:  0.30156755447387695
train gradient:  0.13000026931749753
iteration : 9018
train acc:  0.8671875
train loss:  0.2879895567893982
train gradient:  0.183525353867857
iteration : 9019
train acc:  0.84375
train loss:  0.3085309565067291
train gradient:  0.13592585537777344
iteration : 9020
train acc:  0.8828125
train loss:  0.3285524547100067
train gradient:  0.15298453789570166
iteration : 9021
train acc:  0.8515625
train loss:  0.32228440046310425
train gradient:  0.1452612459595733
iteration : 9022
train acc:  0.875
train loss:  0.2907230854034424
train gradient:  0.10992662449844295
iteration : 9023
train acc:  0.875
train loss:  0.32891541719436646
train gradient:  0.12793498494308236
iteration : 9024
train acc:  0.90625
train loss:  0.2158544957637787
train gradient:  0.07372252507841323
iteration : 9025
train acc:  0.8359375
train loss:  0.3415484130382538
train gradient:  0.16903945398033476
iteration : 9026
train acc:  0.8515625
train loss:  0.3521864712238312
train gradient:  0.15291752853946539
iteration : 9027
train acc:  0.8828125
train loss:  0.31162798404693604
train gradient:  0.16201280862060424
iteration : 9028
train acc:  0.8515625
train loss:  0.32272329926490784
train gradient:  0.2356996312209047
iteration : 9029
train acc:  0.8046875
train loss:  0.3833739757537842
train gradient:  0.24689927736667988
iteration : 9030
train acc:  0.828125
train loss:  0.4209623336791992
train gradient:  0.30364403400917317
iteration : 9031
train acc:  0.8515625
train loss:  0.3447451591491699
train gradient:  0.30012350497235707
iteration : 9032
train acc:  0.8515625
train loss:  0.30678316950798035
train gradient:  0.22161741572901744
iteration : 9033
train acc:  0.8359375
train loss:  0.3572113513946533
train gradient:  0.16532616765150468
iteration : 9034
train acc:  0.859375
train loss:  0.3646766245365143
train gradient:  0.19485173974373798
iteration : 9035
train acc:  0.8828125
train loss:  0.28836095333099365
train gradient:  0.11258398710415157
iteration : 9036
train acc:  0.8671875
train loss:  0.32023513317108154
train gradient:  0.15700245247255973
iteration : 9037
train acc:  0.8203125
train loss:  0.38371437788009644
train gradient:  0.18013593999304786
iteration : 9038
train acc:  0.875
train loss:  0.2930983901023865
train gradient:  0.11422408786357169
iteration : 9039
train acc:  0.84375
train loss:  0.2933201491832733
train gradient:  0.16555234827187848
iteration : 9040
train acc:  0.875
train loss:  0.3152340054512024
train gradient:  0.19668275704367902
iteration : 9041
train acc:  0.8125
train loss:  0.41353434324264526
train gradient:  0.20505613812418805
iteration : 9042
train acc:  0.828125
train loss:  0.4110988974571228
train gradient:  0.26309919724313235
iteration : 9043
train acc:  0.8828125
train loss:  0.2902775704860687
train gradient:  0.14091833032429596
iteration : 9044
train acc:  0.875
train loss:  0.32185232639312744
train gradient:  0.17651784185986022
iteration : 9045
train acc:  0.875
train loss:  0.2903159260749817
train gradient:  0.14764927289319557
iteration : 9046
train acc:  0.8828125
train loss:  0.3287534713745117
train gradient:  0.17387493794908765
iteration : 9047
train acc:  0.796875
train loss:  0.3828098773956299
train gradient:  0.17281024387970254
iteration : 9048
train acc:  0.828125
train loss:  0.3984766900539398
train gradient:  0.22393857100495973
iteration : 9049
train acc:  0.890625
train loss:  0.29959994554519653
train gradient:  0.1468965626939021
iteration : 9050
train acc:  0.8828125
train loss:  0.2856367826461792
train gradient:  0.1443439372699057
iteration : 9051
train acc:  0.8984375
train loss:  0.2696521282196045
train gradient:  0.09160786713898422
iteration : 9052
train acc:  0.8515625
train loss:  0.35508909821510315
train gradient:  0.17456373986000903
iteration : 9053
train acc:  0.90625
train loss:  0.2626240849494934
train gradient:  0.12815321805906826
iteration : 9054
train acc:  0.875
train loss:  0.3247833847999573
train gradient:  0.17727859096084414
iteration : 9055
train acc:  0.8203125
train loss:  0.41626888513565063
train gradient:  0.2209573071903441
iteration : 9056
train acc:  0.8515625
train loss:  0.30173203349113464
train gradient:  0.18740502584467822
iteration : 9057
train acc:  0.875
train loss:  0.3313857614994049
train gradient:  0.15236222853387948
iteration : 9058
train acc:  0.90625
train loss:  0.2854476571083069
train gradient:  0.1770300582366977
iteration : 9059
train acc:  0.8203125
train loss:  0.3460701107978821
train gradient:  0.18733524501978113
iteration : 9060
train acc:  0.84375
train loss:  0.31730690598487854
train gradient:  0.15233733195397736
iteration : 9061
train acc:  0.828125
train loss:  0.375024676322937
train gradient:  0.27445902486096113
iteration : 9062
train acc:  0.890625
train loss:  0.3407641053199768
train gradient:  0.14349821465045623
iteration : 9063
train acc:  0.8671875
train loss:  0.3191526532173157
train gradient:  0.1659287256182464
iteration : 9064
train acc:  0.8671875
train loss:  0.3305911421775818
train gradient:  0.23028201120964553
iteration : 9065
train acc:  0.8359375
train loss:  0.3775523602962494
train gradient:  0.18213885833065743
iteration : 9066
train acc:  0.875
train loss:  0.35637539625167847
train gradient:  0.2528207116229363
iteration : 9067
train acc:  0.8984375
train loss:  0.31312185525894165
train gradient:  0.16777427071043377
iteration : 9068
train acc:  0.8828125
train loss:  0.25341078639030457
train gradient:  0.17091194135056967
iteration : 9069
train acc:  0.84375
train loss:  0.3583992123603821
train gradient:  0.1395400463655705
iteration : 9070
train acc:  0.8359375
train loss:  0.3932768702507019
train gradient:  0.2035347218585194
iteration : 9071
train acc:  0.8671875
train loss:  0.33212897181510925
train gradient:  0.21085349712138352
iteration : 9072
train acc:  0.890625
train loss:  0.3023598790168762
train gradient:  0.13145328221873293
iteration : 9073
train acc:  0.84375
train loss:  0.3049418330192566
train gradient:  0.14973779717494087
iteration : 9074
train acc:  0.796875
train loss:  0.38843265175819397
train gradient:  0.183656138199708
iteration : 9075
train acc:  0.859375
train loss:  0.3527992069721222
train gradient:  0.17234679752675858
iteration : 9076
train acc:  0.796875
train loss:  0.4016970098018646
train gradient:  0.2489751071999568
iteration : 9077
train acc:  0.8828125
train loss:  0.3149098753929138
train gradient:  0.1361270453108008
iteration : 9078
train acc:  0.8359375
train loss:  0.40966325998306274
train gradient:  0.24870846489652618
iteration : 9079
train acc:  0.84375
train loss:  0.3467249274253845
train gradient:  0.22143779760557403
iteration : 9080
train acc:  0.859375
train loss:  0.31579723954200745
train gradient:  0.18158517009898847
iteration : 9081
train acc:  0.859375
train loss:  0.40047359466552734
train gradient:  0.274735471705668
iteration : 9082
train acc:  0.859375
train loss:  0.385684072971344
train gradient:  0.17754790650111088
iteration : 9083
train acc:  0.8671875
train loss:  0.29497867822647095
train gradient:  0.1680937517323242
iteration : 9084
train acc:  0.8671875
train loss:  0.2644917964935303
train gradient:  0.13575345479049522
iteration : 9085
train acc:  0.8828125
train loss:  0.3094657063484192
train gradient:  0.17398336060315173
iteration : 9086
train acc:  0.890625
train loss:  0.27512168884277344
train gradient:  0.1494808870345451
iteration : 9087
train acc:  0.890625
train loss:  0.262160062789917
train gradient:  0.1121944579386102
iteration : 9088
train acc:  0.875
train loss:  0.3133462369441986
train gradient:  0.1427666856931175
iteration : 9089
train acc:  0.8515625
train loss:  0.3344804644584656
train gradient:  0.15029388527112805
iteration : 9090
train acc:  0.8359375
train loss:  0.3610599637031555
train gradient:  0.30556373821025157
iteration : 9091
train acc:  0.8671875
train loss:  0.31235969066619873
train gradient:  0.1382572416788586
iteration : 9092
train acc:  0.859375
train loss:  0.2734811305999756
train gradient:  0.12194654330740315
iteration : 9093
train acc:  0.8515625
train loss:  0.29027706384658813
train gradient:  0.1422771546189293
iteration : 9094
train acc:  0.8359375
train loss:  0.40328869223594666
train gradient:  0.284272175111502
iteration : 9095
train acc:  0.8125
train loss:  0.4357033967971802
train gradient:  0.3132317003657495
iteration : 9096
train acc:  0.859375
train loss:  0.29338014125823975
train gradient:  0.14398047167129857
iteration : 9097
train acc:  0.9140625
train loss:  0.2488892525434494
train gradient:  0.10948810468461086
iteration : 9098
train acc:  0.796875
train loss:  0.4618690013885498
train gradient:  0.2526911230885193
iteration : 9099
train acc:  0.875
train loss:  0.29301726818084717
train gradient:  0.15899732246302586
iteration : 9100
train acc:  0.84375
train loss:  0.35312119126319885
train gradient:  0.1776376884906897
iteration : 9101
train acc:  0.8671875
train loss:  0.28056612610816956
train gradient:  0.14154456037829705
iteration : 9102
train acc:  0.8515625
train loss:  0.32167527079582214
train gradient:  0.21023905742140334
iteration : 9103
train acc:  0.8515625
train loss:  0.3253541588783264
train gradient:  0.19693530919667268
iteration : 9104
train acc:  0.8984375
train loss:  0.25274497270584106
train gradient:  0.10373770337009162
iteration : 9105
train acc:  0.8515625
train loss:  0.3286241888999939
train gradient:  0.18534742900479917
iteration : 9106
train acc:  0.8515625
train loss:  0.3906533718109131
train gradient:  0.25874605781724086
iteration : 9107
train acc:  0.875
train loss:  0.2494867742061615
train gradient:  0.11518938416501286
iteration : 9108
train acc:  0.8984375
train loss:  0.31556236743927
train gradient:  0.12795067450075792
iteration : 9109
train acc:  0.84375
train loss:  0.4083814024925232
train gradient:  0.23503675989093686
iteration : 9110
train acc:  0.828125
train loss:  0.3337568938732147
train gradient:  0.24242021471499842
iteration : 9111
train acc:  0.859375
train loss:  0.2959102690219879
train gradient:  0.1234820651195513
iteration : 9112
train acc:  0.859375
train loss:  0.3655523657798767
train gradient:  0.21851356408564426
iteration : 9113
train acc:  0.890625
train loss:  0.2646203637123108
train gradient:  0.13065415154121202
iteration : 9114
train acc:  0.859375
train loss:  0.37915006279945374
train gradient:  0.24354318784783952
iteration : 9115
train acc:  0.8828125
train loss:  0.32433220744132996
train gradient:  0.1397937490451036
iteration : 9116
train acc:  0.84375
train loss:  0.32606297731399536
train gradient:  0.1839245073277811
iteration : 9117
train acc:  0.8984375
train loss:  0.2677609324455261
train gradient:  0.19790001746323582
iteration : 9118
train acc:  0.890625
train loss:  0.28698280453681946
train gradient:  0.16667586315727398
iteration : 9119
train acc:  0.859375
train loss:  0.36181575059890747
train gradient:  0.21632873734437097
iteration : 9120
train acc:  0.8515625
train loss:  0.3167269229888916
train gradient:  0.17322803696026645
iteration : 9121
train acc:  0.8828125
train loss:  0.30616796016693115
train gradient:  0.14039842103331665
iteration : 9122
train acc:  0.828125
train loss:  0.36160656809806824
train gradient:  0.3155179056434349
iteration : 9123
train acc:  0.828125
train loss:  0.35219067335128784
train gradient:  0.23316135109284292
iteration : 9124
train acc:  0.8203125
train loss:  0.3784545361995697
train gradient:  0.23507462554276215
iteration : 9125
train acc:  0.8359375
train loss:  0.3785792887210846
train gradient:  0.21483463551707116
iteration : 9126
train acc:  0.859375
train loss:  0.33635127544403076
train gradient:  0.17926316446965762
iteration : 9127
train acc:  0.8984375
train loss:  0.321388304233551
train gradient:  0.17383210480986844
iteration : 9128
train acc:  0.8359375
train loss:  0.38956815004348755
train gradient:  0.23120025688878948
iteration : 9129
train acc:  0.8359375
train loss:  0.3492743968963623
train gradient:  0.24215608981753645
iteration : 9130
train acc:  0.8125
train loss:  0.3506641089916229
train gradient:  0.21642091664959948
iteration : 9131
train acc:  0.84375
train loss:  0.3743075430393219
train gradient:  0.24396896204291804
iteration : 9132
train acc:  0.8671875
train loss:  0.33959394693374634
train gradient:  0.14583895749078443
iteration : 9133
train acc:  0.84375
train loss:  0.35950377583503723
train gradient:  0.2285731488533721
iteration : 9134
train acc:  0.8984375
train loss:  0.2791770100593567
train gradient:  0.22654095963440105
iteration : 9135
train acc:  0.859375
train loss:  0.30754297971725464
train gradient:  0.13089301318419722
iteration : 9136
train acc:  0.8828125
train loss:  0.28257647156715393
train gradient:  0.11377137448802298
iteration : 9137
train acc:  0.875
train loss:  0.3104862868785858
train gradient:  0.12662500914301147
iteration : 9138
train acc:  0.8515625
train loss:  0.3809461295604706
train gradient:  0.16555042039711793
iteration : 9139
train acc:  0.90625
train loss:  0.259579062461853
train gradient:  0.10997589710123819
iteration : 9140
train acc:  0.8828125
train loss:  0.3174954056739807
train gradient:  0.13516282586114492
iteration : 9141
train acc:  0.8671875
train loss:  0.3552900552749634
train gradient:  0.1570941023892833
iteration : 9142
train acc:  0.828125
train loss:  0.3455638289451599
train gradient:  0.24060612195903905
iteration : 9143
train acc:  0.890625
train loss:  0.36229148507118225
train gradient:  0.2606868001845069
iteration : 9144
train acc:  0.859375
train loss:  0.29684141278266907
train gradient:  0.11442668638184988
iteration : 9145
train acc:  0.84375
train loss:  0.33907774090766907
train gradient:  0.17129238198429322
iteration : 9146
train acc:  0.84375
train loss:  0.39544394612312317
train gradient:  0.19719437691449448
iteration : 9147
train acc:  0.8359375
train loss:  0.40757787227630615
train gradient:  0.2621656303820224
iteration : 9148
train acc:  0.828125
train loss:  0.40798115730285645
train gradient:  0.20516762068099584
iteration : 9149
train acc:  0.890625
train loss:  0.29218554496765137
train gradient:  0.154073050888982
iteration : 9150
train acc:  0.8828125
train loss:  0.33092039823532104
train gradient:  0.11825163332758179
iteration : 9151
train acc:  0.90625
train loss:  0.2564106583595276
train gradient:  0.10505334016150103
iteration : 9152
train acc:  0.8359375
train loss:  0.390250027179718
train gradient:  0.17900356538504109
iteration : 9153
train acc:  0.8125
train loss:  0.3653058111667633
train gradient:  0.18149919464319225
iteration : 9154
train acc:  0.859375
train loss:  0.37490832805633545
train gradient:  0.16361205150953662
iteration : 9155
train acc:  0.8828125
train loss:  0.2912183403968811
train gradient:  0.10497209757254025
iteration : 9156
train acc:  0.875
train loss:  0.3174327611923218
train gradient:  0.20253737183197165
iteration : 9157
train acc:  0.890625
train loss:  0.3088345229625702
train gradient:  0.15863627126679175
iteration : 9158
train acc:  0.84375
train loss:  0.3896433413028717
train gradient:  0.21534206568013375
iteration : 9159
train acc:  0.8359375
train loss:  0.3367636799812317
train gradient:  0.12931067657265566
iteration : 9160
train acc:  0.84375
train loss:  0.46690604090690613
train gradient:  0.2594509230523808
iteration : 9161
train acc:  0.8984375
train loss:  0.2740127444267273
train gradient:  0.11987809863738653
iteration : 9162
train acc:  0.828125
train loss:  0.37411296367645264
train gradient:  0.17685642901305973
iteration : 9163
train acc:  0.890625
train loss:  0.2827221155166626
train gradient:  0.14185575255434715
iteration : 9164
train acc:  0.8515625
train loss:  0.3113393485546112
train gradient:  0.11909614743145305
iteration : 9165
train acc:  0.8828125
train loss:  0.3369905352592468
train gradient:  0.2297268011601331
iteration : 9166
train acc:  0.859375
train loss:  0.29801034927368164
train gradient:  0.15819308028430606
iteration : 9167
train acc:  0.8984375
train loss:  0.2626931667327881
train gradient:  0.12202925752948482
iteration : 9168
train acc:  0.875
train loss:  0.29832392930984497
train gradient:  0.10774767467886412
iteration : 9169
train acc:  0.859375
train loss:  0.3477739095687866
train gradient:  0.18982249476137847
iteration : 9170
train acc:  0.859375
train loss:  0.2858009934425354
train gradient:  0.14192610107306622
iteration : 9171
train acc:  0.875
train loss:  0.25645142793655396
train gradient:  0.114466054188965
iteration : 9172
train acc:  0.8359375
train loss:  0.31605249643325806
train gradient:  0.19769048050002164
iteration : 9173
train acc:  0.859375
train loss:  0.31223544478416443
train gradient:  0.15873760690633829
iteration : 9174
train acc:  0.828125
train loss:  0.3824479877948761
train gradient:  0.1749625516178236
iteration : 9175
train acc:  0.8984375
train loss:  0.29996001720428467
train gradient:  0.14433381455612274
iteration : 9176
train acc:  0.84375
train loss:  0.43147969245910645
train gradient:  0.24274491970592812
iteration : 9177
train acc:  0.8359375
train loss:  0.41148900985717773
train gradient:  0.250664021875845
iteration : 9178
train acc:  0.84375
train loss:  0.3912866413593292
train gradient:  0.2346882730522079
iteration : 9179
train acc:  0.8515625
train loss:  0.39250046014785767
train gradient:  0.25480457793089234
iteration : 9180
train acc:  0.8515625
train loss:  0.35141709446907043
train gradient:  0.23257241291571087
iteration : 9181
train acc:  0.8828125
train loss:  0.2465679943561554
train gradient:  0.09640242922635063
iteration : 9182
train acc:  0.8515625
train loss:  0.32208251953125
train gradient:  0.15231666683803374
iteration : 9183
train acc:  0.8046875
train loss:  0.4296049475669861
train gradient:  0.2720285827955123
iteration : 9184
train acc:  0.875
train loss:  0.2860868573188782
train gradient:  0.15709178362183518
iteration : 9185
train acc:  0.8046875
train loss:  0.4061387777328491
train gradient:  0.2542934284847937
iteration : 9186
train acc:  0.8984375
train loss:  0.2999497056007385
train gradient:  0.13235806015488127
iteration : 9187
train acc:  0.84375
train loss:  0.36727291345596313
train gradient:  0.1999505340083634
iteration : 9188
train acc:  0.8515625
train loss:  0.3489457964897156
train gradient:  0.3065570655778309
iteration : 9189
train acc:  0.8515625
train loss:  0.3941245973110199
train gradient:  0.19814130192210594
iteration : 9190
train acc:  0.84375
train loss:  0.3444182276725769
train gradient:  0.1761211836820118
iteration : 9191
train acc:  0.84375
train loss:  0.37945470213890076
train gradient:  0.2192885028309432
iteration : 9192
train acc:  0.859375
train loss:  0.3696964383125305
train gradient:  0.16370442290530476
iteration : 9193
train acc:  0.78125
train loss:  0.4266633093357086
train gradient:  0.26540112819999684
iteration : 9194
train acc:  0.9140625
train loss:  0.24726015329360962
train gradient:  0.0794061490920192
iteration : 9195
train acc:  0.9140625
train loss:  0.27605509757995605
train gradient:  0.13369921315463695
iteration : 9196
train acc:  0.84375
train loss:  0.35272493958473206
train gradient:  0.1816949398806222
iteration : 9197
train acc:  0.859375
train loss:  0.4047212600708008
train gradient:  0.3442779480484319
iteration : 9198
train acc:  0.828125
train loss:  0.32480761408805847
train gradient:  0.15501356472491523
iteration : 9199
train acc:  0.8671875
train loss:  0.33865606784820557
train gradient:  0.2670989991311477
iteration : 9200
train acc:  0.8671875
train loss:  0.28792619705200195
train gradient:  0.13064706963028483
iteration : 9201
train acc:  0.875
train loss:  0.27678346633911133
train gradient:  0.1467543226800574
iteration : 9202
train acc:  0.8125
train loss:  0.3715664744377136
train gradient:  0.23139644852707775
iteration : 9203
train acc:  0.890625
train loss:  0.27330392599105835
train gradient:  0.09620582673271502
iteration : 9204
train acc:  0.8359375
train loss:  0.33474305272102356
train gradient:  0.20682195011540322
iteration : 9205
train acc:  0.875
train loss:  0.2746928930282593
train gradient:  0.10901520679534941
iteration : 9206
train acc:  0.8203125
train loss:  0.3495006561279297
train gradient:  0.23580879575408012
iteration : 9207
train acc:  0.90625
train loss:  0.24014601111412048
train gradient:  0.13759217054058853
iteration : 9208
train acc:  0.90625
train loss:  0.24152329564094543
train gradient:  0.09734552043250426
iteration : 9209
train acc:  0.8203125
train loss:  0.34451162815093994
train gradient:  0.1798963612256803
iteration : 9210
train acc:  0.859375
train loss:  0.32555100321769714
train gradient:  0.12502918166777993
iteration : 9211
train acc:  0.859375
train loss:  0.3379601836204529
train gradient:  0.20115931322035852
iteration : 9212
train acc:  0.8671875
train loss:  0.3094004690647125
train gradient:  0.14041863768652932
iteration : 9213
train acc:  0.8359375
train loss:  0.3778427839279175
train gradient:  0.2277928249083546
iteration : 9214
train acc:  0.84375
train loss:  0.3594285547733307
train gradient:  0.1644784019353301
iteration : 9215
train acc:  0.796875
train loss:  0.41308820247650146
train gradient:  0.29478525664600747
iteration : 9216
train acc:  0.875
train loss:  0.2708643078804016
train gradient:  0.10647383739137402
iteration : 9217
train acc:  0.8515625
train loss:  0.36249932646751404
train gradient:  0.14828207460548853
iteration : 9218
train acc:  0.90625
train loss:  0.23754289746284485
train gradient:  0.10620153637845489
iteration : 9219
train acc:  0.8671875
train loss:  0.29669225215911865
train gradient:  0.12215133013624253
iteration : 9220
train acc:  0.78125
train loss:  0.3831605017185211
train gradient:  0.20284658271998035
iteration : 9221
train acc:  0.828125
train loss:  0.3592706322669983
train gradient:  0.19368554923951928
iteration : 9222
train acc:  0.7890625
train loss:  0.45280760526657104
train gradient:  0.36952573873121575
iteration : 9223
train acc:  0.8359375
train loss:  0.3120458126068115
train gradient:  0.15418240489774016
iteration : 9224
train acc:  0.8359375
train loss:  0.3425925672054291
train gradient:  0.3165339261134122
iteration : 9225
train acc:  0.8125
train loss:  0.3669254183769226
train gradient:  0.22498177679380324
iteration : 9226
train acc:  0.8515625
train loss:  0.37343984842300415
train gradient:  0.23668266040552216
iteration : 9227
train acc:  0.8203125
train loss:  0.42929908633232117
train gradient:  0.28188717472519437
iteration : 9228
train acc:  0.8046875
train loss:  0.37309885025024414
train gradient:  0.14285915696843654
iteration : 9229
train acc:  0.8125
train loss:  0.4299042224884033
train gradient:  0.27421530694692514
iteration : 9230
train acc:  0.90625
train loss:  0.26582929491996765
train gradient:  0.134940548503267
iteration : 9231
train acc:  0.859375
train loss:  0.31067955493927
train gradient:  0.15625486244199643
iteration : 9232
train acc:  0.859375
train loss:  0.31885048747062683
train gradient:  0.21667421999740166
iteration : 9233
train acc:  0.8515625
train loss:  0.3296016454696655
train gradient:  0.2329451344244774
iteration : 9234
train acc:  0.84375
train loss:  0.31920838356018066
train gradient:  0.22992454283642058
iteration : 9235
train acc:  0.890625
train loss:  0.2547628879547119
train gradient:  0.10463815858012121
iteration : 9236
train acc:  0.8515625
train loss:  0.32941934466362
train gradient:  0.20654642528384037
iteration : 9237
train acc:  0.8515625
train loss:  0.33621060848236084
train gradient:  0.1722239777435264
iteration : 9238
train acc:  0.859375
train loss:  0.37119901180267334
train gradient:  0.18956537615520327
iteration : 9239
train acc:  0.875
train loss:  0.3034965991973877
train gradient:  0.14878909399812557
iteration : 9240
train acc:  0.859375
train loss:  0.3579663634300232
train gradient:  0.17884436048519292
iteration : 9241
train acc:  0.8203125
train loss:  0.40064021944999695
train gradient:  0.2626135772824545
iteration : 9242
train acc:  0.9140625
train loss:  0.24362044036388397
train gradient:  0.1155007907728629
iteration : 9243
train acc:  0.8203125
train loss:  0.40042567253112793
train gradient:  0.21701389232831583
iteration : 9244
train acc:  0.8984375
train loss:  0.2853255569934845
train gradient:  0.09817744498378794
iteration : 9245
train acc:  0.8203125
train loss:  0.38511091470718384
train gradient:  0.21714304574499219
iteration : 9246
train acc:  0.84375
train loss:  0.42106711864471436
train gradient:  0.2358964675636418
iteration : 9247
train acc:  0.78125
train loss:  0.44884127378463745
train gradient:  0.2129952183250452
iteration : 9248
train acc:  0.90625
train loss:  0.24176302552223206
train gradient:  0.1421126570162623
iteration : 9249
train acc:  0.8515625
train loss:  0.32464489340782166
train gradient:  0.11532071170276442
iteration : 9250
train acc:  0.8046875
train loss:  0.3577825129032135
train gradient:  0.22315830981708118
iteration : 9251
train acc:  0.9140625
train loss:  0.2618491053581238
train gradient:  0.15877375050122838
iteration : 9252
train acc:  0.859375
train loss:  0.3606264591217041
train gradient:  0.18597284930400146
iteration : 9253
train acc:  0.84375
train loss:  0.3468478322029114
train gradient:  0.14645965099316893
iteration : 9254
train acc:  0.859375
train loss:  0.3174844980239868
train gradient:  0.11923978620375024
iteration : 9255
train acc:  0.8359375
train loss:  0.29513993859291077
train gradient:  0.1506219554748876
iteration : 9256
train acc:  0.859375
train loss:  0.3170168101787567
train gradient:  0.14406671295566711
iteration : 9257
train acc:  0.859375
train loss:  0.32115066051483154
train gradient:  0.13665792890804623
iteration : 9258
train acc:  0.8671875
train loss:  0.29970481991767883
train gradient:  0.17297772956577595
iteration : 9259
train acc:  0.8828125
train loss:  0.2691449522972107
train gradient:  0.12997639206851214
iteration : 9260
train acc:  0.8359375
train loss:  0.36730557680130005
train gradient:  0.2611832563041014
iteration : 9261
train acc:  0.8125
train loss:  0.45175424218177795
train gradient:  0.2203898481187958
iteration : 9262
train acc:  0.8125
train loss:  0.4613466262817383
train gradient:  0.3641147028435353
iteration : 9263
train acc:  0.828125
train loss:  0.3589012920856476
train gradient:  0.2545949430634438
iteration : 9264
train acc:  0.859375
train loss:  0.3085848093032837
train gradient:  0.12388676597133015
iteration : 9265
train acc:  0.8046875
train loss:  0.3804997205734253
train gradient:  0.25925886227830236
iteration : 9266
train acc:  0.8359375
train loss:  0.3624257743358612
train gradient:  0.13875274577066088
iteration : 9267
train acc:  0.828125
train loss:  0.3407934308052063
train gradient:  0.19336247883552374
iteration : 9268
train acc:  0.875
train loss:  0.330213725566864
train gradient:  0.3062973463164615
iteration : 9269
train acc:  0.8671875
train loss:  0.3509058654308319
train gradient:  0.17512385000986114
iteration : 9270
train acc:  0.8671875
train loss:  0.3194354772567749
train gradient:  0.1507087191676203
iteration : 9271
train acc:  0.8828125
train loss:  0.26429539918899536
train gradient:  0.1185318396755383
iteration : 9272
train acc:  0.8671875
train loss:  0.36481067538261414
train gradient:  0.26932148594297645
iteration : 9273
train acc:  0.8671875
train loss:  0.3183627128601074
train gradient:  0.11069053747500263
iteration : 9274
train acc:  0.8359375
train loss:  0.3839239478111267
train gradient:  0.2072170701969911
iteration : 9275
train acc:  0.8828125
train loss:  0.28296953439712524
train gradient:  0.19724279790715166
iteration : 9276
train acc:  0.8203125
train loss:  0.384834885597229
train gradient:  0.17340939680420908
iteration : 9277
train acc:  0.828125
train loss:  0.3707953691482544
train gradient:  0.2413577188641349
iteration : 9278
train acc:  0.84375
train loss:  0.32966676354408264
train gradient:  0.211600607537147
iteration : 9279
train acc:  0.84375
train loss:  0.2924688458442688
train gradient:  0.13165478676268288
iteration : 9280
train acc:  0.859375
train loss:  0.3098626732826233
train gradient:  0.152426669236705
iteration : 9281
train acc:  0.78125
train loss:  0.44362038373947144
train gradient:  0.34851400457805937
iteration : 9282
train acc:  0.8984375
train loss:  0.28749901056289673
train gradient:  0.2028333200058519
iteration : 9283
train acc:  0.890625
train loss:  0.31898123025894165
train gradient:  0.19794861028277394
iteration : 9284
train acc:  0.8828125
train loss:  0.29107141494750977
train gradient:  0.13736106236240678
iteration : 9285
train acc:  0.8359375
train loss:  0.3698144555091858
train gradient:  0.18439010498882488
iteration : 9286
train acc:  0.8671875
train loss:  0.3245730400085449
train gradient:  0.12475664346376314
iteration : 9287
train acc:  0.7734375
train loss:  0.4650304913520813
train gradient:  0.27370715104243976
iteration : 9288
train acc:  0.8359375
train loss:  0.35810214281082153
train gradient:  0.15434502931578475
iteration : 9289
train acc:  0.8203125
train loss:  0.41798070073127747
train gradient:  0.27532586840285384
iteration : 9290
train acc:  0.875
train loss:  0.2856234908103943
train gradient:  0.16128723569899822
iteration : 9291
train acc:  0.890625
train loss:  0.2605716586112976
train gradient:  0.21684037017041088
iteration : 9292
train acc:  0.8515625
train loss:  0.3587667644023895
train gradient:  0.207370292340596
iteration : 9293
train acc:  0.8359375
train loss:  0.39300471544265747
train gradient:  0.20217280442634927
iteration : 9294
train acc:  0.890625
train loss:  0.2880621552467346
train gradient:  0.1605329380659315
iteration : 9295
train acc:  0.828125
train loss:  0.37734347581863403
train gradient:  0.18525780650253287
iteration : 9296
train acc:  0.8671875
train loss:  0.36859551072120667
train gradient:  0.2553823523541197
iteration : 9297
train acc:  0.90625
train loss:  0.26086798310279846
train gradient:  0.13777555097693991
iteration : 9298
train acc:  0.90625
train loss:  0.2692878246307373
train gradient:  0.16065774962040402
iteration : 9299
train acc:  0.84375
train loss:  0.3465811312198639
train gradient:  0.1404414397054408
iteration : 9300
train acc:  0.765625
train loss:  0.45716360211372375
train gradient:  0.27355180669191054
iteration : 9301
train acc:  0.859375
train loss:  0.3206964433193207
train gradient:  0.15340925327798433
iteration : 9302
train acc:  0.875
train loss:  0.2979724705219269
train gradient:  0.16704317250738676
iteration : 9303
train acc:  0.78125
train loss:  0.42761656641960144
train gradient:  0.21577774771215058
iteration : 9304
train acc:  0.875
train loss:  0.31289413571357727
train gradient:  0.1610696446237607
iteration : 9305
train acc:  0.8671875
train loss:  0.3125920295715332
train gradient:  0.1779035033335136
iteration : 9306
train acc:  0.8828125
train loss:  0.2989254593849182
train gradient:  0.13838568215782968
iteration : 9307
train acc:  0.8203125
train loss:  0.4006977081298828
train gradient:  0.20938290702545545
iteration : 9308
train acc:  0.9140625
train loss:  0.23333218693733215
train gradient:  0.10850718980890299
iteration : 9309
train acc:  0.8984375
train loss:  0.26111817359924316
train gradient:  0.11335574408836203
iteration : 9310
train acc:  0.8671875
train loss:  0.31123876571655273
train gradient:  0.1370173452237729
iteration : 9311
train acc:  0.875
train loss:  0.28395867347717285
train gradient:  0.1313421593425841
iteration : 9312
train acc:  0.90625
train loss:  0.27657756209373474
train gradient:  0.10882930872053205
iteration : 9313
train acc:  0.8671875
train loss:  0.323251873254776
train gradient:  0.1459247182638223
iteration : 9314
train acc:  0.859375
train loss:  0.3364272117614746
train gradient:  0.15563961110586239
iteration : 9315
train acc:  0.8359375
train loss:  0.38054579496383667
train gradient:  0.17414748171848415
iteration : 9316
train acc:  0.8125
train loss:  0.3916131854057312
train gradient:  0.20877916885838996
iteration : 9317
train acc:  0.796875
train loss:  0.39002591371536255
train gradient:  0.16718439636593777
iteration : 9318
train acc:  0.9140625
train loss:  0.23403973877429962
train gradient:  0.10924384241201689
iteration : 9319
train acc:  0.796875
train loss:  0.4329237639904022
train gradient:  0.20514808683893326
iteration : 9320
train acc:  0.859375
train loss:  0.34627774357795715
train gradient:  0.21386429387384048
iteration : 9321
train acc:  0.84375
train loss:  0.34751787781715393
train gradient:  0.2804049186888615
iteration : 9322
train acc:  0.828125
train loss:  0.3233879804611206
train gradient:  0.20023068304472408
iteration : 9323
train acc:  0.8984375
train loss:  0.2832692861557007
train gradient:  0.19466107629623602
iteration : 9324
train acc:  0.8515625
train loss:  0.3244732618331909
train gradient:  0.1484513968152034
iteration : 9325
train acc:  0.8359375
train loss:  0.3892044425010681
train gradient:  0.26407723889064233
iteration : 9326
train acc:  0.8671875
train loss:  0.2974369525909424
train gradient:  0.14425737347372566
iteration : 9327
train acc:  0.890625
train loss:  0.2948790490627289
train gradient:  0.09393901238057185
iteration : 9328
train acc:  0.875
train loss:  0.3102110028266907
train gradient:  0.11834016773878639
iteration : 9329
train acc:  0.8828125
train loss:  0.2399456650018692
train gradient:  0.13504111062074603
iteration : 9330
train acc:  0.8828125
train loss:  0.28510236740112305
train gradient:  0.16197025402074972
iteration : 9331
train acc:  0.8203125
train loss:  0.35563918948173523
train gradient:  0.19455573928906927
iteration : 9332
train acc:  0.875
train loss:  0.34547245502471924
train gradient:  0.20376810337828366
iteration : 9333
train acc:  0.859375
train loss:  0.3661596179008484
train gradient:  0.2244674603792518
iteration : 9334
train acc:  0.8359375
train loss:  0.3095270097255707
train gradient:  0.13399615969195527
iteration : 9335
train acc:  0.84375
train loss:  0.3528260886669159
train gradient:  0.24897934394795346
iteration : 9336
train acc:  0.84375
train loss:  0.3405970335006714
train gradient:  0.28624693054910766
iteration : 9337
train acc:  0.7890625
train loss:  0.35439327359199524
train gradient:  0.1770024445147699
iteration : 9338
train acc:  0.890625
train loss:  0.29068127274513245
train gradient:  0.1985715915896737
iteration : 9339
train acc:  0.875
train loss:  0.28751420974731445
train gradient:  0.14669362148173157
iteration : 9340
train acc:  0.8984375
train loss:  0.2686595022678375
train gradient:  0.15534506444046028
iteration : 9341
train acc:  0.8828125
train loss:  0.26295191049575806
train gradient:  0.16399609005438073
iteration : 9342
train acc:  0.8515625
train loss:  0.3340776264667511
train gradient:  0.19874189594293415
iteration : 9343
train acc:  0.8203125
train loss:  0.4008840322494507
train gradient:  0.1904135166402196
iteration : 9344
train acc:  0.8828125
train loss:  0.2837649881839752
train gradient:  0.10127112946126898
iteration : 9345
train acc:  0.859375
train loss:  0.38454416394233704
train gradient:  0.20905128928526728
iteration : 9346
train acc:  0.8046875
train loss:  0.4497823119163513
train gradient:  0.30071873756747514
iteration : 9347
train acc:  0.859375
train loss:  0.35321861505508423
train gradient:  0.2464440291065289
iteration : 9348
train acc:  0.8203125
train loss:  0.37205782532691956
train gradient:  0.2083137970584177
iteration : 9349
train acc:  0.8359375
train loss:  0.3496449589729309
train gradient:  0.20825392760625583
iteration : 9350
train acc:  0.875
train loss:  0.28179749846458435
train gradient:  0.13332389084018187
iteration : 9351
train acc:  0.8515625
train loss:  0.3596292734146118
train gradient:  0.19587638868894797
iteration : 9352
train acc:  0.8671875
train loss:  0.3067733645439148
train gradient:  0.13879226088649863
iteration : 9353
train acc:  0.9140625
train loss:  0.26144281029701233
train gradient:  0.10840133411030614
iteration : 9354
train acc:  0.8671875
train loss:  0.3802430033683777
train gradient:  0.17218394057608186
iteration : 9355
train acc:  0.8984375
train loss:  0.3279608488082886
train gradient:  0.17109871345551275
iteration : 9356
train acc:  0.8359375
train loss:  0.3497663736343384
train gradient:  0.18619308804994505
iteration : 9357
train acc:  0.7734375
train loss:  0.39261940121650696
train gradient:  0.18455564462816512
iteration : 9358
train acc:  0.84375
train loss:  0.3191320300102234
train gradient:  0.11238555724619377
iteration : 9359
train acc:  0.8515625
train loss:  0.35651764273643494
train gradient:  0.19803854234846796
iteration : 9360
train acc:  0.8359375
train loss:  0.3730122745037079
train gradient:  0.17185873103903587
iteration : 9361
train acc:  0.8359375
train loss:  0.3810287415981293
train gradient:  0.17610650734792382
iteration : 9362
train acc:  0.8828125
train loss:  0.314517080783844
train gradient:  0.1803730537468745
iteration : 9363
train acc:  0.828125
train loss:  0.39598724246025085
train gradient:  0.18013182305045333
iteration : 9364
train acc:  0.8828125
train loss:  0.31081438064575195
train gradient:  0.1951617397517894
iteration : 9365
train acc:  0.8671875
train loss:  0.3213922083377838
train gradient:  0.21312552799701134
iteration : 9366
train acc:  0.8359375
train loss:  0.33907848596572876
train gradient:  0.11255781083118577
iteration : 9367
train acc:  0.8515625
train loss:  0.3498497009277344
train gradient:  0.16854322627297075
iteration : 9368
train acc:  0.875
train loss:  0.27453896403312683
train gradient:  0.17135005009583598
iteration : 9369
train acc:  0.8203125
train loss:  0.4085816740989685
train gradient:  0.21403940711037878
iteration : 9370
train acc:  0.8671875
train loss:  0.28139740228652954
train gradient:  0.18445024559847298
iteration : 9371
train acc:  0.859375
train loss:  0.3163800835609436
train gradient:  0.1398070823311542
iteration : 9372
train acc:  0.8359375
train loss:  0.4425264000892639
train gradient:  0.22945532179849287
iteration : 9373
train acc:  0.8359375
train loss:  0.33269500732421875
train gradient:  0.17323086812094868
iteration : 9374
train acc:  0.8671875
train loss:  0.34494778513908386
train gradient:  0.13366543202970765
iteration : 9375
train acc:  0.828125
train loss:  0.3711013197898865
train gradient:  0.21103107046029873
iteration : 9376
train acc:  0.765625
train loss:  0.42055344581604004
train gradient:  0.3333363855075425
iteration : 9377
train acc:  0.84375
train loss:  0.29579564929008484
train gradient:  0.20966487051526506
iteration : 9378
train acc:  0.875
train loss:  0.2923336327075958
train gradient:  0.07951470564264668
iteration : 9379
train acc:  0.8203125
train loss:  0.43089866638183594
train gradient:  0.2425672290691681
iteration : 9380
train acc:  0.8515625
train loss:  0.36419326066970825
train gradient:  0.23837524145849134
iteration : 9381
train acc:  0.8203125
train loss:  0.40868791937828064
train gradient:  0.23793218804156369
iteration : 9382
train acc:  0.859375
train loss:  0.3524187505245209
train gradient:  0.17990746640415609
iteration : 9383
train acc:  0.890625
train loss:  0.3151140809059143
train gradient:  0.13528972041807055
iteration : 9384
train acc:  0.8515625
train loss:  0.28994858264923096
train gradient:  0.12143755620882617
iteration : 9385
train acc:  0.859375
train loss:  0.3299260139465332
train gradient:  0.18149780254355694
iteration : 9386
train acc:  0.84375
train loss:  0.32548537850379944
train gradient:  0.16110621218075008
iteration : 9387
train acc:  0.9140625
train loss:  0.25140467286109924
train gradient:  0.10138094524452367
iteration : 9388
train acc:  0.8203125
train loss:  0.3981807231903076
train gradient:  0.20017535939501102
iteration : 9389
train acc:  0.828125
train loss:  0.3649081587791443
train gradient:  0.16601316288075438
iteration : 9390
train acc:  0.8671875
train loss:  0.3065502643585205
train gradient:  0.14799481085508226
iteration : 9391
train acc:  0.8515625
train loss:  0.33610451221466064
train gradient:  0.16586719479723333
iteration : 9392
train acc:  0.84375
train loss:  0.3359450101852417
train gradient:  0.17339590007927164
iteration : 9393
train acc:  0.8515625
train loss:  0.37436535954475403
train gradient:  0.16361334373938424
iteration : 9394
train acc:  0.828125
train loss:  0.3739887475967407
train gradient:  0.14451985228295242
iteration : 9395
train acc:  0.8515625
train loss:  0.3168541193008423
train gradient:  0.14164911513594586
iteration : 9396
train acc:  0.8828125
train loss:  0.2852407693862915
train gradient:  0.1602972033599268
iteration : 9397
train acc:  0.890625
train loss:  0.2785213887691498
train gradient:  0.13581941716260487
iteration : 9398
train acc:  0.8046875
train loss:  0.3855282962322235
train gradient:  0.20826824411154457
iteration : 9399
train acc:  0.7890625
train loss:  0.39785677194595337
train gradient:  0.23685201935769978
iteration : 9400
train acc:  0.828125
train loss:  0.3253995180130005
train gradient:  0.10822594311190314
iteration : 9401
train acc:  0.84375
train loss:  0.3137800991535187
train gradient:  0.12363302400516417
iteration : 9402
train acc:  0.9140625
train loss:  0.27264729142189026
train gradient:  0.11222775601772708
iteration : 9403
train acc:  0.8125
train loss:  0.31445613503456116
train gradient:  0.1265549512601636
iteration : 9404
train acc:  0.8125
train loss:  0.34512805938720703
train gradient:  0.16253205290440165
iteration : 9405
train acc:  0.8359375
train loss:  0.3595947325229645
train gradient:  0.19983717534488102
iteration : 9406
train acc:  0.8671875
train loss:  0.3016706109046936
train gradient:  0.21775815731909454
iteration : 9407
train acc:  0.8515625
train loss:  0.3174790143966675
train gradient:  0.17294988642341558
iteration : 9408
train acc:  0.8515625
train loss:  0.3321360647678375
train gradient:  0.2139812867668966
iteration : 9409
train acc:  0.8984375
train loss:  0.2629038393497467
train gradient:  0.15905006691126697
iteration : 9410
train acc:  0.84375
train loss:  0.3427627682685852
train gradient:  0.16885255744072714
iteration : 9411
train acc:  0.828125
train loss:  0.29287731647491455
train gradient:  0.12231603151755262
iteration : 9412
train acc:  0.8671875
train loss:  0.3483922481536865
train gradient:  0.18454297164640404
iteration : 9413
train acc:  0.8984375
train loss:  0.29334843158721924
train gradient:  0.15550324633769716
iteration : 9414
train acc:  0.859375
train loss:  0.3117738366127014
train gradient:  0.12879204471984684
iteration : 9415
train acc:  0.8671875
train loss:  0.28780317306518555
train gradient:  0.18779122963443384
iteration : 9416
train acc:  0.828125
train loss:  0.31768542528152466
train gradient:  0.13787427719555334
iteration : 9417
train acc:  0.8828125
train loss:  0.33254721760749817
train gradient:  0.1203904542478305
iteration : 9418
train acc:  0.84375
train loss:  0.3172740638256073
train gradient:  0.13226037206319194
iteration : 9419
train acc:  0.8203125
train loss:  0.3642128109931946
train gradient:  0.19566534853693263
iteration : 9420
train acc:  0.9140625
train loss:  0.2635408639907837
train gradient:  0.11694642603920272
iteration : 9421
train acc:  0.859375
train loss:  0.3167014718055725
train gradient:  0.16494234490287774
iteration : 9422
train acc:  0.859375
train loss:  0.32107168436050415
train gradient:  0.13205035290685624
iteration : 9423
train acc:  0.890625
train loss:  0.27150267362594604
train gradient:  0.10697519543967231
iteration : 9424
train acc:  0.9140625
train loss:  0.18949627876281738
train gradient:  0.07734867876386277
iteration : 9425
train acc:  0.8984375
train loss:  0.2530059814453125
train gradient:  0.09361928184748919
iteration : 9426
train acc:  0.859375
train loss:  0.2849278151988983
train gradient:  0.0907673910973652
iteration : 9427
train acc:  0.8515625
train loss:  0.2941393554210663
train gradient:  0.13200696673263324
iteration : 9428
train acc:  0.828125
train loss:  0.39387157559394836
train gradient:  0.25244354132457103
iteration : 9429
train acc:  0.8984375
train loss:  0.28884702920913696
train gradient:  0.16139943094345277
iteration : 9430
train acc:  0.8203125
train loss:  0.34566739201545715
train gradient:  0.13945212677851007
iteration : 9431
train acc:  0.8203125
train loss:  0.33256757259368896
train gradient:  0.15889549989369095
iteration : 9432
train acc:  0.8515625
train loss:  0.3337860405445099
train gradient:  0.20480863769559055
iteration : 9433
train acc:  0.8984375
train loss:  0.24002835154533386
train gradient:  0.11234485488741673
iteration : 9434
train acc:  0.859375
train loss:  0.3301684260368347
train gradient:  0.19884724075144325
iteration : 9435
train acc:  0.875
train loss:  0.30356937646865845
train gradient:  0.15824152795199053
iteration : 9436
train acc:  0.8203125
train loss:  0.39516133069992065
train gradient:  0.24168260137590553
iteration : 9437
train acc:  0.890625
train loss:  0.2631418704986572
train gradient:  0.14661038653021008
iteration : 9438
train acc:  0.875
train loss:  0.30538350343704224
train gradient:  0.15058120200722203
iteration : 9439
train acc:  0.859375
train loss:  0.28082275390625
train gradient:  0.16591667791919104
iteration : 9440
train acc:  0.8515625
train loss:  0.3202660083770752
train gradient:  0.17997147486736842
iteration : 9441
train acc:  0.84375
train loss:  0.3204649090766907
train gradient:  0.1855041212178561
iteration : 9442
train acc:  0.8515625
train loss:  0.34363338351249695
train gradient:  0.19804734320159476
iteration : 9443
train acc:  0.8515625
train loss:  0.374540776014328
train gradient:  0.255617162797418
iteration : 9444
train acc:  0.84375
train loss:  0.3514046370983124
train gradient:  0.19938341220280703
iteration : 9445
train acc:  0.84375
train loss:  0.36963316798210144
train gradient:  0.28244840381699454
iteration : 9446
train acc:  0.78125
train loss:  0.43431025743484497
train gradient:  0.2206150075255215
iteration : 9447
train acc:  0.828125
train loss:  0.3606351315975189
train gradient:  0.16602285099442296
iteration : 9448
train acc:  0.8671875
train loss:  0.3007280230522156
train gradient:  0.17973304358953907
iteration : 9449
train acc:  0.796875
train loss:  0.35305100679397583
train gradient:  0.28357560462001974
iteration : 9450
train acc:  0.8671875
train loss:  0.317249596118927
train gradient:  0.18207536236948885
iteration : 9451
train acc:  0.84375
train loss:  0.399413526058197
train gradient:  0.2578830417505951
iteration : 9452
train acc:  0.875
train loss:  0.31538325548171997
train gradient:  0.15243484831316179
iteration : 9453
train acc:  0.8828125
train loss:  0.2672439217567444
train gradient:  0.1491898102941946
iteration : 9454
train acc:  0.875
train loss:  0.28131967782974243
train gradient:  0.14192475775557578
iteration : 9455
train acc:  0.8515625
train loss:  0.35584115982055664
train gradient:  0.20058791125424624
iteration : 9456
train acc:  0.7890625
train loss:  0.39707309007644653
train gradient:  0.23824200766986253
iteration : 9457
train acc:  0.8046875
train loss:  0.39471495151519775
train gradient:  0.2187117239591217
iteration : 9458
train acc:  0.875
train loss:  0.27644309401512146
train gradient:  0.12419737368661381
iteration : 9459
train acc:  0.84375
train loss:  0.3695124387741089
train gradient:  0.32033420703317556
iteration : 9460
train acc:  0.90625
train loss:  0.2588813304901123
train gradient:  0.11890842087469751
iteration : 9461
train acc:  0.8046875
train loss:  0.4056888520717621
train gradient:  0.30491121437204477
iteration : 9462
train acc:  0.890625
train loss:  0.2811003625392914
train gradient:  0.18533947302397824
iteration : 9463
train acc:  0.8671875
train loss:  0.3081284165382385
train gradient:  0.17930475829021836
iteration : 9464
train acc:  0.8671875
train loss:  0.36071640253067017
train gradient:  0.24636296863652157
iteration : 9465
train acc:  0.7421875
train loss:  0.47766637802124023
train gradient:  0.32533316403860474
iteration : 9466
train acc:  0.8359375
train loss:  0.33000800013542175
train gradient:  0.13981743975141137
iteration : 9467
train acc:  0.8671875
train loss:  0.3380599915981293
train gradient:  0.15314158341114514
iteration : 9468
train acc:  0.8515625
train loss:  0.32508155703544617
train gradient:  0.14137073491580066
iteration : 9469
train acc:  0.8515625
train loss:  0.3232397437095642
train gradient:  0.20536181116791075
iteration : 9470
train acc:  0.875
train loss:  0.27375340461730957
train gradient:  0.13833230169579241
iteration : 9471
train acc:  0.859375
train loss:  0.33021873235702515
train gradient:  0.18131723304400088
iteration : 9472
train acc:  0.890625
train loss:  0.2905997633934021
train gradient:  0.1218674166948882
iteration : 9473
train acc:  0.8046875
train loss:  0.4447375535964966
train gradient:  0.22714654234498505
iteration : 9474
train acc:  0.8046875
train loss:  0.36942076683044434
train gradient:  0.20934862827156908
iteration : 9475
train acc:  0.84375
train loss:  0.3451695442199707
train gradient:  0.13472352787097133
iteration : 9476
train acc:  0.8515625
train loss:  0.34217575192451477
train gradient:  0.19464818063005285
iteration : 9477
train acc:  0.84375
train loss:  0.30147460103034973
train gradient:  0.1297011348590708
iteration : 9478
train acc:  0.8203125
train loss:  0.36239516735076904
train gradient:  0.2420366221341273
iteration : 9479
train acc:  0.8125
train loss:  0.33286333084106445
train gradient:  0.14883066438828207
iteration : 9480
train acc:  0.8515625
train loss:  0.3639327585697174
train gradient:  0.18901119993791177
iteration : 9481
train acc:  0.8671875
train loss:  0.3159366250038147
train gradient:  0.1367240997521969
iteration : 9482
train acc:  0.84375
train loss:  0.32087671756744385
train gradient:  0.17071630753244754
iteration : 9483
train acc:  0.8125
train loss:  0.4293420910835266
train gradient:  0.2547915759803864
iteration : 9484
train acc:  0.875
train loss:  0.33814775943756104
train gradient:  0.21811335013996475
iteration : 9485
train acc:  0.8125
train loss:  0.39604341983795166
train gradient:  0.21378406747453976
iteration : 9486
train acc:  0.8515625
train loss:  0.2929728925228119
train gradient:  0.1393356421603218
iteration : 9487
train acc:  0.8046875
train loss:  0.40435972809791565
train gradient:  0.2372823957928189
iteration : 9488
train acc:  0.9296875
train loss:  0.24442856013774872
train gradient:  0.13861030564806986
iteration : 9489
train acc:  0.8984375
train loss:  0.30985206365585327
train gradient:  0.15988379657716112
iteration : 9490
train acc:  0.859375
train loss:  0.3610299229621887
train gradient:  0.20707446230204168
iteration : 9491
train acc:  0.875
train loss:  0.3337618112564087
train gradient:  0.1731704188995446
iteration : 9492
train acc:  0.828125
train loss:  0.3534618020057678
train gradient:  0.18520633305788403
iteration : 9493
train acc:  0.890625
train loss:  0.30654221773147583
train gradient:  0.15839278907285245
iteration : 9494
train acc:  0.890625
train loss:  0.2976868748664856
train gradient:  0.17928270063752821
iteration : 9495
train acc:  0.7890625
train loss:  0.39780575037002563
train gradient:  0.24167990846181647
iteration : 9496
train acc:  0.8125
train loss:  0.3681485950946808
train gradient:  0.21568735337180256
iteration : 9497
train acc:  0.859375
train loss:  0.339506596326828
train gradient:  0.22362655680128504
iteration : 9498
train acc:  0.890625
train loss:  0.2968534827232361
train gradient:  0.1417810260693843
iteration : 9499
train acc:  0.8828125
train loss:  0.2551218569278717
train gradient:  0.12953210214222097
iteration : 9500
train acc:  0.84375
train loss:  0.35194694995880127
train gradient:  0.24376463478173718
iteration : 9501
train acc:  0.8671875
train loss:  0.33292490243911743
train gradient:  0.1556566091689021
iteration : 9502
train acc:  0.875
train loss:  0.3151564300060272
train gradient:  0.16164704461718862
iteration : 9503
train acc:  0.8125
train loss:  0.4687422811985016
train gradient:  0.3214136390129281
iteration : 9504
train acc:  0.875
train loss:  0.3426135182380676
train gradient:  0.16778611551410935
iteration : 9505
train acc:  0.8515625
train loss:  0.3250875473022461
train gradient:  0.2570285272223778
iteration : 9506
train acc:  0.8125
train loss:  0.348091185092926
train gradient:  0.1823701867726884
iteration : 9507
train acc:  0.84375
train loss:  0.34235137701034546
train gradient:  0.1792161547701591
iteration : 9508
train acc:  0.8671875
train loss:  0.3544808328151703
train gradient:  0.22530048414509807
iteration : 9509
train acc:  0.8828125
train loss:  0.30817872285842896
train gradient:  0.17094368959057993
iteration : 9510
train acc:  0.8359375
train loss:  0.3909143805503845
train gradient:  0.20901091552814918
iteration : 9511
train acc:  0.8828125
train loss:  0.26773178577423096
train gradient:  0.13541279167437975
iteration : 9512
train acc:  0.921875
train loss:  0.2724728584289551
train gradient:  0.1499861088401851
iteration : 9513
train acc:  0.859375
train loss:  0.3038191795349121
train gradient:  0.1223545993252797
iteration : 9514
train acc:  0.90625
train loss:  0.2456945925951004
train gradient:  0.1342352126339759
iteration : 9515
train acc:  0.8828125
train loss:  0.3176802396774292
train gradient:  0.230713660458508
iteration : 9516
train acc:  0.8515625
train loss:  0.3060963749885559
train gradient:  0.22020711427076922
iteration : 9517
train acc:  0.8203125
train loss:  0.36378416419029236
train gradient:  0.20840392932335344
iteration : 9518
train acc:  0.8515625
train loss:  0.3182770609855652
train gradient:  0.15210024243528192
iteration : 9519
train acc:  0.78125
train loss:  0.41316723823547363
train gradient:  0.2655837550568487
iteration : 9520
train acc:  0.9140625
train loss:  0.25323686003685
train gradient:  0.11738984850066103
iteration : 9521
train acc:  0.8515625
train loss:  0.30489206314086914
train gradient:  0.19021218935314474
iteration : 9522
train acc:  0.828125
train loss:  0.3491607904434204
train gradient:  0.21300607817709633
iteration : 9523
train acc:  0.84375
train loss:  0.3755655288696289
train gradient:  0.2373776101770198
iteration : 9524
train acc:  0.859375
train loss:  0.3069941997528076
train gradient:  0.17802443904487525
iteration : 9525
train acc:  0.875
train loss:  0.2971845269203186
train gradient:  0.1683409353417273
iteration : 9526
train acc:  0.828125
train loss:  0.32539740204811096
train gradient:  0.1878267213845402
iteration : 9527
train acc:  0.8125
train loss:  0.4196305572986603
train gradient:  0.3506523210776061
iteration : 9528
train acc:  0.890625
train loss:  0.2675538659095764
train gradient:  0.18394115654508575
iteration : 9529
train acc:  0.859375
train loss:  0.3059348464012146
train gradient:  0.19993996123153984
iteration : 9530
train acc:  0.8359375
train loss:  0.34801554679870605
train gradient:  0.16684587989650806
iteration : 9531
train acc:  0.890625
train loss:  0.2952432334423065
train gradient:  0.19447090438103315
iteration : 9532
train acc:  0.84375
train loss:  0.3391379714012146
train gradient:  0.2574579817315566
iteration : 9533
train acc:  0.9296875
train loss:  0.25244662165641785
train gradient:  0.09315871862776558
iteration : 9534
train acc:  0.8828125
train loss:  0.27815085649490356
train gradient:  0.11022307748991624
iteration : 9535
train acc:  0.90625
train loss:  0.22293126583099365
train gradient:  0.12233620822570666
iteration : 9536
train acc:  0.875
train loss:  0.3092558979988098
train gradient:  0.18247619614817337
iteration : 9537
train acc:  0.8515625
train loss:  0.34463372826576233
train gradient:  0.162171516902267
iteration : 9538
train acc:  0.890625
train loss:  0.2599062919616699
train gradient:  0.13277799716880884
iteration : 9539
train acc:  0.875
train loss:  0.30439063906669617
train gradient:  0.16883958418985298
iteration : 9540
train acc:  0.8671875
train loss:  0.28487467765808105
train gradient:  0.1478792648720568
iteration : 9541
train acc:  0.8125
train loss:  0.3110125660896301
train gradient:  0.1913145920540975
iteration : 9542
train acc:  0.796875
train loss:  0.40249019861221313
train gradient:  0.17872749574462424
iteration : 9543
train acc:  0.7890625
train loss:  0.4042757749557495
train gradient:  0.26453002704645656
iteration : 9544
train acc:  0.859375
train loss:  0.3507843315601349
train gradient:  0.19116079294964278
iteration : 9545
train acc:  0.8515625
train loss:  0.3483072519302368
train gradient:  0.2803956178015276
iteration : 9546
train acc:  0.859375
train loss:  0.3243722915649414
train gradient:  0.14220880173123857
iteration : 9547
train acc:  0.8671875
train loss:  0.29146653413772583
train gradient:  0.16790792795407367
iteration : 9548
train acc:  0.859375
train loss:  0.33825504779815674
train gradient:  0.15677180595604395
iteration : 9549
train acc:  0.8515625
train loss:  0.29861488938331604
train gradient:  0.16978698580584684
iteration : 9550
train acc:  0.8984375
train loss:  0.2740175724029541
train gradient:  0.1234247604208418
iteration : 9551
train acc:  0.828125
train loss:  0.37837278842926025
train gradient:  0.26185314168076834
iteration : 9552
train acc:  0.875
train loss:  0.26383161544799805
train gradient:  0.16669592067419287
iteration : 9553
train acc:  0.8984375
train loss:  0.2616475224494934
train gradient:  0.12074023411258186
iteration : 9554
train acc:  0.84375
train loss:  0.41018128395080566
train gradient:  0.28301546669310546
iteration : 9555
train acc:  0.8671875
train loss:  0.3366783857345581
train gradient:  0.1615742870812508
iteration : 9556
train acc:  0.84375
train loss:  0.2972533702850342
train gradient:  0.24660428210564636
iteration : 9557
train acc:  0.8828125
train loss:  0.3197184205055237
train gradient:  0.23873406994900764
iteration : 9558
train acc:  0.875
train loss:  0.32425081729888916
train gradient:  0.20633023109604717
iteration : 9559
train acc:  0.875
train loss:  0.3104228973388672
train gradient:  0.18010637619892755
iteration : 9560
train acc:  0.8203125
train loss:  0.4163270592689514
train gradient:  0.242764990424915
iteration : 9561
train acc:  0.8515625
train loss:  0.33725589513778687
train gradient:  0.2637800537301944
iteration : 9562
train acc:  0.8359375
train loss:  0.3517027497291565
train gradient:  0.24031225754368965
iteration : 9563
train acc:  0.8671875
train loss:  0.3824605643749237
train gradient:  0.2662185649237422
iteration : 9564
train acc:  0.828125
train loss:  0.36923155188560486
train gradient:  0.17772172486073096
iteration : 9565
train acc:  0.796875
train loss:  0.42679375410079956
train gradient:  0.42903464646040473
iteration : 9566
train acc:  0.8515625
train loss:  0.3111746907234192
train gradient:  0.19827360628224186
iteration : 9567
train acc:  0.8359375
train loss:  0.35559767484664917
train gradient:  0.31628053878163953
iteration : 9568
train acc:  0.8828125
train loss:  0.2879939675331116
train gradient:  0.18927547002431377
iteration : 9569
train acc:  0.875
train loss:  0.3230118453502655
train gradient:  0.1966803307499861
iteration : 9570
train acc:  0.84375
train loss:  0.3702625036239624
train gradient:  0.21163726982902292
iteration : 9571
train acc:  0.8828125
train loss:  0.29149049520492554
train gradient:  0.15816862124739434
iteration : 9572
train acc:  0.8046875
train loss:  0.3778995871543884
train gradient:  0.1957469418710135
iteration : 9573
train acc:  0.8828125
train loss:  0.261860728263855
train gradient:  0.08820265133650082
iteration : 9574
train acc:  0.84375
train loss:  0.34184691309928894
train gradient:  0.18894960093974095
iteration : 9575
train acc:  0.8359375
train loss:  0.29071786999702454
train gradient:  0.13020446112636958
iteration : 9576
train acc:  0.8671875
train loss:  0.27686071395874023
train gradient:  0.08825065481691115
iteration : 9577
train acc:  0.8828125
train loss:  0.41010236740112305
train gradient:  0.2448603087733628
iteration : 9578
train acc:  0.84375
train loss:  0.33824682235717773
train gradient:  0.16590159829605605
iteration : 9579
train acc:  0.8671875
train loss:  0.3412153124809265
train gradient:  0.20206077398254363
iteration : 9580
train acc:  0.8203125
train loss:  0.3671839237213135
train gradient:  0.16529550271878274
iteration : 9581
train acc:  0.84375
train loss:  0.37612906098365784
train gradient:  0.20379166822034037
iteration : 9582
train acc:  0.8515625
train loss:  0.3228054642677307
train gradient:  0.1774587406500347
iteration : 9583
train acc:  0.828125
train loss:  0.3676825165748596
train gradient:  0.2753190906972988
iteration : 9584
train acc:  0.84375
train loss:  0.3686756491661072
train gradient:  0.1858558863847456
iteration : 9585
train acc:  0.859375
train loss:  0.2746666669845581
train gradient:  0.13573067384936435
iteration : 9586
train acc:  0.8203125
train loss:  0.39871230721473694
train gradient:  0.18669307200641455
iteration : 9587
train acc:  0.8046875
train loss:  0.37036335468292236
train gradient:  0.19123114550724513
iteration : 9588
train acc:  0.8359375
train loss:  0.373300701379776
train gradient:  0.25931477062629854
iteration : 9589
train acc:  0.8671875
train loss:  0.32466718554496765
train gradient:  0.1820997424540463
iteration : 9590
train acc:  0.890625
train loss:  0.27937763929367065
train gradient:  0.1377635191875955
iteration : 9591
train acc:  0.8828125
train loss:  0.2957974970340729
train gradient:  0.1408245723076379
iteration : 9592
train acc:  0.8984375
train loss:  0.2649669647216797
train gradient:  0.15002266362006428
iteration : 9593
train acc:  0.8671875
train loss:  0.3244631290435791
train gradient:  0.131296643717418
iteration : 9594
train acc:  0.90625
train loss:  0.2538340389728546
train gradient:  0.08179783783379119
iteration : 9595
train acc:  0.875
train loss:  0.3048419952392578
train gradient:  0.1715715911300328
iteration : 9596
train acc:  0.8203125
train loss:  0.5044313669204712
train gradient:  0.37196090090890493
iteration : 9597
train acc:  0.859375
train loss:  0.2847219705581665
train gradient:  0.12596922457236986
iteration : 9598
train acc:  0.859375
train loss:  0.3169967532157898
train gradient:  0.11233154127213361
iteration : 9599
train acc:  0.90625
train loss:  0.2462031990289688
train gradient:  0.08491811518894914
iteration : 9600
train acc:  0.8359375
train loss:  0.36299413442611694
train gradient:  0.16121578615766974
iteration : 9601
train acc:  0.8359375
train loss:  0.3877791166305542
train gradient:  0.2164423991615489
iteration : 9602
train acc:  0.8359375
train loss:  0.34634530544281006
train gradient:  0.21003186243840655
iteration : 9603
train acc:  0.8671875
train loss:  0.2970709204673767
train gradient:  0.1306838934652911
iteration : 9604
train acc:  0.8515625
train loss:  0.4051592946052551
train gradient:  0.26645125753365906
iteration : 9605
train acc:  0.8828125
train loss:  0.2832888066768646
train gradient:  0.11573979669912628
iteration : 9606
train acc:  0.875
train loss:  0.30692172050476074
train gradient:  0.14430874826161788
iteration : 9607
train acc:  0.875
train loss:  0.34275373816490173
train gradient:  0.3551125510033633
iteration : 9608
train acc:  0.8828125
train loss:  0.2938191592693329
train gradient:  0.1396977390366685
iteration : 9609
train acc:  0.8828125
train loss:  0.26912859082221985
train gradient:  0.1173321546483272
iteration : 9610
train acc:  0.796875
train loss:  0.4626148045063019
train gradient:  0.26257488367207565
iteration : 9611
train acc:  0.8671875
train loss:  0.3624778389930725
train gradient:  0.15021900960973375
iteration : 9612
train acc:  0.8984375
train loss:  0.279308021068573
train gradient:  0.13206961693714875
iteration : 9613
train acc:  0.8125
train loss:  0.38772931694984436
train gradient:  0.19931614802928754
iteration : 9614
train acc:  0.8671875
train loss:  0.3269551992416382
train gradient:  0.14453781702988203
iteration : 9615
train acc:  0.828125
train loss:  0.35209766030311584
train gradient:  0.2213571178237267
iteration : 9616
train acc:  0.90625
train loss:  0.30168119072914124
train gradient:  0.15091498071412748
iteration : 9617
train acc:  0.84375
train loss:  0.330966591835022
train gradient:  0.15564872113332892
iteration : 9618
train acc:  0.90625
train loss:  0.2968630790710449
train gradient:  0.15605133507070929
iteration : 9619
train acc:  0.828125
train loss:  0.3764677345752716
train gradient:  0.21291280949554164
iteration : 9620
train acc:  0.875
train loss:  0.32932248711586
train gradient:  0.17314673738601113
iteration : 9621
train acc:  0.859375
train loss:  0.279498815536499
train gradient:  0.10677531045299307
iteration : 9622
train acc:  0.828125
train loss:  0.3304818272590637
train gradient:  0.16276912342193223
iteration : 9623
train acc:  0.875
train loss:  0.27104270458221436
train gradient:  0.16566354145384293
iteration : 9624
train acc:  0.8671875
train loss:  0.3117799758911133
train gradient:  0.11723077848198546
iteration : 9625
train acc:  0.8515625
train loss:  0.362321674823761
train gradient:  0.20975119196862205
iteration : 9626
train acc:  0.84375
train loss:  0.3709173798561096
train gradient:  0.2185134206454335
iteration : 9627
train acc:  0.9296875
train loss:  0.23854920268058777
train gradient:  0.09564183015709293
iteration : 9628
train acc:  0.8515625
train loss:  0.3958418369293213
train gradient:  0.269452774488982
iteration : 9629
train acc:  0.8046875
train loss:  0.3823372721672058
train gradient:  0.31990801358541765
iteration : 9630
train acc:  0.8671875
train loss:  0.31639522314071655
train gradient:  0.19789666512248136
iteration : 9631
train acc:  0.8984375
train loss:  0.2532977759838104
train gradient:  0.12018693598797044
iteration : 9632
train acc:  0.796875
train loss:  0.4594227075576782
train gradient:  0.29882464506178885
iteration : 9633
train acc:  0.828125
train loss:  0.3800355792045593
train gradient:  0.21023132613522932
iteration : 9634
train acc:  0.8359375
train loss:  0.3584159314632416
train gradient:  0.2081936601816325
iteration : 9635
train acc:  0.796875
train loss:  0.4104078412055969
train gradient:  0.2597024276722529
iteration : 9636
train acc:  0.859375
train loss:  0.3326857388019562
train gradient:  0.17929103972294969
iteration : 9637
train acc:  0.8359375
train loss:  0.34172773361206055
train gradient:  0.14432158150494045
iteration : 9638
train acc:  0.8671875
train loss:  0.32068634033203125
train gradient:  0.16422197191051174
iteration : 9639
train acc:  0.8515625
train loss:  0.3149639666080475
train gradient:  0.12001814744201587
iteration : 9640
train acc:  0.8671875
train loss:  0.33581212162971497
train gradient:  0.13183404381177177
iteration : 9641
train acc:  0.921875
train loss:  0.23040226101875305
train gradient:  0.12020990106506689
iteration : 9642
train acc:  0.859375
train loss:  0.36244702339172363
train gradient:  0.1378633353372585
iteration : 9643
train acc:  0.890625
train loss:  0.24188393354415894
train gradient:  0.10890265734683692
iteration : 9644
train acc:  0.8671875
train loss:  0.3027845025062561
train gradient:  0.1595511855304148
iteration : 9645
train acc:  0.828125
train loss:  0.3865940570831299
train gradient:  0.19670920392440888
iteration : 9646
train acc:  0.90625
train loss:  0.22083847224712372
train gradient:  0.10027089484514368
iteration : 9647
train acc:  0.859375
train loss:  0.2874777615070343
train gradient:  0.14665084271920573
iteration : 9648
train acc:  0.828125
train loss:  0.41210001707077026
train gradient:  0.28527461588659747
iteration : 9649
train acc:  0.828125
train loss:  0.3352809548377991
train gradient:  0.20281499631828898
iteration : 9650
train acc:  0.8125
train loss:  0.4099169075489044
train gradient:  0.2614946672089
iteration : 9651
train acc:  0.8671875
train loss:  0.30038800835609436
train gradient:  0.16582515306796608
iteration : 9652
train acc:  0.859375
train loss:  0.3050847053527832
train gradient:  0.16460761816241604
iteration : 9653
train acc:  0.8125
train loss:  0.3807327151298523
train gradient:  0.3142766538053075
iteration : 9654
train acc:  0.8671875
train loss:  0.26588746905326843
train gradient:  0.23260404104227192
iteration : 9655
train acc:  0.8359375
train loss:  0.3115983009338379
train gradient:  0.16904249678179817
iteration : 9656
train acc:  0.875
train loss:  0.277506560087204
train gradient:  0.1778622860060774
iteration : 9657
train acc:  0.8515625
train loss:  0.370071679353714
train gradient:  0.31104725353548535
iteration : 9658
train acc:  0.8984375
train loss:  0.2963513731956482
train gradient:  0.20023699132341183
iteration : 9659
train acc:  0.875
train loss:  0.33010536432266235
train gradient:  0.22798167138716274
iteration : 9660
train acc:  0.8046875
train loss:  0.4236224293708801
train gradient:  0.2333603409460762
iteration : 9661
train acc:  0.8359375
train loss:  0.3356701135635376
train gradient:  0.19912708273467689
iteration : 9662
train acc:  0.828125
train loss:  0.35216882824897766
train gradient:  0.20546615053773107
iteration : 9663
train acc:  0.875
train loss:  0.3212665319442749
train gradient:  0.14399713473834286
iteration : 9664
train acc:  0.8984375
train loss:  0.2653241455554962
train gradient:  0.121431692993326
iteration : 9665
train acc:  0.8359375
train loss:  0.38618940114974976
train gradient:  0.3656638899382157
iteration : 9666
train acc:  0.84375
train loss:  0.32993432879447937
train gradient:  0.1783484505187797
iteration : 9667
train acc:  0.8671875
train loss:  0.32090824842453003
train gradient:  0.18225505371773398
iteration : 9668
train acc:  0.921875
train loss:  0.27118009328842163
train gradient:  0.11209857732276134
iteration : 9669
train acc:  0.8671875
train loss:  0.3504387140274048
train gradient:  0.20227055728771873
iteration : 9670
train acc:  0.875
train loss:  0.2953110337257385
train gradient:  0.18032411815667426
iteration : 9671
train acc:  0.828125
train loss:  0.35848182439804077
train gradient:  0.14950018247536248
iteration : 9672
train acc:  0.859375
train loss:  0.3394910991191864
train gradient:  0.12611707292481505
iteration : 9673
train acc:  0.8828125
train loss:  0.27822011709213257
train gradient:  0.09493281355008533
iteration : 9674
train acc:  0.8125
train loss:  0.4159408211708069
train gradient:  0.23648488035788712
iteration : 9675
train acc:  0.8671875
train loss:  0.3376484513282776
train gradient:  0.1904265410407354
iteration : 9676
train acc:  0.8359375
train loss:  0.3274659812450409
train gradient:  0.18476772852180387
iteration : 9677
train acc:  0.78125
train loss:  0.40991872549057007
train gradient:  0.36932602334938447
iteration : 9678
train acc:  0.890625
train loss:  0.3016851246356964
train gradient:  0.15833461849697508
iteration : 9679
train acc:  0.8515625
train loss:  0.3545759916305542
train gradient:  0.20277120074664368
iteration : 9680
train acc:  0.84375
train loss:  0.2800232172012329
train gradient:  0.13860319187706463
iteration : 9681
train acc:  0.90625
train loss:  0.27825236320495605
train gradient:  0.12577219092651798
iteration : 9682
train acc:  0.859375
train loss:  0.3399801254272461
train gradient:  0.18069388982935494
iteration : 9683
train acc:  0.8828125
train loss:  0.30104777216911316
train gradient:  0.13039441152783782
iteration : 9684
train acc:  0.8984375
train loss:  0.2557867765426636
train gradient:  0.11286322374541015
iteration : 9685
train acc:  0.84375
train loss:  0.3627713918685913
train gradient:  0.1878626991371368
iteration : 9686
train acc:  0.8671875
train loss:  0.26526308059692383
train gradient:  0.1464431153494877
iteration : 9687
train acc:  0.8828125
train loss:  0.34188878536224365
train gradient:  0.16654610700910125
iteration : 9688
train acc:  0.828125
train loss:  0.34989026188850403
train gradient:  0.224688312444691
iteration : 9689
train acc:  0.90625
train loss:  0.22084960341453552
train gradient:  0.11931627037829812
iteration : 9690
train acc:  0.8515625
train loss:  0.33662956953048706
train gradient:  0.1577692046175732
iteration : 9691
train acc:  0.859375
train loss:  0.3491474986076355
train gradient:  0.24302039168271022
iteration : 9692
train acc:  0.8828125
train loss:  0.2699596881866455
train gradient:  0.12165835945402254
iteration : 9693
train acc:  0.90625
train loss:  0.24794703722000122
train gradient:  0.1437683599393928
iteration : 9694
train acc:  0.875
train loss:  0.27113640308380127
train gradient:  0.09811814086006125
iteration : 9695
train acc:  0.828125
train loss:  0.40566447377204895
train gradient:  0.26505225452583886
iteration : 9696
train acc:  0.9140625
train loss:  0.2375650554895401
train gradient:  0.09855628226908872
iteration : 9697
train acc:  0.8671875
train loss:  0.3089155852794647
train gradient:  0.15433872875876953
iteration : 9698
train acc:  0.8203125
train loss:  0.3846310079097748
train gradient:  0.19822570640376672
iteration : 9699
train acc:  0.8671875
train loss:  0.3543744683265686
train gradient:  0.19375894463619903
iteration : 9700
train acc:  0.875
train loss:  0.32587212324142456
train gradient:  0.1564955961127406
iteration : 9701
train acc:  0.84375
train loss:  0.34699487686157227
train gradient:  0.16696107525678477
iteration : 9702
train acc:  0.875
train loss:  0.30168288946151733
train gradient:  0.14869611364424276
iteration : 9703
train acc:  0.8515625
train loss:  0.34407109022140503
train gradient:  0.15897543747499587
iteration : 9704
train acc:  0.8828125
train loss:  0.2774069607257843
train gradient:  0.13526043851954347
iteration : 9705
train acc:  0.8515625
train loss:  0.2824852466583252
train gradient:  0.1687450185565549
iteration : 9706
train acc:  0.8203125
train loss:  0.3445759415626526
train gradient:  0.3238599721276478
iteration : 9707
train acc:  0.859375
train loss:  0.35879841446876526
train gradient:  0.26680794242986217
iteration : 9708
train acc:  0.8671875
train loss:  0.2705039083957672
train gradient:  0.19790506358600762
iteration : 9709
train acc:  0.828125
train loss:  0.3972233533859253
train gradient:  0.21730468371329092
iteration : 9710
train acc:  0.8671875
train loss:  0.3130495548248291
train gradient:  0.11840843358028012
iteration : 9711
train acc:  0.796875
train loss:  0.4205320477485657
train gradient:  0.2592604116982056
iteration : 9712
train acc:  0.8671875
train loss:  0.3039954602718353
train gradient:  0.11313736402629096
iteration : 9713
train acc:  0.8359375
train loss:  0.3194490969181061
train gradient:  0.2087129089045697
iteration : 9714
train acc:  0.8203125
train loss:  0.33986014127731323
train gradient:  0.17610185378671184
iteration : 9715
train acc:  0.7890625
train loss:  0.4231516122817993
train gradient:  0.25795993500683867
iteration : 9716
train acc:  0.9140625
train loss:  0.2794782817363739
train gradient:  0.11742246937923576
iteration : 9717
train acc:  0.8359375
train loss:  0.39343342185020447
train gradient:  0.313526912211946
iteration : 9718
train acc:  0.8984375
train loss:  0.26220542192459106
train gradient:  0.11843452994861381
iteration : 9719
train acc:  0.875
train loss:  0.3386636972427368
train gradient:  0.17193474513341542
iteration : 9720
train acc:  0.8984375
train loss:  0.29082152247428894
train gradient:  0.09754012205729755
iteration : 9721
train acc:  0.953125
train loss:  0.2130676805973053
train gradient:  0.0881520112292189
iteration : 9722
train acc:  0.8359375
train loss:  0.352092981338501
train gradient:  0.1699826195626995
iteration : 9723
train acc:  0.859375
train loss:  0.31248146295547485
train gradient:  0.15392362962814246
iteration : 9724
train acc:  0.859375
train loss:  0.3398187756538391
train gradient:  0.19668583925884003
iteration : 9725
train acc:  0.8046875
train loss:  0.3752819895744324
train gradient:  0.2341582339496067
iteration : 9726
train acc:  0.84375
train loss:  0.3603466749191284
train gradient:  0.2679522322824854
iteration : 9727
train acc:  0.8125
train loss:  0.414543092250824
train gradient:  0.3428898441928484
iteration : 9728
train acc:  0.8359375
train loss:  0.3480835258960724
train gradient:  0.1767865997868251
iteration : 9729
train acc:  0.859375
train loss:  0.3123428523540497
train gradient:  0.1794714153372433
iteration : 9730
train acc:  0.8671875
train loss:  0.3175119459629059
train gradient:  0.15195719525247026
iteration : 9731
train acc:  0.890625
train loss:  0.29338496923446655
train gradient:  0.12862766718769286
iteration : 9732
train acc:  0.875
train loss:  0.30470675230026245
train gradient:  0.1373213756964058
iteration : 9733
train acc:  0.875
train loss:  0.24947698414325714
train gradient:  0.094535962089244
iteration : 9734
train acc:  0.828125
train loss:  0.368784099817276
train gradient:  0.19293553974886907
iteration : 9735
train acc:  0.84375
train loss:  0.3269389569759369
train gradient:  0.2836824450191457
iteration : 9736
train acc:  0.8828125
train loss:  0.31870776414871216
train gradient:  0.17911544655793463
iteration : 9737
train acc:  0.859375
train loss:  0.3272450566291809
train gradient:  0.1357120274862943
iteration : 9738
train acc:  0.859375
train loss:  0.34195077419281006
train gradient:  0.19699060981364674
iteration : 9739
train acc:  0.890625
train loss:  0.2742370069026947
train gradient:  0.1351421345757272
iteration : 9740
train acc:  0.84375
train loss:  0.35448187589645386
train gradient:  0.21348407856224563
iteration : 9741
train acc:  0.8828125
train loss:  0.28328895568847656
train gradient:  0.14409744164626653
iteration : 9742
train acc:  0.84375
train loss:  0.3297659158706665
train gradient:  0.28862818159824877
iteration : 9743
train acc:  0.84375
train loss:  0.35955411195755005
train gradient:  0.1658967258842417
iteration : 9744
train acc:  0.8671875
train loss:  0.2566191256046295
train gradient:  0.13466765070182793
iteration : 9745
train acc:  0.828125
train loss:  0.32950881123542786
train gradient:  0.16623991549912787
iteration : 9746
train acc:  0.84375
train loss:  0.34524649381637573
train gradient:  0.12646615374285214
iteration : 9747
train acc:  0.84375
train loss:  0.37040936946868896
train gradient:  0.2015591446390757
iteration : 9748
train acc:  0.8203125
train loss:  0.37106671929359436
train gradient:  0.16327234280476818
iteration : 9749
train acc:  0.8515625
train loss:  0.28287237882614136
train gradient:  0.17009187343281526
iteration : 9750
train acc:  0.84375
train loss:  0.3229571580886841
train gradient:  0.38190255271647544
iteration : 9751
train acc:  0.859375
train loss:  0.3195013999938965
train gradient:  0.10616102043139583
iteration : 9752
train acc:  0.8984375
train loss:  0.24849945306777954
train gradient:  0.1208598187428709
iteration : 9753
train acc:  0.890625
train loss:  0.25834256410598755
train gradient:  0.1561902528403542
iteration : 9754
train acc:  0.796875
train loss:  0.37318938970565796
train gradient:  0.20787293885848968
iteration : 9755
train acc:  0.8515625
train loss:  0.39000025391578674
train gradient:  0.19271524056286682
iteration : 9756
train acc:  0.828125
train loss:  0.34445691108703613
train gradient:  0.17224534234060018
iteration : 9757
train acc:  0.8203125
train loss:  0.35974788665771484
train gradient:  0.2337631032058369
iteration : 9758
train acc:  0.84375
train loss:  0.3527221083641052
train gradient:  0.21028368931103428
iteration : 9759
train acc:  0.84375
train loss:  0.36069348454475403
train gradient:  0.27113583301361355
iteration : 9760
train acc:  0.8671875
train loss:  0.27194589376449585
train gradient:  0.1404507903079305
iteration : 9761
train acc:  0.8671875
train loss:  0.33641159534454346
train gradient:  0.16950979438117464
iteration : 9762
train acc:  0.84375
train loss:  0.3763253688812256
train gradient:  0.20215353035049533
iteration : 9763
train acc:  0.84375
train loss:  0.3731992244720459
train gradient:  0.19964035863282137
iteration : 9764
train acc:  0.84375
train loss:  0.37827959656715393
train gradient:  0.274441050140563
iteration : 9765
train acc:  0.84375
train loss:  0.3071369528770447
train gradient:  0.2216479925834557
iteration : 9766
train acc:  0.8828125
train loss:  0.2963302433490753
train gradient:  0.22573321713470684
iteration : 9767
train acc:  0.8046875
train loss:  0.3581734001636505
train gradient:  0.2651485305570528
iteration : 9768
train acc:  0.84375
train loss:  0.3583599925041199
train gradient:  0.15173349464233923
iteration : 9769
train acc:  0.84375
train loss:  0.37780576944351196
train gradient:  0.1652584797157923
iteration : 9770
train acc:  0.875
train loss:  0.27797931432724
train gradient:  0.12013762821656217
iteration : 9771
train acc:  0.8671875
train loss:  0.30464351177215576
train gradient:  0.14830133300421144
iteration : 9772
train acc:  0.875
train loss:  0.3754822015762329
train gradient:  0.22396414071343584
iteration : 9773
train acc:  0.796875
train loss:  0.38969242572784424
train gradient:  0.25727460214703723
iteration : 9774
train acc:  0.84375
train loss:  0.30804336071014404
train gradient:  0.12195171846044275
iteration : 9775
train acc:  0.8671875
train loss:  0.32384204864501953
train gradient:  0.16506705216661605
iteration : 9776
train acc:  0.859375
train loss:  0.34380295872688293
train gradient:  0.2234942564190201
iteration : 9777
train acc:  0.875
train loss:  0.3308754563331604
train gradient:  0.16753888620189716
iteration : 9778
train acc:  0.796875
train loss:  0.40878379344940186
train gradient:  0.28099228802341125
iteration : 9779
train acc:  0.875
train loss:  0.28574228286743164
train gradient:  0.10170883728256225
iteration : 9780
train acc:  0.8671875
train loss:  0.3274810314178467
train gradient:  0.18304554214815388
iteration : 9781
train acc:  0.875
train loss:  0.2652011513710022
train gradient:  0.12766952994224195
iteration : 9782
train acc:  0.8359375
train loss:  0.3193662166595459
train gradient:  0.15623031235236456
iteration : 9783
train acc:  0.8515625
train loss:  0.31275251507759094
train gradient:  0.17007571300025665
iteration : 9784
train acc:  0.8984375
train loss:  0.2658523619174957
train gradient:  0.19146493048792065
iteration : 9785
train acc:  0.921875
train loss:  0.252236545085907
train gradient:  0.09554867446120664
iteration : 9786
train acc:  0.9140625
train loss:  0.2606280744075775
train gradient:  0.15724615019860447
iteration : 9787
train acc:  0.796875
train loss:  0.4338310956954956
train gradient:  0.2662487387932286
iteration : 9788
train acc:  0.78125
train loss:  0.44560888409614563
train gradient:  0.21885249209140417
iteration : 9789
train acc:  0.8671875
train loss:  0.28751641511917114
train gradient:  0.09749002863916593
iteration : 9790
train acc:  0.8359375
train loss:  0.35338425636291504
train gradient:  0.20103243557482142
iteration : 9791
train acc:  0.8828125
train loss:  0.2852579653263092
train gradient:  0.14601608301542057
iteration : 9792
train acc:  0.8671875
train loss:  0.3124731183052063
train gradient:  0.15955622433613947
iteration : 9793
train acc:  0.84375
train loss:  0.36982911825180054
train gradient:  0.28762938871637717
iteration : 9794
train acc:  0.8359375
train loss:  0.33492812514305115
train gradient:  0.2615495464410707
iteration : 9795
train acc:  0.890625
train loss:  0.26454660296440125
train gradient:  0.1213105650228099
iteration : 9796
train acc:  0.84375
train loss:  0.3640182316303253
train gradient:  0.25408984546789976
iteration : 9797
train acc:  0.828125
train loss:  0.35441139340400696
train gradient:  0.16775609130310587
iteration : 9798
train acc:  0.8125
train loss:  0.44121110439300537
train gradient:  0.2070002743698851
iteration : 9799
train acc:  0.8203125
train loss:  0.4132434129714966
train gradient:  0.23987686596832303
iteration : 9800
train acc:  0.8515625
train loss:  0.3048688769340515
train gradient:  0.18096303426767968
iteration : 9801
train acc:  0.7890625
train loss:  0.4276800751686096
train gradient:  0.1764658060941508
iteration : 9802
train acc:  0.8984375
train loss:  0.257049024105072
train gradient:  0.10118239915115892
iteration : 9803
train acc:  0.828125
train loss:  0.34809601306915283
train gradient:  0.17120257278827253
iteration : 9804
train acc:  0.828125
train loss:  0.3363890051841736
train gradient:  0.16095552135316182
iteration : 9805
train acc:  0.8125
train loss:  0.33594202995300293
train gradient:  0.26813998376153436
iteration : 9806
train acc:  0.859375
train loss:  0.30192631483078003
train gradient:  0.14692333309181374
iteration : 9807
train acc:  0.8671875
train loss:  0.28912943601608276
train gradient:  0.12489492650005908
iteration : 9808
train acc:  0.828125
train loss:  0.3306097984313965
train gradient:  0.15499293486824528
iteration : 9809
train acc:  0.875
train loss:  0.3592792749404907
train gradient:  0.16295791148897776
iteration : 9810
train acc:  0.84375
train loss:  0.32868266105651855
train gradient:  0.15477752065728176
iteration : 9811
train acc:  0.84375
train loss:  0.3221561908721924
train gradient:  0.12424360646383226
iteration : 9812
train acc:  0.8671875
train loss:  0.2785864472389221
train gradient:  0.11915577500740845
iteration : 9813
train acc:  0.890625
train loss:  0.28142884373664856
train gradient:  0.11947786655287346
iteration : 9814
train acc:  0.8671875
train loss:  0.27205270528793335
train gradient:  0.1009338165376543
iteration : 9815
train acc:  0.84375
train loss:  0.3255331218242645
train gradient:  0.14483128413355528
iteration : 9816
train acc:  0.859375
train loss:  0.3601446747779846
train gradient:  0.1650477567105474
iteration : 9817
train acc:  0.8125
train loss:  0.3309534192085266
train gradient:  0.17575924795001568
iteration : 9818
train acc:  0.859375
train loss:  0.31552791595458984
train gradient:  0.17283385493232883
iteration : 9819
train acc:  0.859375
train loss:  0.33785122632980347
train gradient:  0.15259288416603373
iteration : 9820
train acc:  0.859375
train loss:  0.2927015423774719
train gradient:  0.11974156038809172
iteration : 9821
train acc:  0.84375
train loss:  0.31634363532066345
train gradient:  0.12295872134606282
iteration : 9822
train acc:  0.859375
train loss:  0.3367488980293274
train gradient:  0.149559541595212
iteration : 9823
train acc:  0.8828125
train loss:  0.2881864905357361
train gradient:  0.10364491970082045
iteration : 9824
train acc:  0.8671875
train loss:  0.2888559103012085
train gradient:  0.13400296309010915
iteration : 9825
train acc:  0.8828125
train loss:  0.2686910033226013
train gradient:  0.16230301324466728
iteration : 9826
train acc:  0.890625
train loss:  0.3147624731063843
train gradient:  0.15775548207717136
iteration : 9827
train acc:  0.875
train loss:  0.31869786977767944
train gradient:  0.1751350088497368
iteration : 9828
train acc:  0.9140625
train loss:  0.2981340289115906
train gradient:  0.11953567501957092
iteration : 9829
train acc:  0.8671875
train loss:  0.36319416761398315
train gradient:  0.2051143055670043
iteration : 9830
train acc:  0.890625
train loss:  0.24403493106365204
train gradient:  0.1035913634025749
iteration : 9831
train acc:  0.8359375
train loss:  0.3862055838108063
train gradient:  0.27678278943892176
iteration : 9832
train acc:  0.890625
train loss:  0.30583804845809937
train gradient:  0.1865328861904298
iteration : 9833
train acc:  0.8203125
train loss:  0.34280282258987427
train gradient:  0.13847261547052941
iteration : 9834
train acc:  0.890625
train loss:  0.25726795196533203
train gradient:  0.13514445025273752
iteration : 9835
train acc:  0.7734375
train loss:  0.42867517471313477
train gradient:  0.19470638463162793
iteration : 9836
train acc:  0.875
train loss:  0.27859362959861755
train gradient:  0.1213025768008737
iteration : 9837
train acc:  0.8203125
train loss:  0.36443620920181274
train gradient:  0.21374925675054585
iteration : 9838
train acc:  0.8671875
train loss:  0.3586300313472748
train gradient:  0.22392797644591567
iteration : 9839
train acc:  0.8671875
train loss:  0.31669777631759644
train gradient:  0.13499400120921912
iteration : 9840
train acc:  0.90625
train loss:  0.2691575288772583
train gradient:  0.08091227232925917
iteration : 9841
train acc:  0.8828125
train loss:  0.2741054892539978
train gradient:  0.12188521014000839
iteration : 9842
train acc:  0.8359375
train loss:  0.35904067754745483
train gradient:  0.24285254095752792
iteration : 9843
train acc:  0.8515625
train loss:  0.3411332666873932
train gradient:  0.17757309687267625
iteration : 9844
train acc:  0.84375
train loss:  0.341693639755249
train gradient:  0.16799971900304517
iteration : 9845
train acc:  0.875
train loss:  0.2804102301597595
train gradient:  0.09233764662231367
iteration : 9846
train acc:  0.8515625
train loss:  0.29319751262664795
train gradient:  0.12061645961512553
iteration : 9847
train acc:  0.84375
train loss:  0.4129331409931183
train gradient:  0.2145445473588301
iteration : 9848
train acc:  0.8359375
train loss:  0.34869131445884705
train gradient:  0.23508583501420693
iteration : 9849
train acc:  0.859375
train loss:  0.29306477308273315
train gradient:  0.11182638373907192
iteration : 9850
train acc:  0.8671875
train loss:  0.3389485478401184
train gradient:  0.1756849243110893
iteration : 9851
train acc:  0.859375
train loss:  0.3617652654647827
train gradient:  0.6229416724272624
iteration : 9852
train acc:  0.8671875
train loss:  0.3603092432022095
train gradient:  0.1741662592403186
iteration : 9853
train acc:  0.8515625
train loss:  0.3685925006866455
train gradient:  0.16927397319781878
iteration : 9854
train acc:  0.875
train loss:  0.3191677927970886
train gradient:  0.12823085834654954
iteration : 9855
train acc:  0.8359375
train loss:  0.35264065861701965
train gradient:  0.19620661423381913
iteration : 9856
train acc:  0.90625
train loss:  0.2578084170818329
train gradient:  0.15694535149766964
iteration : 9857
train acc:  0.828125
train loss:  0.34394553303718567
train gradient:  0.18592034796354318
iteration : 9858
train acc:  0.859375
train loss:  0.27657073736190796
train gradient:  0.13553602810853838
iteration : 9859
train acc:  0.859375
train loss:  0.38978835940361023
train gradient:  0.18768638598840104
iteration : 9860
train acc:  0.875
train loss:  0.31594324111938477
train gradient:  0.1120157269480736
iteration : 9861
train acc:  0.84375
train loss:  0.34404441714286804
train gradient:  0.1746293132074582
iteration : 9862
train acc:  0.8515625
train loss:  0.35102903842926025
train gradient:  0.18830175081034087
iteration : 9863
train acc:  0.8359375
train loss:  0.34328269958496094
train gradient:  0.1951026508166465
iteration : 9864
train acc:  0.8046875
train loss:  0.3836362361907959
train gradient:  0.15398203507075492
iteration : 9865
train acc:  0.859375
train loss:  0.3303425908088684
train gradient:  0.16794062022791237
iteration : 9866
train acc:  0.8671875
train loss:  0.3612293601036072
train gradient:  0.14600994420438496
iteration : 9867
train acc:  0.9140625
train loss:  0.22472718358039856
train gradient:  0.18556190930840516
iteration : 9868
train acc:  0.8671875
train loss:  0.345171183347702
train gradient:  0.21183523500221135
iteration : 9869
train acc:  0.8671875
train loss:  0.34337443113327026
train gradient:  0.155058779569921
iteration : 9870
train acc:  0.875
train loss:  0.29759329557418823
train gradient:  0.14582570451249666
iteration : 9871
train acc:  0.8359375
train loss:  0.41774484515190125
train gradient:  0.268092957540182
iteration : 9872
train acc:  0.8359375
train loss:  0.42399442195892334
train gradient:  0.2303159271196653
iteration : 9873
train acc:  0.84375
train loss:  0.3410215973854065
train gradient:  0.10349545685509391
iteration : 9874
train acc:  0.8203125
train loss:  0.36665934324264526
train gradient:  0.20221280157434765
iteration : 9875
train acc:  0.8046875
train loss:  0.3732038736343384
train gradient:  0.1649500219574347
iteration : 9876
train acc:  0.8515625
train loss:  0.295648455619812
train gradient:  0.12690021451981853
iteration : 9877
train acc:  0.84375
train loss:  0.3041537404060364
train gradient:  0.13479892469417815
iteration : 9878
train acc:  0.8515625
train loss:  0.3106970191001892
train gradient:  0.14466522255238962
iteration : 9879
train acc:  0.859375
train loss:  0.33256566524505615
train gradient:  0.13167890672013033
iteration : 9880
train acc:  0.8671875
train loss:  0.2941253185272217
train gradient:  0.12673723433848044
iteration : 9881
train acc:  0.9140625
train loss:  0.23470935225486755
train gradient:  0.13389797183423924
iteration : 9882
train acc:  0.828125
train loss:  0.3657170534133911
train gradient:  0.18008622517813228
iteration : 9883
train acc:  0.8828125
train loss:  0.3111908435821533
train gradient:  0.14892552601191827
iteration : 9884
train acc:  0.859375
train loss:  0.31068331003189087
train gradient:  0.13395943995757395
iteration : 9885
train acc:  0.8828125
train loss:  0.3021557927131653
train gradient:  0.11903609230379841
iteration : 9886
train acc:  0.8671875
train loss:  0.30643993616104126
train gradient:  0.16842535947571569
iteration : 9887
train acc:  0.875
train loss:  0.3072271943092346
train gradient:  0.11608540999040193
iteration : 9888
train acc:  0.8515625
train loss:  0.3370317816734314
train gradient:  0.1489626508162134
iteration : 9889
train acc:  0.796875
train loss:  0.4457055330276489
train gradient:  0.35063540300417917
iteration : 9890
train acc:  0.828125
train loss:  0.38053497672080994
train gradient:  0.2338934263588912
iteration : 9891
train acc:  0.7890625
train loss:  0.44503939151763916
train gradient:  0.2317667180571364
iteration : 9892
train acc:  0.859375
train loss:  0.3806697726249695
train gradient:  0.23911771189466652
iteration : 9893
train acc:  0.84375
train loss:  0.34461715817451477
train gradient:  0.14914588747903404
iteration : 9894
train acc:  0.8671875
train loss:  0.3032569885253906
train gradient:  0.21051499483009847
iteration : 9895
train acc:  0.8828125
train loss:  0.33906090259552
train gradient:  0.1440596434432323
iteration : 9896
train acc:  0.8671875
train loss:  0.26679807901382446
train gradient:  0.12611727398568123
iteration : 9897
train acc:  0.875
train loss:  0.27571892738342285
train gradient:  0.11729374892632828
iteration : 9898
train acc:  0.828125
train loss:  0.385231077671051
train gradient:  0.1843998469229469
iteration : 9899
train acc:  0.859375
train loss:  0.3137274384498596
train gradient:  0.11849183407363749
iteration : 9900
train acc:  0.8828125
train loss:  0.31491315364837646
train gradient:  0.16769228512086037
iteration : 9901
train acc:  0.859375
train loss:  0.3390974998474121
train gradient:  0.1560276586719641
iteration : 9902
train acc:  0.8359375
train loss:  0.3307111859321594
train gradient:  0.13038662824148078
iteration : 9903
train acc:  0.859375
train loss:  0.2925490140914917
train gradient:  0.12641604765346764
iteration : 9904
train acc:  0.8515625
train loss:  0.3155156970024109
train gradient:  0.14954041126061968
iteration : 9905
train acc:  0.8359375
train loss:  0.3377480208873749
train gradient:  0.14246208311596903
iteration : 9906
train acc:  0.8515625
train loss:  0.3416241407394409
train gradient:  0.21977967736993387
iteration : 9907
train acc:  0.9140625
train loss:  0.25686144828796387
train gradient:  0.12951179494170126
iteration : 9908
train acc:  0.8203125
train loss:  0.3185523748397827
train gradient:  0.15405625402279993
iteration : 9909
train acc:  0.8984375
train loss:  0.27591121196746826
train gradient:  0.12280470400797126
iteration : 9910
train acc:  0.8515625
train loss:  0.32575929164886475
train gradient:  0.165160655995507
iteration : 9911
train acc:  0.890625
train loss:  0.27062928676605225
train gradient:  0.13625296201758913
iteration : 9912
train acc:  0.9140625
train loss:  0.2520519196987152
train gradient:  0.1229691218015221
iteration : 9913
train acc:  0.9140625
train loss:  0.34799495339393616
train gradient:  0.14551242670973974
iteration : 9914
train acc:  0.8046875
train loss:  0.4250437021255493
train gradient:  0.3270618311902834
iteration : 9915
train acc:  0.875
train loss:  0.26713237166404724
train gradient:  0.11632651819041735
iteration : 9916
train acc:  0.8515625
train loss:  0.39869874715805054
train gradient:  0.25778772507999986
iteration : 9917
train acc:  0.8828125
train loss:  0.3763304352760315
train gradient:  0.1521095677629311
iteration : 9918
train acc:  0.875
train loss:  0.2903214693069458
train gradient:  0.10507797016549213
iteration : 9919
train acc:  0.890625
train loss:  0.27492159605026245
train gradient:  0.10760440657122232
iteration : 9920
train acc:  0.8671875
train loss:  0.2862243056297302
train gradient:  0.12542510590302344
iteration : 9921
train acc:  0.84375
train loss:  0.40160879492759705
train gradient:  0.2638789007176071
iteration : 9922
train acc:  0.8515625
train loss:  0.29346963763237
train gradient:  0.13945762253137886
iteration : 9923
train acc:  0.8828125
train loss:  0.30675745010375977
train gradient:  0.10526357838927422
iteration : 9924
train acc:  0.890625
train loss:  0.30608007311820984
train gradient:  0.14602270129088157
iteration : 9925
train acc:  0.8671875
train loss:  0.30250152945518494
train gradient:  0.13904306311848544
iteration : 9926
train acc:  0.8359375
train loss:  0.37558281421661377
train gradient:  0.23748376124439569
iteration : 9927
train acc:  0.8125
train loss:  0.3852888345718384
train gradient:  0.21673221119102787
iteration : 9928
train acc:  0.859375
train loss:  0.3176383972167969
train gradient:  0.14686125997605656
iteration : 9929
train acc:  0.8671875
train loss:  0.3629167079925537
train gradient:  0.1323621387814732
iteration : 9930
train acc:  0.859375
train loss:  0.3088076412677765
train gradient:  0.23526588954187966
iteration : 9931
train acc:  0.8515625
train loss:  0.36655566096305847
train gradient:  0.20122023826666358
iteration : 9932
train acc:  0.8984375
train loss:  0.24797791242599487
train gradient:  0.12074356492751577
iteration : 9933
train acc:  0.84375
train loss:  0.3367547392845154
train gradient:  0.1545025630599881
iteration : 9934
train acc:  0.8671875
train loss:  0.30109336972236633
train gradient:  0.28417503989380194
iteration : 9935
train acc:  0.8515625
train loss:  0.3546815812587738
train gradient:  0.13595530603542497
iteration : 9936
train acc:  0.84375
train loss:  0.3841487169265747
train gradient:  0.2131489642300131
iteration : 9937
train acc:  0.828125
train loss:  0.31911253929138184
train gradient:  0.15980073301827807
iteration : 9938
train acc:  0.8203125
train loss:  0.33369892835617065
train gradient:  0.23056892181962751
iteration : 9939
train acc:  0.8125
train loss:  0.4194788932800293
train gradient:  0.23978573854756546
iteration : 9940
train acc:  0.859375
train loss:  0.36162152886390686
train gradient:  0.2066793307026743
iteration : 9941
train acc:  0.84375
train loss:  0.3555258512496948
train gradient:  0.32002047959205904
iteration : 9942
train acc:  0.8125
train loss:  0.36269381642341614
train gradient:  0.19867014777007316
iteration : 9943
train acc:  0.8515625
train loss:  0.31163132190704346
train gradient:  0.17861294742269987
iteration : 9944
train acc:  0.859375
train loss:  0.37551164627075195
train gradient:  0.19147874978702778
iteration : 9945
train acc:  0.8828125
train loss:  0.3058752715587616
train gradient:  0.14817878614507804
iteration : 9946
train acc:  0.875
train loss:  0.34497493505477905
train gradient:  0.18588606426406473
iteration : 9947
train acc:  0.8359375
train loss:  0.39463067054748535
train gradient:  0.2999128781726592
iteration : 9948
train acc:  0.8515625
train loss:  0.34430164098739624
train gradient:  0.19474496928048124
iteration : 9949
train acc:  0.8984375
train loss:  0.31976014375686646
train gradient:  0.13838889194742005
iteration : 9950
train acc:  0.8671875
train loss:  0.3105708062648773
train gradient:  0.17323544528739915
iteration : 9951
train acc:  0.875
train loss:  0.3119574189186096
train gradient:  0.16752765353152052
iteration : 9952
train acc:  0.875
train loss:  0.3152092695236206
train gradient:  0.1403152621336371
iteration : 9953
train acc:  0.859375
train loss:  0.3539498448371887
train gradient:  0.24428202077399044
iteration : 9954
train acc:  0.90625
train loss:  0.2233853042125702
train gradient:  0.07015238831394392
iteration : 9955
train acc:  0.875
train loss:  0.3055281937122345
train gradient:  0.11745418641589331
iteration : 9956
train acc:  0.8984375
train loss:  0.2623293697834015
train gradient:  0.1331643620761082
iteration : 9957
train acc:  0.84375
train loss:  0.3277972936630249
train gradient:  0.15899397723054193
iteration : 9958
train acc:  0.8828125
train loss:  0.28108450770378113
train gradient:  0.11041252395922126
iteration : 9959
train acc:  0.875
train loss:  0.2966670095920563
train gradient:  0.12954518549141683
iteration : 9960
train acc:  0.8671875
train loss:  0.28146621584892273
train gradient:  0.1413862087920642
iteration : 9961
train acc:  0.828125
train loss:  0.3359561562538147
train gradient:  0.20876272242887167
iteration : 9962
train acc:  0.828125
train loss:  0.3824017643928528
train gradient:  0.17199953345136493
iteration : 9963
train acc:  0.875
train loss:  0.3420713543891907
train gradient:  0.18169550834614823
iteration : 9964
train acc:  0.84375
train loss:  0.32994380593299866
train gradient:  0.14983137108180886
iteration : 9965
train acc:  0.875
train loss:  0.2762777507305145
train gradient:  0.12071719031621936
iteration : 9966
train acc:  0.8828125
train loss:  0.26070770621299744
train gradient:  0.1228133851902109
iteration : 9967
train acc:  0.9140625
train loss:  0.2547370195388794
train gradient:  0.12335745228123318
iteration : 9968
train acc:  0.8671875
train loss:  0.2999249994754791
train gradient:  0.14935383528458115
iteration : 9969
train acc:  0.9296875
train loss:  0.23691710829734802
train gradient:  0.10313261702644284
iteration : 9970
train acc:  0.890625
train loss:  0.28824368119239807
train gradient:  0.13349257873935794
iteration : 9971
train acc:  0.890625
train loss:  0.29029250144958496
train gradient:  0.1677333284285773
iteration : 9972
train acc:  0.84375
train loss:  0.34379130601882935
train gradient:  0.227195453775844
iteration : 9973
train acc:  0.8515625
train loss:  0.349001407623291
train gradient:  0.17523669398644645
iteration : 9974
train acc:  0.859375
train loss:  0.33571451902389526
train gradient:  0.2592661118541798
iteration : 9975
train acc:  0.8671875
train loss:  0.30245137214660645
train gradient:  0.19221791083111284
iteration : 9976
train acc:  0.8671875
train loss:  0.2784865200519562
train gradient:  0.12863424997047676
iteration : 9977
train acc:  0.8671875
train loss:  0.32027336955070496
train gradient:  0.16631939385099373
iteration : 9978
train acc:  0.8984375
train loss:  0.2457239329814911
train gradient:  0.09612860909507226
iteration : 9979
train acc:  0.828125
train loss:  0.3114304542541504
train gradient:  0.1722163890181498
iteration : 9980
train acc:  0.8984375
train loss:  0.2655029892921448
train gradient:  0.13947677631598823
iteration : 9981
train acc:  0.875
train loss:  0.3272227644920349
train gradient:  0.2976140055588813
iteration : 9982
train acc:  0.8203125
train loss:  0.3235153555870056
train gradient:  0.1450060764961831
iteration : 9983
train acc:  0.828125
train loss:  0.3511989414691925
train gradient:  0.21661902165576113
iteration : 9984
train acc:  0.875
train loss:  0.2933780252933502
train gradient:  0.16532090904878782
iteration : 9985
train acc:  0.890625
train loss:  0.28921204805374146
train gradient:  0.17500155444549634
iteration : 9986
train acc:  0.890625
train loss:  0.2917264401912689
train gradient:  0.23397006301971018
iteration : 9987
train acc:  0.8515625
train loss:  0.3426229953765869
train gradient:  0.14612348090395855
iteration : 9988
train acc:  0.84375
train loss:  0.3497859835624695
train gradient:  0.17223612637861352
iteration : 9989
train acc:  0.859375
train loss:  0.30260613560676575
train gradient:  0.10916003215932385
iteration : 9990
train acc:  0.890625
train loss:  0.2688921391963959
train gradient:  0.1294436010224466
iteration : 9991
train acc:  0.84375
train loss:  0.350729376077652
train gradient:  0.2073732162225132
iteration : 9992
train acc:  0.8984375
train loss:  0.2306102216243744
train gradient:  0.14077419821637793
iteration : 9993
train acc:  0.859375
train loss:  0.3461126983165741
train gradient:  0.20381032887290723
iteration : 9994
train acc:  0.8359375
train loss:  0.3201116919517517
train gradient:  0.16855843315931457
iteration : 9995
train acc:  0.828125
train loss:  0.3766581416130066
train gradient:  0.22044354661288385
iteration : 9996
train acc:  0.859375
train loss:  0.3127930164337158
train gradient:  0.13535583011744723
iteration : 9997
train acc:  0.84375
train loss:  0.3278849422931671
train gradient:  0.16012817942556423
iteration : 9998
train acc:  0.8671875
train loss:  0.30576443672180176
train gradient:  0.27914413111656794
iteration : 9999
train acc:  0.8515625
train loss:  0.4320565462112427
train gradient:  0.20562521751456858
iteration : 10000
train acc:  0.859375
train loss:  0.27862218022346497
train gradient:  0.16411089955713415
iteration : 10001
train acc:  0.8046875
train loss:  0.4502309560775757
train gradient:  0.34246666986610896
iteration : 10002
train acc:  0.796875
train loss:  0.44472628831863403
train gradient:  0.265470978160632
iteration : 10003
train acc:  0.8515625
train loss:  0.321236252784729
train gradient:  0.16414964464773713
iteration : 10004
train acc:  0.8515625
train loss:  0.3845134973526001
train gradient:  0.14954763965916928
iteration : 10005
train acc:  0.8984375
train loss:  0.27826449275016785
train gradient:  0.12444919715671135
iteration : 10006
train acc:  0.90625
train loss:  0.2726982831954956
train gradient:  0.1291295939237045
iteration : 10007
train acc:  0.8828125
train loss:  0.3013836145401001
train gradient:  0.14321415942413157
iteration : 10008
train acc:  0.8359375
train loss:  0.40718576312065125
train gradient:  0.20691441761975132
iteration : 10009
train acc:  0.8671875
train loss:  0.30660250782966614
train gradient:  0.11637144814351824
iteration : 10010
train acc:  0.8984375
train loss:  0.2800266444683075
train gradient:  0.08811497440080075
iteration : 10011
train acc:  0.84375
train loss:  0.3637356460094452
train gradient:  0.1811105912949576
iteration : 10012
train acc:  0.8515625
train loss:  0.2968817949295044
train gradient:  0.21375036497778988
iteration : 10013
train acc:  0.859375
train loss:  0.35629141330718994
train gradient:  0.22116285992456378
iteration : 10014
train acc:  0.8515625
train loss:  0.3127100467681885
train gradient:  0.133718151647521
iteration : 10015
train acc:  0.84375
train loss:  0.3278747797012329
train gradient:  0.17217087241227874
iteration : 10016
train acc:  0.84375
train loss:  0.3529075086116791
train gradient:  0.16226947535629443
iteration : 10017
train acc:  0.890625
train loss:  0.28514260053634644
train gradient:  0.11562047668145424
iteration : 10018
train acc:  0.8203125
train loss:  0.3286270499229431
train gradient:  0.14601044920107087
iteration : 10019
train acc:  0.8125
train loss:  0.3625548481941223
train gradient:  0.1895946985994833
iteration : 10020
train acc:  0.8984375
train loss:  0.24040617048740387
train gradient:  0.1032807167902209
iteration : 10021
train acc:  0.8671875
train loss:  0.3133486807346344
train gradient:  0.14832041880199576
iteration : 10022
train acc:  0.8515625
train loss:  0.2784144878387451
train gradient:  0.1343184733357714
iteration : 10023
train acc:  0.8515625
train loss:  0.2895878851413727
train gradient:  0.17154059819757733
iteration : 10024
train acc:  0.890625
train loss:  0.247956320643425
train gradient:  0.12510796122145296
iteration : 10025
train acc:  0.8203125
train loss:  0.36889463663101196
train gradient:  0.26458129036667194
iteration : 10026
train acc:  0.890625
train loss:  0.31676721572875977
train gradient:  0.15835293389203292
iteration : 10027
train acc:  0.875
train loss:  0.3105592131614685
train gradient:  0.17308724842898432
iteration : 10028
train acc:  0.875
train loss:  0.31637197732925415
train gradient:  0.137223446954308
iteration : 10029
train acc:  0.8515625
train loss:  0.31462782621383667
train gradient:  0.2520908581245951
iteration : 10030
train acc:  0.84375
train loss:  0.3037276864051819
train gradient:  0.12814431094055964
iteration : 10031
train acc:  0.8203125
train loss:  0.3648488521575928
train gradient:  0.21599930135197726
iteration : 10032
train acc:  0.8671875
train loss:  0.3512541353702545
train gradient:  0.1617274929269471
iteration : 10033
train acc:  0.8515625
train loss:  0.3358677625656128
train gradient:  0.19494087975902136
iteration : 10034
train acc:  0.859375
train loss:  0.30147305130958557
train gradient:  0.1523563987373001
iteration : 10035
train acc:  0.859375
train loss:  0.31813058257102966
train gradient:  0.15104143600935163
iteration : 10036
train acc:  0.90625
train loss:  0.25294676423072815
train gradient:  0.10039407208279136
iteration : 10037
train acc:  0.8125
train loss:  0.4401649236679077
train gradient:  0.3298853363290989
iteration : 10038
train acc:  0.859375
train loss:  0.3177716135978699
train gradient:  0.183769997573407
iteration : 10039
train acc:  0.84375
train loss:  0.3782004117965698
train gradient:  0.26097570182381097
iteration : 10040
train acc:  0.84375
train loss:  0.305520236492157
train gradient:  0.14430651024681437
iteration : 10041
train acc:  0.84375
train loss:  0.31858861446380615
train gradient:  0.2122816887262562
iteration : 10042
train acc:  0.84375
train loss:  0.3291855752468109
train gradient:  0.14549609824144433
iteration : 10043
train acc:  0.8359375
train loss:  0.2987818717956543
train gradient:  0.13692503180557622
iteration : 10044
train acc:  0.875
train loss:  0.3006620407104492
train gradient:  0.15929378261793314
iteration : 10045
train acc:  0.8515625
train loss:  0.3326184153556824
train gradient:  0.13936592759900918
iteration : 10046
train acc:  0.875
train loss:  0.3224836587905884
train gradient:  0.18260390359370895
iteration : 10047
train acc:  0.875
train loss:  0.3085026144981384
train gradient:  0.11384160954317456
iteration : 10048
train acc:  0.875
train loss:  0.29820698499679565
train gradient:  0.17148320418063595
iteration : 10049
train acc:  0.859375
train loss:  0.34984755516052246
train gradient:  0.22454282076374196
iteration : 10050
train acc:  0.8515625
train loss:  0.3007175028324127
train gradient:  0.12986213769642355
iteration : 10051
train acc:  0.84375
train loss:  0.3595283627510071
train gradient:  0.15259573730913367
iteration : 10052
train acc:  0.8359375
train loss:  0.36291345953941345
train gradient:  0.20851274340358691
iteration : 10053
train acc:  0.890625
train loss:  0.2870836853981018
train gradient:  0.17592231559021693
iteration : 10054
train acc:  0.84375
train loss:  0.367489755153656
train gradient:  0.15191895323391824
iteration : 10055
train acc:  0.921875
train loss:  0.25060179829597473
train gradient:  0.07280896275313896
iteration : 10056
train acc:  0.828125
train loss:  0.3894314169883728
train gradient:  0.18472535426781866
iteration : 10057
train acc:  0.8671875
train loss:  0.2961135804653168
train gradient:  0.14988365526067204
iteration : 10058
train acc:  0.8203125
train loss:  0.3497220277786255
train gradient:  0.18021311453743302
iteration : 10059
train acc:  0.859375
train loss:  0.3213788568973541
train gradient:  0.18910087757978544
iteration : 10060
train acc:  0.8828125
train loss:  0.2541985809803009
train gradient:  0.17864161965099032
iteration : 10061
train acc:  0.8515625
train loss:  0.3302662670612335
train gradient:  0.13967096369631343
iteration : 10062
train acc:  0.8359375
train loss:  0.33960482478141785
train gradient:  0.20644623476540758
iteration : 10063
train acc:  0.890625
train loss:  0.2842613458633423
train gradient:  0.10967877163595104
iteration : 10064
train acc:  0.859375
train loss:  0.30886754393577576
train gradient:  0.17740446303516802
iteration : 10065
train acc:  0.8125
train loss:  0.34866398572921753
train gradient:  0.2462580420276669
iteration : 10066
train acc:  0.8203125
train loss:  0.3760959506034851
train gradient:  0.2856785448531674
iteration : 10067
train acc:  0.828125
train loss:  0.36522990465164185
train gradient:  0.18500309420524955
iteration : 10068
train acc:  0.828125
train loss:  0.3884153962135315
train gradient:  0.2942462843960275
iteration : 10069
train acc:  0.8515625
train loss:  0.341249942779541
train gradient:  0.1891468740473095
iteration : 10070
train acc:  0.8359375
train loss:  0.3498091697692871
train gradient:  0.14694088623310658
iteration : 10071
train acc:  0.8828125
train loss:  0.30486297607421875
train gradient:  0.15685237016621673
iteration : 10072
train acc:  0.8515625
train loss:  0.32332679629325867
train gradient:  0.18043806186231054
iteration : 10073
train acc:  0.828125
train loss:  0.34994977712631226
train gradient:  0.17727068013322084
iteration : 10074
train acc:  0.859375
train loss:  0.3335053026676178
train gradient:  0.16691804322211806
iteration : 10075
train acc:  0.8984375
train loss:  0.26420649886131287
train gradient:  0.1278851345854975
iteration : 10076
train acc:  0.8828125
train loss:  0.30382347106933594
train gradient:  0.14274319086534135
iteration : 10077
train acc:  0.8359375
train loss:  0.34427401423454285
train gradient:  0.1921817494888609
iteration : 10078
train acc:  0.8984375
train loss:  0.2903222143650055
train gradient:  0.08302914881597283
iteration : 10079
train acc:  0.8671875
train loss:  0.32545414566993713
train gradient:  0.18678463393782557
iteration : 10080
train acc:  0.8828125
train loss:  0.23455479741096497
train gradient:  0.10411463506531182
iteration : 10081
train acc:  0.8046875
train loss:  0.4695449471473694
train gradient:  0.28070740297461627
iteration : 10082
train acc:  0.890625
train loss:  0.3148486614227295
train gradient:  0.15023095971357003
iteration : 10083
train acc:  0.8359375
train loss:  0.3572237491607666
train gradient:  0.1778522595434172
iteration : 10084
train acc:  0.8671875
train loss:  0.34447821974754333
train gradient:  0.25607388061329267
iteration : 10085
train acc:  0.859375
train loss:  0.3453398048877716
train gradient:  0.18589638076328918
iteration : 10086
train acc:  0.828125
train loss:  0.319840669631958
train gradient:  0.15590074986392732
iteration : 10087
train acc:  0.8203125
train loss:  0.40425652265548706
train gradient:  0.20011620007064562
iteration : 10088
train acc:  0.84375
train loss:  0.3266456127166748
train gradient:  0.21156855115047352
iteration : 10089
train acc:  0.84375
train loss:  0.305466890335083
train gradient:  0.14043168061720765
iteration : 10090
train acc:  0.875
train loss:  0.3276504576206207
train gradient:  0.19478385564243256
iteration : 10091
train acc:  0.859375
train loss:  0.2872990071773529
train gradient:  0.12050465246919638
iteration : 10092
train acc:  0.8515625
train loss:  0.353455513715744
train gradient:  0.1364401434636379
iteration : 10093
train acc:  0.859375
train loss:  0.33768174052238464
train gradient:  0.12762248396843667
iteration : 10094
train acc:  0.890625
train loss:  0.34165140986442566
train gradient:  0.16173617797610407
iteration : 10095
train acc:  0.8828125
train loss:  0.28349238634109497
train gradient:  0.133092966242159
iteration : 10096
train acc:  0.90625
train loss:  0.21933725476264954
train gradient:  0.12229720937941738
iteration : 10097
train acc:  0.8359375
train loss:  0.3657878637313843
train gradient:  0.19850544735996217
iteration : 10098
train acc:  0.8828125
train loss:  0.2996442914009094
train gradient:  0.11035135009262269
iteration : 10099
train acc:  0.890625
train loss:  0.2584323585033417
train gradient:  0.13352596983363002
iteration : 10100
train acc:  0.8515625
train loss:  0.35668766498565674
train gradient:  0.22334308074275583
iteration : 10101
train acc:  0.9140625
train loss:  0.2902851104736328
train gradient:  0.11974911245812124
iteration : 10102
train acc:  0.8984375
train loss:  0.27573996782302856
train gradient:  0.18131014054808767
iteration : 10103
train acc:  0.828125
train loss:  0.3469861149787903
train gradient:  0.18449926579905013
iteration : 10104
train acc:  0.9140625
train loss:  0.2587852478027344
train gradient:  0.09386379416394215
iteration : 10105
train acc:  0.8671875
train loss:  0.33951669931411743
train gradient:  0.14183437981965055
iteration : 10106
train acc:  0.8203125
train loss:  0.4649330973625183
train gradient:  0.35916299704181204
iteration : 10107
train acc:  0.8984375
train loss:  0.28356683254241943
train gradient:  0.1262465579742318
iteration : 10108
train acc:  0.8359375
train loss:  0.4114561080932617
train gradient:  0.21752789192739186
iteration : 10109
train acc:  0.859375
train loss:  0.3343302011489868
train gradient:  0.21280812788703543
iteration : 10110
train acc:  0.8671875
train loss:  0.30792444944381714
train gradient:  0.16561809103595443
iteration : 10111
train acc:  0.8671875
train loss:  0.32595622539520264
train gradient:  0.1582588827136886
iteration : 10112
train acc:  0.8046875
train loss:  0.3838002681732178
train gradient:  0.22431107078235885
iteration : 10113
train acc:  0.828125
train loss:  0.32577604055404663
train gradient:  0.16699956128560473
iteration : 10114
train acc:  0.8046875
train loss:  0.33494991064071655
train gradient:  0.17725919115123762
iteration : 10115
train acc:  0.796875
train loss:  0.40101155638694763
train gradient:  0.32391323891629586
iteration : 10116
train acc:  0.8203125
train loss:  0.3916160762310028
train gradient:  0.20977313667336428
iteration : 10117
train acc:  0.90625
train loss:  0.27049991488456726
train gradient:  0.12888927873901207
iteration : 10118
train acc:  0.8046875
train loss:  0.4471082091331482
train gradient:  0.2945077686157561
iteration : 10119
train acc:  0.8359375
train loss:  0.3553382456302643
train gradient:  0.19462322944036353
iteration : 10120
train acc:  0.8515625
train loss:  0.376390665769577
train gradient:  0.2161599250894251
iteration : 10121
train acc:  0.875
train loss:  0.2825312912464142
train gradient:  0.11960796259237484
iteration : 10122
train acc:  0.828125
train loss:  0.32193657755851746
train gradient:  0.16968011775972003
iteration : 10123
train acc:  0.875
train loss:  0.2964310050010681
train gradient:  0.11057282265650269
iteration : 10124
train acc:  0.8515625
train loss:  0.34688031673431396
train gradient:  0.19406428890832247
iteration : 10125
train acc:  0.8984375
train loss:  0.2343614399433136
train gradient:  0.09318741726545764
iteration : 10126
train acc:  0.875
train loss:  0.257204532623291
train gradient:  0.14509290618639042
iteration : 10127
train acc:  0.828125
train loss:  0.32768309116363525
train gradient:  0.15602154536377183
iteration : 10128
train acc:  0.859375
train loss:  0.3098766803741455
train gradient:  0.14037853715309184
iteration : 10129
train acc:  0.828125
train loss:  0.4381662607192993
train gradient:  0.3759910533680149
iteration : 10130
train acc:  0.875
train loss:  0.2991909384727478
train gradient:  0.14680680742399121
iteration : 10131
train acc:  0.8671875
train loss:  0.2930981516838074
train gradient:  0.2606697113259629
iteration : 10132
train acc:  0.859375
train loss:  0.2736632525920868
train gradient:  0.13925448853219413
iteration : 10133
train acc:  0.84375
train loss:  0.33394742012023926
train gradient:  0.19744646248551323
iteration : 10134
train acc:  0.890625
train loss:  0.3123079836368561
train gradient:  0.17623267460608832
iteration : 10135
train acc:  0.890625
train loss:  0.2757985293865204
train gradient:  0.09801558762850215
iteration : 10136
train acc:  0.859375
train loss:  0.3216916620731354
train gradient:  0.13295363094692875
iteration : 10137
train acc:  0.8515625
train loss:  0.35860440135002136
train gradient:  0.18686586754509638
iteration : 10138
train acc:  0.9375
train loss:  0.2775847315788269
train gradient:  0.13643380248776382
iteration : 10139
train acc:  0.828125
train loss:  0.3446597456932068
train gradient:  0.15587847999756335
iteration : 10140
train acc:  0.8671875
train loss:  0.33734598755836487
train gradient:  0.15152415336804254
iteration : 10141
train acc:  0.859375
train loss:  0.36254727840423584
train gradient:  0.21043920220898382
iteration : 10142
train acc:  0.828125
train loss:  0.36067378520965576
train gradient:  0.22366336086255717
iteration : 10143
train acc:  0.8828125
train loss:  0.25779247283935547
train gradient:  0.15218762505583205
iteration : 10144
train acc:  0.859375
train loss:  0.3220217227935791
train gradient:  0.14938149249245647
iteration : 10145
train acc:  0.84375
train loss:  0.34174415469169617
train gradient:  0.17070387137375248
iteration : 10146
train acc:  0.8828125
train loss:  0.30036109685897827
train gradient:  0.1474424066056117
iteration : 10147
train acc:  0.8828125
train loss:  0.25922906398773193
train gradient:  0.10083906983857838
iteration : 10148
train acc:  0.890625
train loss:  0.2590411305427551
train gradient:  0.13505754154315386
iteration : 10149
train acc:  0.859375
train loss:  0.29955509305000305
train gradient:  0.15789736695080092
iteration : 10150
train acc:  0.8359375
train loss:  0.37342947721481323
train gradient:  0.2294677741290686
iteration : 10151
train acc:  0.8671875
train loss:  0.3181191086769104
train gradient:  0.15487735312720183
iteration : 10152
train acc:  0.8515625
train loss:  0.27182069420814514
train gradient:  0.14844969491706012
iteration : 10153
train acc:  0.8515625
train loss:  0.328097403049469
train gradient:  0.15164192869812823
iteration : 10154
train acc:  0.890625
train loss:  0.3097504675388336
train gradient:  0.11914466558430518
iteration : 10155
train acc:  0.84375
train loss:  0.35403895378112793
train gradient:  0.223113121456727
iteration : 10156
train acc:  0.8671875
train loss:  0.3804790675640106
train gradient:  0.20326349125995943
iteration : 10157
train acc:  0.8203125
train loss:  0.3668370544910431
train gradient:  0.1487907324287821
iteration : 10158
train acc:  0.828125
train loss:  0.33906394243240356
train gradient:  0.2034524291199276
iteration : 10159
train acc:  0.890625
train loss:  0.25540560483932495
train gradient:  0.15362053092909467
iteration : 10160
train acc:  0.828125
train loss:  0.3795073330402374
train gradient:  0.20908983396164915
iteration : 10161
train acc:  0.859375
train loss:  0.4140705466270447
train gradient:  0.2087540690897188
iteration : 10162
train acc:  0.84375
train loss:  0.3062393069267273
train gradient:  0.18834685100562287
iteration : 10163
train acc:  0.7890625
train loss:  0.3919488191604614
train gradient:  0.2046994768150815
iteration : 10164
train acc:  0.8515625
train loss:  0.34394943714141846
train gradient:  0.17644862001937361
iteration : 10165
train acc:  0.8046875
train loss:  0.3351740837097168
train gradient:  0.17640172185861103
iteration : 10166
train acc:  0.8203125
train loss:  0.36890262365341187
train gradient:  0.16198734799913397
iteration : 10167
train acc:  0.8671875
train loss:  0.2858555018901825
train gradient:  0.15430019804770986
iteration : 10168
train acc:  0.890625
train loss:  0.28200364112854004
train gradient:  0.10795459854139833
iteration : 10169
train acc:  0.890625
train loss:  0.2803162932395935
train gradient:  0.13625149546172846
iteration : 10170
train acc:  0.875
train loss:  0.26077333092689514
train gradient:  0.11798676472659665
iteration : 10171
train acc:  0.7890625
train loss:  0.43621256947517395
train gradient:  0.2141435176749141
iteration : 10172
train acc:  0.8984375
train loss:  0.3174246549606323
train gradient:  0.11727934818421233
iteration : 10173
train acc:  0.8671875
train loss:  0.3046073317527771
train gradient:  0.15092649754733964
iteration : 10174
train acc:  0.8203125
train loss:  0.40241917967796326
train gradient:  0.24439419940579082
iteration : 10175
train acc:  0.8671875
train loss:  0.3226042687892914
train gradient:  0.11054002851204614
iteration : 10176
train acc:  0.8046875
train loss:  0.39130038022994995
train gradient:  0.18270950571315653
iteration : 10177
train acc:  0.84375
train loss:  0.329801082611084
train gradient:  0.16053355705211403
iteration : 10178
train acc:  0.84375
train loss:  0.36195504665374756
train gradient:  0.17237163140922945
iteration : 10179
train acc:  0.90625
train loss:  0.2587045431137085
train gradient:  0.08070104519535333
iteration : 10180
train acc:  0.921875
train loss:  0.24344047904014587
train gradient:  0.08299288738082375
iteration : 10181
train acc:  0.875
train loss:  0.30472081899642944
train gradient:  0.12189869210163344
iteration : 10182
train acc:  0.859375
train loss:  0.32844069600105286
train gradient:  0.13783241550800218
iteration : 10183
train acc:  0.8671875
train loss:  0.34240254759788513
train gradient:  0.15497026698379757
iteration : 10184
train acc:  0.875
train loss:  0.30371153354644775
train gradient:  0.11115173490054113
iteration : 10185
train acc:  0.90625
train loss:  0.28251802921295166
train gradient:  0.14142240673442796
iteration : 10186
train acc:  0.8671875
train loss:  0.3334670662879944
train gradient:  0.14565599585694175
iteration : 10187
train acc:  0.8671875
train loss:  0.31761634349823
train gradient:  0.12139720251722262
iteration : 10188
train acc:  0.8046875
train loss:  0.3641347885131836
train gradient:  0.17579746167874472
iteration : 10189
train acc:  0.8203125
train loss:  0.36566251516342163
train gradient:  0.22790107960589367
iteration : 10190
train acc:  0.84375
train loss:  0.3000432848930359
train gradient:  0.12610719182793978
iteration : 10191
train acc:  0.8359375
train loss:  0.3485815227031708
train gradient:  0.17067132300756133
iteration : 10192
train acc:  0.84375
train loss:  0.31137603521347046
train gradient:  0.14054860757526044
iteration : 10193
train acc:  0.8984375
train loss:  0.2729947865009308
train gradient:  0.1693084724928381
iteration : 10194
train acc:  0.8671875
train loss:  0.29549092054367065
train gradient:  0.14061559103542393
iteration : 10195
train acc:  0.8828125
train loss:  0.3012118637561798
train gradient:  0.1300274459032614
iteration : 10196
train acc:  0.84375
train loss:  0.3798360228538513
train gradient:  0.1970267395677608
iteration : 10197
train acc:  0.8671875
train loss:  0.31419700384140015
train gradient:  0.15022475493770623
iteration : 10198
train acc:  0.859375
train loss:  0.32478439807891846
train gradient:  0.16058382954032413
iteration : 10199
train acc:  0.84375
train loss:  0.3259252607822418
train gradient:  0.1361527906741554
iteration : 10200
train acc:  0.875
train loss:  0.30779504776000977
train gradient:  0.15671654616822991
iteration : 10201
train acc:  0.859375
train loss:  0.32346075773239136
train gradient:  0.1041299505770067
iteration : 10202
train acc:  0.921875
train loss:  0.2126464545726776
train gradient:  0.07897242070694768
iteration : 10203
train acc:  0.875
train loss:  0.287858247756958
train gradient:  0.24027667347590187
iteration : 10204
train acc:  0.8359375
train loss:  0.3581075668334961
train gradient:  0.11465178281406452
iteration : 10205
train acc:  0.8671875
train loss:  0.29213881492614746
train gradient:  0.13365740286380812
iteration : 10206
train acc:  0.890625
train loss:  0.28357505798339844
train gradient:  0.17507098937912163
iteration : 10207
train acc:  0.8125
train loss:  0.39593392610549927
train gradient:  0.21918259927869316
iteration : 10208
train acc:  0.859375
train loss:  0.3258935213088989
train gradient:  0.11040084610377145
iteration : 10209
train acc:  0.8046875
train loss:  0.39427632093429565
train gradient:  0.22543695113550433
iteration : 10210
train acc:  0.8828125
train loss:  0.3063361644744873
train gradient:  0.14631354968066917
iteration : 10211
train acc:  0.890625
train loss:  0.27946531772613525
train gradient:  0.09577340748818866
iteration : 10212
train acc:  0.875
train loss:  0.288019061088562
train gradient:  0.09580809001921926
iteration : 10213
train acc:  0.8046875
train loss:  0.37681692838668823
train gradient:  0.1833564801354873
iteration : 10214
train acc:  0.8671875
train loss:  0.3403760492801666
train gradient:  0.1846973321245751
iteration : 10215
train acc:  0.8203125
train loss:  0.32760390639305115
train gradient:  0.12147473000231447
iteration : 10216
train acc:  0.8203125
train loss:  0.40415865182876587
train gradient:  0.21620424021963713
iteration : 10217
train acc:  0.8828125
train loss:  0.323909193277359
train gradient:  0.17035550904987043
iteration : 10218
train acc:  0.8984375
train loss:  0.2607855200767517
train gradient:  0.14197768446202136
iteration : 10219
train acc:  0.828125
train loss:  0.3796594440937042
train gradient:  0.20130774647948002
iteration : 10220
train acc:  0.875
train loss:  0.2796889543533325
train gradient:  0.16688399952277277
iteration : 10221
train acc:  0.8828125
train loss:  0.28181320428848267
train gradient:  0.10312923707460388
iteration : 10222
train acc:  0.9140625
train loss:  0.2165907323360443
train gradient:  0.08645731860038831
iteration : 10223
train acc:  0.8828125
train loss:  0.2924429774284363
train gradient:  0.11794510506978192
iteration : 10224
train acc:  0.8515625
train loss:  0.3393205404281616
train gradient:  0.1634803035871661
iteration : 10225
train acc:  0.8125
train loss:  0.3378261625766754
train gradient:  0.22141851227070575
iteration : 10226
train acc:  0.890625
train loss:  0.3076971769332886
train gradient:  0.21477878445564036
iteration : 10227
train acc:  0.890625
train loss:  0.2871476113796234
train gradient:  0.13708110069359886
iteration : 10228
train acc:  0.8671875
train loss:  0.29754573106765747
train gradient:  0.12520520889708522
iteration : 10229
train acc:  0.8359375
train loss:  0.3558683395385742
train gradient:  0.13444104573118967
iteration : 10230
train acc:  0.890625
train loss:  0.33892756700515747
train gradient:  0.23687855639327265
iteration : 10231
train acc:  0.8359375
train loss:  0.3268955647945404
train gradient:  0.164928416693006
iteration : 10232
train acc:  0.84375
train loss:  0.33902257680892944
train gradient:  0.17570998147773975
iteration : 10233
train acc:  0.921875
train loss:  0.2828138768672943
train gradient:  0.139032258103909
iteration : 10234
train acc:  0.859375
train loss:  0.3370494842529297
train gradient:  0.1601423988347171
iteration : 10235
train acc:  0.890625
train loss:  0.2737177014350891
train gradient:  0.12921544069797736
iteration : 10236
train acc:  0.890625
train loss:  0.2771396338939667
train gradient:  0.10543500726503627
iteration : 10237
train acc:  0.8203125
train loss:  0.3860127031803131
train gradient:  0.2062899454652398
iteration : 10238
train acc:  0.90625
train loss:  0.24845823645591736
train gradient:  0.07609761180578688
iteration : 10239
train acc:  0.84375
train loss:  0.2649737000465393
train gradient:  0.08909041729029586
iteration : 10240
train acc:  0.8125
train loss:  0.39269617199897766
train gradient:  0.32323085729131557
iteration : 10241
train acc:  0.8515625
train loss:  0.3193158209323883
train gradient:  0.1947620499865353
iteration : 10242
train acc:  0.8515625
train loss:  0.3341690003871918
train gradient:  0.1962018218141019
iteration : 10243
train acc:  0.890625
train loss:  0.3819173574447632
train gradient:  0.18503821702785733
iteration : 10244
train acc:  0.8828125
train loss:  0.2545698285102844
train gradient:  0.08546667850790761
iteration : 10245
train acc:  0.8515625
train loss:  0.3799781799316406
train gradient:  0.16652035711547009
iteration : 10246
train acc:  0.875
train loss:  0.3108710050582886
train gradient:  0.1626080170738275
iteration : 10247
train acc:  0.90625
train loss:  0.24604029953479767
train gradient:  0.13301234299628
iteration : 10248
train acc:  0.859375
train loss:  0.3328469693660736
train gradient:  0.17618333129276104
iteration : 10249
train acc:  0.796875
train loss:  0.38791173696517944
train gradient:  0.19763479971164402
iteration : 10250
train acc:  0.84375
train loss:  0.3360661268234253
train gradient:  0.309648598835872
iteration : 10251
train acc:  0.8828125
train loss:  0.25530678033828735
train gradient:  0.11921404275601312
iteration : 10252
train acc:  0.8671875
train loss:  0.3082699179649353
train gradient:  0.17151411604865618
iteration : 10253
train acc:  0.875
train loss:  0.3376246392726898
train gradient:  0.2177123667308447
iteration : 10254
train acc:  0.8671875
train loss:  0.321502149105072
train gradient:  0.13968920622539238
iteration : 10255
train acc:  0.8828125
train loss:  0.3040370047092438
train gradient:  0.1963454410509224
iteration : 10256
train acc:  0.8125
train loss:  0.4023502767086029
train gradient:  0.19014421635594198
iteration : 10257
train acc:  0.8203125
train loss:  0.379285603761673
train gradient:  0.19046666258996653
iteration : 10258
train acc:  0.875
train loss:  0.28739598393440247
train gradient:  0.17484940392353884
iteration : 10259
train acc:  0.8203125
train loss:  0.37835025787353516
train gradient:  0.19590742814572543
iteration : 10260
train acc:  0.8984375
train loss:  0.2841171324253082
train gradient:  0.3445383999378982
iteration : 10261
train acc:  0.859375
train loss:  0.32636362314224243
train gradient:  0.1656130634557193
iteration : 10262
train acc:  0.8671875
train loss:  0.32391852140426636
train gradient:  0.1840082486094093
iteration : 10263
train acc:  0.828125
train loss:  0.3987363278865814
train gradient:  0.17734384496895117
iteration : 10264
train acc:  0.8828125
train loss:  0.28682464361190796
train gradient:  0.15491404261052652
iteration : 10265
train acc:  0.8515625
train loss:  0.3568921387195587
train gradient:  0.19031325165018087
iteration : 10266
train acc:  0.890625
train loss:  0.28516465425491333
train gradient:  0.14721953937877602
iteration : 10267
train acc:  0.84375
train loss:  0.3713022470474243
train gradient:  0.24498890616612615
iteration : 10268
train acc:  0.8515625
train loss:  0.30034488439559937
train gradient:  0.16852325314070138
iteration : 10269
train acc:  0.84375
train loss:  0.34034794569015503
train gradient:  0.18035062430348733
iteration : 10270
train acc:  0.84375
train loss:  0.3380163311958313
train gradient:  0.16984919011382174
iteration : 10271
train acc:  0.8828125
train loss:  0.29111915826797485
train gradient:  0.17517039848250143
iteration : 10272
train acc:  0.84375
train loss:  0.4395211935043335
train gradient:  0.3627669802251564
iteration : 10273
train acc:  0.875
train loss:  0.2844313085079193
train gradient:  0.17708296788567784
iteration : 10274
train acc:  0.8515625
train loss:  0.30854111909866333
train gradient:  0.248465967836364
iteration : 10275
train acc:  0.84375
train loss:  0.34534287452697754
train gradient:  0.17561510851305956
iteration : 10276
train acc:  0.8984375
train loss:  0.2516951262950897
train gradient:  0.09397275439386846
iteration : 10277
train acc:  0.859375
train loss:  0.27090853452682495
train gradient:  0.15269774007199866
iteration : 10278
train acc:  0.890625
train loss:  0.2651074528694153
train gradient:  0.13781116707897806
iteration : 10279
train acc:  0.90625
train loss:  0.23310458660125732
train gradient:  0.08222192273997522
iteration : 10280
train acc:  0.875
train loss:  0.3369220495223999
train gradient:  0.22710901496416175
iteration : 10281
train acc:  0.90625
train loss:  0.3710528016090393
train gradient:  0.23972047270437097
iteration : 10282
train acc:  0.875
train loss:  0.2635877728462219
train gradient:  0.11740810727821546
iteration : 10283
train acc:  0.8125
train loss:  0.3591417372226715
train gradient:  0.18821159686351954
iteration : 10284
train acc:  0.890625
train loss:  0.3372279405593872
train gradient:  0.19276025791608442
iteration : 10285
train acc:  0.875
train loss:  0.2506086230278015
train gradient:  0.0993648286135524
iteration : 10286
train acc:  0.890625
train loss:  0.27790212631225586
train gradient:  0.10695094822054083
iteration : 10287
train acc:  0.8203125
train loss:  0.3230524957180023
train gradient:  0.15924028689751402
iteration : 10288
train acc:  0.8984375
train loss:  0.24969179928302765
train gradient:  0.10557451420382241
iteration : 10289
train acc:  0.8125
train loss:  0.39673930406570435
train gradient:  0.23311956165247955
iteration : 10290
train acc:  0.8828125
train loss:  0.2695557177066803
train gradient:  0.12476175093245667
iteration : 10291
train acc:  0.859375
train loss:  0.3238515853881836
train gradient:  0.12732431973024444
iteration : 10292
train acc:  0.9140625
train loss:  0.2817010283470154
train gradient:  0.10692850350700689
iteration : 10293
train acc:  0.8671875
train loss:  0.3270973861217499
train gradient:  0.18933205426252114
iteration : 10294
train acc:  0.859375
train loss:  0.29585176706314087
train gradient:  0.15732605087979834
iteration : 10295
train acc:  0.828125
train loss:  0.4066709876060486
train gradient:  0.22771105679432252
iteration : 10296
train acc:  0.90625
train loss:  0.2531065344810486
train gradient:  0.10612039640717175
iteration : 10297
train acc:  0.8515625
train loss:  0.38590943813323975
train gradient:  0.2767950061535644
iteration : 10298
train acc:  0.90625
train loss:  0.2762516140937805
train gradient:  0.12605192757277517
iteration : 10299
train acc:  0.8203125
train loss:  0.4028638005256653
train gradient:  0.22730442197750855
iteration : 10300
train acc:  0.8671875
train loss:  0.32053059339523315
train gradient:  0.12093875869767255
iteration : 10301
train acc:  0.8671875
train loss:  0.2728559970855713
train gradient:  0.1255904606416513
iteration : 10302
train acc:  0.796875
train loss:  0.44898390769958496
train gradient:  0.2110443054619514
iteration : 10303
train acc:  0.9296875
train loss:  0.22646301984786987
train gradient:  0.11808241898380717
iteration : 10304
train acc:  0.8671875
train loss:  0.3028707206249237
train gradient:  0.13479611148666665
iteration : 10305
train acc:  0.8671875
train loss:  0.3108358383178711
train gradient:  0.11491016688682236
iteration : 10306
train acc:  0.84375
train loss:  0.3184555470943451
train gradient:  0.13003946038445832
iteration : 10307
train acc:  0.875
train loss:  0.2931779623031616
train gradient:  0.16725583921468545
iteration : 10308
train acc:  0.8828125
train loss:  0.31881797313690186
train gradient:  0.18144207895957531
iteration : 10309
train acc:  0.8671875
train loss:  0.25951945781707764
train gradient:  0.10722201637572523
iteration : 10310
train acc:  0.8046875
train loss:  0.3731195032596588
train gradient:  0.20833355145511265
iteration : 10311
train acc:  0.8203125
train loss:  0.392630398273468
train gradient:  0.2996441250818994
iteration : 10312
train acc:  0.8984375
train loss:  0.2989213466644287
train gradient:  0.18021292636906527
iteration : 10313
train acc:  0.8515625
train loss:  0.27100515365600586
train gradient:  0.10861829543021335
iteration : 10314
train acc:  0.8359375
train loss:  0.2828209102153778
train gradient:  0.16933324303547692
iteration : 10315
train acc:  0.84375
train loss:  0.320749968290329
train gradient:  0.15488327462248141
iteration : 10316
train acc:  0.859375
train loss:  0.32275381684303284
train gradient:  0.18355038865120693
iteration : 10317
train acc:  0.90625
train loss:  0.26558467745780945
train gradient:  0.11239695079692871
iteration : 10318
train acc:  0.8515625
train loss:  0.37603721022605896
train gradient:  0.2003592867650057
iteration : 10319
train acc:  0.875
train loss:  0.31245890259742737
train gradient:  0.12432269190595317
iteration : 10320
train acc:  0.78125
train loss:  0.43828707933425903
train gradient:  0.33503880932742564
iteration : 10321
train acc:  0.8671875
train loss:  0.34088635444641113
train gradient:  0.14445377789871666
iteration : 10322
train acc:  0.8984375
train loss:  0.3024975061416626
train gradient:  0.1447238814635093
iteration : 10323
train acc:  0.859375
train loss:  0.30507469177246094
train gradient:  0.13438454507384917
iteration : 10324
train acc:  0.859375
train loss:  0.348238468170166
train gradient:  0.20240265155732245
iteration : 10325
train acc:  0.8359375
train loss:  0.32339242100715637
train gradient:  0.12696212973237525
iteration : 10326
train acc:  0.859375
train loss:  0.2842198610305786
train gradient:  0.09336711633503254
iteration : 10327
train acc:  0.8671875
train loss:  0.3237918019294739
train gradient:  0.1338026058120781
iteration : 10328
train acc:  0.859375
train loss:  0.310613214969635
train gradient:  0.12333566287476928
iteration : 10329
train acc:  0.828125
train loss:  0.35842496156692505
train gradient:  0.21127494108303463
iteration : 10330
train acc:  0.90625
train loss:  0.24382032454013824
train gradient:  0.14975873657833944
iteration : 10331
train acc:  0.7578125
train loss:  0.45711272954940796
train gradient:  0.33183507670042756
iteration : 10332
train acc:  0.8984375
train loss:  0.3143177330493927
train gradient:  0.18765183939181218
iteration : 10333
train acc:  0.8984375
train loss:  0.3034244775772095
train gradient:  0.11683177951997131
iteration : 10334
train acc:  0.84375
train loss:  0.40802305936813354
train gradient:  0.27796174391065276
iteration : 10335
train acc:  0.890625
train loss:  0.23903658986091614
train gradient:  0.11672767074601365
iteration : 10336
train acc:  0.8125
train loss:  0.38663405179977417
train gradient:  0.2512620607493906
iteration : 10337
train acc:  0.8203125
train loss:  0.3975813388824463
train gradient:  0.29844827994498085
iteration : 10338
train acc:  0.90625
train loss:  0.27041304111480713
train gradient:  0.13344226791988367
iteration : 10339
train acc:  0.828125
train loss:  0.3448762595653534
train gradient:  0.3377712325730874
iteration : 10340
train acc:  0.890625
train loss:  0.254111111164093
train gradient:  0.13380655041136838
iteration : 10341
train acc:  0.8203125
train loss:  0.3453788161277771
train gradient:  0.18845795426448028
iteration : 10342
train acc:  0.8984375
train loss:  0.2681962549686432
train gradient:  0.1405112675248773
iteration : 10343
train acc:  0.8125
train loss:  0.43932074308395386
train gradient:  0.3115111580226238
iteration : 10344
train acc:  0.890625
train loss:  0.28363049030303955
train gradient:  0.14859026546517018
iteration : 10345
train acc:  0.8515625
train loss:  0.3033587336540222
train gradient:  0.1404567194242887
iteration : 10346
train acc:  0.828125
train loss:  0.3263755142688751
train gradient:  0.153983298562849
iteration : 10347
train acc:  0.8125
train loss:  0.3182760775089264
train gradient:  0.18457019891342588
iteration : 10348
train acc:  0.859375
train loss:  0.3011413812637329
train gradient:  0.14034420383698565
iteration : 10349
train acc:  0.8515625
train loss:  0.2894599735736847
train gradient:  0.10339005555046157
iteration : 10350
train acc:  0.8828125
train loss:  0.2683839797973633
train gradient:  0.15933969653489194
iteration : 10351
train acc:  0.8515625
train loss:  0.3366251289844513
train gradient:  0.18216895094728636
iteration : 10352
train acc:  0.8515625
train loss:  0.30526477098464966
train gradient:  0.14866065289963923
iteration : 10353
train acc:  0.8515625
train loss:  0.33159202337265015
train gradient:  0.13260564391743543
iteration : 10354
train acc:  0.8828125
train loss:  0.26873013377189636
train gradient:  0.1303936844742476
iteration : 10355
train acc:  0.875
train loss:  0.3971080780029297
train gradient:  0.2839889531825289
iteration : 10356
train acc:  0.8515625
train loss:  0.31436485052108765
train gradient:  0.19798134062208822
iteration : 10357
train acc:  0.796875
train loss:  0.40527522563934326
train gradient:  0.24179925393696589
iteration : 10358
train acc:  0.8671875
train loss:  0.3488060235977173
train gradient:  0.19213014263671996
iteration : 10359
train acc:  0.8125
train loss:  0.37736111879348755
train gradient:  0.18982632741183705
iteration : 10360
train acc:  0.875
train loss:  0.3030204772949219
train gradient:  0.1870628679069884
iteration : 10361
train acc:  0.84375
train loss:  0.31340330839157104
train gradient:  0.17284487261930354
iteration : 10362
train acc:  0.875
train loss:  0.3189981281757355
train gradient:  0.18391922773108552
iteration : 10363
train acc:  0.828125
train loss:  0.3641203045845032
train gradient:  0.26620066577568025
iteration : 10364
train acc:  0.859375
train loss:  0.31820112466812134
train gradient:  0.1663427164511629
iteration : 10365
train acc:  0.875
train loss:  0.31041020154953003
train gradient:  0.14420031497454597
iteration : 10366
train acc:  0.8671875
train loss:  0.3099856376647949
train gradient:  0.111583654247985
iteration : 10367
train acc:  0.90625
train loss:  0.31186866760253906
train gradient:  0.15232390991447253
iteration : 10368
train acc:  0.8359375
train loss:  0.36814069747924805
train gradient:  0.2697899933328352
iteration : 10369
train acc:  0.8671875
train loss:  0.32514578104019165
train gradient:  0.17970532184685445
iteration : 10370
train acc:  0.890625
train loss:  0.236826092004776
train gradient:  0.132234589464741
iteration : 10371
train acc:  0.84375
train loss:  0.3720487952232361
train gradient:  0.17233716411654287
iteration : 10372
train acc:  0.8359375
train loss:  0.40235474705696106
train gradient:  0.20147077145385112
iteration : 10373
train acc:  0.890625
train loss:  0.2849344313144684
train gradient:  0.12600048263075153
iteration : 10374
train acc:  0.8359375
train loss:  0.3734249174594879
train gradient:  0.2238638458883561
iteration : 10375
train acc:  0.8359375
train loss:  0.3341294527053833
train gradient:  0.17342475030911253
iteration : 10376
train acc:  0.84375
train loss:  0.33997058868408203
train gradient:  0.18480727532264452
iteration : 10377
train acc:  0.8671875
train loss:  0.37827152013778687
train gradient:  0.16936847828940066
iteration : 10378
train acc:  0.8046875
train loss:  0.37585335969924927
train gradient:  0.19680384907993292
iteration : 10379
train acc:  0.8359375
train loss:  0.3448435068130493
train gradient:  0.16783972702495156
iteration : 10380
train acc:  0.890625
train loss:  0.3221847116947174
train gradient:  0.16073270859395944
iteration : 10381
train acc:  0.8359375
train loss:  0.3263177275657654
train gradient:  0.2401476249427953
iteration : 10382
train acc:  0.890625
train loss:  0.29405879974365234
train gradient:  0.14323688612464514
iteration : 10383
train acc:  0.8671875
train loss:  0.35593438148498535
train gradient:  0.1605013969863833
iteration : 10384
train acc:  0.9140625
train loss:  0.24651338160037994
train gradient:  0.11047610613198384
iteration : 10385
train acc:  0.8359375
train loss:  0.31413698196411133
train gradient:  0.15910359109699912
iteration : 10386
train acc:  0.875
train loss:  0.28707441687583923
train gradient:  0.13072465019535964
iteration : 10387
train acc:  0.8359375
train loss:  0.3537602722644806
train gradient:  0.23914756142739363
iteration : 10388
train acc:  0.8125
train loss:  0.36373502016067505
train gradient:  0.16341294107291326
iteration : 10389
train acc:  0.828125
train loss:  0.38592803478240967
train gradient:  0.24703398637643925
iteration : 10390
train acc:  0.859375
train loss:  0.35086631774902344
train gradient:  0.18549115671785602
iteration : 10391
train acc:  0.8203125
train loss:  0.32862788438796997
train gradient:  0.17000889454554474
iteration : 10392
train acc:  0.8984375
train loss:  0.3291899263858795
train gradient:  0.17401174926640822
iteration : 10393
train acc:  0.75
train loss:  0.4477217197418213
train gradient:  0.21007106174012002
iteration : 10394
train acc:  0.859375
train loss:  0.3082018196582794
train gradient:  0.2623120969535931
iteration : 10395
train acc:  0.8515625
train loss:  0.33414575457572937
train gradient:  0.17503442134459812
iteration : 10396
train acc:  0.8984375
train loss:  0.2495807409286499
train gradient:  0.13033030484150168
iteration : 10397
train acc:  0.8828125
train loss:  0.2970399260520935
train gradient:  0.1477712344483828
iteration : 10398
train acc:  0.8359375
train loss:  0.3404409885406494
train gradient:  0.22176326280848557
iteration : 10399
train acc:  0.890625
train loss:  0.33297672867774963
train gradient:  0.17080765202611514
iteration : 10400
train acc:  0.8828125
train loss:  0.29953885078430176
train gradient:  0.11329638367952134
iteration : 10401
train acc:  0.78125
train loss:  0.4274539351463318
train gradient:  0.2910479757692108
iteration : 10402
train acc:  0.84375
train loss:  0.3550128638744354
train gradient:  0.1686606585039071
iteration : 10403
train acc:  0.8671875
train loss:  0.34382784366607666
train gradient:  0.16776307037338792
iteration : 10404
train acc:  0.8515625
train loss:  0.33631429076194763
train gradient:  0.21626928355414374
iteration : 10405
train acc:  0.8984375
train loss:  0.28705543279647827
train gradient:  0.1273476345389571
iteration : 10406
train acc:  0.90625
train loss:  0.2679305672645569
train gradient:  0.12189653481002882
iteration : 10407
train acc:  0.859375
train loss:  0.36314859986305237
train gradient:  0.13581912634062565
iteration : 10408
train acc:  0.8671875
train loss:  0.2585820257663727
train gradient:  0.10854302347116233
iteration : 10409
train acc:  0.859375
train loss:  0.3310210704803467
train gradient:  0.24790902720964605
iteration : 10410
train acc:  0.890625
train loss:  0.27649936079978943
train gradient:  0.12215935639293969
iteration : 10411
train acc:  0.84375
train loss:  0.321061372756958
train gradient:  0.2290504442202598
iteration : 10412
train acc:  0.8125
train loss:  0.43258777260780334
train gradient:  0.18334970226832759
iteration : 10413
train acc:  0.875
train loss:  0.31440770626068115
train gradient:  0.15443538607364482
iteration : 10414
train acc:  0.875
train loss:  0.3034704923629761
train gradient:  0.13050458745769392
iteration : 10415
train acc:  0.875
train loss:  0.2833299934864044
train gradient:  0.17348837187439486
iteration : 10416
train acc:  0.875
train loss:  0.28209906816482544
train gradient:  0.1732780509359643
iteration : 10417
train acc:  0.859375
train loss:  0.3197069466114044
train gradient:  0.12031413026529815
iteration : 10418
train acc:  0.8515625
train loss:  0.2986885905265808
train gradient:  0.08917729439257224
iteration : 10419
train acc:  0.8984375
train loss:  0.288063645362854
train gradient:  0.13781837599701185
iteration : 10420
train acc:  0.859375
train loss:  0.3071150481700897
train gradient:  0.18312536805456195
iteration : 10421
train acc:  0.8984375
train loss:  0.29165488481521606
train gradient:  0.11944198481275779
iteration : 10422
train acc:  0.828125
train loss:  0.36041510105133057
train gradient:  0.14837623684939327
iteration : 10423
train acc:  0.8359375
train loss:  0.3823879659175873
train gradient:  0.16563262983437083
iteration : 10424
train acc:  0.8203125
train loss:  0.3202243745326996
train gradient:  0.17102198722809725
iteration : 10425
train acc:  0.90625
train loss:  0.26895010471343994
train gradient:  0.08871286629286661
iteration : 10426
train acc:  0.875
train loss:  0.2558687925338745
train gradient:  0.12977001698218085
iteration : 10427
train acc:  0.796875
train loss:  0.4125632643699646
train gradient:  0.2141704050648349
iteration : 10428
train acc:  0.8671875
train loss:  0.35491982102394104
train gradient:  0.14859601271655215
iteration : 10429
train acc:  0.796875
train loss:  0.3841601014137268
train gradient:  0.17143859525470018
iteration : 10430
train acc:  0.8515625
train loss:  0.3642713129520416
train gradient:  0.16747625014528664
iteration : 10431
train acc:  0.8125
train loss:  0.42608022689819336
train gradient:  0.21522098025423253
iteration : 10432
train acc:  0.8203125
train loss:  0.390847384929657
train gradient:  0.15438690387435514
iteration : 10433
train acc:  0.859375
train loss:  0.3467220067977905
train gradient:  0.1384172362196543
iteration : 10434
train acc:  0.8046875
train loss:  0.38650280237197876
train gradient:  0.14659294728181077
iteration : 10435
train acc:  0.8359375
train loss:  0.3370121419429779
train gradient:  0.15620699638907065
iteration : 10436
train acc:  0.8828125
train loss:  0.2845773696899414
train gradient:  0.13129100734196947
iteration : 10437
train acc:  0.90625
train loss:  0.23498302698135376
train gradient:  0.12598992225796685
iteration : 10438
train acc:  0.828125
train loss:  0.37037912011146545
train gradient:  0.21594193556975916
iteration : 10439
train acc:  0.84375
train loss:  0.3647717833518982
train gradient:  0.18232978866364527
iteration : 10440
train acc:  0.8359375
train loss:  0.32185792922973633
train gradient:  0.2519138579944713
iteration : 10441
train acc:  0.8671875
train loss:  0.3560895323753357
train gradient:  0.16150617228435593
iteration : 10442
train acc:  0.78125
train loss:  0.4638102650642395
train gradient:  0.2307723577795102
iteration : 10443
train acc:  0.875
train loss:  0.3196289539337158
train gradient:  0.12738841026728917
iteration : 10444
train acc:  0.828125
train loss:  0.3709460496902466
train gradient:  0.1735801324591671
iteration : 10445
train acc:  0.828125
train loss:  0.3725265562534332
train gradient:  0.2519777446559688
iteration : 10446
train acc:  0.8671875
train loss:  0.3417495787143707
train gradient:  0.13626626399012276
iteration : 10447
train acc:  0.84375
train loss:  0.34987249970436096
train gradient:  0.2128099699582961
iteration : 10448
train acc:  0.828125
train loss:  0.34749317169189453
train gradient:  0.21029503363404564
iteration : 10449
train acc:  0.890625
train loss:  0.2928421199321747
train gradient:  0.09178006274531474
iteration : 10450
train acc:  0.8828125
train loss:  0.24892374873161316
train gradient:  0.16124446793828567
iteration : 10451
train acc:  0.828125
train loss:  0.3508082926273346
train gradient:  0.22074141711147688
iteration : 10452
train acc:  0.8515625
train loss:  0.3369999825954437
train gradient:  0.1173162012950929
iteration : 10453
train acc:  0.890625
train loss:  0.2897205650806427
train gradient:  0.1226504798724476
iteration : 10454
train acc:  0.8984375
train loss:  0.2862437963485718
train gradient:  0.1400841027871138
iteration : 10455
train acc:  0.8203125
train loss:  0.36729517579078674
train gradient:  0.18799046663805047
iteration : 10456
train acc:  0.84375
train loss:  0.3838563859462738
train gradient:  0.18237532081073493
iteration : 10457
train acc:  0.875
train loss:  0.34572142362594604
train gradient:  0.16562883693327818
iteration : 10458
train acc:  0.875
train loss:  0.34799182415008545
train gradient:  0.19351341237856093
iteration : 10459
train acc:  0.8671875
train loss:  0.3439847230911255
train gradient:  0.14023924178635727
iteration : 10460
train acc:  0.796875
train loss:  0.3577865958213806
train gradient:  0.26323589096665917
iteration : 10461
train acc:  0.8203125
train loss:  0.37586021423339844
train gradient:  0.15063344859396258
iteration : 10462
train acc:  0.875
train loss:  0.29917842149734497
train gradient:  0.4369138845120952
iteration : 10463
train acc:  0.84375
train loss:  0.3217306435108185
train gradient:  0.1207070326457073
iteration : 10464
train acc:  0.8203125
train loss:  0.3986748158931732
train gradient:  0.18456023824906567
iteration : 10465
train acc:  0.8984375
train loss:  0.2639382481575012
train gradient:  0.15836813453001558
iteration : 10466
train acc:  0.84375
train loss:  0.3366137146949768
train gradient:  0.14859327420296128
iteration : 10467
train acc:  0.8515625
train loss:  0.29724839329719543
train gradient:  0.11168287945568987
iteration : 10468
train acc:  0.875
train loss:  0.3328409194946289
train gradient:  0.18750397356182652
iteration : 10469
train acc:  0.8984375
train loss:  0.28138577938079834
train gradient:  0.10614345189569992
iteration : 10470
train acc:  0.859375
train loss:  0.3458031117916107
train gradient:  0.17386100824010697
iteration : 10471
train acc:  0.8203125
train loss:  0.4184708893299103
train gradient:  0.23106292659411004
iteration : 10472
train acc:  0.8671875
train loss:  0.32206064462661743
train gradient:  0.12960407626156578
iteration : 10473
train acc:  0.90625
train loss:  0.2513398230075836
train gradient:  0.09243957357660192
iteration : 10474
train acc:  0.875
train loss:  0.3044365644454956
train gradient:  0.1319050174441573
iteration : 10475
train acc:  0.890625
train loss:  0.30559396743774414
train gradient:  0.13053664844014876
iteration : 10476
train acc:  0.84375
train loss:  0.38048115372657776
train gradient:  0.1862393038189154
iteration : 10477
train acc:  0.8828125
train loss:  0.28262418508529663
train gradient:  0.11223795801697524
iteration : 10478
train acc:  0.765625
train loss:  0.4835827946662903
train gradient:  0.3067401177762215
iteration : 10479
train acc:  0.84375
train loss:  0.3354896605014801
train gradient:  0.1302432703780202
iteration : 10480
train acc:  0.875
train loss:  0.2939847707748413
train gradient:  0.09789909342892378
iteration : 10481
train acc:  0.921875
train loss:  0.2558477222919464
train gradient:  0.09154731770342199
iteration : 10482
train acc:  0.890625
train loss:  0.3063242435455322
train gradient:  0.21462464540390777
iteration : 10483
train acc:  0.84375
train loss:  0.329414963722229
train gradient:  0.15344188084647284
iteration : 10484
train acc:  0.890625
train loss:  0.25384244322776794
train gradient:  0.10475705920735646
iteration : 10485
train acc:  0.9140625
train loss:  0.23520871996879578
train gradient:  0.07823974577419383
iteration : 10486
train acc:  0.84375
train loss:  0.363656610250473
train gradient:  0.2957394542353245
iteration : 10487
train acc:  0.8828125
train loss:  0.3138304352760315
train gradient:  0.09881943635926377
iteration : 10488
train acc:  0.84375
train loss:  0.37092429399490356
train gradient:  0.17733390715671787
iteration : 10489
train acc:  0.8203125
train loss:  0.3874131441116333
train gradient:  0.16236890809065962
iteration : 10490
train acc:  0.875
train loss:  0.3282260298728943
train gradient:  0.10359094438755087
iteration : 10491
train acc:  0.8671875
train loss:  0.27213573455810547
train gradient:  0.11735884919995497
iteration : 10492
train acc:  0.84375
train loss:  0.3250085413455963
train gradient:  0.17306886452208295
iteration : 10493
train acc:  0.890625
train loss:  0.2671584188938141
train gradient:  0.16077566534724957
iteration : 10494
train acc:  0.7578125
train loss:  0.46830788254737854
train gradient:  0.4321726841826916
iteration : 10495
train acc:  0.8359375
train loss:  0.30286866426467896
train gradient:  0.15142066207608457
iteration : 10496
train acc:  0.8984375
train loss:  0.2580650746822357
train gradient:  0.10728554638646566
iteration : 10497
train acc:  0.8515625
train loss:  0.3078327476978302
train gradient:  0.16204857401322045
iteration : 10498
train acc:  0.8515625
train loss:  0.3375730812549591
train gradient:  0.15360829552776575
iteration : 10499
train acc:  0.859375
train loss:  0.3163430094718933
train gradient:  0.2347879889352743
iteration : 10500
train acc:  0.8828125
train loss:  0.3097173869609833
train gradient:  0.11573568853613293
iteration : 10501
train acc:  0.890625
train loss:  0.2851521074771881
train gradient:  0.13176580822529443
iteration : 10502
train acc:  0.859375
train loss:  0.3075653314590454
train gradient:  0.1277109805922142
iteration : 10503
train acc:  0.8203125
train loss:  0.3651949167251587
train gradient:  0.15514496590526528
iteration : 10504
train acc:  0.8515625
train loss:  0.32473817467689514
train gradient:  0.12338441230967208
iteration : 10505
train acc:  0.828125
train loss:  0.34148138761520386
train gradient:  0.18244821138797523
iteration : 10506
train acc:  0.859375
train loss:  0.34078580141067505
train gradient:  0.17357231718711807
iteration : 10507
train acc:  0.8515625
train loss:  0.3738206624984741
train gradient:  0.1937990742803659
iteration : 10508
train acc:  0.8984375
train loss:  0.24458906054496765
train gradient:  0.07394077591313152
iteration : 10509
train acc:  0.890625
train loss:  0.2835574746131897
train gradient:  0.09796788493779891
iteration : 10510
train acc:  0.8515625
train loss:  0.3183547258377075
train gradient:  0.20523689885321067
iteration : 10511
train acc:  0.8515625
train loss:  0.36137181520462036
train gradient:  0.13807456225962186
iteration : 10512
train acc:  0.8828125
train loss:  0.28851377964019775
train gradient:  0.1200129933764279
iteration : 10513
train acc:  0.8671875
train loss:  0.3109414875507355
train gradient:  0.1722942933529805
iteration : 10514
train acc:  0.8828125
train loss:  0.3180723190307617
train gradient:  0.18376941457478072
iteration : 10515
train acc:  0.8046875
train loss:  0.3993191123008728
train gradient:  0.2146856199463194
iteration : 10516
train acc:  0.8359375
train loss:  0.3509508967399597
train gradient:  0.18003614068688129
iteration : 10517
train acc:  0.8984375
train loss:  0.2713524401187897
train gradient:  0.09976471708716073
iteration : 10518
train acc:  0.8828125
train loss:  0.26869523525238037
train gradient:  0.11448136335194244
iteration : 10519
train acc:  0.828125
train loss:  0.39517343044281006
train gradient:  0.2638556178987603
iteration : 10520
train acc:  0.8671875
train loss:  0.30501610040664673
train gradient:  0.13330556229895835
iteration : 10521
train acc:  0.8515625
train loss:  0.3217095136642456
train gradient:  0.13056433902580322
iteration : 10522
train acc:  0.8203125
train loss:  0.364531546831131
train gradient:  0.17132089920909516
iteration : 10523
train acc:  0.875
train loss:  0.28411534428596497
train gradient:  0.14326104907703752
iteration : 10524
train acc:  0.796875
train loss:  0.39127033948898315
train gradient:  0.2069717607253624
iteration : 10525
train acc:  0.8828125
train loss:  0.2730162441730499
train gradient:  0.10416248464267337
iteration : 10526
train acc:  0.90625
train loss:  0.2459702491760254
train gradient:  0.13479410235070827
iteration : 10527
train acc:  0.8671875
train loss:  0.3031945824623108
train gradient:  0.1318214371297503
iteration : 10528
train acc:  0.8984375
train loss:  0.27370935678482056
train gradient:  0.1197690790693081
iteration : 10529
train acc:  0.8046875
train loss:  0.47056466341018677
train gradient:  0.25955625908574365
iteration : 10530
train acc:  0.890625
train loss:  0.2963801622390747
train gradient:  0.1758193739916819
iteration : 10531
train acc:  0.8515625
train loss:  0.3341212570667267
train gradient:  0.15726659808704507
iteration : 10532
train acc:  0.875
train loss:  0.34268918633461
train gradient:  0.2130555475490663
iteration : 10533
train acc:  0.859375
train loss:  0.36262673139572144
train gradient:  0.2742138755523724
iteration : 10534
train acc:  0.859375
train loss:  0.3311869502067566
train gradient:  0.17661536082059562
iteration : 10535
train acc:  0.84375
train loss:  0.3306041955947876
train gradient:  0.14207833809630988
iteration : 10536
train acc:  0.859375
train loss:  0.36467671394348145
train gradient:  0.22755813549939052
iteration : 10537
train acc:  0.8828125
train loss:  0.2937018871307373
train gradient:  0.1453014972864629
iteration : 10538
train acc:  0.8515625
train loss:  0.30826741456985474
train gradient:  0.13558943235654958
iteration : 10539
train acc:  0.8515625
train loss:  0.28864896297454834
train gradient:  0.08954260932205144
iteration : 10540
train acc:  0.8984375
train loss:  0.23666003346443176
train gradient:  0.0943601822912439
iteration : 10541
train acc:  0.8671875
train loss:  0.37147510051727295
train gradient:  0.23219343457088398
iteration : 10542
train acc:  0.890625
train loss:  0.27795878052711487
train gradient:  0.17247126814955482
iteration : 10543
train acc:  0.90625
train loss:  0.2648474872112274
train gradient:  0.10443482458932345
iteration : 10544
train acc:  0.890625
train loss:  0.28692030906677246
train gradient:  0.1484009761361142
iteration : 10545
train acc:  0.859375
train loss:  0.38683778047561646
train gradient:  0.2245758401630747
iteration : 10546
train acc:  0.8359375
train loss:  0.311344712972641
train gradient:  0.15429729756602859
iteration : 10547
train acc:  0.890625
train loss:  0.280701220035553
train gradient:  0.17466701101026827
iteration : 10548
train acc:  0.9453125
train loss:  0.21298377215862274
train gradient:  0.09099287030712878
iteration : 10549
train acc:  0.8671875
train loss:  0.3264251947402954
train gradient:  0.12322082540596581
iteration : 10550
train acc:  0.8828125
train loss:  0.3004254698753357
train gradient:  0.12008297577074763
iteration : 10551
train acc:  0.8046875
train loss:  0.3880542516708374
train gradient:  0.28194304409373483
iteration : 10552
train acc:  0.8203125
train loss:  0.4885607063770294
train gradient:  0.42573797386633183
iteration : 10553
train acc:  0.890625
train loss:  0.3620590567588806
train gradient:  0.16990977432894802
iteration : 10554
train acc:  0.8671875
train loss:  0.2977815866470337
train gradient:  0.15712065140653933
iteration : 10555
train acc:  0.875
train loss:  0.2487218677997589
train gradient:  0.10526126648978815
iteration : 10556
train acc:  0.8984375
train loss:  0.2189178466796875
train gradient:  0.09184449129512062
iteration : 10557
train acc:  0.8515625
train loss:  0.35486504435539246
train gradient:  0.24272971077550518
iteration : 10558
train acc:  0.8515625
train loss:  0.3304625153541565
train gradient:  0.23421687759517973
iteration : 10559
train acc:  0.8828125
train loss:  0.27703753113746643
train gradient:  0.11587122741124356
iteration : 10560
train acc:  0.859375
train loss:  0.33180463314056396
train gradient:  0.13440131259792593
iteration : 10561
train acc:  0.8359375
train loss:  0.3382023572921753
train gradient:  0.16763242375768692
iteration : 10562
train acc:  0.890625
train loss:  0.29142269492149353
train gradient:  0.18248398538619803
iteration : 10563
train acc:  0.84375
train loss:  0.2872748076915741
train gradient:  0.14551634858087797
iteration : 10564
train acc:  0.859375
train loss:  0.296616792678833
train gradient:  0.22193166967785322
iteration : 10565
train acc:  0.8203125
train loss:  0.33160391449928284
train gradient:  0.15491837623414179
iteration : 10566
train acc:  0.8359375
train loss:  0.3842619061470032
train gradient:  0.24504606076847552
iteration : 10567
train acc:  0.828125
train loss:  0.307670533657074
train gradient:  0.15715202816572627
iteration : 10568
train acc:  0.8671875
train loss:  0.29571467638015747
train gradient:  0.14501274878587844
iteration : 10569
train acc:  0.8828125
train loss:  0.31165993213653564
train gradient:  0.13383251487702252
iteration : 10570
train acc:  0.8125
train loss:  0.44685620069503784
train gradient:  0.2226485058847338
iteration : 10571
train acc:  0.8515625
train loss:  0.3091353178024292
train gradient:  0.17907648281715982
iteration : 10572
train acc:  0.8984375
train loss:  0.2931250333786011
train gradient:  0.15009682275339675
iteration : 10573
train acc:  0.8671875
train loss:  0.28368550539016724
train gradient:  0.19399534969561916
iteration : 10574
train acc:  0.859375
train loss:  0.3460617959499359
train gradient:  0.15173490500952555
iteration : 10575
train acc:  0.890625
train loss:  0.2639632225036621
train gradient:  0.1715888637212375
iteration : 10576
train acc:  0.84375
train loss:  0.3510393500328064
train gradient:  0.1786267946283474
iteration : 10577
train acc:  0.8828125
train loss:  0.2801929712295532
train gradient:  0.1345387845057081
iteration : 10578
train acc:  0.8828125
train loss:  0.33236178755760193
train gradient:  0.15976183891456266
iteration : 10579
train acc:  0.828125
train loss:  0.31976181268692017
train gradient:  0.12986310778140062
iteration : 10580
train acc:  0.875
train loss:  0.29274553060531616
train gradient:  0.1374613382860259
iteration : 10581
train acc:  0.796875
train loss:  0.42750176787376404
train gradient:  0.2516986057819558
iteration : 10582
train acc:  0.859375
train loss:  0.3100227117538452
train gradient:  0.1428398674502333
iteration : 10583
train acc:  0.8359375
train loss:  0.3731698989868164
train gradient:  0.1799603487580687
iteration : 10584
train acc:  0.90625
train loss:  0.2211810052394867
train gradient:  0.1183310418111146
iteration : 10585
train acc:  0.8359375
train loss:  0.32338058948516846
train gradient:  0.16481302625213962
iteration : 10586
train acc:  0.8359375
train loss:  0.34552597999572754
train gradient:  0.13259037246803312
iteration : 10587
train acc:  0.8359375
train loss:  0.32053202390670776
train gradient:  0.14201135918997493
iteration : 10588
train acc:  0.890625
train loss:  0.26404982805252075
train gradient:  0.10139840185503642
iteration : 10589
train acc:  0.8984375
train loss:  0.3418906331062317
train gradient:  0.17406507426230905
iteration : 10590
train acc:  0.8359375
train loss:  0.3660210967063904
train gradient:  0.1966876630747707
iteration : 10591
train acc:  0.84375
train loss:  0.3262319266796112
train gradient:  0.12341200822014864
iteration : 10592
train acc:  0.8125
train loss:  0.43979570269584656
train gradient:  0.31226598259041494
iteration : 10593
train acc:  0.8359375
train loss:  0.3898858428001404
train gradient:  0.27620531150279426
iteration : 10594
train acc:  0.8828125
train loss:  0.2418650984764099
train gradient:  0.08293031036381096
iteration : 10595
train acc:  0.859375
train loss:  0.28574737906455994
train gradient:  0.13573585374575897
iteration : 10596
train acc:  0.8671875
train loss:  0.3488765358924866
train gradient:  0.18381951671799932
iteration : 10597
train acc:  0.84375
train loss:  0.3590339422225952
train gradient:  0.1729421471887259
iteration : 10598
train acc:  0.8203125
train loss:  0.31725192070007324
train gradient:  0.1840350966270728
iteration : 10599
train acc:  0.828125
train loss:  0.371130108833313
train gradient:  0.26520829979578686
iteration : 10600
train acc:  0.8984375
train loss:  0.27465111017227173
train gradient:  0.14157401621021257
iteration : 10601
train acc:  0.828125
train loss:  0.35493922233581543
train gradient:  0.1713582830617736
iteration : 10602
train acc:  0.828125
train loss:  0.35664525628089905
train gradient:  0.1466715402293127
iteration : 10603
train acc:  0.859375
train loss:  0.36195269227027893
train gradient:  0.14388363117381767
iteration : 10604
train acc:  0.8671875
train loss:  0.3129400908946991
train gradient:  0.17683170075568855
iteration : 10605
train acc:  0.90625
train loss:  0.2553843855857849
train gradient:  0.10270936694079968
iteration : 10606
train acc:  0.8671875
train loss:  0.31533336639404297
train gradient:  0.19589655931661898
iteration : 10607
train acc:  0.8671875
train loss:  0.26323753595352173
train gradient:  0.10514418680082938
iteration : 10608
train acc:  0.8828125
train loss:  0.29745718836784363
train gradient:  0.13956804754049384
iteration : 10609
train acc:  0.8984375
train loss:  0.2462655007839203
train gradient:  0.10969114165283768
iteration : 10610
train acc:  0.890625
train loss:  0.25833916664123535
train gradient:  0.16341611635315342
iteration : 10611
train acc:  0.875
train loss:  0.4048108160495758
train gradient:  0.246471237464228
iteration : 10612
train acc:  0.890625
train loss:  0.24031460285186768
train gradient:  0.11843033154552511
iteration : 10613
train acc:  0.8125
train loss:  0.34884804487228394
train gradient:  0.262977549951959
iteration : 10614
train acc:  0.921875
train loss:  0.2485896497964859
train gradient:  0.115040946260262
iteration : 10615
train acc:  0.859375
train loss:  0.28468334674835205
train gradient:  0.10745996812140533
iteration : 10616
train acc:  0.84375
train loss:  0.358721524477005
train gradient:  0.1519950051836791
iteration : 10617
train acc:  0.890625
train loss:  0.24909433722496033
train gradient:  0.10182604999895603
iteration : 10618
train acc:  0.859375
train loss:  0.3068588376045227
train gradient:  0.20393978176930452
iteration : 10619
train acc:  0.875
train loss:  0.33412766456604004
train gradient:  0.22508199020166608
iteration : 10620
train acc:  0.828125
train loss:  0.3527464270591736
train gradient:  0.17332058852576782
iteration : 10621
train acc:  0.7734375
train loss:  0.4454096555709839
train gradient:  0.28877319296625137
iteration : 10622
train acc:  0.875
train loss:  0.3482872545719147
train gradient:  0.1599306798880229
iteration : 10623
train acc:  0.875
train loss:  0.2868083715438843
train gradient:  0.16626156779465173
iteration : 10624
train acc:  0.859375
train loss:  0.3217262625694275
train gradient:  0.10382698918403091
iteration : 10625
train acc:  0.8828125
train loss:  0.27127599716186523
train gradient:  0.10917098191917247
iteration : 10626
train acc:  0.8828125
train loss:  0.3127795457839966
train gradient:  0.13939287135753511
iteration : 10627
train acc:  0.875
train loss:  0.3478614389896393
train gradient:  0.17177676879747733
iteration : 10628
train acc:  0.921875
train loss:  0.2348904013633728
train gradient:  0.11166902254174703
iteration : 10629
train acc:  0.8359375
train loss:  0.33680546283721924
train gradient:  0.17834287166504448
iteration : 10630
train acc:  0.8828125
train loss:  0.3040670156478882
train gradient:  0.10582942037914
iteration : 10631
train acc:  0.8828125
train loss:  0.30860811471939087
train gradient:  0.10541724535438061
iteration : 10632
train acc:  0.8203125
train loss:  0.40892335772514343
train gradient:  0.2832357840609407
iteration : 10633
train acc:  0.890625
train loss:  0.28731048107147217
train gradient:  0.10530160002007721
iteration : 10634
train acc:  0.796875
train loss:  0.4533015489578247
train gradient:  0.2611724796172513
iteration : 10635
train acc:  0.828125
train loss:  0.3723866939544678
train gradient:  0.2329097560057632
iteration : 10636
train acc:  0.875
train loss:  0.3399684727191925
train gradient:  0.16043497362955506
iteration : 10637
train acc:  0.8828125
train loss:  0.25550132989883423
train gradient:  0.07686394731048879
iteration : 10638
train acc:  0.875
train loss:  0.297950804233551
train gradient:  0.12603959960522432
iteration : 10639
train acc:  0.859375
train loss:  0.32847321033477783
train gradient:  0.17263578467005186
iteration : 10640
train acc:  0.859375
train loss:  0.33195942640304565
train gradient:  0.18195645815680372
iteration : 10641
train acc:  0.8203125
train loss:  0.35898616909980774
train gradient:  0.17539434470429277
iteration : 10642
train acc:  0.8359375
train loss:  0.3526061773300171
train gradient:  0.16045973565885888
iteration : 10643
train acc:  0.8515625
train loss:  0.3387334942817688
train gradient:  0.1598108910734376
iteration : 10644
train acc:  0.8671875
train loss:  0.29692381620407104
train gradient:  0.1792546583386132
iteration : 10645
train acc:  0.8359375
train loss:  0.34999990463256836
train gradient:  0.14296835336239555
iteration : 10646
train acc:  0.8828125
train loss:  0.2750079333782196
train gradient:  0.10867755628387663
iteration : 10647
train acc:  0.875
train loss:  0.30408668518066406
train gradient:  0.16023307891024086
iteration : 10648
train acc:  0.8359375
train loss:  0.3379625380039215
train gradient:  0.16352680276519238
iteration : 10649
train acc:  0.8828125
train loss:  0.30974775552749634
train gradient:  0.1683205106268111
iteration : 10650
train acc:  0.84375
train loss:  0.2994629740715027
train gradient:  0.14317423527957587
iteration : 10651
train acc:  0.8671875
train loss:  0.34798407554626465
train gradient:  0.17641579139278774
iteration : 10652
train acc:  0.921875
train loss:  0.22280588746070862
train gradient:  0.08738242631111119
iteration : 10653
train acc:  0.875
train loss:  0.2953643202781677
train gradient:  0.1923795676708872
iteration : 10654
train acc:  0.8203125
train loss:  0.3814672827720642
train gradient:  0.3130162372355494
iteration : 10655
train acc:  0.8046875
train loss:  0.39775264263153076
train gradient:  0.21494270045184882
iteration : 10656
train acc:  0.8671875
train loss:  0.2850190997123718
train gradient:  0.1415746164730503
iteration : 10657
train acc:  0.8359375
train loss:  0.39183127880096436
train gradient:  0.26747252329597104
iteration : 10658
train acc:  0.8203125
train loss:  0.3669459819793701
train gradient:  0.15761405479910415
iteration : 10659
train acc:  0.84375
train loss:  0.33150964975357056
train gradient:  0.2107258833879885
iteration : 10660
train acc:  0.875
train loss:  0.2855682373046875
train gradient:  0.1299763348537537
iteration : 10661
train acc:  0.859375
train loss:  0.32392418384552
train gradient:  0.1982135586908319
iteration : 10662
train acc:  0.8671875
train loss:  0.3600535988807678
train gradient:  0.19833228855701407
iteration : 10663
train acc:  0.859375
train loss:  0.36947551369667053
train gradient:  0.24783674576267306
iteration : 10664
train acc:  0.8515625
train loss:  0.3526427149772644
train gradient:  0.12886017316263929
iteration : 10665
train acc:  0.890625
train loss:  0.31359314918518066
train gradient:  0.15694785494946112
iteration : 10666
train acc:  0.828125
train loss:  0.3718748688697815
train gradient:  0.23946439453631074
iteration : 10667
train acc:  0.84375
train loss:  0.3475162386894226
train gradient:  0.22568345602395634
iteration : 10668
train acc:  0.765625
train loss:  0.5050809383392334
train gradient:  0.29779250148786457
iteration : 10669
train acc:  0.8671875
train loss:  0.3182138502597809
train gradient:  0.19213810462051667
iteration : 10670
train acc:  0.875
train loss:  0.3357045650482178
train gradient:  0.2761776022183095
iteration : 10671
train acc:  0.890625
train loss:  0.28458839654922485
train gradient:  0.10668764977565552
iteration : 10672
train acc:  0.9140625
train loss:  0.23309126496315002
train gradient:  0.11466337297609382
iteration : 10673
train acc:  0.8671875
train loss:  0.2733151614665985
train gradient:  0.146452924345165
iteration : 10674
train acc:  0.859375
train loss:  0.30087989568710327
train gradient:  0.13624049406387223
iteration : 10675
train acc:  0.8359375
train loss:  0.3280102610588074
train gradient:  0.13656973727225097
iteration : 10676
train acc:  0.859375
train loss:  0.3408755362033844
train gradient:  0.25637023051720176
iteration : 10677
train acc:  0.859375
train loss:  0.3153812885284424
train gradient:  0.13691612645011708
iteration : 10678
train acc:  0.8359375
train loss:  0.357390820980072
train gradient:  0.17451642182178145
iteration : 10679
train acc:  0.90625
train loss:  0.26285040378570557
train gradient:  0.09656083351621621
iteration : 10680
train acc:  0.84375
train loss:  0.3497598171234131
train gradient:  0.19225533880752727
iteration : 10681
train acc:  0.8984375
train loss:  0.24586905539035797
train gradient:  0.10499411075285199
iteration : 10682
train acc:  0.859375
train loss:  0.3018386960029602
train gradient:  0.09247176207445088
iteration : 10683
train acc:  0.796875
train loss:  0.44050878286361694
train gradient:  0.35594824736974584
iteration : 10684
train acc:  0.84375
train loss:  0.3635183274745941
train gradient:  0.15739610410926053
iteration : 10685
train acc:  0.8125
train loss:  0.3699861764907837
train gradient:  0.2615373065787698
iteration : 10686
train acc:  0.875
train loss:  0.29480916261672974
train gradient:  0.16456127509681284
iteration : 10687
train acc:  0.8671875
train loss:  0.2794944941997528
train gradient:  0.12832676553938593
iteration : 10688
train acc:  0.8359375
train loss:  0.3184148073196411
train gradient:  0.12560785626035675
iteration : 10689
train acc:  0.8984375
train loss:  0.327869176864624
train gradient:  0.164787148424326
iteration : 10690
train acc:  0.859375
train loss:  0.3041131794452667
train gradient:  0.1729598557107131
iteration : 10691
train acc:  0.8359375
train loss:  0.35544657707214355
train gradient:  0.1437452850174653
iteration : 10692
train acc:  0.90625
train loss:  0.3246666491031647
train gradient:  0.12535075800941192
iteration : 10693
train acc:  0.8515625
train loss:  0.3395956754684448
train gradient:  0.18593276343688708
iteration : 10694
train acc:  0.7890625
train loss:  0.3701938986778259
train gradient:  0.18533618231888316
iteration : 10695
train acc:  0.828125
train loss:  0.3521520495414734
train gradient:  0.2047917318478754
iteration : 10696
train acc:  0.90625
train loss:  0.2844524085521698
train gradient:  0.16063958017288527
iteration : 10697
train acc:  0.8671875
train loss:  0.3019237816333771
train gradient:  0.14773294345034804
iteration : 10698
train acc:  0.890625
train loss:  0.27231502532958984
train gradient:  0.15039655872061689
iteration : 10699
train acc:  0.8671875
train loss:  0.3122694492340088
train gradient:  0.11267646072510526
iteration : 10700
train acc:  0.828125
train loss:  0.35918211936950684
train gradient:  0.31430934818961276
iteration : 10701
train acc:  0.890625
train loss:  0.24942326545715332
train gradient:  0.15383386101262725
iteration : 10702
train acc:  0.8828125
train loss:  0.2875700891017914
train gradient:  0.17052479294581951
iteration : 10703
train acc:  0.8515625
train loss:  0.33503809571266174
train gradient:  0.14067876498166226
iteration : 10704
train acc:  0.875
train loss:  0.33879879117012024
train gradient:  0.1469241204799765
iteration : 10705
train acc:  0.890625
train loss:  0.2984483242034912
train gradient:  0.11669651857895172
iteration : 10706
train acc:  0.8671875
train loss:  0.3241211175918579
train gradient:  0.1391460257326898
iteration : 10707
train acc:  0.8515625
train loss:  0.3678308129310608
train gradient:  0.2083421762884553
iteration : 10708
train acc:  0.8359375
train loss:  0.34351813793182373
train gradient:  0.17694497562568023
iteration : 10709
train acc:  0.8359375
train loss:  0.299323707818985
train gradient:  0.228960180493697
iteration : 10710
train acc:  0.8828125
train loss:  0.28792494535446167
train gradient:  0.1024152516215839
iteration : 10711
train acc:  0.890625
train loss:  0.2547164559364319
train gradient:  0.12247431006234087
iteration : 10712
train acc:  0.84375
train loss:  0.35996100306510925
train gradient:  0.20685756633971045
iteration : 10713
train acc:  0.859375
train loss:  0.3617372214794159
train gradient:  0.15330661725297484
iteration : 10714
train acc:  0.90625
train loss:  0.2798508107662201
train gradient:  0.148607778469141
iteration : 10715
train acc:  0.8359375
train loss:  0.3840346336364746
train gradient:  0.22031954294766964
iteration : 10716
train acc:  0.8671875
train loss:  0.28720736503601074
train gradient:  0.10083532232571546
iteration : 10717
train acc:  0.8671875
train loss:  0.3414388597011566
train gradient:  0.16745924908169318
iteration : 10718
train acc:  0.8828125
train loss:  0.29512354731559753
train gradient:  0.1209284019568431
iteration : 10719
train acc:  0.8203125
train loss:  0.3818433880805969
train gradient:  0.22113013425861527
iteration : 10720
train acc:  0.84375
train loss:  0.36527398228645325
train gradient:  0.19815390003513253
iteration : 10721
train acc:  0.875
train loss:  0.33095991611480713
train gradient:  0.18648296312832657
iteration : 10722
train acc:  0.875
train loss:  0.3296663463115692
train gradient:  0.17864754863131926
iteration : 10723
train acc:  0.8515625
train loss:  0.3179548978805542
train gradient:  0.17639580866691434
iteration : 10724
train acc:  0.8203125
train loss:  0.35148096084594727
train gradient:  0.17614442306961314
iteration : 10725
train acc:  0.84375
train loss:  0.4029495120048523
train gradient:  0.3404085464891113
iteration : 10726
train acc:  0.8359375
train loss:  0.37018248438835144
train gradient:  0.18107299002199612
iteration : 10727
train acc:  0.8671875
train loss:  0.32057303190231323
train gradient:  0.11971635902590853
iteration : 10728
train acc:  0.8984375
train loss:  0.28188663721084595
train gradient:  0.21690784136202215
iteration : 10729
train acc:  0.8203125
train loss:  0.41135624051094055
train gradient:  0.23339031092850404
iteration : 10730
train acc:  0.9140625
train loss:  0.2222544550895691
train gradient:  0.0963552657514651
iteration : 10731
train acc:  0.8515625
train loss:  0.3807026147842407
train gradient:  0.1763032840466897
iteration : 10732
train acc:  0.921875
train loss:  0.23911362886428833
train gradient:  0.0873461918398058
iteration : 10733
train acc:  0.890625
train loss:  0.2763269543647766
train gradient:  0.12925418955047951
iteration : 10734
train acc:  0.7734375
train loss:  0.46878188848495483
train gradient:  0.26513403069290936
iteration : 10735
train acc:  0.875
train loss:  0.28577953577041626
train gradient:  0.09640927010275593
iteration : 10736
train acc:  0.8984375
train loss:  0.26189619302749634
train gradient:  0.10505489287559312
iteration : 10737
train acc:  0.8125
train loss:  0.4241319000720978
train gradient:  0.24526138086548507
iteration : 10738
train acc:  0.8359375
train loss:  0.34319016337394714
train gradient:  0.1717039957764897
iteration : 10739
train acc:  0.859375
train loss:  0.2965770363807678
train gradient:  0.12391074145155541
iteration : 10740
train acc:  0.84375
train loss:  0.3731001019477844
train gradient:  0.20285181814034675
iteration : 10741
train acc:  0.8671875
train loss:  0.32762178778648376
train gradient:  0.12481069275978349
iteration : 10742
train acc:  0.8984375
train loss:  0.25124579668045044
train gradient:  0.11597967429179483
iteration : 10743
train acc:  0.8125
train loss:  0.46905291080474854
train gradient:  0.3183028339706397
iteration : 10744
train acc:  0.8203125
train loss:  0.3561398386955261
train gradient:  0.24801412593838773
iteration : 10745
train acc:  0.859375
train loss:  0.2927282750606537
train gradient:  0.18754779714810346
iteration : 10746
train acc:  0.90625
train loss:  0.273966521024704
train gradient:  0.10892093627204698
iteration : 10747
train acc:  0.8671875
train loss:  0.325045645236969
train gradient:  0.1925024127500349
iteration : 10748
train acc:  0.875
train loss:  0.2679470181465149
train gradient:  0.11681656324797553
iteration : 10749
train acc:  0.8828125
train loss:  0.2911050319671631
train gradient:  0.11409381066117986
iteration : 10750
train acc:  0.8203125
train loss:  0.4700931906700134
train gradient:  0.28530980790429905
iteration : 10751
train acc:  0.859375
train loss:  0.3074345290660858
train gradient:  0.12849066175050883
iteration : 10752
train acc:  0.890625
train loss:  0.35277828574180603
train gradient:  0.14834344072222516
iteration : 10753
train acc:  0.8984375
train loss:  0.28450220823287964
train gradient:  0.10515333442445372
iteration : 10754
train acc:  0.8671875
train loss:  0.2959470748901367
train gradient:  0.12038403574786696
iteration : 10755
train acc:  0.8671875
train loss:  0.28569120168685913
train gradient:  0.1282075302578567
iteration : 10756
train acc:  0.859375
train loss:  0.33680474758148193
train gradient:  0.16801192339407275
iteration : 10757
train acc:  0.875
train loss:  0.2996581792831421
train gradient:  0.13147364406198342
iteration : 10758
train acc:  0.828125
train loss:  0.36872148513793945
train gradient:  0.17571599229677445
iteration : 10759
train acc:  0.8828125
train loss:  0.3014304041862488
train gradient:  0.14406172727942046
iteration : 10760
train acc:  0.8515625
train loss:  0.30490002036094666
train gradient:  0.10739475641703414
iteration : 10761
train acc:  0.890625
train loss:  0.26828962564468384
train gradient:  0.1390351680844733
iteration : 10762
train acc:  0.84375
train loss:  0.33058539032936096
train gradient:  0.14091339558113639
iteration : 10763
train acc:  0.84375
train loss:  0.32042139768600464
train gradient:  0.1495309378241856
iteration : 10764
train acc:  0.8515625
train loss:  0.2934003472328186
train gradient:  0.13675811886041994
iteration : 10765
train acc:  0.8046875
train loss:  0.3788421154022217
train gradient:  0.2015534160031645
iteration : 10766
train acc:  0.859375
train loss:  0.3174474239349365
train gradient:  0.13891317913247148
iteration : 10767
train acc:  0.875
train loss:  0.2700085937976837
train gradient:  0.13918025127536576
iteration : 10768
train acc:  0.875
train loss:  0.31481289863586426
train gradient:  0.1396475919281406
iteration : 10769
train acc:  0.859375
train loss:  0.3500733971595764
train gradient:  0.1630097964625973
iteration : 10770
train acc:  0.8671875
train loss:  0.3217428922653198
train gradient:  0.15414411384365567
iteration : 10771
train acc:  0.8984375
train loss:  0.27311256527900696
train gradient:  0.12178607634453704
iteration : 10772
train acc:  0.9296875
train loss:  0.23836109042167664
train gradient:  0.09426177873243845
iteration : 10773
train acc:  0.8828125
train loss:  0.2795153856277466
train gradient:  0.13371534737168872
iteration : 10774
train acc:  0.875
train loss:  0.2969505488872528
train gradient:  0.1293309647048993
iteration : 10775
train acc:  0.828125
train loss:  0.42590615153312683
train gradient:  0.30499631798320137
iteration : 10776
train acc:  0.828125
train loss:  0.3619654178619385
train gradient:  0.18183547549690043
iteration : 10777
train acc:  0.8515625
train loss:  0.2800482511520386
train gradient:  0.1353341331386057
iteration : 10778
train acc:  0.8671875
train loss:  0.2802295684814453
train gradient:  0.10008989430701175
iteration : 10779
train acc:  0.8359375
train loss:  0.333133727312088
train gradient:  0.15404216274624472
iteration : 10780
train acc:  0.8671875
train loss:  0.37488093972206116
train gradient:  0.22648112894607553
iteration : 10781
train acc:  0.859375
train loss:  0.298648864030838
train gradient:  0.11704155181869336
iteration : 10782
train acc:  0.8515625
train loss:  0.30459627509117126
train gradient:  0.1484831365771731
iteration : 10783
train acc:  0.859375
train loss:  0.3632534444332123
train gradient:  0.18819402479787545
iteration : 10784
train acc:  0.8828125
train loss:  0.2785894274711609
train gradient:  0.1638261018333026
iteration : 10785
train acc:  0.90625
train loss:  0.23900678753852844
train gradient:  0.08953127096294601
iteration : 10786
train acc:  0.8671875
train loss:  0.30207639932632446
train gradient:  0.11960174391132296
iteration : 10787
train acc:  0.84375
train loss:  0.34634360671043396
train gradient:  0.20464798589954464
iteration : 10788
train acc:  0.890625
train loss:  0.26060056686401367
train gradient:  0.10907735780021295
iteration : 10789
train acc:  0.859375
train loss:  0.39570939540863037
train gradient:  0.19461901034233525
iteration : 10790
train acc:  0.84375
train loss:  0.38865745067596436
train gradient:  0.16193173478265166
iteration : 10791
train acc:  0.8515625
train loss:  0.34443148970603943
train gradient:  0.13471616735909764
iteration : 10792
train acc:  0.875
train loss:  0.33818140625953674
train gradient:  0.11903859454337146
iteration : 10793
train acc:  0.8515625
train loss:  0.2976635992527008
train gradient:  0.1284415220605488
iteration : 10794
train acc:  0.8359375
train loss:  0.31576603651046753
train gradient:  0.21263239383073868
iteration : 10795
train acc:  0.859375
train loss:  0.3737824261188507
train gradient:  0.2740719548176682
iteration : 10796
train acc:  0.828125
train loss:  0.375312477350235
train gradient:  0.20583074358401113
iteration : 10797
train acc:  0.7734375
train loss:  0.4370444715023041
train gradient:  0.32919785895271503
iteration : 10798
train acc:  0.8671875
train loss:  0.3095240592956543
train gradient:  0.10763058792647617
iteration : 10799
train acc:  0.8203125
train loss:  0.3504527807235718
train gradient:  0.13870979793543764
iteration : 10800
train acc:  0.875
train loss:  0.38177746534347534
train gradient:  0.15388993917300547
iteration : 10801
train acc:  0.8125
train loss:  0.3395317494869232
train gradient:  0.2612909059949844
iteration : 10802
train acc:  0.8828125
train loss:  0.2675219476222992
train gradient:  0.09314519620423983
iteration : 10803
train acc:  0.8125
train loss:  0.36635708808898926
train gradient:  0.18280525645535806
iteration : 10804
train acc:  0.8828125
train loss:  0.26505014300346375
train gradient:  0.11940183155854642
iteration : 10805
train acc:  0.84375
train loss:  0.29099974036216736
train gradient:  0.12532317403298443
iteration : 10806
train acc:  0.9140625
train loss:  0.280940979719162
train gradient:  0.14227750610145515
iteration : 10807
train acc:  0.8671875
train loss:  0.28356218338012695
train gradient:  0.13149052205120731
iteration : 10808
train acc:  0.8046875
train loss:  0.4582045078277588
train gradient:  0.26285958888281474
iteration : 10809
train acc:  0.8203125
train loss:  0.35898828506469727
train gradient:  0.1608450847987012
iteration : 10810
train acc:  0.8203125
train loss:  0.3668389916419983
train gradient:  0.2039211596445914
iteration : 10811
train acc:  0.875
train loss:  0.30118340253829956
train gradient:  0.09317739406627062
iteration : 10812
train acc:  0.875
train loss:  0.3803224563598633
train gradient:  0.17042510401121153
iteration : 10813
train acc:  0.8125
train loss:  0.37979570031166077
train gradient:  0.19038575579202116
iteration : 10814
train acc:  0.875
train loss:  0.3176004886627197
train gradient:  0.17767306636407956
iteration : 10815
train acc:  0.8828125
train loss:  0.2822311520576477
train gradient:  0.091586111427137
iteration : 10816
train acc:  0.828125
train loss:  0.3647230863571167
train gradient:  0.15996127087152368
iteration : 10817
train acc:  0.9296875
train loss:  0.18800543248653412
train gradient:  0.05978261242382904
iteration : 10818
train acc:  0.765625
train loss:  0.4275844395160675
train gradient:  0.3253758983078421
iteration : 10819
train acc:  0.859375
train loss:  0.37505343556404114
train gradient:  0.17846345195161756
iteration : 10820
train acc:  0.7890625
train loss:  0.43277406692504883
train gradient:  0.2630012103006894
iteration : 10821
train acc:  0.859375
train loss:  0.3632354736328125
train gradient:  0.1582787191633435
iteration : 10822
train acc:  0.8359375
train loss:  0.2982150614261627
train gradient:  0.12610766111549532
iteration : 10823
train acc:  0.859375
train loss:  0.34882378578186035
train gradient:  0.17087502571656435
iteration : 10824
train acc:  0.8125
train loss:  0.41266876459121704
train gradient:  0.22874463375226095
iteration : 10825
train acc:  0.84375
train loss:  0.3238113820552826
train gradient:  0.16951375096966342
iteration : 10826
train acc:  0.859375
train loss:  0.26979517936706543
train gradient:  0.11812624709064268
iteration : 10827
train acc:  0.8984375
train loss:  0.25824493169784546
train gradient:  0.08770468100568347
iteration : 10828
train acc:  0.84375
train loss:  0.3346933126449585
train gradient:  0.17212502579496386
iteration : 10829
train acc:  0.890625
train loss:  0.2779000997543335
train gradient:  0.1364224455383788
iteration : 10830
train acc:  0.875
train loss:  0.2944026291370392
train gradient:  0.1044328043532629
iteration : 10831
train acc:  0.84375
train loss:  0.36242252588272095
train gradient:  0.13669645544319717
iteration : 10832
train acc:  0.8671875
train loss:  0.28428763151168823
train gradient:  0.14023438165439311
iteration : 10833
train acc:  0.8984375
train loss:  0.2313411831855774
train gradient:  0.12689633420891377
iteration : 10834
train acc:  0.8828125
train loss:  0.29774153232574463
train gradient:  0.09034527920791673
iteration : 10835
train acc:  0.8828125
train loss:  0.3318261504173279
train gradient:  0.12785886756705112
iteration : 10836
train acc:  0.828125
train loss:  0.3677239418029785
train gradient:  0.21336293809141582
iteration : 10837
train acc:  0.875
train loss:  0.29421061277389526
train gradient:  0.13807814954408032
iteration : 10838
train acc:  0.8671875
train loss:  0.3324645757675171
train gradient:  0.16732199405820009
iteration : 10839
train acc:  0.84375
train loss:  0.32974112033843994
train gradient:  0.19772367519432715
iteration : 10840
train acc:  0.8515625
train loss:  0.36436688899993896
train gradient:  0.14998129109220779
iteration : 10841
train acc:  0.8828125
train loss:  0.28501003980636597
train gradient:  0.2175726466569295
iteration : 10842
train acc:  0.8359375
train loss:  0.3263621926307678
train gradient:  0.1746437640126693
iteration : 10843
train acc:  0.7890625
train loss:  0.3909728527069092
train gradient:  0.27356949445413453
iteration : 10844
train acc:  0.84375
train loss:  0.3614809513092041
train gradient:  0.19524713796627471
iteration : 10845
train acc:  0.875
train loss:  0.2768818140029907
train gradient:  0.13736190790680086
iteration : 10846
train acc:  0.8671875
train loss:  0.35501277446746826
train gradient:  0.21421013203704048
iteration : 10847
train acc:  0.8828125
train loss:  0.3198733329772949
train gradient:  0.12953459162854972
iteration : 10848
train acc:  0.875
train loss:  0.3295586109161377
train gradient:  0.12744743359072486
iteration : 10849
train acc:  0.8515625
train loss:  0.3447796106338501
train gradient:  0.16024752431987993
iteration : 10850
train acc:  0.8203125
train loss:  0.4152776300907135
train gradient:  0.21282488693737056
iteration : 10851
train acc:  0.7578125
train loss:  0.413647323846817
train gradient:  0.2117881340178575
iteration : 10852
train acc:  0.90625
train loss:  0.25615066289901733
train gradient:  0.09496443240058067
iteration : 10853
train acc:  0.8515625
train loss:  0.3386383354663849
train gradient:  0.1338821454301798
iteration : 10854
train acc:  0.8671875
train loss:  0.2860831618309021
train gradient:  0.08703384800158265
iteration : 10855
train acc:  0.765625
train loss:  0.4653322100639343
train gradient:  0.22147983336523275
iteration : 10856
train acc:  0.8359375
train loss:  0.36314019560813904
train gradient:  0.20714698963244796
iteration : 10857
train acc:  0.8828125
train loss:  0.3007861077785492
train gradient:  0.12164835823451171
iteration : 10858
train acc:  0.8671875
train loss:  0.35331714153289795
train gradient:  0.1740346150104219
iteration : 10859
train acc:  0.890625
train loss:  0.28771916031837463
train gradient:  0.11549914447128833
iteration : 10860
train acc:  0.875
train loss:  0.2829757332801819
train gradient:  0.18734828906841794
iteration : 10861
train acc:  0.8515625
train loss:  0.35471391677856445
train gradient:  0.13507436324399336
iteration : 10862
train acc:  0.875
train loss:  0.3639554977416992
train gradient:  0.1324873700704241
iteration : 10863
train acc:  0.84375
train loss:  0.33020156621932983
train gradient:  0.16264959028967946
iteration : 10864
train acc:  0.8203125
train loss:  0.3366486132144928
train gradient:  0.12131800589065934
iteration : 10865
train acc:  0.8984375
train loss:  0.2861679196357727
train gradient:  0.10537406552359822
iteration : 10866
train acc:  0.8828125
train loss:  0.29446712136268616
train gradient:  0.11634991374044924
iteration : 10867
train acc:  0.8203125
train loss:  0.3851001262664795
train gradient:  0.18126071249719364
iteration : 10868
train acc:  0.8515625
train loss:  0.3524431884288788
train gradient:  0.12852352428926928
iteration : 10869
train acc:  0.875
train loss:  0.3730030655860901
train gradient:  0.1747148193385965
iteration : 10870
train acc:  0.875
train loss:  0.3076462745666504
train gradient:  0.1280661687703714
iteration : 10871
train acc:  0.8515625
train loss:  0.3127218186855316
train gradient:  0.16961456904834604
iteration : 10872
train acc:  0.8828125
train loss:  0.28362607955932617
train gradient:  0.149239732469164
iteration : 10873
train acc:  0.8828125
train loss:  0.2607060670852661
train gradient:  0.07794548158182173
iteration : 10874
train acc:  0.84375
train loss:  0.2754412889480591
train gradient:  0.1437718315938598
iteration : 10875
train acc:  0.8125
train loss:  0.3834446668624878
train gradient:  0.15433341552513052
iteration : 10876
train acc:  0.8671875
train loss:  0.2797566056251526
train gradient:  0.13268915747927434
iteration : 10877
train acc:  0.8359375
train loss:  0.28994423151016235
train gradient:  0.092320126759053
iteration : 10878
train acc:  0.890625
train loss:  0.25828060507774353
train gradient:  0.11579598082245532
iteration : 10879
train acc:  0.8828125
train loss:  0.3177739381790161
train gradient:  0.13308787257673343
iteration : 10880
train acc:  0.84375
train loss:  0.37687283754348755
train gradient:  0.21924150997160216
iteration : 10881
train acc:  0.8515625
train loss:  0.34280556440353394
train gradient:  0.2240791224934362
iteration : 10882
train acc:  0.875
train loss:  0.3104149401187897
train gradient:  0.12238961384591464
iteration : 10883
train acc:  0.84375
train loss:  0.34370189905166626
train gradient:  0.2435078120213428
iteration : 10884
train acc:  0.8828125
train loss:  0.27015528082847595
train gradient:  0.11860859953643922
iteration : 10885
train acc:  0.84375
train loss:  0.3365612328052521
train gradient:  0.20038159376716166
iteration : 10886
train acc:  0.859375
train loss:  0.32597288489341736
train gradient:  0.20048512003606328
iteration : 10887
train acc:  0.84375
train loss:  0.2999410629272461
train gradient:  0.09713071742125938
iteration : 10888
train acc:  0.8359375
train loss:  0.37353867292404175
train gradient:  0.22084263696829415
iteration : 10889
train acc:  0.8984375
train loss:  0.2642151117324829
train gradient:  0.13283977717322284
iteration : 10890
train acc:  0.890625
train loss:  0.25000181794166565
train gradient:  0.10090873125983914
iteration : 10891
train acc:  0.859375
train loss:  0.4009391665458679
train gradient:  0.2207108115959362
iteration : 10892
train acc:  0.8359375
train loss:  0.43168240785598755
train gradient:  0.2040228479708153
iteration : 10893
train acc:  0.8828125
train loss:  0.2919818162918091
train gradient:  0.16890006607630367
iteration : 10894
train acc:  0.859375
train loss:  0.2984350323677063
train gradient:  0.10974449443574356
iteration : 10895
train acc:  0.875
train loss:  0.28235670924186707
train gradient:  0.15117608992739495
iteration : 10896
train acc:  0.859375
train loss:  0.29457059502601624
train gradient:  0.17537450105561242
iteration : 10897
train acc:  0.8984375
train loss:  0.2801107168197632
train gradient:  0.14472298910580733
iteration : 10898
train acc:  0.8671875
train loss:  0.3181389272212982
train gradient:  0.15045497845450934
iteration : 10899
train acc:  0.828125
train loss:  0.4241940975189209
train gradient:  0.2064445692174976
iteration : 10900
train acc:  0.8984375
train loss:  0.2863044738769531
train gradient:  0.12908412738750769
iteration : 10901
train acc:  0.8828125
train loss:  0.28397539258003235
train gradient:  0.11910680198486769
iteration : 10902
train acc:  0.8125
train loss:  0.3768056333065033
train gradient:  0.25937777508519977
iteration : 10903
train acc:  0.8515625
train loss:  0.34913724660873413
train gradient:  0.22533655731360597
iteration : 10904
train acc:  0.8984375
train loss:  0.30784744024276733
train gradient:  0.12963046938678574
iteration : 10905
train acc:  0.8984375
train loss:  0.27458974719047546
train gradient:  0.11704275646547677
iteration : 10906
train acc:  0.9140625
train loss:  0.2551896572113037
train gradient:  0.12489342646670648
iteration : 10907
train acc:  0.8515625
train loss:  0.3250984251499176
train gradient:  0.16010833722772128
iteration : 10908
train acc:  0.8125
train loss:  0.3606637716293335
train gradient:  0.15602364599418309
iteration : 10909
train acc:  0.859375
train loss:  0.2779475450515747
train gradient:  0.08633225120641218
iteration : 10910
train acc:  0.8046875
train loss:  0.42375779151916504
train gradient:  0.24861642268948686
iteration : 10911
train acc:  0.796875
train loss:  0.46437203884124756
train gradient:  0.3298807389813856
iteration : 10912
train acc:  0.859375
train loss:  0.39451855421066284
train gradient:  0.23282711909542728
iteration : 10913
train acc:  0.859375
train loss:  0.3522903025150299
train gradient:  0.24230203145024276
iteration : 10914
train acc:  0.875
train loss:  0.2940904498100281
train gradient:  0.1658593105027506
iteration : 10915
train acc:  0.8671875
train loss:  0.29312658309936523
train gradient:  0.19054780571429933
iteration : 10916
train acc:  0.8359375
train loss:  0.364513099193573
train gradient:  0.19610771056261053
iteration : 10917
train acc:  0.828125
train loss:  0.38354867696762085
train gradient:  0.1443108095110716
iteration : 10918
train acc:  0.8359375
train loss:  0.38443097472190857
train gradient:  0.14962457334200985
iteration : 10919
train acc:  0.875
train loss:  0.3239039182662964
train gradient:  0.1702066274131304
iteration : 10920
train acc:  0.890625
train loss:  0.2604057192802429
train gradient:  0.11576413360868376
iteration : 10921
train acc:  0.84375
train loss:  0.3665579557418823
train gradient:  0.2941485069211755
iteration : 10922
train acc:  0.8828125
train loss:  0.3174046277999878
train gradient:  0.19803602297317874
iteration : 10923
train acc:  0.8125
train loss:  0.3458278477191925
train gradient:  0.18390869467348586
iteration : 10924
train acc:  0.8125
train loss:  0.3779677152633667
train gradient:  0.2153739063333477
iteration : 10925
train acc:  0.875
train loss:  0.278026282787323
train gradient:  0.14050170298347583
iteration : 10926
train acc:  0.8359375
train loss:  0.2964015603065491
train gradient:  0.11875356973859398
iteration : 10927
train acc:  0.8515625
train loss:  0.34996098279953003
train gradient:  0.20101895420140217
iteration : 10928
train acc:  0.859375
train loss:  0.31615984439849854
train gradient:  0.15346880350451403
iteration : 10929
train acc:  0.875
train loss:  0.32086265087127686
train gradient:  0.15971208554625305
iteration : 10930
train acc:  0.828125
train loss:  0.3468325734138489
train gradient:  0.27634801138286924
iteration : 10931
train acc:  0.8359375
train loss:  0.3595070540904999
train gradient:  0.2365576168743494
iteration : 10932
train acc:  0.8359375
train loss:  0.3647402822971344
train gradient:  0.19468120395543353
iteration : 10933
train acc:  0.859375
train loss:  0.2786818742752075
train gradient:  0.14353207924507672
iteration : 10934
train acc:  0.8984375
train loss:  0.28190386295318604
train gradient:  0.12672565730561516
iteration : 10935
train acc:  0.8671875
train loss:  0.3726070821285248
train gradient:  0.17612649650317047
iteration : 10936
train acc:  0.875
train loss:  0.28658878803253174
train gradient:  0.13559941784262233
iteration : 10937
train acc:  0.890625
train loss:  0.285756915807724
train gradient:  0.12435497041648287
iteration : 10938
train acc:  0.8515625
train loss:  0.36926954984664917
train gradient:  0.15698946201973163
iteration : 10939
train acc:  0.8203125
train loss:  0.3530104160308838
train gradient:  0.1494861695842425
iteration : 10940
train acc:  0.8671875
train loss:  0.30086347460746765
train gradient:  0.16475978869961483
iteration : 10941
train acc:  0.8984375
train loss:  0.2617069482803345
train gradient:  0.11888479384395773
iteration : 10942
train acc:  0.8359375
train loss:  0.4226074814796448
train gradient:  0.20688672220416704
iteration : 10943
train acc:  0.8515625
train loss:  0.3962336778640747
train gradient:  0.22690537702059554
iteration : 10944
train acc:  0.84375
train loss:  0.35103967785835266
train gradient:  0.1581670460115945
iteration : 10945
train acc:  0.859375
train loss:  0.33100664615631104
train gradient:  0.1420024730759995
iteration : 10946
train acc:  0.921875
train loss:  0.22312608361244202
train gradient:  0.0792326263639324
iteration : 10947
train acc:  0.8515625
train loss:  0.30315926671028137
train gradient:  0.1326623293419074
iteration : 10948
train acc:  0.90625
train loss:  0.26741790771484375
train gradient:  0.09190414564136812
iteration : 10949
train acc:  0.859375
train loss:  0.29604998230934143
train gradient:  0.09351890953756259
iteration : 10950
train acc:  0.8203125
train loss:  0.3856222927570343
train gradient:  0.19523143902558077
iteration : 10951
train acc:  0.8515625
train loss:  0.32618778944015503
train gradient:  0.1796062419151133
iteration : 10952
train acc:  0.8515625
train loss:  0.3585001826286316
train gradient:  0.16660204513813842
iteration : 10953
train acc:  0.90625
train loss:  0.27885693311691284
train gradient:  0.10074755037704107
iteration : 10954
train acc:  0.859375
train loss:  0.3165411949157715
train gradient:  0.09301115820649017
iteration : 10955
train acc:  0.875
train loss:  0.2747586965560913
train gradient:  0.09401663211655216
iteration : 10956
train acc:  0.8828125
train loss:  0.2874690890312195
train gradient:  0.15210027806302484
iteration : 10957
train acc:  0.890625
train loss:  0.2632710039615631
train gradient:  0.0974290182802476
iteration : 10958
train acc:  0.875
train loss:  0.30459368228912354
train gradient:  0.12100065857182941
iteration : 10959
train acc:  0.84375
train loss:  0.35839369893074036
train gradient:  0.13814624610384618
iteration : 10960
train acc:  0.8515625
train loss:  0.36227360367774963
train gradient:  0.14628815524740063
iteration : 10961
train acc:  0.8203125
train loss:  0.40400075912475586
train gradient:  0.2718591347397787
iteration : 10962
train acc:  0.84375
train loss:  0.35760626196861267
train gradient:  0.15848997476909715
iteration : 10963
train acc:  0.8203125
train loss:  0.3856167197227478
train gradient:  0.1821662028637758
iteration : 10964
train acc:  0.8828125
train loss:  0.27899879217147827
train gradient:  0.18057908653672206
iteration : 10965
train acc:  0.859375
train loss:  0.31971222162246704
train gradient:  0.17941690545733213
iteration : 10966
train acc:  0.84375
train loss:  0.33018094301223755
train gradient:  0.13725611514437255
iteration : 10967
train acc:  0.8671875
train loss:  0.29592829942703247
train gradient:  0.1583051905560118
iteration : 10968
train acc:  0.8046875
train loss:  0.4420478343963623
train gradient:  0.22430656140056815
iteration : 10969
train acc:  0.828125
train loss:  0.38198530673980713
train gradient:  0.21820796504833495
iteration : 10970
train acc:  0.84375
train loss:  0.3080265522003174
train gradient:  0.13604791113050402
iteration : 10971
train acc:  0.8359375
train loss:  0.37558722496032715
train gradient:  0.19709403079283952
iteration : 10972
train acc:  0.796875
train loss:  0.3793652057647705
train gradient:  0.20943182975313254
iteration : 10973
train acc:  0.890625
train loss:  0.34477943181991577
train gradient:  0.13067048353006824
iteration : 10974
train acc:  0.8359375
train loss:  0.4039468765258789
train gradient:  0.22683994348708497
iteration : 10975
train acc:  0.8515625
train loss:  0.27803319692611694
train gradient:  0.14879017835893554
iteration : 10976
train acc:  0.875
train loss:  0.32510143518447876
train gradient:  0.17092364480409816
iteration : 10977
train acc:  0.8828125
train loss:  0.29405438899993896
train gradient:  0.16768461327859674
iteration : 10978
train acc:  0.8671875
train loss:  0.2914547324180603
train gradient:  0.11612733162256303
iteration : 10979
train acc:  0.84375
train loss:  0.36731866002082825
train gradient:  0.19324407306519914
iteration : 10980
train acc:  0.828125
train loss:  0.39125293493270874
train gradient:  0.2530834102193748
iteration : 10981
train acc:  0.8671875
train loss:  0.32618284225463867
train gradient:  0.10156634098174197
iteration : 10982
train acc:  0.8671875
train loss:  0.3058907985687256
train gradient:  0.12535237063941743
iteration : 10983
train acc:  0.859375
train loss:  0.3274223804473877
train gradient:  0.09395966553173501
iteration : 10984
train acc:  0.890625
train loss:  0.3047568202018738
train gradient:  0.12904210717323567
iteration : 10985
train acc:  0.8671875
train loss:  0.29113978147506714
train gradient:  0.1427094642308519
iteration : 10986
train acc:  0.859375
train loss:  0.35571005940437317
train gradient:  0.16799152260269323
iteration : 10987
train acc:  0.828125
train loss:  0.31151139736175537
train gradient:  0.12436959843500224
iteration : 10988
train acc:  0.828125
train loss:  0.40845251083374023
train gradient:  0.15125380296642266
iteration : 10989
train acc:  0.9296875
train loss:  0.2741137444972992
train gradient:  0.10198432748625397
iteration : 10990
train acc:  0.84375
train loss:  0.29394230246543884
train gradient:  0.11727471321416988
iteration : 10991
train acc:  0.8671875
train loss:  0.29156866669654846
train gradient:  0.12330108611424047
iteration : 10992
train acc:  0.8515625
train loss:  0.3405149579048157
train gradient:  0.1501523319136251
iteration : 10993
train acc:  0.875
train loss:  0.28562793135643005
train gradient:  0.12022385685826516
iteration : 10994
train acc:  0.90625
train loss:  0.32270461320877075
train gradient:  0.13045054131473596
iteration : 10995
train acc:  0.8984375
train loss:  0.3009266257286072
train gradient:  0.08409405009931341
iteration : 10996
train acc:  0.875
train loss:  0.3293958306312561
train gradient:  0.14823860426932942
iteration : 10997
train acc:  0.8203125
train loss:  0.3327120244503021
train gradient:  0.13638367055779843
iteration : 10998
train acc:  0.90625
train loss:  0.2736574411392212
train gradient:  0.09229658990944661
iteration : 10999
train acc:  0.890625
train loss:  0.2838762700557709
train gradient:  0.1337067661395848
iteration : 11000
train acc:  0.8515625
train loss:  0.35311359167099
train gradient:  0.14053865533126414
iteration : 11001
train acc:  0.84375
train loss:  0.37013673782348633
train gradient:  0.13809172705632106
iteration : 11002
train acc:  0.8671875
train loss:  0.28110355138778687
train gradient:  0.12896683786278954
iteration : 11003
train acc:  0.8359375
train loss:  0.3753012418746948
train gradient:  0.2133074354452663
iteration : 11004
train acc:  0.828125
train loss:  0.3901534676551819
train gradient:  0.18476731401612959
iteration : 11005
train acc:  0.8828125
train loss:  0.2612636685371399
train gradient:  0.10159505218811987
iteration : 11006
train acc:  0.8671875
train loss:  0.3592129647731781
train gradient:  0.2317995955061516
iteration : 11007
train acc:  0.8671875
train loss:  0.2974911332130432
train gradient:  0.12581568053956393
iteration : 11008
train acc:  0.8203125
train loss:  0.423930287361145
train gradient:  0.2485657684077609
iteration : 11009
train acc:  0.796875
train loss:  0.42884761095046997
train gradient:  0.20450746407313353
iteration : 11010
train acc:  0.875
train loss:  0.2956312298774719
train gradient:  0.12964605715504463
iteration : 11011
train acc:  0.828125
train loss:  0.3464290499687195
train gradient:  0.2383997642257517
iteration : 11012
train acc:  0.875
train loss:  0.2813219428062439
train gradient:  0.1145750081271088
iteration : 11013
train acc:  0.75
train loss:  0.48905810713768005
train gradient:  0.2906708837080063
iteration : 11014
train acc:  0.875
train loss:  0.2205170840024948
train gradient:  0.08941942627572659
iteration : 11015
train acc:  0.8515625
train loss:  0.349093496799469
train gradient:  0.1989446195317155
iteration : 11016
train acc:  0.8125
train loss:  0.36305439472198486
train gradient:  0.18314596368814978
iteration : 11017
train acc:  0.828125
train loss:  0.3572956919670105
train gradient:  0.12166740064199341
iteration : 11018
train acc:  0.90625
train loss:  0.23267970979213715
train gradient:  0.13645170581085453
iteration : 11019
train acc:  0.8984375
train loss:  0.298259437084198
train gradient:  0.11385141509899772
iteration : 11020
train acc:  0.875
train loss:  0.307979553937912
train gradient:  0.16749982403246183
iteration : 11021
train acc:  0.859375
train loss:  0.38425973057746887
train gradient:  0.25633572425647044
iteration : 11022
train acc:  0.859375
train loss:  0.27740001678466797
train gradient:  0.14696690638603796
iteration : 11023
train acc:  0.890625
train loss:  0.2745804190635681
train gradient:  0.11677665348312236
iteration : 11024
train acc:  0.84375
train loss:  0.29283976554870605
train gradient:  0.13227084043211995
iteration : 11025
train acc:  0.84375
train loss:  0.35449451208114624
train gradient:  0.1764617755221406
iteration : 11026
train acc:  0.8828125
train loss:  0.29739946126937866
train gradient:  0.14322368400756103
iteration : 11027
train acc:  0.8203125
train loss:  0.3650108873844147
train gradient:  0.17870098985527932
iteration : 11028
train acc:  0.8984375
train loss:  0.284370094537735
train gradient:  0.09039188458735231
iteration : 11029
train acc:  0.8671875
train loss:  0.3058459758758545
train gradient:  0.13984117711586064
iteration : 11030
train acc:  0.890625
train loss:  0.2946314811706543
train gradient:  0.14491663650970982
iteration : 11031
train acc:  0.90625
train loss:  0.23770375549793243
train gradient:  0.10470856380917369
iteration : 11032
train acc:  0.890625
train loss:  0.2570878863334656
train gradient:  0.0976928285894615
iteration : 11033
train acc:  0.8671875
train loss:  0.3363379240036011
train gradient:  0.1625866386567544
iteration : 11034
train acc:  0.90625
train loss:  0.3073040246963501
train gradient:  0.11845382595473693
iteration : 11035
train acc:  0.859375
train loss:  0.3797140121459961
train gradient:  0.1976966921658511
iteration : 11036
train acc:  0.8125
train loss:  0.4057561159133911
train gradient:  0.18641078138241995
iteration : 11037
train acc:  0.859375
train loss:  0.288327693939209
train gradient:  0.1555963283777845
iteration : 11038
train acc:  0.828125
train loss:  0.374301552772522
train gradient:  0.1956863374775843
iteration : 11039
train acc:  0.8515625
train loss:  0.3559260368347168
train gradient:  0.1492655641818859
iteration : 11040
train acc:  0.859375
train loss:  0.3331076502799988
train gradient:  0.11599134864196307
iteration : 11041
train acc:  0.859375
train loss:  0.3489677906036377
train gradient:  0.1475128758013798
iteration : 11042
train acc:  0.828125
train loss:  0.40516000986099243
train gradient:  0.14787655685278175
iteration : 11043
train acc:  0.890625
train loss:  0.295979768037796
train gradient:  0.13891507130805925
iteration : 11044
train acc:  0.796875
train loss:  0.3694501519203186
train gradient:  0.19928747832187033
iteration : 11045
train acc:  0.8125
train loss:  0.3885863423347473
train gradient:  0.23744227542321028
iteration : 11046
train acc:  0.890625
train loss:  0.3094106614589691
train gradient:  0.10879145666052643
iteration : 11047
train acc:  0.8828125
train loss:  0.33057063817977905
train gradient:  0.14962343240461906
iteration : 11048
train acc:  0.8828125
train loss:  0.2980669140815735
train gradient:  0.13602754129239786
iteration : 11049
train acc:  0.859375
train loss:  0.3318304419517517
train gradient:  0.1273901033851313
iteration : 11050
train acc:  0.875
train loss:  0.2842453122138977
train gradient:  0.11543138366311673
iteration : 11051
train acc:  0.9140625
train loss:  0.278998464345932
train gradient:  0.17468209849014682
iteration : 11052
train acc:  0.828125
train loss:  0.3654597997665405
train gradient:  0.1890458230916203
iteration : 11053
train acc:  0.8671875
train loss:  0.33118191361427307
train gradient:  0.1789942350857254
iteration : 11054
train acc:  0.8125
train loss:  0.4417504072189331
train gradient:  0.2083543444770231
iteration : 11055
train acc:  0.84375
train loss:  0.30653268098831177
train gradient:  0.10732027983311516
iteration : 11056
train acc:  0.890625
train loss:  0.29570433497428894
train gradient:  0.1567194068158023
iteration : 11057
train acc:  0.8515625
train loss:  0.3017555773258209
train gradient:  0.12416426095121287
iteration : 11058
train acc:  0.875
train loss:  0.25535592436790466
train gradient:  0.14046329159932142
iteration : 11059
train acc:  0.8671875
train loss:  0.34521859884262085
train gradient:  0.13990707031380173
iteration : 11060
train acc:  0.875
train loss:  0.3240189552307129
train gradient:  0.1373907036758286
iteration : 11061
train acc:  0.8125
train loss:  0.3757452070713043
train gradient:  0.1605602587665702
iteration : 11062
train acc:  0.8359375
train loss:  0.3732265830039978
train gradient:  0.17719921114272894
iteration : 11063
train acc:  0.890625
train loss:  0.23454861342906952
train gradient:  0.0996389926671294
iteration : 11064
train acc:  0.8515625
train loss:  0.31336337327957153
train gradient:  0.16330829452933998
iteration : 11065
train acc:  0.8515625
train loss:  0.3213081359863281
train gradient:  0.1743323440622962
iteration : 11066
train acc:  0.8359375
train loss:  0.3502960801124573
train gradient:  0.15138574196198518
iteration : 11067
train acc:  0.9140625
train loss:  0.2627403140068054
train gradient:  0.13670665461009784
iteration : 11068
train acc:  0.828125
train loss:  0.37753018736839294
train gradient:  0.28544557887123023
iteration : 11069
train acc:  0.84375
train loss:  0.3178827166557312
train gradient:  0.16137217483778
iteration : 11070
train acc:  0.828125
train loss:  0.3529133200645447
train gradient:  0.18869868144416724
iteration : 11071
train acc:  0.875
train loss:  0.30143144726753235
train gradient:  0.16947221363743864
iteration : 11072
train acc:  0.859375
train loss:  0.3010065257549286
train gradient:  0.12685763013167067
iteration : 11073
train acc:  0.8359375
train loss:  0.3140086233615875
train gradient:  0.1545041217915838
iteration : 11074
train acc:  0.8359375
train loss:  0.377654492855072
train gradient:  0.2615082567656326
iteration : 11075
train acc:  0.8359375
train loss:  0.3617122173309326
train gradient:  0.20288803935200322
iteration : 11076
train acc:  0.8359375
train loss:  0.35543933510780334
train gradient:  0.15439002070287394
iteration : 11077
train acc:  0.859375
train loss:  0.2951352000236511
train gradient:  0.1334526275110393
iteration : 11078
train acc:  0.8125
train loss:  0.3698573708534241
train gradient:  0.22441082780382265
iteration : 11079
train acc:  0.8671875
train loss:  0.29613399505615234
train gradient:  0.17807734819493665
iteration : 11080
train acc:  0.8984375
train loss:  0.3323367238044739
train gradient:  0.19823422373512625
iteration : 11081
train acc:  0.890625
train loss:  0.2624596953392029
train gradient:  0.1394955540585645
iteration : 11082
train acc:  0.9140625
train loss:  0.2476184368133545
train gradient:  0.13922897341272855
iteration : 11083
train acc:  0.8359375
train loss:  0.3487451672554016
train gradient:  0.1787745075040175
iteration : 11084
train acc:  0.84375
train loss:  0.34934869408607483
train gradient:  0.16181545277635614
iteration : 11085
train acc:  0.9140625
train loss:  0.2402610182762146
train gradient:  0.12148654255524903
iteration : 11086
train acc:  0.8515625
train loss:  0.3268621265888214
train gradient:  0.20782682663467836
iteration : 11087
train acc:  0.8359375
train loss:  0.3583833575248718
train gradient:  0.14610495493652717
iteration : 11088
train acc:  0.84375
train loss:  0.36667579412460327
train gradient:  0.17962138688024082
iteration : 11089
train acc:  0.84375
train loss:  0.36662471294403076
train gradient:  0.1612695841485278
iteration : 11090
train acc:  0.8671875
train loss:  0.31302952766418457
train gradient:  0.14116707314995827
iteration : 11091
train acc:  0.828125
train loss:  0.39005011320114136
train gradient:  0.21743661642162376
iteration : 11092
train acc:  0.828125
train loss:  0.34476780891418457
train gradient:  0.12358936782173142
iteration : 11093
train acc:  0.890625
train loss:  0.2679452896118164
train gradient:  0.142648577344487
iteration : 11094
train acc:  0.8515625
train loss:  0.3156972825527191
train gradient:  0.14044801974179422
iteration : 11095
train acc:  0.8828125
train loss:  0.3214641809463501
train gradient:  0.16209357937710317
iteration : 11096
train acc:  0.890625
train loss:  0.2787865400314331
train gradient:  0.130601791045871
iteration : 11097
train acc:  0.8828125
train loss:  0.30504685640335083
train gradient:  0.16208373194505613
iteration : 11098
train acc:  0.875
train loss:  0.2923271358013153
train gradient:  0.12718359361220072
iteration : 11099
train acc:  0.859375
train loss:  0.370664119720459
train gradient:  0.17339662342350937
iteration : 11100
train acc:  0.875
train loss:  0.31784266233444214
train gradient:  0.12574588767289402
iteration : 11101
train acc:  0.8515625
train loss:  0.29920127987861633
train gradient:  0.121010460515808
iteration : 11102
train acc:  0.828125
train loss:  0.3203650116920471
train gradient:  0.13426231985428025
iteration : 11103
train acc:  0.828125
train loss:  0.3089304268360138
train gradient:  0.14351693382070768
iteration : 11104
train acc:  0.890625
train loss:  0.2748311460018158
train gradient:  0.07596105253324832
iteration : 11105
train acc:  0.84375
train loss:  0.3673269748687744
train gradient:  0.2129400117403696
iteration : 11106
train acc:  0.875
train loss:  0.3087414503097534
train gradient:  0.14006000050999612
iteration : 11107
train acc:  0.8671875
train loss:  0.2643323242664337
train gradient:  0.13217248450505764
iteration : 11108
train acc:  0.8828125
train loss:  0.26053285598754883
train gradient:  0.09119538585651067
iteration : 11109
train acc:  0.828125
train loss:  0.3934105336666107
train gradient:  0.22185269958566847
iteration : 11110
train acc:  0.8828125
train loss:  0.28038835525512695
train gradient:  0.14644702524916584
iteration : 11111
train acc:  0.875
train loss:  0.27106666564941406
train gradient:  0.11495626198982976
iteration : 11112
train acc:  0.890625
train loss:  0.25197845697402954
train gradient:  0.10204240966186487
iteration : 11113
train acc:  0.8671875
train loss:  0.28104740381240845
train gradient:  0.11332479816567023
iteration : 11114
train acc:  0.859375
train loss:  0.3021414279937744
train gradient:  0.12050805523498684
iteration : 11115
train acc:  0.8359375
train loss:  0.32375359535217285
train gradient:  0.11687061089181557
iteration : 11116
train acc:  0.8984375
train loss:  0.30336886644363403
train gradient:  0.09656634445950399
iteration : 11117
train acc:  0.84375
train loss:  0.3471023738384247
train gradient:  0.19686175726081973
iteration : 11118
train acc:  0.8515625
train loss:  0.36569446325302124
train gradient:  0.24573770547233192
iteration : 11119
train acc:  0.84375
train loss:  0.34549176692962646
train gradient:  0.3109372981580931
iteration : 11120
train acc:  0.8671875
train loss:  0.31818294525146484
train gradient:  0.13894715576274644
iteration : 11121
train acc:  0.8515625
train loss:  0.3582659065723419
train gradient:  0.1581542346136175
iteration : 11122
train acc:  0.8203125
train loss:  0.4066665768623352
train gradient:  0.23131521656995074
iteration : 11123
train acc:  0.8046875
train loss:  0.4238930344581604
train gradient:  0.22691859965699224
iteration : 11124
train acc:  0.8671875
train loss:  0.26921236515045166
train gradient:  0.13369518401527025
iteration : 11125
train acc:  0.859375
train loss:  0.3652864694595337
train gradient:  0.1646653661216394
iteration : 11126
train acc:  0.8359375
train loss:  0.3595876097679138
train gradient:  0.147789843446907
iteration : 11127
train acc:  0.78125
train loss:  0.4660176932811737
train gradient:  0.2678182355799711
iteration : 11128
train acc:  0.8671875
train loss:  0.3088921010494232
train gradient:  0.1460312344534531
iteration : 11129
train acc:  0.8359375
train loss:  0.4222705066204071
train gradient:  0.23343720095464543
iteration : 11130
train acc:  0.8203125
train loss:  0.33619487285614014
train gradient:  0.12550861890303633
iteration : 11131
train acc:  0.859375
train loss:  0.3527078628540039
train gradient:  0.1875707552297462
iteration : 11132
train acc:  0.8515625
train loss:  0.3882829546928406
train gradient:  0.1700865293490961
iteration : 11133
train acc:  0.8203125
train loss:  0.32932519912719727
train gradient:  0.21625863452482594
iteration : 11134
train acc:  0.890625
train loss:  0.27849844098091125
train gradient:  0.18037633188884472
iteration : 11135
train acc:  0.8515625
train loss:  0.3590608835220337
train gradient:  0.14955510159677254
iteration : 11136
train acc:  0.859375
train loss:  0.30774086713790894
train gradient:  0.1192460990708736
iteration : 11137
train acc:  0.8828125
train loss:  0.3205375075340271
train gradient:  0.2011074151600018
iteration : 11138
train acc:  0.890625
train loss:  0.3254812955856323
train gradient:  0.11449490947805646
iteration : 11139
train acc:  0.859375
train loss:  0.30720192193984985
train gradient:  0.10567123986518827
iteration : 11140
train acc:  0.84375
train loss:  0.2780666649341583
train gradient:  0.07090930190543202
iteration : 11141
train acc:  0.8828125
train loss:  0.29311296343803406
train gradient:  0.17210684352964833
iteration : 11142
train acc:  0.8515625
train loss:  0.3570829927921295
train gradient:  0.15278042085077465
iteration : 11143
train acc:  0.890625
train loss:  0.28718623518943787
train gradient:  0.10742225154209507
iteration : 11144
train acc:  0.8359375
train loss:  0.3691522479057312
train gradient:  0.141599274757035
iteration : 11145
train acc:  0.8203125
train loss:  0.38354289531707764
train gradient:  0.17492940127208367
iteration : 11146
train acc:  0.859375
train loss:  0.31734928488731384
train gradient:  0.1379644793292112
iteration : 11147
train acc:  0.9140625
train loss:  0.27748870849609375
train gradient:  0.08508399705191563
iteration : 11148
train acc:  0.8515625
train loss:  0.34307044744491577
train gradient:  0.1319096638181586
iteration : 11149
train acc:  0.90625
train loss:  0.28349995613098145
train gradient:  0.15792873133664528
iteration : 11150
train acc:  0.8671875
train loss:  0.31559357047080994
train gradient:  0.11396668671603151
iteration : 11151
train acc:  0.7890625
train loss:  0.4587516784667969
train gradient:  0.2667681731650072
iteration : 11152
train acc:  0.8515625
train loss:  0.36748939752578735
train gradient:  0.1763229943111755
iteration : 11153
train acc:  0.9140625
train loss:  0.2877107262611389
train gradient:  0.1767225398191809
iteration : 11154
train acc:  0.8984375
train loss:  0.30737781524658203
train gradient:  0.11756572208298921
iteration : 11155
train acc:  0.875
train loss:  0.32959216833114624
train gradient:  0.18424661140274223
iteration : 11156
train acc:  0.90625
train loss:  0.2711772322654724
train gradient:  0.1097451449576642
iteration : 11157
train acc:  0.875
train loss:  0.30382150411605835
train gradient:  0.09734428121477748
iteration : 11158
train acc:  0.8515625
train loss:  0.3367105722427368
train gradient:  0.18588076087123956
iteration : 11159
train acc:  0.84375
train loss:  0.3331363797187805
train gradient:  0.13947893912665318
iteration : 11160
train acc:  0.8515625
train loss:  0.30605044960975647
train gradient:  0.14115026841650213
iteration : 11161
train acc:  0.8515625
train loss:  0.3014240860939026
train gradient:  0.12869132143663128
iteration : 11162
train acc:  0.8359375
train loss:  0.3270634412765503
train gradient:  0.14806995302656667
iteration : 11163
train acc:  0.859375
train loss:  0.3325576186180115
train gradient:  0.2183495119828534
iteration : 11164
train acc:  0.8046875
train loss:  0.4006081819534302
train gradient:  0.2764034237874488
iteration : 11165
train acc:  0.8515625
train loss:  0.31606388092041016
train gradient:  0.15358319562634848
iteration : 11166
train acc:  0.8671875
train loss:  0.29600557684898376
train gradient:  0.21119390024421122
iteration : 11167
train acc:  0.8515625
train loss:  0.439588725566864
train gradient:  0.2034073260074358
iteration : 11168
train acc:  0.84375
train loss:  0.30168417096138
train gradient:  0.12936430285038725
iteration : 11169
train acc:  0.8984375
train loss:  0.2633909285068512
train gradient:  0.09445260752204858
iteration : 11170
train acc:  0.8046875
train loss:  0.40798643231391907
train gradient:  0.21740243127938524
iteration : 11171
train acc:  0.828125
train loss:  0.4140145778656006
train gradient:  0.22716401658607893
iteration : 11172
train acc:  0.8984375
train loss:  0.299473375082016
train gradient:  0.10671811058820349
iteration : 11173
train acc:  0.828125
train loss:  0.3756074905395508
train gradient:  0.18432090845681004
iteration : 11174
train acc:  0.84375
train loss:  0.40411484241485596
train gradient:  0.18220300790779273
iteration : 11175
train acc:  0.90625
train loss:  0.2319965958595276
train gradient:  0.07186653364798695
iteration : 11176
train acc:  0.8984375
train loss:  0.24458128213882446
train gradient:  0.08125606419474475
iteration : 11177
train acc:  0.8203125
train loss:  0.3448072671890259
train gradient:  0.18246715692688006
iteration : 11178
train acc:  0.78125
train loss:  0.43590861558914185
train gradient:  0.15919573270567935
iteration : 11179
train acc:  0.8671875
train loss:  0.2589103579521179
train gradient:  0.11035338777869866
iteration : 11180
train acc:  0.890625
train loss:  0.33273565769195557
train gradient:  0.16532292736504178
iteration : 11181
train acc:  0.875
train loss:  0.32399141788482666
train gradient:  0.1439774771251785
iteration : 11182
train acc:  0.8359375
train loss:  0.36479616165161133
train gradient:  0.17879832420167618
iteration : 11183
train acc:  0.8671875
train loss:  0.29909294843673706
train gradient:  0.11729181270463652
iteration : 11184
train acc:  0.90625
train loss:  0.25854724645614624
train gradient:  0.11734374642233607
iteration : 11185
train acc:  0.859375
train loss:  0.3503742814064026
train gradient:  0.15784863233912838
iteration : 11186
train acc:  0.8515625
train loss:  0.3102404475212097
train gradient:  0.11661902675815806
iteration : 11187
train acc:  0.828125
train loss:  0.3487207591533661
train gradient:  0.1682785817919763
iteration : 11188
train acc:  0.9140625
train loss:  0.2176317274570465
train gradient:  0.0872423106828925
iteration : 11189
train acc:  0.9140625
train loss:  0.27534013986587524
train gradient:  0.10081594083610984
iteration : 11190
train acc:  0.8671875
train loss:  0.28371545672416687
train gradient:  0.09511154525991543
iteration : 11191
train acc:  0.84375
train loss:  0.3164708614349365
train gradient:  0.2015464122238459
iteration : 11192
train acc:  0.875
train loss:  0.3416239023208618
train gradient:  0.1761502826078849
iteration : 11193
train acc:  0.890625
train loss:  0.27522897720336914
train gradient:  0.10848728047930423
iteration : 11194
train acc:  0.8515625
train loss:  0.3333062529563904
train gradient:  0.15413438162483095
iteration : 11195
train acc:  0.8203125
train loss:  0.3050234615802765
train gradient:  0.12632132017503
iteration : 11196
train acc:  0.875
train loss:  0.3372681736946106
train gradient:  0.21862248683144433
iteration : 11197
train acc:  0.890625
train loss:  0.2985963225364685
train gradient:  0.1795744707719782
iteration : 11198
train acc:  0.8359375
train loss:  0.2856328785419464
train gradient:  0.14707745122756843
iteration : 11199
train acc:  0.8515625
train loss:  0.3358766436576843
train gradient:  0.16352926407306745
iteration : 11200
train acc:  0.8515625
train loss:  0.29910168051719666
train gradient:  0.10402352345239516
iteration : 11201
train acc:  0.8828125
train loss:  0.28133469820022583
train gradient:  0.09024804408941778
iteration : 11202
train acc:  0.890625
train loss:  0.29200392961502075
train gradient:  0.22877409572016671
iteration : 11203
train acc:  0.8828125
train loss:  0.24541184306144714
train gradient:  0.07731294368054228
iteration : 11204
train acc:  0.8515625
train loss:  0.379874587059021
train gradient:  0.20920241358037123
iteration : 11205
train acc:  0.8515625
train loss:  0.2947062849998474
train gradient:  0.14257533715842363
iteration : 11206
train acc:  0.859375
train loss:  0.3366239666938782
train gradient:  0.16836270208512175
iteration : 11207
train acc:  0.90625
train loss:  0.2644447088241577
train gradient:  0.10254428668326834
iteration : 11208
train acc:  0.8828125
train loss:  0.30341994762420654
train gradient:  0.26915652612334257
iteration : 11209
train acc:  0.796875
train loss:  0.38435208797454834
train gradient:  0.16230460442232242
iteration : 11210
train acc:  0.828125
train loss:  0.4244147539138794
train gradient:  0.21957828798159107
iteration : 11211
train acc:  0.8515625
train loss:  0.33602139353752136
train gradient:  0.19607115692646693
iteration : 11212
train acc:  0.859375
train loss:  0.37696728110313416
train gradient:  0.1847667907907083
iteration : 11213
train acc:  0.8515625
train loss:  0.3386300504207611
train gradient:  0.17217298339677373
iteration : 11214
train acc:  0.8515625
train loss:  0.3526526987552643
train gradient:  0.17371104943002902
iteration : 11215
train acc:  0.8671875
train loss:  0.2892289161682129
train gradient:  0.1027772469161154
iteration : 11216
train acc:  0.78125
train loss:  0.4754030406475067
train gradient:  0.282118168301148
iteration : 11217
train acc:  0.875
train loss:  0.3033093214035034
train gradient:  0.14779281948744716
iteration : 11218
train acc:  0.796875
train loss:  0.4657468795776367
train gradient:  0.2705939125605917
iteration : 11219
train acc:  0.8359375
train loss:  0.3293440341949463
train gradient:  0.16347451522598938
iteration : 11220
train acc:  0.8828125
train loss:  0.3599856495857239
train gradient:  0.2600894738868463
iteration : 11221
train acc:  0.84375
train loss:  0.31984585523605347
train gradient:  0.15556115702697776
iteration : 11222
train acc:  0.8515625
train loss:  0.3108401298522949
train gradient:  0.17302940412327078
iteration : 11223
train acc:  0.84375
train loss:  0.3649815320968628
train gradient:  0.1672107241764547
iteration : 11224
train acc:  0.890625
train loss:  0.24625185132026672
train gradient:  0.08139789996856903
iteration : 11225
train acc:  0.9140625
train loss:  0.24526365101337433
train gradient:  0.1508495429159814
iteration : 11226
train acc:  0.875
train loss:  0.3038944900035858
train gradient:  0.1097662677752202
iteration : 11227
train acc:  0.875
train loss:  0.2917831242084503
train gradient:  0.10684871647066932
iteration : 11228
train acc:  0.8359375
train loss:  0.37911486625671387
train gradient:  0.1899309491714115
iteration : 11229
train acc:  0.78125
train loss:  0.3740517497062683
train gradient:  0.2872008024939314
iteration : 11230
train acc:  0.8671875
train loss:  0.29384663701057434
train gradient:  0.13045365118071925
iteration : 11231
train acc:  0.8828125
train loss:  0.30157220363616943
train gradient:  0.11037398263325342
iteration : 11232
train acc:  0.8984375
train loss:  0.2489182949066162
train gradient:  0.08669319205548905
iteration : 11233
train acc:  0.8515625
train loss:  0.3560566306114197
train gradient:  0.23348904915893837
iteration : 11234
train acc:  0.875
train loss:  0.29999664425849915
train gradient:  0.1575486089994226
iteration : 11235
train acc:  0.875
train loss:  0.2986994981765747
train gradient:  0.12455206206735966
iteration : 11236
train acc:  0.890625
train loss:  0.24814432859420776
train gradient:  0.09771734764367285
iteration : 11237
train acc:  0.84375
train loss:  0.31218191981315613
train gradient:  0.13321221037801828
iteration : 11238
train acc:  0.8046875
train loss:  0.4045168161392212
train gradient:  0.2604836947555977
iteration : 11239
train acc:  0.8046875
train loss:  0.3516567349433899
train gradient:  0.15539512408019174
iteration : 11240
train acc:  0.859375
train loss:  0.32140815258026123
train gradient:  0.16803635094159008
iteration : 11241
train acc:  0.8828125
train loss:  0.24877554178237915
train gradient:  0.1238597415604724
iteration : 11242
train acc:  0.875
train loss:  0.3289661109447479
train gradient:  0.1573542297013127
iteration : 11243
train acc:  0.8671875
train loss:  0.3042207658290863
train gradient:  0.24600027571945188
iteration : 11244
train acc:  0.8203125
train loss:  0.35123389959335327
train gradient:  0.1526097352795823
iteration : 11245
train acc:  0.8671875
train loss:  0.30249103903770447
train gradient:  0.08326287533113876
iteration : 11246
train acc:  0.8984375
train loss:  0.31162193417549133
train gradient:  0.11544627696310034
iteration : 11247
train acc:  0.859375
train loss:  0.321993887424469
train gradient:  0.16234479176535355
iteration : 11248
train acc:  0.8828125
train loss:  0.3283470571041107
train gradient:  0.23723972485351918
iteration : 11249
train acc:  0.875
train loss:  0.23092852532863617
train gradient:  0.08955773380967191
iteration : 11250
train acc:  0.8515625
train loss:  0.3445315957069397
train gradient:  0.20690024906249754
iteration : 11251
train acc:  0.8828125
train loss:  0.2901322841644287
train gradient:  0.13048103487168874
iteration : 11252
train acc:  0.8984375
train loss:  0.3200993239879608
train gradient:  0.17966375003219492
iteration : 11253
train acc:  0.859375
train loss:  0.2899921238422394
train gradient:  0.14788067786166628
iteration : 11254
train acc:  0.8984375
train loss:  0.2586587965488434
train gradient:  0.1382800305501205
iteration : 11255
train acc:  0.828125
train loss:  0.4284799098968506
train gradient:  0.22862252730027283
iteration : 11256
train acc:  0.890625
train loss:  0.2979443371295929
train gradient:  0.11015449952850132
iteration : 11257
train acc:  0.8515625
train loss:  0.3032723069190979
train gradient:  0.13032813900852058
iteration : 11258
train acc:  0.875
train loss:  0.31131741404533386
train gradient:  0.1654471032632152
iteration : 11259
train acc:  0.890625
train loss:  0.2712555229663849
train gradient:  0.11314643699852352
iteration : 11260
train acc:  0.8515625
train loss:  0.36579450964927673
train gradient:  0.14830378888666845
iteration : 11261
train acc:  0.8515625
train loss:  0.3091769218444824
train gradient:  0.2605088725857311
iteration : 11262
train acc:  0.8671875
train loss:  0.2933591604232788
train gradient:  0.17976684047860392
iteration : 11263
train acc:  0.90625
train loss:  0.31342628598213196
train gradient:  0.17918995622852646
iteration : 11264
train acc:  0.921875
train loss:  0.2298651933670044
train gradient:  0.08953592648935929
iteration : 11265
train acc:  0.8671875
train loss:  0.34885647892951965
train gradient:  0.1735136460802297
iteration : 11266
train acc:  0.8125
train loss:  0.3378387689590454
train gradient:  0.14965285698969882
iteration : 11267
train acc:  0.859375
train loss:  0.2917456030845642
train gradient:  0.10465856213546211
iteration : 11268
train acc:  0.8203125
train loss:  0.35376420617103577
train gradient:  0.16219496546269024
iteration : 11269
train acc:  0.8984375
train loss:  0.2735820412635803
train gradient:  0.21011370487578035
iteration : 11270
train acc:  0.84375
train loss:  0.35471320152282715
train gradient:  0.1499902729147629
iteration : 11271
train acc:  0.8671875
train loss:  0.3137947916984558
train gradient:  0.16677933302192266
iteration : 11272
train acc:  0.8515625
train loss:  0.3074387311935425
train gradient:  0.13505370732026123
iteration : 11273
train acc:  0.8203125
train loss:  0.4291541576385498
train gradient:  0.2499024584674167
iteration : 11274
train acc:  0.8125
train loss:  0.39186298847198486
train gradient:  0.1928078785563071
iteration : 11275
train acc:  0.8828125
train loss:  0.29772526025772095
train gradient:  0.09987063036303341
iteration : 11276
train acc:  0.8515625
train loss:  0.3452831506729126
train gradient:  0.1930637483879134
iteration : 11277
train acc:  0.890625
train loss:  0.2886018753051758
train gradient:  0.1419218909524035
iteration : 11278
train acc:  0.8828125
train loss:  0.26364320516586304
train gradient:  0.10892134787842146
iteration : 11279
train acc:  0.828125
train loss:  0.3860427141189575
train gradient:  0.1427502170382548
iteration : 11280
train acc:  0.8984375
train loss:  0.28514277935028076
train gradient:  0.14337196961820375
iteration : 11281
train acc:  0.8984375
train loss:  0.2543269693851471
train gradient:  0.09443263617303764
iteration : 11282
train acc:  0.859375
train loss:  0.28866487741470337
train gradient:  0.10272159502108467
iteration : 11283
train acc:  0.8671875
train loss:  0.31069666147232056
train gradient:  0.15510053472349494
iteration : 11284
train acc:  0.859375
train loss:  0.2927324175834656
train gradient:  0.1866947986423804
iteration : 11285
train acc:  0.7734375
train loss:  0.48302793502807617
train gradient:  0.30846768927253465
iteration : 11286
train acc:  0.859375
train loss:  0.32992327213287354
train gradient:  0.12017734034792993
iteration : 11287
train acc:  0.9140625
train loss:  0.24301546812057495
train gradient:  0.11316372288608455
iteration : 11288
train acc:  0.8359375
train loss:  0.3608192801475525
train gradient:  0.20616464448697056
iteration : 11289
train acc:  0.8046875
train loss:  0.4588291347026825
train gradient:  0.3359006636561119
iteration : 11290
train acc:  0.875
train loss:  0.30413493514060974
train gradient:  0.16486281489132437
iteration : 11291
train acc:  0.8671875
train loss:  0.3125058710575104
train gradient:  0.14008868002331099
iteration : 11292
train acc:  0.8671875
train loss:  0.31131622195243835
train gradient:  0.1535229485111651
iteration : 11293
train acc:  0.9140625
train loss:  0.19108079373836517
train gradient:  0.07094254143184907
iteration : 11294
train acc:  0.8125
train loss:  0.3762408494949341
train gradient:  0.21790337232183798
iteration : 11295
train acc:  0.8125
train loss:  0.3798721432685852
train gradient:  0.20155382794676824
iteration : 11296
train acc:  0.890625
train loss:  0.3214471936225891
train gradient:  0.09088237666685098
iteration : 11297
train acc:  0.8671875
train loss:  0.2765682339668274
train gradient:  0.12939330754135658
iteration : 11298
train acc:  0.8515625
train loss:  0.4312138557434082
train gradient:  0.32978800466847996
iteration : 11299
train acc:  0.8671875
train loss:  0.3116784691810608
train gradient:  0.1578776954233453
iteration : 11300
train acc:  0.875
train loss:  0.2737165689468384
train gradient:  0.1234895947290375
iteration : 11301
train acc:  0.890625
train loss:  0.2733874022960663
train gradient:  0.1139138919233099
iteration : 11302
train acc:  0.90625
train loss:  0.28650033473968506
train gradient:  0.11112533854296278
iteration : 11303
train acc:  0.890625
train loss:  0.2501358985900879
train gradient:  0.07950816369541386
iteration : 11304
train acc:  0.8515625
train loss:  0.32362082600593567
train gradient:  0.11544463914326114
iteration : 11305
train acc:  0.859375
train loss:  0.3061075210571289
train gradient:  0.17470367769370704
iteration : 11306
train acc:  0.8359375
train loss:  0.3699803650379181
train gradient:  0.222631781412013
iteration : 11307
train acc:  0.8671875
train loss:  0.29566872119903564
train gradient:  0.1259489413557586
iteration : 11308
train acc:  0.796875
train loss:  0.3832402527332306
train gradient:  0.19507762632113718
iteration : 11309
train acc:  0.796875
train loss:  0.43187403678894043
train gradient:  0.2375947425149741
iteration : 11310
train acc:  0.8515625
train loss:  0.3411247730255127
train gradient:  0.19447648272387336
iteration : 11311
train acc:  0.84375
train loss:  0.3145650625228882
train gradient:  0.13858703788113674
iteration : 11312
train acc:  0.859375
train loss:  0.30210497975349426
train gradient:  0.10374986338996783
iteration : 11313
train acc:  0.8828125
train loss:  0.26722192764282227
train gradient:  0.08302947199977787
iteration : 11314
train acc:  0.8125
train loss:  0.3594205975532532
train gradient:  0.1760839788222316
iteration : 11315
train acc:  0.875
train loss:  0.3128707706928253
train gradient:  0.1323786912143181
iteration : 11316
train acc:  0.875
train loss:  0.3425109386444092
train gradient:  0.3321657717894581
iteration : 11317
train acc:  0.859375
train loss:  0.3958832025527954
train gradient:  0.16773843961781604
iteration : 11318
train acc:  0.84375
train loss:  0.3535059094429016
train gradient:  0.17803402877749075
iteration : 11319
train acc:  0.8828125
train loss:  0.29295283555984497
train gradient:  0.10972082558513241
iteration : 11320
train acc:  0.8203125
train loss:  0.35829317569732666
train gradient:  0.13406256065817035
iteration : 11321
train acc:  0.8125
train loss:  0.38405442237854004
train gradient:  0.1456301371383404
iteration : 11322
train acc:  0.9140625
train loss:  0.2791132926940918
train gradient:  0.12103486464191748
iteration : 11323
train acc:  0.8203125
train loss:  0.39196979999542236
train gradient:  0.25799810636726495
iteration : 11324
train acc:  0.8046875
train loss:  0.38415104150772095
train gradient:  0.1843824728222098
iteration : 11325
train acc:  0.875
train loss:  0.29577991366386414
train gradient:  0.13019982349817016
iteration : 11326
train acc:  0.875
train loss:  0.3834651708602905
train gradient:  0.15159322057170982
iteration : 11327
train acc:  0.828125
train loss:  0.39620262384414673
train gradient:  0.19962609989169852
iteration : 11328
train acc:  0.84375
train loss:  0.3276101350784302
train gradient:  0.15774733747412842
iteration : 11329
train acc:  0.8671875
train loss:  0.2695038616657257
train gradient:  0.11317653060263476
iteration : 11330
train acc:  0.90625
train loss:  0.2519620656967163
train gradient:  0.08880849336644127
iteration : 11331
train acc:  0.8671875
train loss:  0.34178993105888367
train gradient:  0.13457803714206518
iteration : 11332
train acc:  0.8125
train loss:  0.3910113573074341
train gradient:  0.19918589965718275
iteration : 11333
train acc:  0.8359375
train loss:  0.3088257312774658
train gradient:  0.10425499191677425
iteration : 11334
train acc:  0.8515625
train loss:  0.3189482092857361
train gradient:  0.1707941754846089
iteration : 11335
train acc:  0.84375
train loss:  0.38288840651512146
train gradient:  0.1941947921536029
iteration : 11336
train acc:  0.8359375
train loss:  0.33479997515678406
train gradient:  0.16813098592679832
iteration : 11337
train acc:  0.9140625
train loss:  0.2557229995727539
train gradient:  0.1102927023386545
iteration : 11338
train acc:  0.8828125
train loss:  0.31777793169021606
train gradient:  0.130350245689924
iteration : 11339
train acc:  0.8671875
train loss:  0.30379921197891235
train gradient:  0.1334077167781752
iteration : 11340
train acc:  0.8515625
train loss:  0.3487820327281952
train gradient:  0.14775245960884872
iteration : 11341
train acc:  0.9375
train loss:  0.22117479145526886
train gradient:  0.07520813032508941
iteration : 11342
train acc:  0.8671875
train loss:  0.27869993448257446
train gradient:  0.08422159774829162
iteration : 11343
train acc:  0.859375
train loss:  0.3588671088218689
train gradient:  0.15574913604985685
iteration : 11344
train acc:  0.859375
train loss:  0.3630937337875366
train gradient:  0.14432443107949847
iteration : 11345
train acc:  0.8125
train loss:  0.35945650935173035
train gradient:  0.12539949439153297
iteration : 11346
train acc:  0.8203125
train loss:  0.34592747688293457
train gradient:  0.1970966825965753
iteration : 11347
train acc:  0.8515625
train loss:  0.3479394316673279
train gradient:  0.14600953537733774
iteration : 11348
train acc:  0.8359375
train loss:  0.3899461328983307
train gradient:  0.24193507447424745
iteration : 11349
train acc:  0.78125
train loss:  0.46889516711235046
train gradient:  0.2476346796762242
iteration : 11350
train acc:  0.8828125
train loss:  0.2837967872619629
train gradient:  0.10535303984277652
iteration : 11351
train acc:  0.875
train loss:  0.34904375672340393
train gradient:  0.1578886981510087
iteration : 11352
train acc:  0.8828125
train loss:  0.3020603358745575
train gradient:  0.12113246858402164
iteration : 11353
train acc:  0.8828125
train loss:  0.28946900367736816
train gradient:  0.1084925542224945
iteration : 11354
train acc:  0.8515625
train loss:  0.3378838300704956
train gradient:  0.16932870803349445
iteration : 11355
train acc:  0.8515625
train loss:  0.35853415727615356
train gradient:  0.2216365947174213
iteration : 11356
train acc:  0.8828125
train loss:  0.264279305934906
train gradient:  0.1304066001453504
iteration : 11357
train acc:  0.8515625
train loss:  0.35731446743011475
train gradient:  0.1631658032700234
iteration : 11358
train acc:  0.8984375
train loss:  0.2675778865814209
train gradient:  0.13976690905150171
iteration : 11359
train acc:  0.8671875
train loss:  0.4027666449546814
train gradient:  0.17172076834898242
iteration : 11360
train acc:  0.8515625
train loss:  0.3713991641998291
train gradient:  0.17432562161996956
iteration : 11361
train acc:  0.8515625
train loss:  0.3773694634437561
train gradient:  0.19337157225594873
iteration : 11362
train acc:  0.8671875
train loss:  0.39400434494018555
train gradient:  0.15223854660214595
iteration : 11363
train acc:  0.84375
train loss:  0.3465192914009094
train gradient:  0.15781021030007764
iteration : 11364
train acc:  0.8671875
train loss:  0.2820724844932556
train gradient:  0.12266718978369019
iteration : 11365
train acc:  0.8515625
train loss:  0.27973824739456177
train gradient:  0.11065729004920095
iteration : 11366
train acc:  0.90625
train loss:  0.3099384903907776
train gradient:  0.15154237688546654
iteration : 11367
train acc:  0.8984375
train loss:  0.2817648947238922
train gradient:  0.10956048994072799
iteration : 11368
train acc:  0.84375
train loss:  0.3428281247615814
train gradient:  0.12017602088754645
iteration : 11369
train acc:  0.875
train loss:  0.2520887553691864
train gradient:  0.10208945224589623
iteration : 11370
train acc:  0.859375
train loss:  0.3379223346710205
train gradient:  0.13227416276598808
iteration : 11371
train acc:  0.8359375
train loss:  0.356670081615448
train gradient:  0.1277658760645397
iteration : 11372
train acc:  0.7734375
train loss:  0.4357028007507324
train gradient:  0.20785097939846095
iteration : 11373
train acc:  0.8203125
train loss:  0.3848915100097656
train gradient:  0.22867148798572143
iteration : 11374
train acc:  0.875
train loss:  0.3067939579486847
train gradient:  0.09879732447726854
iteration : 11375
train acc:  0.84375
train loss:  0.31755244731903076
train gradient:  0.15901483737638408
iteration : 11376
train acc:  0.8828125
train loss:  0.3078446388244629
train gradient:  0.1092639967523599
iteration : 11377
train acc:  0.8515625
train loss:  0.3406699001789093
train gradient:  0.16322303654624218
iteration : 11378
train acc:  0.8828125
train loss:  0.2711936831474304
train gradient:  0.07234308542217513
iteration : 11379
train acc:  0.8515625
train loss:  0.35608214139938354
train gradient:  0.1655247627160678
iteration : 11380
train acc:  0.8359375
train loss:  0.3959994912147522
train gradient:  0.14612976598250899
iteration : 11381
train acc:  0.8671875
train loss:  0.3500351309776306
train gradient:  0.12813278835077624
iteration : 11382
train acc:  0.84375
train loss:  0.371807336807251
train gradient:  0.13330733802405081
iteration : 11383
train acc:  0.8984375
train loss:  0.25588855147361755
train gradient:  0.0959753706803755
iteration : 11384
train acc:  0.828125
train loss:  0.30191174149513245
train gradient:  0.13032460396669324
iteration : 11385
train acc:  0.8671875
train loss:  0.38048112392425537
train gradient:  0.16262108985639798
iteration : 11386
train acc:  0.828125
train loss:  0.3207889199256897
train gradient:  0.14722801422199058
iteration : 11387
train acc:  0.84375
train loss:  0.3311581611633301
train gradient:  0.11565368258211665
iteration : 11388
train acc:  0.8359375
train loss:  0.3650661110877991
train gradient:  0.16147161243505542
iteration : 11389
train acc:  0.8125
train loss:  0.36639463901519775
train gradient:  0.2213663070425721
iteration : 11390
train acc:  0.8125
train loss:  0.34891629219055176
train gradient:  0.19082419904304176
iteration : 11391
train acc:  0.796875
train loss:  0.3538842499256134
train gradient:  0.24172312643083682
iteration : 11392
train acc:  0.875
train loss:  0.2540525794029236
train gradient:  0.12783228533249225
iteration : 11393
train acc:  0.828125
train loss:  0.366557776927948
train gradient:  0.15205596244743985
iteration : 11394
train acc:  0.84375
train loss:  0.3052186667919159
train gradient:  0.0974766564702496
iteration : 11395
train acc:  0.8984375
train loss:  0.242301344871521
train gradient:  0.07848167404136966
iteration : 11396
train acc:  0.875
train loss:  0.3052646517753601
train gradient:  0.0983231120400116
iteration : 11397
train acc:  0.875
train loss:  0.3051404356956482
train gradient:  0.1415041673123729
iteration : 11398
train acc:  0.8515625
train loss:  0.3722624182701111
train gradient:  0.14576109579708946
iteration : 11399
train acc:  0.8203125
train loss:  0.363486111164093
train gradient:  0.22908038504839973
iteration : 11400
train acc:  0.84375
train loss:  0.32011890411376953
train gradient:  0.11425823619309443
iteration : 11401
train acc:  0.8359375
train loss:  0.4018072485923767
train gradient:  0.28076604724255155
iteration : 11402
train acc:  0.8515625
train loss:  0.27583545446395874
train gradient:  0.18700593913548968
iteration : 11403
train acc:  0.8828125
train loss:  0.3019639849662781
train gradient:  0.0885295683667162
iteration : 11404
train acc:  0.8203125
train loss:  0.32917433977127075
train gradient:  0.16616335384772768
iteration : 11405
train acc:  0.90625
train loss:  0.23849692940711975
train gradient:  0.10381502713404328
iteration : 11406
train acc:  0.8828125
train loss:  0.2779011130332947
train gradient:  0.11835334979337464
iteration : 11407
train acc:  0.90625
train loss:  0.2403503954410553
train gradient:  0.0988236615386309
iteration : 11408
train acc:  0.828125
train loss:  0.32534369826316833
train gradient:  0.15611352105693355
iteration : 11409
train acc:  0.8515625
train loss:  0.33689337968826294
train gradient:  0.18133278262577004
iteration : 11410
train acc:  0.890625
train loss:  0.28819945454597473
train gradient:  0.11925421643365215
iteration : 11411
train acc:  0.8671875
train loss:  0.27912458777427673
train gradient:  0.16454361143445045
iteration : 11412
train acc:  0.859375
train loss:  0.34589603543281555
train gradient:  0.13824183697623227
iteration : 11413
train acc:  0.875
train loss:  0.26439571380615234
train gradient:  0.1261984280041633
iteration : 11414
train acc:  0.859375
train loss:  0.3299393057823181
train gradient:  0.1650737048321468
iteration : 11415
train acc:  0.796875
train loss:  0.37720736861228943
train gradient:  0.16273167994231003
iteration : 11416
train acc:  0.8671875
train loss:  0.3083382844924927
train gradient:  0.1628741518679359
iteration : 11417
train acc:  0.8359375
train loss:  0.409329354763031
train gradient:  0.20047283912922634
iteration : 11418
train acc:  0.84375
train loss:  0.2963564693927765
train gradient:  0.11124271017155377
iteration : 11419
train acc:  0.859375
train loss:  0.3609299659729004
train gradient:  0.17356979408434287
iteration : 11420
train acc:  0.90625
train loss:  0.2551252841949463
train gradient:  0.1122574063901819
iteration : 11421
train acc:  0.859375
train loss:  0.3382423520088196
train gradient:  0.1609708776500191
iteration : 11422
train acc:  0.8671875
train loss:  0.3019412159919739
train gradient:  0.2967344140997039
iteration : 11423
train acc:  0.8515625
train loss:  0.3161700963973999
train gradient:  0.13916160906317407
iteration : 11424
train acc:  0.8203125
train loss:  0.37748581171035767
train gradient:  0.2833992990043597
iteration : 11425
train acc:  0.875
train loss:  0.26479825377464294
train gradient:  0.14237889298943313
iteration : 11426
train acc:  0.890625
train loss:  0.304760217666626
train gradient:  0.1730235822957732
iteration : 11427
train acc:  0.8359375
train loss:  0.34566301107406616
train gradient:  0.1289901567403251
iteration : 11428
train acc:  0.8828125
train loss:  0.3449089527130127
train gradient:  0.13757686525568685
iteration : 11429
train acc:  0.859375
train loss:  0.3335679769515991
train gradient:  0.14412241440459705
iteration : 11430
train acc:  0.859375
train loss:  0.29904937744140625
train gradient:  0.11631864232694585
iteration : 11431
train acc:  0.890625
train loss:  0.285977303981781
train gradient:  0.1246638936861842
iteration : 11432
train acc:  0.8359375
train loss:  0.3817681670188904
train gradient:  0.18255538155070022
iteration : 11433
train acc:  0.875
train loss:  0.25658515095710754
train gradient:  0.1380665232785784
iteration : 11434
train acc:  0.84375
train loss:  0.3312382698059082
train gradient:  0.1236750245465147
iteration : 11435
train acc:  0.875
train loss:  0.3013502359390259
train gradient:  0.12584079457057606
iteration : 11436
train acc:  0.8359375
train loss:  0.3664734959602356
train gradient:  0.19250189406314727
iteration : 11437
train acc:  0.796875
train loss:  0.40736082196235657
train gradient:  0.20706692703814347
iteration : 11438
train acc:  0.859375
train loss:  0.27799633145332336
train gradient:  0.09092793344802384
iteration : 11439
train acc:  0.84375
train loss:  0.34219110012054443
train gradient:  0.19576088687473636
iteration : 11440
train acc:  0.828125
train loss:  0.33243703842163086
train gradient:  0.1524796832867079
iteration : 11441
train acc:  0.828125
train loss:  0.35998502373695374
train gradient:  0.21919335655195152
iteration : 11442
train acc:  0.8515625
train loss:  0.33335959911346436
train gradient:  0.15441379247126683
iteration : 11443
train acc:  0.890625
train loss:  0.24044263362884521
train gradient:  0.06714218823097522
iteration : 11444
train acc:  0.8984375
train loss:  0.2550988495349884
train gradient:  0.07664600800982953
iteration : 11445
train acc:  0.84375
train loss:  0.3558694124221802
train gradient:  0.15742223525742552
iteration : 11446
train acc:  0.8671875
train loss:  0.28757283091545105
train gradient:  0.12913916393955438
iteration : 11447
train acc:  0.8984375
train loss:  0.30170637369155884
train gradient:  0.09461548254851809
iteration : 11448
train acc:  0.8671875
train loss:  0.36158373951911926
train gradient:  0.21804356329550598
iteration : 11449
train acc:  0.890625
train loss:  0.2917940616607666
train gradient:  0.0906603116085984
iteration : 11450
train acc:  0.8515625
train loss:  0.3486185371875763
train gradient:  0.1565041395054387
iteration : 11451
train acc:  0.8671875
train loss:  0.29234737157821655
train gradient:  0.12092424724695054
iteration : 11452
train acc:  0.8515625
train loss:  0.27651798725128174
train gradient:  0.1083628062525337
iteration : 11453
train acc:  0.8828125
train loss:  0.2938145399093628
train gradient:  0.17493259581528964
iteration : 11454
train acc:  0.8359375
train loss:  0.3033398985862732
train gradient:  0.14608351813139972
iteration : 11455
train acc:  0.8671875
train loss:  0.3091912865638733
train gradient:  0.11938908038864533
iteration : 11456
train acc:  0.8359375
train loss:  0.4190061688423157
train gradient:  0.24632919718305832
iteration : 11457
train acc:  0.8828125
train loss:  0.28701263666152954
train gradient:  0.10817556982668815
iteration : 11458
train acc:  0.8203125
train loss:  0.349548876285553
train gradient:  0.15145317869021402
iteration : 11459
train acc:  0.8671875
train loss:  0.31687507033348083
train gradient:  0.1265823999656547
iteration : 11460
train acc:  0.90625
train loss:  0.2734736204147339
train gradient:  0.13846191430950786
iteration : 11461
train acc:  0.8671875
train loss:  0.3463227450847626
train gradient:  0.2513218837644879
iteration : 11462
train acc:  0.90625
train loss:  0.27077487111091614
train gradient:  0.09604731011492292
iteration : 11463
train acc:  0.890625
train loss:  0.25173187255859375
train gradient:  0.09242655800763785
iteration : 11464
train acc:  0.8203125
train loss:  0.33245164155960083
train gradient:  0.15975322701072278
iteration : 11465
train acc:  0.828125
train loss:  0.3410796523094177
train gradient:  0.14636797060913795
iteration : 11466
train acc:  0.84375
train loss:  0.333578497171402
train gradient:  0.13975719559887628
iteration : 11467
train acc:  0.84375
train loss:  0.34368354082107544
train gradient:  0.15087892807598496
iteration : 11468
train acc:  0.8203125
train loss:  0.3241651654243469
train gradient:  0.21295594635259238
iteration : 11469
train acc:  0.8984375
train loss:  0.265705943107605
train gradient:  0.1246560044374559
iteration : 11470
train acc:  0.875
train loss:  0.32635098695755005
train gradient:  0.15593493080322496
iteration : 11471
train acc:  0.8203125
train loss:  0.3585997223854065
train gradient:  0.21166568770243652
iteration : 11472
train acc:  0.8515625
train loss:  0.31737181544303894
train gradient:  0.14536137899247847
iteration : 11473
train acc:  0.875
train loss:  0.24628306925296783
train gradient:  0.12962190597236914
iteration : 11474
train acc:  0.8515625
train loss:  0.37574613094329834
train gradient:  0.20838423972213405
iteration : 11475
train acc:  0.8359375
train loss:  0.2979458272457123
train gradient:  0.17196519187566345
iteration : 11476
train acc:  0.8515625
train loss:  0.3641495108604431
train gradient:  0.15827094974928074
iteration : 11477
train acc:  0.828125
train loss:  0.4155982434749603
train gradient:  0.21698668146498443
iteration : 11478
train acc:  0.8125
train loss:  0.41017377376556396
train gradient:  0.21072438386813133
iteration : 11479
train acc:  0.859375
train loss:  0.31356650590896606
train gradient:  0.11290155173961557
iteration : 11480
train acc:  0.8359375
train loss:  0.29910552501678467
train gradient:  0.10041063188594515
iteration : 11481
train acc:  0.859375
train loss:  0.3290858864784241
train gradient:  0.10586357480741035
iteration : 11482
train acc:  0.8359375
train loss:  0.3865395486354828
train gradient:  0.1942945800674001
iteration : 11483
train acc:  0.84375
train loss:  0.30234166979789734
train gradient:  0.11171230721959427
iteration : 11484
train acc:  0.828125
train loss:  0.34183821082115173
train gradient:  0.1598353014389738
iteration : 11485
train acc:  0.90625
train loss:  0.2548179626464844
train gradient:  0.09515857790359251
iteration : 11486
train acc:  0.8828125
train loss:  0.26930391788482666
train gradient:  0.09879738866229296
iteration : 11487
train acc:  0.90625
train loss:  0.2377566248178482
train gradient:  0.09128089111879903
iteration : 11488
train acc:  0.8671875
train loss:  0.32214754819869995
train gradient:  0.10997341976383818
iteration : 11489
train acc:  0.8515625
train loss:  0.308808833360672
train gradient:  0.15573554072331475
iteration : 11490
train acc:  0.890625
train loss:  0.2894613742828369
train gradient:  0.10692374579500269
iteration : 11491
train acc:  0.8359375
train loss:  0.34073305130004883
train gradient:  0.16386098780878544
iteration : 11492
train acc:  0.8515625
train loss:  0.31078267097473145
train gradient:  0.12731472169005326
iteration : 11493
train acc:  0.7890625
train loss:  0.4000716209411621
train gradient:  0.21451295914374147
iteration : 11494
train acc:  0.8671875
train loss:  0.3087041676044464
train gradient:  0.13164438994720118
iteration : 11495
train acc:  0.8828125
train loss:  0.3022462725639343
train gradient:  0.12735220477045178
iteration : 11496
train acc:  0.9140625
train loss:  0.2520853877067566
train gradient:  0.09531316029692218
iteration : 11497
train acc:  0.8359375
train loss:  0.34504398703575134
train gradient:  0.14543290835146946
iteration : 11498
train acc:  0.9140625
train loss:  0.29758399724960327
train gradient:  0.11913396728769382
iteration : 11499
train acc:  0.8828125
train loss:  0.2694874107837677
train gradient:  0.1620642824364455
iteration : 11500
train acc:  0.8046875
train loss:  0.4663679599761963
train gradient:  0.20442396456238254
iteration : 11501
train acc:  0.8359375
train loss:  0.3660168945789337
train gradient:  0.17120942582348622
iteration : 11502
train acc:  0.8984375
train loss:  0.2570575475692749
train gradient:  0.08445015608950963
iteration : 11503
train acc:  0.875
train loss:  0.280892014503479
train gradient:  0.10055412909300372
iteration : 11504
train acc:  0.890625
train loss:  0.2981465458869934
train gradient:  0.1127389810585693
iteration : 11505
train acc:  0.8671875
train loss:  0.2554377019405365
train gradient:  0.094658876407724
iteration : 11506
train acc:  0.8515625
train loss:  0.3249621093273163
train gradient:  0.14821115702923945
iteration : 11507
train acc:  0.875
train loss:  0.2675783038139343
train gradient:  0.11842200341173714
iteration : 11508
train acc:  0.875
train loss:  0.2996528148651123
train gradient:  0.13467053221144165
iteration : 11509
train acc:  0.8984375
train loss:  0.264789342880249
train gradient:  0.16880625145986794
iteration : 11510
train acc:  0.84375
train loss:  0.33406320214271545
train gradient:  0.21482497639097178
iteration : 11511
train acc:  0.875
train loss:  0.3296610116958618
train gradient:  0.24018794717294462
iteration : 11512
train acc:  0.875
train loss:  0.3152751922607422
train gradient:  0.14788074013124825
iteration : 11513
train acc:  0.8671875
train loss:  0.32096123695373535
train gradient:  0.13837710580622442
iteration : 11514
train acc:  0.84375
train loss:  0.32399728894233704
train gradient:  0.15471346906936267
iteration : 11515
train acc:  0.8203125
train loss:  0.3642207086086273
train gradient:  0.16185914417195904
iteration : 11516
train acc:  0.8828125
train loss:  0.3339745104312897
train gradient:  0.14865551799593416
iteration : 11517
train acc:  0.7734375
train loss:  0.49090132117271423
train gradient:  0.2982859504281766
iteration : 11518
train acc:  0.8515625
train loss:  0.3630290627479553
train gradient:  0.15212636400510499
iteration : 11519
train acc:  0.8515625
train loss:  0.3688393831253052
train gradient:  0.20135154767702157
iteration : 11520
train acc:  0.890625
train loss:  0.27981841564178467
train gradient:  0.10166000927770516
iteration : 11521
train acc:  0.8828125
train loss:  0.27487027645111084
train gradient:  0.10829678424998498
iteration : 11522
train acc:  0.890625
train loss:  0.3360997438430786
train gradient:  0.1138995335113933
iteration : 11523
train acc:  0.84375
train loss:  0.3186591863632202
train gradient:  0.15262801426314543
iteration : 11524
train acc:  0.90625
train loss:  0.24406880140304565
train gradient:  0.09381930727075763
iteration : 11525
train acc:  0.8984375
train loss:  0.2754967212677002
train gradient:  0.11845049480498573
iteration : 11526
train acc:  0.8203125
train loss:  0.3314402103424072
train gradient:  0.1538565561764071
iteration : 11527
train acc:  0.8984375
train loss:  0.26559776067733765
train gradient:  0.11057392883501213
iteration : 11528
train acc:  0.8828125
train loss:  0.29692983627319336
train gradient:  0.1378227526629358
iteration : 11529
train acc:  0.90625
train loss:  0.25699442625045776
train gradient:  0.10675461260153815
iteration : 11530
train acc:  0.875
train loss:  0.37115398049354553
train gradient:  0.1413378640479979
iteration : 11531
train acc:  0.8828125
train loss:  0.2698914408683777
train gradient:  0.1294873810123804
iteration : 11532
train acc:  0.8515625
train loss:  0.3367322087287903
train gradient:  0.17040491856863538
iteration : 11533
train acc:  0.859375
train loss:  0.30437248945236206
train gradient:  0.1491482106555258
iteration : 11534
train acc:  0.8515625
train loss:  0.31697118282318115
train gradient:  0.19267365296679403
iteration : 11535
train acc:  0.8125
train loss:  0.41370272636413574
train gradient:  0.2490867954819468
iteration : 11536
train acc:  0.890625
train loss:  0.27114108204841614
train gradient:  0.10999165668527729
iteration : 11537
train acc:  0.8203125
train loss:  0.3452874422073364
train gradient:  0.13003422697319536
iteration : 11538
train acc:  0.90625
train loss:  0.2714391052722931
train gradient:  0.14922369791543422
iteration : 11539
train acc:  0.8359375
train loss:  0.31081223487854004
train gradient:  0.12140441250501868
iteration : 11540
train acc:  0.890625
train loss:  0.31393963098526
train gradient:  0.1584078140821379
iteration : 11541
train acc:  0.890625
train loss:  0.28939807415008545
train gradient:  0.10703747025594165
iteration : 11542
train acc:  0.8671875
train loss:  0.31596851348876953
train gradient:  0.13236243742041365
iteration : 11543
train acc:  0.8515625
train loss:  0.35959386825561523
train gradient:  0.13719620335967778
iteration : 11544
train acc:  0.84375
train loss:  0.38454604148864746
train gradient:  0.16387441879761216
iteration : 11545
train acc:  0.8828125
train loss:  0.2744906544685364
train gradient:  0.11334417870205825
iteration : 11546
train acc:  0.8984375
train loss:  0.29645347595214844
train gradient:  0.1957669495927722
iteration : 11547
train acc:  0.8671875
train loss:  0.3202756643295288
train gradient:  0.1481428911753741
iteration : 11548
train acc:  0.90625
train loss:  0.3200889527797699
train gradient:  0.21753799722536246
iteration : 11549
train acc:  0.8515625
train loss:  0.35026535391807556
train gradient:  0.20852648628314205
iteration : 11550
train acc:  0.84375
train loss:  0.3427966237068176
train gradient:  0.13672025521392792
iteration : 11551
train acc:  0.8046875
train loss:  0.32824742794036865
train gradient:  0.17136885752477105
iteration : 11552
train acc:  0.828125
train loss:  0.34619638323783875
train gradient:  0.1880779453830313
iteration : 11553
train acc:  0.8203125
train loss:  0.4492989182472229
train gradient:  0.2657182195589147
iteration : 11554
train acc:  0.84375
train loss:  0.30695945024490356
train gradient:  0.18926248578998395
iteration : 11555
train acc:  0.90625
train loss:  0.2917064428329468
train gradient:  0.12558496193250218
iteration : 11556
train acc:  0.90625
train loss:  0.295081228017807
train gradient:  0.10753242980284286
iteration : 11557
train acc:  0.8515625
train loss:  0.28623902797698975
train gradient:  0.11416524986252452
iteration : 11558
train acc:  0.8359375
train loss:  0.34494438767433167
train gradient:  0.1689893165838256
iteration : 11559
train acc:  0.84375
train loss:  0.3249957859516144
train gradient:  0.1829310752443207
iteration : 11560
train acc:  0.875
train loss:  0.31366002559661865
train gradient:  0.12744800316159344
iteration : 11561
train acc:  0.796875
train loss:  0.4581958055496216
train gradient:  0.2697647595976599
iteration : 11562
train acc:  0.8984375
train loss:  0.28241169452667236
train gradient:  0.12660137037370034
iteration : 11563
train acc:  0.8828125
train loss:  0.3276309370994568
train gradient:  0.15205860313814198
iteration : 11564
train acc:  0.8671875
train loss:  0.35092252492904663
train gradient:  0.131421317157244
iteration : 11565
train acc:  0.859375
train loss:  0.35374242067337036
train gradient:  0.22130358592626403
iteration : 11566
train acc:  0.90625
train loss:  0.24035806953907013
train gradient:  0.07788962359921191
iteration : 11567
train acc:  0.84375
train loss:  0.3108086585998535
train gradient:  0.1411296204278617
iteration : 11568
train acc:  0.828125
train loss:  0.4038274884223938
train gradient:  0.2752670730440111
iteration : 11569
train acc:  0.890625
train loss:  0.2587701380252838
train gradient:  0.1248721111260801
iteration : 11570
train acc:  0.859375
train loss:  0.319964200258255
train gradient:  0.2540782877773321
iteration : 11571
train acc:  0.8046875
train loss:  0.41895124316215515
train gradient:  0.24825319069408916
iteration : 11572
train acc:  0.78125
train loss:  0.428058385848999
train gradient:  0.26937865451348936
iteration : 11573
train acc:  0.8203125
train loss:  0.3507462739944458
train gradient:  0.20609862730745948
iteration : 11574
train acc:  0.8359375
train loss:  0.3133545219898224
train gradient:  0.11448005243494307
iteration : 11575
train acc:  0.875
train loss:  0.2918281555175781
train gradient:  0.09948800570197548
iteration : 11576
train acc:  0.859375
train loss:  0.33499252796173096
train gradient:  0.1600878207444301
iteration : 11577
train acc:  0.90625
train loss:  0.2669934034347534
train gradient:  0.09215862639507108
iteration : 11578
train acc:  0.859375
train loss:  0.33604511618614197
train gradient:  0.16308153407979187
iteration : 11579
train acc:  0.8203125
train loss:  0.33201998472213745
train gradient:  0.11411753777077481
iteration : 11580
train acc:  0.8828125
train loss:  0.3048056960105896
train gradient:  0.12950420289831668
iteration : 11581
train acc:  0.8828125
train loss:  0.3040997385978699
train gradient:  0.10987982411409376
iteration : 11582
train acc:  0.8984375
train loss:  0.2568032741546631
train gradient:  0.13462484762971916
iteration : 11583
train acc:  0.9140625
train loss:  0.2641114592552185
train gradient:  0.1073454396714731
iteration : 11584
train acc:  0.90625
train loss:  0.23090887069702148
train gradient:  0.08756930884394343
iteration : 11585
train acc:  0.8359375
train loss:  0.3560991883277893
train gradient:  0.18061532239904066
iteration : 11586
train acc:  0.8203125
train loss:  0.395232617855072
train gradient:  0.16734058314750366
iteration : 11587
train acc:  0.828125
train loss:  0.3699174225330353
train gradient:  0.15218388887004758
iteration : 11588
train acc:  0.8828125
train loss:  0.3382876515388489
train gradient:  0.18687287649040826
iteration : 11589
train acc:  0.8359375
train loss:  0.3576061427593231
train gradient:  0.17233854562974626
iteration : 11590
train acc:  0.875
train loss:  0.31744351983070374
train gradient:  0.1071240150932914
iteration : 11591
train acc:  0.8828125
train loss:  0.3032901883125305
train gradient:  0.12822914902744542
iteration : 11592
train acc:  0.8046875
train loss:  0.36707016825675964
train gradient:  0.19302137911006786
iteration : 11593
train acc:  0.8671875
train loss:  0.31369397044181824
train gradient:  0.15415185860553166
iteration : 11594
train acc:  0.8984375
train loss:  0.26402926445007324
train gradient:  0.09571370298927033
iteration : 11595
train acc:  0.8359375
train loss:  0.360170841217041
train gradient:  0.19084806677961325
iteration : 11596
train acc:  0.875
train loss:  0.31990015506744385
train gradient:  0.16555985396232603
iteration : 11597
train acc:  0.890625
train loss:  0.33171236515045166
train gradient:  0.1650544577385421
iteration : 11598
train acc:  0.8203125
train loss:  0.3664284944534302
train gradient:  0.15035460167516101
iteration : 11599
train acc:  0.890625
train loss:  0.2845267057418823
train gradient:  0.2077884667783398
iteration : 11600
train acc:  0.8203125
train loss:  0.43181073665618896
train gradient:  0.23036425208297268
iteration : 11601
train acc:  0.84375
train loss:  0.38092443346977234
train gradient:  0.21410919815058452
iteration : 11602
train acc:  0.8671875
train loss:  0.2919330596923828
train gradient:  0.1162127321024437
iteration : 11603
train acc:  0.90625
train loss:  0.26189476251602173
train gradient:  0.1412888182391291
iteration : 11604
train acc:  0.875
train loss:  0.3018673062324524
train gradient:  0.11168153072265581
iteration : 11605
train acc:  0.875
train loss:  0.34046727418899536
train gradient:  0.2142161741104727
iteration : 11606
train acc:  0.890625
train loss:  0.28166162967681885
train gradient:  0.10357640469862016
iteration : 11607
train acc:  0.875
train loss:  0.2821260690689087
train gradient:  0.10493590254398552
iteration : 11608
train acc:  0.84375
train loss:  0.31761622428894043
train gradient:  0.11942793013935034
iteration : 11609
train acc:  0.890625
train loss:  0.30972087383270264
train gradient:  0.1349899691813039
iteration : 11610
train acc:  0.8828125
train loss:  0.3354230523109436
train gradient:  0.1115366988548449
iteration : 11611
train acc:  0.8828125
train loss:  0.3008243143558502
train gradient:  0.15709723651147978
iteration : 11612
train acc:  0.9296875
train loss:  0.2489294409751892
train gradient:  0.11783033960094749
iteration : 11613
train acc:  0.8359375
train loss:  0.36625874042510986
train gradient:  0.16019164934968205
iteration : 11614
train acc:  0.890625
train loss:  0.2897034287452698
train gradient:  0.12788810795306751
iteration : 11615
train acc:  0.8671875
train loss:  0.2967894673347473
train gradient:  0.1670295157368133
iteration : 11616
train acc:  0.8671875
train loss:  0.3425336480140686
train gradient:  0.1161209488042049
iteration : 11617
train acc:  0.8828125
train loss:  0.32563015818595886
train gradient:  0.17298153636353247
iteration : 11618
train acc:  0.859375
train loss:  0.271564245223999
train gradient:  0.11065424907681021
iteration : 11619
train acc:  0.8828125
train loss:  0.2587030529975891
train gradient:  0.1225226414025372
iteration : 11620
train acc:  0.875
train loss:  0.3810812830924988
train gradient:  0.20918128630248578
iteration : 11621
train acc:  0.8515625
train loss:  0.3107379674911499
train gradient:  0.14443339286502468
iteration : 11622
train acc:  0.8046875
train loss:  0.5245358943939209
train gradient:  0.37944801321473753
iteration : 11623
train acc:  0.875
train loss:  0.37085193395614624
train gradient:  0.24732236675025593
iteration : 11624
train acc:  0.8828125
train loss:  0.2934902608394623
train gradient:  0.2858780306744327
iteration : 11625
train acc:  0.859375
train loss:  0.284198522567749
train gradient:  0.15498248199446155
iteration : 11626
train acc:  0.875
train loss:  0.27000296115875244
train gradient:  0.13005779192318118
iteration : 11627
train acc:  0.8046875
train loss:  0.38725873827934265
train gradient:  0.18939776802771974
iteration : 11628
train acc:  0.8984375
train loss:  0.24696466326713562
train gradient:  0.11053069476544089
iteration : 11629
train acc:  0.8515625
train loss:  0.2919754981994629
train gradient:  0.16923990044627474
iteration : 11630
train acc:  0.90625
train loss:  0.29336339235305786
train gradient:  0.13843455505238444
iteration : 11631
train acc:  0.8203125
train loss:  0.3990893065929413
train gradient:  0.28791163246232543
iteration : 11632
train acc:  0.8828125
train loss:  0.30000951886177063
train gradient:  0.12261199604179635
iteration : 11633
train acc:  0.8359375
train loss:  0.3343820571899414
train gradient:  0.13170535966323288
iteration : 11634
train acc:  0.8515625
train loss:  0.3525961637496948
train gradient:  0.12631381171506972
iteration : 11635
train acc:  0.8828125
train loss:  0.3053915202617645
train gradient:  0.14283410445120526
iteration : 11636
train acc:  0.8125
train loss:  0.3851664364337921
train gradient:  0.20587241942773052
iteration : 11637
train acc:  0.859375
train loss:  0.3739456534385681
train gradient:  0.2128733685810186
iteration : 11638
train acc:  0.84375
train loss:  0.36878305673599243
train gradient:  0.20550957949950177
iteration : 11639
train acc:  0.8671875
train loss:  0.35964056849479675
train gradient:  0.16338270253860804
iteration : 11640
train acc:  0.90625
train loss:  0.2385852038860321
train gradient:  0.16284308009598067
iteration : 11641
train acc:  0.875
train loss:  0.3104870319366455
train gradient:  0.15289610404831344
iteration : 11642
train acc:  0.921875
train loss:  0.26724565029144287
train gradient:  0.1036395241772381
iteration : 11643
train acc:  0.859375
train loss:  0.3644692897796631
train gradient:  0.3298577576700948
iteration : 11644
train acc:  0.8515625
train loss:  0.34539294242858887
train gradient:  0.16423080101599255
iteration : 11645
train acc:  0.828125
train loss:  0.42959073185920715
train gradient:  0.2883457616226831
iteration : 11646
train acc:  0.828125
train loss:  0.3597007691860199
train gradient:  0.17177336970543278
iteration : 11647
train acc:  0.828125
train loss:  0.35025104880332947
train gradient:  0.15516869023257207
iteration : 11648
train acc:  0.8515625
train loss:  0.3668372631072998
train gradient:  0.20465492290630713
iteration : 11649
train acc:  0.90625
train loss:  0.27737584710121155
train gradient:  0.0818282228719373
iteration : 11650
train acc:  0.8359375
train loss:  0.3438720703125
train gradient:  0.14537948091655972
iteration : 11651
train acc:  0.859375
train loss:  0.30722740292549133
train gradient:  0.12434190555065022
iteration : 11652
train acc:  0.9140625
train loss:  0.2365233600139618
train gradient:  0.1107416537735879
iteration : 11653
train acc:  0.84375
train loss:  0.37330320477485657
train gradient:  0.1523970348606376
iteration : 11654
train acc:  0.8203125
train loss:  0.37588363885879517
train gradient:  0.2089163488775646
iteration : 11655
train acc:  0.8125
train loss:  0.40245240926742554
train gradient:  0.19339901399628073
iteration : 11656
train acc:  0.8984375
train loss:  0.295396625995636
train gradient:  0.12730484953538596
iteration : 11657
train acc:  0.8671875
train loss:  0.2943466901779175
train gradient:  0.14832677595227792
iteration : 11658
train acc:  0.8203125
train loss:  0.3937758207321167
train gradient:  0.2141548648798095
iteration : 11659
train acc:  0.875
train loss:  0.30831849575042725
train gradient:  0.14184523265613203
iteration : 11660
train acc:  0.859375
train loss:  0.38352012634277344
train gradient:  0.17324715893150927
iteration : 11661
train acc:  0.8515625
train loss:  0.35778093338012695
train gradient:  0.15393215424108594
iteration : 11662
train acc:  0.84375
train loss:  0.30994468927383423
train gradient:  0.1592815199600999
iteration : 11663
train acc:  0.890625
train loss:  0.2882175147533417
train gradient:  0.1350778695175649
iteration : 11664
train acc:  0.84375
train loss:  0.3285953998565674
train gradient:  0.19416298232282106
iteration : 11665
train acc:  0.8359375
train loss:  0.297349750995636
train gradient:  0.1261337142825656
iteration : 11666
train acc:  0.8828125
train loss:  0.23955292999744415
train gradient:  0.08251257224418297
iteration : 11667
train acc:  0.7890625
train loss:  0.38488394021987915
train gradient:  0.19649871619548204
iteration : 11668
train acc:  0.859375
train loss:  0.35215482115745544
train gradient:  0.1619766153986721
iteration : 11669
train acc:  0.90625
train loss:  0.32563555240631104
train gradient:  0.11251485406635521
iteration : 11670
train acc:  0.828125
train loss:  0.32901251316070557
train gradient:  0.19083652810209034
iteration : 11671
train acc:  0.8203125
train loss:  0.42886883020401
train gradient:  0.29442657746580253
iteration : 11672
train acc:  0.90625
train loss:  0.25764793157577515
train gradient:  0.08646856782208305
iteration : 11673
train acc:  0.890625
train loss:  0.2730194926261902
train gradient:  0.09002736137890446
iteration : 11674
train acc:  0.828125
train loss:  0.3564344644546509
train gradient:  0.1989963095215627
iteration : 11675
train acc:  0.8671875
train loss:  0.3157283067703247
train gradient:  0.13607815430852627
iteration : 11676
train acc:  0.8359375
train loss:  0.37492942810058594
train gradient:  0.18153424836202015
iteration : 11677
train acc:  0.875
train loss:  0.2774050235748291
train gradient:  0.1427832147492126
iteration : 11678
train acc:  0.8671875
train loss:  0.3403856158256531
train gradient:  0.12612512851552304
iteration : 11679
train acc:  0.8046875
train loss:  0.457730770111084
train gradient:  0.288950258644797
iteration : 11680
train acc:  0.84375
train loss:  0.3540404140949249
train gradient:  0.13940971181173278
iteration : 11681
train acc:  0.875
train loss:  0.2438785284757614
train gradient:  0.09327691537929435
iteration : 11682
train acc:  0.8359375
train loss:  0.3834153711795807
train gradient:  0.23289533713516397
iteration : 11683
train acc:  0.828125
train loss:  0.30655109882354736
train gradient:  0.12512010712694482
iteration : 11684
train acc:  0.8828125
train loss:  0.3145838975906372
train gradient:  0.12967121147147825
iteration : 11685
train acc:  0.84375
train loss:  0.30823180079460144
train gradient:  0.13458007673447206
iteration : 11686
train acc:  0.890625
train loss:  0.3041675090789795
train gradient:  0.10879758517135549
iteration : 11687
train acc:  0.828125
train loss:  0.34963321685791016
train gradient:  0.1252629275678267
iteration : 11688
train acc:  0.890625
train loss:  0.26851654052734375
train gradient:  0.09898653261131961
iteration : 11689
train acc:  0.8671875
train loss:  0.31414076685905457
train gradient:  0.27744486057707096
iteration : 11690
train acc:  0.859375
train loss:  0.3033181428909302
train gradient:  0.16488332017070115
iteration : 11691
train acc:  0.875
train loss:  0.2931455969810486
train gradient:  0.1218656121102964
iteration : 11692
train acc:  0.875
train loss:  0.3237364590167999
train gradient:  0.1844485315655443
iteration : 11693
train acc:  0.890625
train loss:  0.27337998151779175
train gradient:  0.08726156886211167
iteration : 11694
train acc:  0.8828125
train loss:  0.27850106358528137
train gradient:  0.10618140312228949
iteration : 11695
train acc:  0.8125
train loss:  0.4463376998901367
train gradient:  0.28281966585047563
iteration : 11696
train acc:  0.8671875
train loss:  0.2934996485710144
train gradient:  0.20187700051144522
iteration : 11697
train acc:  0.875
train loss:  0.2957838773727417
train gradient:  0.1103014088340807
iteration : 11698
train acc:  0.8359375
train loss:  0.337133526802063
train gradient:  0.24886643232115002
iteration : 11699
train acc:  0.8828125
train loss:  0.2865865230560303
train gradient:  0.0933422244749036
iteration : 11700
train acc:  0.90625
train loss:  0.2745800316333771
train gradient:  0.14753019699236591
iteration : 11701
train acc:  0.8828125
train loss:  0.27324870228767395
train gradient:  0.13331147581858885
iteration : 11702
train acc:  0.8125
train loss:  0.34569811820983887
train gradient:  0.18428366312678204
iteration : 11703
train acc:  0.875
train loss:  0.30440449714660645
train gradient:  0.11732494832898219
iteration : 11704
train acc:  0.84375
train loss:  0.3484603464603424
train gradient:  0.16880587808411207
iteration : 11705
train acc:  0.875
train loss:  0.30548691749572754
train gradient:  0.12832929564355064
iteration : 11706
train acc:  0.8984375
train loss:  0.2833729386329651
train gradient:  0.10814888079659364
iteration : 11707
train acc:  0.84375
train loss:  0.3786006569862366
train gradient:  0.15824370186544717
iteration : 11708
train acc:  0.8515625
train loss:  0.26681947708129883
train gradient:  0.12642957754570622
iteration : 11709
train acc:  0.8515625
train loss:  0.4189152717590332
train gradient:  0.22510447106464698
iteration : 11710
train acc:  0.8984375
train loss:  0.2716059684753418
train gradient:  0.12300932767788224
iteration : 11711
train acc:  0.875
train loss:  0.3961363434791565
train gradient:  0.1828843527238958
iteration : 11712
train acc:  0.890625
train loss:  0.2616783082485199
train gradient:  0.11197281018413377
iteration : 11713
train acc:  0.828125
train loss:  0.4385949671268463
train gradient:  0.23607662258695183
iteration : 11714
train acc:  0.8515625
train loss:  0.3267115354537964
train gradient:  0.14128236656407606
iteration : 11715
train acc:  0.84375
train loss:  0.374914288520813
train gradient:  0.25366331483159454
iteration : 11716
train acc:  0.9296875
train loss:  0.24618053436279297
train gradient:  0.13597452312187633
iteration : 11717
train acc:  0.8359375
train loss:  0.32881075143814087
train gradient:  0.13836999750364695
iteration : 11718
train acc:  0.8125
train loss:  0.40899819135665894
train gradient:  0.19941308017568532
iteration : 11719
train acc:  0.8671875
train loss:  0.3374611437320709
train gradient:  0.10282900527604
iteration : 11720
train acc:  0.890625
train loss:  0.2969261407852173
train gradient:  0.16637098217385543
iteration : 11721
train acc:  0.90625
train loss:  0.2732704281806946
train gradient:  0.1389044987549361
iteration : 11722
train acc:  0.859375
train loss:  0.327972412109375
train gradient:  0.11112851356599686
iteration : 11723
train acc:  0.890625
train loss:  0.27771446108818054
train gradient:  0.12447793444290184
iteration : 11724
train acc:  0.8046875
train loss:  0.3762914836406708
train gradient:  0.20569972757222282
iteration : 11725
train acc:  0.8359375
train loss:  0.41272327303886414
train gradient:  0.17033086865989228
iteration : 11726
train acc:  0.90625
train loss:  0.21789458394050598
train gradient:  0.08889473383992161
iteration : 11727
train acc:  0.8359375
train loss:  0.37080296874046326
train gradient:  0.1837915760190172
iteration : 11728
train acc:  0.8515625
train loss:  0.3129885494709015
train gradient:  0.15123660835427577
iteration : 11729
train acc:  0.90625
train loss:  0.24560604989528656
train gradient:  0.12498992690312681
iteration : 11730
train acc:  0.8984375
train loss:  0.27708685398101807
train gradient:  0.12973410983156586
iteration : 11731
train acc:  0.8671875
train loss:  0.3336111307144165
train gradient:  0.13263768955191196
iteration : 11732
train acc:  0.828125
train loss:  0.36359208822250366
train gradient:  0.12304214066193322
iteration : 11733
train acc:  0.8671875
train loss:  0.3156431317329407
train gradient:  0.1371377859845881
iteration : 11734
train acc:  0.875
train loss:  0.3352126479148865
train gradient:  0.16453937570369542
iteration : 11735
train acc:  0.90625
train loss:  0.22250358760356903
train gradient:  0.09147020333452996
iteration : 11736
train acc:  0.8671875
train loss:  0.3271535634994507
train gradient:  0.15647583226921546
iteration : 11737
train acc:  0.8046875
train loss:  0.373163640499115
train gradient:  0.20582801290546252
iteration : 11738
train acc:  0.8359375
train loss:  0.3302256762981415
train gradient:  0.1692998822365898
iteration : 11739
train acc:  0.8828125
train loss:  0.2886424958705902
train gradient:  0.15542415314869426
iteration : 11740
train acc:  0.8984375
train loss:  0.27235233783721924
train gradient:  0.08320311546911105
iteration : 11741
train acc:  0.9140625
train loss:  0.22622039914131165
train gradient:  0.07613818803219204
iteration : 11742
train acc:  0.8984375
train loss:  0.2647748291492462
train gradient:  0.10178046274996924
iteration : 11743
train acc:  0.8984375
train loss:  0.31064045429229736
train gradient:  0.12240566801089
iteration : 11744
train acc:  0.8515625
train loss:  0.3559551239013672
train gradient:  0.13435354338076003
iteration : 11745
train acc:  0.875
train loss:  0.30603575706481934
train gradient:  0.1295936687100202
iteration : 11746
train acc:  0.875
train loss:  0.3070840835571289
train gradient:  0.10571260277772636
iteration : 11747
train acc:  0.8203125
train loss:  0.37406599521636963
train gradient:  0.21920045911973948
iteration : 11748
train acc:  0.859375
train loss:  0.29714900255203247
train gradient:  0.1282020259152941
iteration : 11749
train acc:  0.8828125
train loss:  0.2860192656517029
train gradient:  0.1430285270994391
iteration : 11750
train acc:  0.8671875
train loss:  0.32133638858795166
train gradient:  0.1355194122655517
iteration : 11751
train acc:  0.8515625
train loss:  0.2921197712421417
train gradient:  0.1385530308911873
iteration : 11752
train acc:  0.859375
train loss:  0.327219694852829
train gradient:  0.13213273233797718
iteration : 11753
train acc:  0.859375
train loss:  0.2919961214065552
train gradient:  0.1403114531646555
iteration : 11754
train acc:  0.8359375
train loss:  0.330402672290802
train gradient:  0.16235311215710985
iteration : 11755
train acc:  0.90625
train loss:  0.2473941147327423
train gradient:  0.10309822152945272
iteration : 11756
train acc:  0.875
train loss:  0.2777676582336426
train gradient:  0.13530196670348313
iteration : 11757
train acc:  0.890625
train loss:  0.2619996964931488
train gradient:  0.1363600865326267
iteration : 11758
train acc:  0.796875
train loss:  0.42517930269241333
train gradient:  0.19835356497274464
iteration : 11759
train acc:  0.84375
train loss:  0.41262781620025635
train gradient:  0.16562789596171207
iteration : 11760
train acc:  0.8671875
train loss:  0.27072739601135254
train gradient:  0.1464033364420631
iteration : 11761
train acc:  0.90625
train loss:  0.3019820749759674
train gradient:  0.15408323644861693
iteration : 11762
train acc:  0.8671875
train loss:  0.2873895764350891
train gradient:  0.2258641673484113
iteration : 11763
train acc:  0.8671875
train loss:  0.2978663146495819
train gradient:  0.12260172357083379
iteration : 11764
train acc:  0.8046875
train loss:  0.37026453018188477
train gradient:  0.1951813463177656
iteration : 11765
train acc:  0.8203125
train loss:  0.3716399073600769
train gradient:  0.2939790580817704
iteration : 11766
train acc:  0.890625
train loss:  0.3259255290031433
train gradient:  0.1149819709410761
iteration : 11767
train acc:  0.828125
train loss:  0.3039776086807251
train gradient:  0.14463048050773908
iteration : 11768
train acc:  0.875
train loss:  0.32378220558166504
train gradient:  0.1511503522825231
iteration : 11769
train acc:  0.875
train loss:  0.28049641847610474
train gradient:  0.17382447571138393
iteration : 11770
train acc:  0.875
train loss:  0.3322928547859192
train gradient:  0.23104564357609375
iteration : 11771
train acc:  0.8359375
train loss:  0.2948181927204132
train gradient:  0.14346182337110652
iteration : 11772
train acc:  0.890625
train loss:  0.30148470401763916
train gradient:  0.13964117380212204
iteration : 11773
train acc:  0.8203125
train loss:  0.3980020582675934
train gradient:  0.22880015818784377
iteration : 11774
train acc:  0.875
train loss:  0.28606659173965454
train gradient:  0.13906875403605534
iteration : 11775
train acc:  0.8671875
train loss:  0.295158714056015
train gradient:  0.10940075746829143
iteration : 11776
train acc:  0.8203125
train loss:  0.3749335706233978
train gradient:  0.2325616101562964
iteration : 11777
train acc:  0.8515625
train loss:  0.3274465799331665
train gradient:  0.17159118672517343
iteration : 11778
train acc:  0.8515625
train loss:  0.29383623600006104
train gradient:  0.16832804708957583
iteration : 11779
train acc:  0.890625
train loss:  0.30446746945381165
train gradient:  0.12690880033349078
iteration : 11780
train acc:  0.828125
train loss:  0.4354865849018097
train gradient:  0.2784980785152635
iteration : 11781
train acc:  0.875
train loss:  0.3487379848957062
train gradient:  0.17199615603558865
iteration : 11782
train acc:  0.8359375
train loss:  0.3816656470298767
train gradient:  0.20796793105481393
iteration : 11783
train acc:  0.921875
train loss:  0.2779390215873718
train gradient:  0.08931152696961979
iteration : 11784
train acc:  0.84375
train loss:  0.32463109493255615
train gradient:  0.17226720415315747
iteration : 11785
train acc:  0.9296875
train loss:  0.27310001850128174
train gradient:  0.13179082674004872
iteration : 11786
train acc:  0.875
train loss:  0.3575737178325653
train gradient:  0.18273681505893272
iteration : 11787
train acc:  0.8671875
train loss:  0.2942037582397461
train gradient:  0.10839065466687596
iteration : 11788
train acc:  0.84375
train loss:  0.30277466773986816
train gradient:  0.12840716707773303
iteration : 11789
train acc:  0.8671875
train loss:  0.2900483310222626
train gradient:  0.12725053141851173
iteration : 11790
train acc:  0.8671875
train loss:  0.34755414724349976
train gradient:  0.1710939536107653
iteration : 11791
train acc:  0.8203125
train loss:  0.42510032653808594
train gradient:  0.17557398939838642
iteration : 11792
train acc:  0.8515625
train loss:  0.30711987614631653
train gradient:  0.15105669914906947
iteration : 11793
train acc:  0.875
train loss:  0.28891849517822266
train gradient:  0.14489361619292834
iteration : 11794
train acc:  0.875
train loss:  0.30252403020858765
train gradient:  0.12444257111349433
iteration : 11795
train acc:  0.8671875
train loss:  0.3025984764099121
train gradient:  0.10739608197213449
iteration : 11796
train acc:  0.890625
train loss:  0.28125429153442383
train gradient:  0.14015748604861242
iteration : 11797
train acc:  0.9140625
train loss:  0.25665339827537537
train gradient:  0.09974301853904156
iteration : 11798
train acc:  0.875
train loss:  0.3035767078399658
train gradient:  0.11621152038918718
iteration : 11799
train acc:  0.8671875
train loss:  0.2751803994178772
train gradient:  0.09249203267773584
iteration : 11800
train acc:  0.8828125
train loss:  0.2704547345638275
train gradient:  0.1390255083856789
iteration : 11801
train acc:  0.859375
train loss:  0.29019296169281006
train gradient:  0.14606099338460632
iteration : 11802
train acc:  0.9296875
train loss:  0.2493177354335785
train gradient:  0.1457797060103701
iteration : 11803
train acc:  0.875
train loss:  0.2748633027076721
train gradient:  0.18439710890339672
iteration : 11804
train acc:  0.8671875
train loss:  0.29883161187171936
train gradient:  0.14920598259021262
iteration : 11805
train acc:  0.8359375
train loss:  0.33483701944351196
train gradient:  0.17566053829684156
iteration : 11806
train acc:  0.859375
train loss:  0.3369021415710449
train gradient:  0.16482040192447683
iteration : 11807
train acc:  0.859375
train loss:  0.2748241126537323
train gradient:  0.12239082567468719
iteration : 11808
train acc:  0.8984375
train loss:  0.23900488018989563
train gradient:  0.10576107641125995
iteration : 11809
train acc:  0.8671875
train loss:  0.328708291053772
train gradient:  0.13110294525886407
iteration : 11810
train acc:  0.890625
train loss:  0.3294762372970581
train gradient:  0.1677252740474644
iteration : 11811
train acc:  0.890625
train loss:  0.23519064486026764
train gradient:  0.14441495670082932
iteration : 11812
train acc:  0.859375
train loss:  0.3415381908416748
train gradient:  0.160776294932083
iteration : 11813
train acc:  0.8515625
train loss:  0.32386520504951477
train gradient:  0.18906583258823154
iteration : 11814
train acc:  0.8828125
train loss:  0.29200562834739685
train gradient:  0.14010572509219496
iteration : 11815
train acc:  0.84375
train loss:  0.31079381704330444
train gradient:  0.1512107876599266
iteration : 11816
train acc:  0.8125
train loss:  0.38872721791267395
train gradient:  0.22041948790613258
iteration : 11817
train acc:  0.859375
train loss:  0.34864604473114014
train gradient:  0.22786227665028236
iteration : 11818
train acc:  0.828125
train loss:  0.33280277252197266
train gradient:  0.1797328795334701
iteration : 11819
train acc:  0.7890625
train loss:  0.4236503541469574
train gradient:  0.3263007401702718
iteration : 11820
train acc:  0.859375
train loss:  0.3776467442512512
train gradient:  0.17049478745944854
iteration : 11821
train acc:  0.8671875
train loss:  0.28372812271118164
train gradient:  0.1214925774631026
iteration : 11822
train acc:  0.84375
train loss:  0.4180161952972412
train gradient:  0.3409705081317528
iteration : 11823
train acc:  0.8515625
train loss:  0.3083483874797821
train gradient:  0.16537373249140525
iteration : 11824
train acc:  0.8828125
train loss:  0.3206917345523834
train gradient:  0.13328234466756397
iteration : 11825
train acc:  0.8515625
train loss:  0.40129348635673523
train gradient:  0.22695283566699706
iteration : 11826
train acc:  0.84375
train loss:  0.2739637792110443
train gradient:  0.12464326741080027
iteration : 11827
train acc:  0.8515625
train loss:  0.34054186940193176
train gradient:  0.20233009512937616
iteration : 11828
train acc:  0.84375
train loss:  0.33236122131347656
train gradient:  0.1583820575447245
iteration : 11829
train acc:  0.8203125
train loss:  0.4063229560852051
train gradient:  0.2230798956187035
iteration : 11830
train acc:  0.921875
train loss:  0.19876012206077576
train gradient:  0.13097349491826155
iteration : 11831
train acc:  0.890625
train loss:  0.2615477442741394
train gradient:  0.11704568046098533
iteration : 11832
train acc:  0.8359375
train loss:  0.37223848700523376
train gradient:  0.2136006259058308
iteration : 11833
train acc:  0.84375
train loss:  0.37807172536849976
train gradient:  0.2554527373315792
iteration : 11834
train acc:  0.8359375
train loss:  0.42956069111824036
train gradient:  0.23305224538418806
iteration : 11835
train acc:  0.90625
train loss:  0.22775118052959442
train gradient:  0.08953640758996947
iteration : 11836
train acc:  0.8828125
train loss:  0.3060757517814636
train gradient:  0.14939356197373665
iteration : 11837
train acc:  0.890625
train loss:  0.24180641770362854
train gradient:  0.09438656462012411
iteration : 11838
train acc:  0.8828125
train loss:  0.3320544958114624
train gradient:  0.13636529169750405
iteration : 11839
train acc:  0.8828125
train loss:  0.27086907625198364
train gradient:  0.15678650637554742
iteration : 11840
train acc:  0.9296875
train loss:  0.24180103838443756
train gradient:  0.09373057422618766
iteration : 11841
train acc:  0.828125
train loss:  0.3215497136116028
train gradient:  0.13864014068909578
iteration : 11842
train acc:  0.828125
train loss:  0.38564521074295044
train gradient:  0.16286285513171442
iteration : 11843
train acc:  0.9140625
train loss:  0.2437502145767212
train gradient:  0.13261205589059843
iteration : 11844
train acc:  0.9296875
train loss:  0.23258675634860992
train gradient:  0.09105439685592792
iteration : 11845
train acc:  0.8515625
train loss:  0.3237122893333435
train gradient:  0.11859193775437678
iteration : 11846
train acc:  0.8828125
train loss:  0.24942006170749664
train gradient:  0.11664787329022656
iteration : 11847
train acc:  0.8671875
train loss:  0.2985961139202118
train gradient:  0.16602456857328085
iteration : 11848
train acc:  0.8359375
train loss:  0.3304314911365509
train gradient:  0.1654316327727176
iteration : 11849
train acc:  0.8125
train loss:  0.34445786476135254
train gradient:  0.20603556689851005
iteration : 11850
train acc:  0.8359375
train loss:  0.36096638441085815
train gradient:  0.20983075907626533
iteration : 11851
train acc:  0.8828125
train loss:  0.27197009325027466
train gradient:  0.16248759519610712
iteration : 11852
train acc:  0.859375
train loss:  0.30317285656929016
train gradient:  0.1608770423312298
iteration : 11853
train acc:  0.921875
train loss:  0.21604342758655548
train gradient:  0.12213018239485797
iteration : 11854
train acc:  0.8515625
train loss:  0.3529627323150635
train gradient:  0.1865867920694928
iteration : 11855
train acc:  0.8515625
train loss:  0.3562747836112976
train gradient:  0.17554786329263702
iteration : 11856
train acc:  0.90625
train loss:  0.2743818163871765
train gradient:  0.09393200895603016
iteration : 11857
train acc:  0.859375
train loss:  0.3519739806652069
train gradient:  0.21648292115330237
iteration : 11858
train acc:  0.84375
train loss:  0.38012686371803284
train gradient:  0.20406840533750295
iteration : 11859
train acc:  0.90625
train loss:  0.23361285030841827
train gradient:  0.09389262271115185
iteration : 11860
train acc:  0.8515625
train loss:  0.3248653709888458
train gradient:  0.1309770805769155
iteration : 11861
train acc:  0.875
train loss:  0.28444230556488037
train gradient:  0.13917415453050502
iteration : 11862
train acc:  0.859375
train loss:  0.34119388461112976
train gradient:  0.1565192689179486
iteration : 11863
train acc:  0.8203125
train loss:  0.3387448191642761
train gradient:  0.11435041612671613
iteration : 11864
train acc:  0.890625
train loss:  0.29352256655693054
train gradient:  0.13042194233971427
iteration : 11865
train acc:  0.8515625
train loss:  0.3090772032737732
train gradient:  0.1465085913731755
iteration : 11866
train acc:  0.8203125
train loss:  0.3892872929573059
train gradient:  0.18264321193668442
iteration : 11867
train acc:  0.953125
train loss:  0.20657607913017273
train gradient:  0.11482715718796262
iteration : 11868
train acc:  0.8515625
train loss:  0.3420290946960449
train gradient:  0.16913970262145014
iteration : 11869
train acc:  0.84375
train loss:  0.3460039496421814
train gradient:  0.14761377042067314
iteration : 11870
train acc:  0.90625
train loss:  0.23325452208518982
train gradient:  0.07496417197109002
iteration : 11871
train acc:  0.84375
train loss:  0.32239246368408203
train gradient:  0.20701638918876766
iteration : 11872
train acc:  0.8359375
train loss:  0.39425432682037354
train gradient:  0.20752187051285798
iteration : 11873
train acc:  0.859375
train loss:  0.3157528042793274
train gradient:  0.12891928113453754
iteration : 11874
train acc:  0.7890625
train loss:  0.45467036962509155
train gradient:  0.2958914783779955
iteration : 11875
train acc:  0.8515625
train loss:  0.3228182792663574
train gradient:  0.13304753495229124
iteration : 11876
train acc:  0.8671875
train loss:  0.3300676643848419
train gradient:  0.125594731768279
iteration : 11877
train acc:  0.875
train loss:  0.30271804332733154
train gradient:  0.14656002071144444
iteration : 11878
train acc:  0.859375
train loss:  0.3280714154243469
train gradient:  0.20911247669398098
iteration : 11879
train acc:  0.890625
train loss:  0.30604010820388794
train gradient:  0.1255177640173396
iteration : 11880
train acc:  0.8671875
train loss:  0.3066411018371582
train gradient:  0.10210416206830439
iteration : 11881
train acc:  0.90625
train loss:  0.27607470750808716
train gradient:  0.11671230172956577
iteration : 11882
train acc:  0.84375
train loss:  0.3478811979293823
train gradient:  0.1728740427634077
iteration : 11883
train acc:  0.8359375
train loss:  0.42968910932540894
train gradient:  0.2213595820809735
iteration : 11884
train acc:  0.8671875
train loss:  0.33646857738494873
train gradient:  0.1489292503968022
iteration : 11885
train acc:  0.84375
train loss:  0.37479710578918457
train gradient:  0.1907322641495195
iteration : 11886
train acc:  0.90625
train loss:  0.25527843832969666
train gradient:  0.11456972532976825
iteration : 11887
train acc:  0.90625
train loss:  0.24026301503181458
train gradient:  0.11584321339544454
iteration : 11888
train acc:  0.890625
train loss:  0.31193265318870544
train gradient:  0.15788982228165224
iteration : 11889
train acc:  0.9140625
train loss:  0.2508382201194763
train gradient:  0.13165134481763774
iteration : 11890
train acc:  0.8515625
train loss:  0.32860782742500305
train gradient:  0.13695803928274372
iteration : 11891
train acc:  0.8203125
train loss:  0.3396174907684326
train gradient:  0.16094259356748808
iteration : 11892
train acc:  0.8359375
train loss:  0.36318719387054443
train gradient:  0.20898104911288062
iteration : 11893
train acc:  0.828125
train loss:  0.3728696405887604
train gradient:  0.16226337918832975
iteration : 11894
train acc:  0.875
train loss:  0.2959812581539154
train gradient:  0.12808112194693227
iteration : 11895
train acc:  0.8359375
train loss:  0.3131956160068512
train gradient:  0.15061446445378085
iteration : 11896
train acc:  0.90625
train loss:  0.2643895149230957
train gradient:  0.13418122760078982
iteration : 11897
train acc:  0.8203125
train loss:  0.38415414094924927
train gradient:  0.1952182814257466
iteration : 11898
train acc:  0.8359375
train loss:  0.32699620723724365
train gradient:  0.127694038262528
iteration : 11899
train acc:  0.8359375
train loss:  0.3754100203514099
train gradient:  0.16878865030488505
iteration : 11900
train acc:  0.875
train loss:  0.2634362578392029
train gradient:  0.07773486717606976
iteration : 11901
train acc:  0.7734375
train loss:  0.39580821990966797
train gradient:  0.20712297158441934
iteration : 11902
train acc:  0.875
train loss:  0.32330191135406494
train gradient:  0.1341010973795668
iteration : 11903
train acc:  0.8359375
train loss:  0.35195112228393555
train gradient:  0.1879489656752737
iteration : 11904
train acc:  0.8515625
train loss:  0.32216450572013855
train gradient:  0.11933080096907972
iteration : 11905
train acc:  0.8671875
train loss:  0.25661975145339966
train gradient:  0.11500491824814883
iteration : 11906
train acc:  0.8515625
train loss:  0.39181840419769287
train gradient:  0.1764200506566922
iteration : 11907
train acc:  0.859375
train loss:  0.36917048692703247
train gradient:  0.21126650798061278
iteration : 11908
train acc:  0.828125
train loss:  0.3652746081352234
train gradient:  0.17661123356082645
iteration : 11909
train acc:  0.9453125
train loss:  0.23636537790298462
train gradient:  0.08790937691031994
iteration : 11910
train acc:  0.90625
train loss:  0.24295413494110107
train gradient:  0.08274783736891857
iteration : 11911
train acc:  0.8203125
train loss:  0.3942088186740875
train gradient:  0.26898881999916746
iteration : 11912
train acc:  0.9140625
train loss:  0.2789493799209595
train gradient:  0.1522661809968257
iteration : 11913
train acc:  0.859375
train loss:  0.306942343711853
train gradient:  0.18006042253075272
iteration : 11914
train acc:  0.8515625
train loss:  0.3450215756893158
train gradient:  0.17935978556850385
iteration : 11915
train acc:  0.828125
train loss:  0.31473278999328613
train gradient:  0.1577879927586084
iteration : 11916
train acc:  0.8515625
train loss:  0.3807699978351593
train gradient:  0.1476600953664985
iteration : 11917
train acc:  0.875
train loss:  0.29477056860923767
train gradient:  0.1506912017796605
iteration : 11918
train acc:  0.875
train loss:  0.3141292333602905
train gradient:  0.1651818086600665
iteration : 11919
train acc:  0.859375
train loss:  0.3944994807243347
train gradient:  0.2129371592684341
iteration : 11920
train acc:  0.8515625
train loss:  0.2783319354057312
train gradient:  0.13531408048288285
iteration : 11921
train acc:  0.921875
train loss:  0.22157186269760132
train gradient:  0.09158685715854492
iteration : 11922
train acc:  0.7890625
train loss:  0.43511509895324707
train gradient:  0.3130050464027305
iteration : 11923
train acc:  0.84375
train loss:  0.3793175220489502
train gradient:  0.16612078254801305
iteration : 11924
train acc:  0.8828125
train loss:  0.3131166100502014
train gradient:  0.15428302763650623
iteration : 11925
train acc:  0.9140625
train loss:  0.2722476124763489
train gradient:  0.16532840826248169
iteration : 11926
train acc:  0.8671875
train loss:  0.3324766755104065
train gradient:  0.17803143516668468
iteration : 11927
train acc:  0.875
train loss:  0.22827038168907166
train gradient:  0.08067442977685788
iteration : 11928
train acc:  0.859375
train loss:  0.37932077050209045
train gradient:  0.21107529371515946
iteration : 11929
train acc:  0.7890625
train loss:  0.39287999272346497
train gradient:  0.1919567574219863
iteration : 11930
train acc:  0.8515625
train loss:  0.3092469871044159
train gradient:  0.1563341532046678
iteration : 11931
train acc:  0.875
train loss:  0.29256671667099
train gradient:  0.12616734239594596
iteration : 11932
train acc:  0.8359375
train loss:  0.4214479923248291
train gradient:  0.2661658510303021
iteration : 11933
train acc:  0.859375
train loss:  0.37780505418777466
train gradient:  0.20758303750772833
iteration : 11934
train acc:  0.8671875
train loss:  0.3110619783401489
train gradient:  0.12873602162407422
iteration : 11935
train acc:  0.828125
train loss:  0.33498263359069824
train gradient:  0.1387712406077186
iteration : 11936
train acc:  0.8359375
train loss:  0.2945224344730377
train gradient:  0.10917168071034802
iteration : 11937
train acc:  0.890625
train loss:  0.3072386085987091
train gradient:  0.11732289828627823
iteration : 11938
train acc:  0.84375
train loss:  0.3061433732509613
train gradient:  0.1425728918482735
iteration : 11939
train acc:  0.8671875
train loss:  0.2796981930732727
train gradient:  0.11869652161373698
iteration : 11940
train acc:  0.84375
train loss:  0.2977483868598938
train gradient:  0.12225339327678
iteration : 11941
train acc:  0.8515625
train loss:  0.35152846574783325
train gradient:  0.17409095113189324
iteration : 11942
train acc:  0.8125
train loss:  0.3404155969619751
train gradient:  0.20621394717723152
iteration : 11943
train acc:  0.8515625
train loss:  0.3254641890525818
train gradient:  0.17059416416758622
iteration : 11944
train acc:  0.9140625
train loss:  0.2352411448955536
train gradient:  0.08011794025932474
iteration : 11945
train acc:  0.8671875
train loss:  0.3266255855560303
train gradient:  0.12549622224712018
iteration : 11946
train acc:  0.890625
train loss:  0.27255886793136597
train gradient:  0.11593473205240189
iteration : 11947
train acc:  0.890625
train loss:  0.3195856511592865
train gradient:  0.1973458143491375
iteration : 11948
train acc:  0.875
train loss:  0.34498289227485657
train gradient:  0.1433009689515754
iteration : 11949
train acc:  0.8046875
train loss:  0.34131723642349243
train gradient:  0.13058916803453136
iteration : 11950
train acc:  0.859375
train loss:  0.3331989645957947
train gradient:  0.12386429791670443
iteration : 11951
train acc:  0.8203125
train loss:  0.35864394903182983
train gradient:  0.19380793108437538
iteration : 11952
train acc:  0.859375
train loss:  0.28406715393066406
train gradient:  0.10023246767615562
iteration : 11953
train acc:  0.875
train loss:  0.312700092792511
train gradient:  0.08639224080796144
iteration : 11954
train acc:  0.8125
train loss:  0.3843807578086853
train gradient:  0.22163241571516046
iteration : 11955
train acc:  0.921875
train loss:  0.2932795584201813
train gradient:  0.16118179217177533
iteration : 11956
train acc:  0.8984375
train loss:  0.2358141541481018
train gradient:  0.08969429765846265
iteration : 11957
train acc:  0.859375
train loss:  0.3480095863342285
train gradient:  0.17865104661613995
iteration : 11958
train acc:  0.90625
train loss:  0.2505653500556946
train gradient:  0.11892855442661449
iteration : 11959
train acc:  0.8046875
train loss:  0.39694738388061523
train gradient:  0.2006540627445943
iteration : 11960
train acc:  0.8984375
train loss:  0.2908332347869873
train gradient:  0.14258228088042915
iteration : 11961
train acc:  0.8203125
train loss:  0.4331304728984833
train gradient:  0.21832097301998948
iteration : 11962
train acc:  0.890625
train loss:  0.3238617777824402
train gradient:  0.16577924333768213
iteration : 11963
train acc:  0.8203125
train loss:  0.3882078528404236
train gradient:  0.2386671670082613
iteration : 11964
train acc:  0.9375
train loss:  0.2178758680820465
train gradient:  0.16901985565799907
iteration : 11965
train acc:  0.90625
train loss:  0.2656838595867157
train gradient:  0.14474080956920216
iteration : 11966
train acc:  0.875
train loss:  0.31549152731895447
train gradient:  0.13094477575403266
iteration : 11967
train acc:  0.859375
train loss:  0.34638383984565735
train gradient:  0.10639366992216664
iteration : 11968
train acc:  0.84375
train loss:  0.3316569924354553
train gradient:  0.11797848011433021
iteration : 11969
train acc:  0.8984375
train loss:  0.2768104672431946
train gradient:  0.11351111269772077
iteration : 11970
train acc:  0.8359375
train loss:  0.34187138080596924
train gradient:  0.13810417868814495
iteration : 11971
train acc:  0.8671875
train loss:  0.33382025361061096
train gradient:  0.1284675900507395
iteration : 11972
train acc:  0.875
train loss:  0.3356057405471802
train gradient:  0.22024502822711328
iteration : 11973
train acc:  0.90625
train loss:  0.2509169578552246
train gradient:  0.07636504138140744
iteration : 11974
train acc:  0.8203125
train loss:  0.36764538288116455
train gradient:  0.1916641060421147
iteration : 11975
train acc:  0.859375
train loss:  0.2904701828956604
train gradient:  0.1481607603763207
iteration : 11976
train acc:  0.8671875
train loss:  0.3334946930408478
train gradient:  0.10453606456036245
iteration : 11977
train acc:  0.875
train loss:  0.3303237557411194
train gradient:  0.11016014191791956
iteration : 11978
train acc:  0.8671875
train loss:  0.2930700182914734
train gradient:  0.09576476397604308
iteration : 11979
train acc:  0.84375
train loss:  0.33119210600852966
train gradient:  0.1531318006097667
iteration : 11980
train acc:  0.828125
train loss:  0.3306421637535095
train gradient:  0.18886798593960558
iteration : 11981
train acc:  0.875
train loss:  0.2616614103317261
train gradient:  0.10368641419764565
iteration : 11982
train acc:  0.8671875
train loss:  0.32852160930633545
train gradient:  0.14377823213111135
iteration : 11983
train acc:  0.7890625
train loss:  0.4019707441329956
train gradient:  0.15871035978008963
iteration : 11984
train acc:  0.8203125
train loss:  0.3651893734931946
train gradient:  0.1358841960282953
iteration : 11985
train acc:  0.8828125
train loss:  0.29277467727661133
train gradient:  0.09296145880502833
iteration : 11986
train acc:  0.8203125
train loss:  0.36138224601745605
train gradient:  0.16037339703975545
iteration : 11987
train acc:  0.8359375
train loss:  0.3275798261165619
train gradient:  0.11206581384086464
iteration : 11988
train acc:  0.859375
train loss:  0.30738508701324463
train gradient:  0.1066088360527849
iteration : 11989
train acc:  0.8984375
train loss:  0.30331921577453613
train gradient:  0.14429499662716236
iteration : 11990
train acc:  0.8671875
train loss:  0.33920031785964966
train gradient:  0.19880761950670078
iteration : 11991
train acc:  0.8671875
train loss:  0.29060789942741394
train gradient:  0.10301609972146611
iteration : 11992
train acc:  0.8359375
train loss:  0.29292622208595276
train gradient:  0.15643211281826827
iteration : 11993
train acc:  0.8671875
train loss:  0.3302966356277466
train gradient:  0.14259883210681287
iteration : 11994
train acc:  0.890625
train loss:  0.34435731172561646
train gradient:  0.2543670306708262
iteration : 11995
train acc:  0.875
train loss:  0.32494986057281494
train gradient:  0.12825843403372328
iteration : 11996
train acc:  0.8125
train loss:  0.3985249698162079
train gradient:  0.21243624831533878
iteration : 11997
train acc:  0.859375
train loss:  0.38939422369003296
train gradient:  0.15161359359541116
iteration : 11998
train acc:  0.8359375
train loss:  0.3646432161331177
train gradient:  0.21274123346844317
iteration : 11999
train acc:  0.8515625
train loss:  0.33684295415878296
train gradient:  0.13545301262061274
iteration : 12000
train acc:  0.8671875
train loss:  0.31000715494155884
train gradient:  0.11290382505637003
iteration : 12001
train acc:  0.8203125
train loss:  0.33604133129119873
train gradient:  0.13168227866951343
iteration : 12002
train acc:  0.8828125
train loss:  0.2744254469871521
train gradient:  0.10232509938695612
iteration : 12003
train acc:  0.8515625
train loss:  0.3909473419189453
train gradient:  0.14439528127959445
iteration : 12004
train acc:  0.8515625
train loss:  0.32425349950790405
train gradient:  0.13091558772207115
iteration : 12005
train acc:  0.84375
train loss:  0.3526250720024109
train gradient:  0.21919532058565527
iteration : 12006
train acc:  0.8203125
train loss:  0.33805012702941895
train gradient:  0.1204216421855298
iteration : 12007
train acc:  0.890625
train loss:  0.27253109216690063
train gradient:  0.12087980094097672
iteration : 12008
train acc:  0.8671875
train loss:  0.3163522183895111
train gradient:  0.18255517319386783
iteration : 12009
train acc:  0.84375
train loss:  0.3266783654689789
train gradient:  0.13132184026467902
iteration : 12010
train acc:  0.8828125
train loss:  0.37715816497802734
train gradient:  0.15546149322376146
iteration : 12011
train acc:  0.8359375
train loss:  0.3769156038761139
train gradient:  0.16869322791045255
iteration : 12012
train acc:  0.90625
train loss:  0.20583763718605042
train gradient:  0.10441204865644163
iteration : 12013
train acc:  0.859375
train loss:  0.3211793303489685
train gradient:  0.12104635704031648
iteration : 12014
train acc:  0.8515625
train loss:  0.35426628589630127
train gradient:  0.14531878495026934
iteration : 12015
train acc:  0.8046875
train loss:  0.3938412666320801
train gradient:  0.13596570290409696
iteration : 12016
train acc:  0.8828125
train loss:  0.2502119839191437
train gradient:  0.11839209775638913
iteration : 12017
train acc:  0.8046875
train loss:  0.42817017436027527
train gradient:  0.26927680894199924
iteration : 12018
train acc:  0.7890625
train loss:  0.3763512372970581
train gradient:  0.16320117249365246
iteration : 12019
train acc:  0.828125
train loss:  0.35453611612319946
train gradient:  0.12669139367085291
iteration : 12020
train acc:  0.84375
train loss:  0.36689016222953796
train gradient:  0.1256550563058622
iteration : 12021
train acc:  0.921875
train loss:  0.23203110694885254
train gradient:  0.13665185428084847
iteration : 12022
train acc:  0.828125
train loss:  0.3600451946258545
train gradient:  0.13030601832508024
iteration : 12023
train acc:  0.9140625
train loss:  0.2538243234157562
train gradient:  0.08483080789397215
iteration : 12024
train acc:  0.875
train loss:  0.34535956382751465
train gradient:  0.11769658146194634
iteration : 12025
train acc:  0.8671875
train loss:  0.3407670259475708
train gradient:  0.14018114123360617
iteration : 12026
train acc:  0.8828125
train loss:  0.3138149678707123
train gradient:  0.09866713545082534
iteration : 12027
train acc:  0.8671875
train loss:  0.3025643229484558
train gradient:  0.10331845506680352
iteration : 12028
train acc:  0.8515625
train loss:  0.3400396704673767
train gradient:  0.12117907664798687
iteration : 12029
train acc:  0.8125
train loss:  0.38139376044273376
train gradient:  0.221648133576325
iteration : 12030
train acc:  0.90625
train loss:  0.2922417223453522
train gradient:  0.12857158681734523
iteration : 12031
train acc:  0.8984375
train loss:  0.28899723291397095
train gradient:  0.1066958519777796
iteration : 12032
train acc:  0.8515625
train loss:  0.2918393015861511
train gradient:  0.10537655511627396
iteration : 12033
train acc:  0.84375
train loss:  0.3145182728767395
train gradient:  0.13508053332820025
iteration : 12034
train acc:  0.8359375
train loss:  0.3505517244338989
train gradient:  0.16955075602726494
iteration : 12035
train acc:  0.84375
train loss:  0.33290910720825195
train gradient:  0.18348887510405137
iteration : 12036
train acc:  0.859375
train loss:  0.30912503600120544
train gradient:  0.10676703617479494
iteration : 12037
train acc:  0.9140625
train loss:  0.2591363489627838
train gradient:  0.10243230452843015
iteration : 12038
train acc:  0.875
train loss:  0.30182385444641113
train gradient:  0.1338431401659741
iteration : 12039
train acc:  0.8515625
train loss:  0.3258323073387146
train gradient:  0.11086311423689912
iteration : 12040
train acc:  0.84375
train loss:  0.34746092557907104
train gradient:  0.17488521243355448
iteration : 12041
train acc:  0.8671875
train loss:  0.25273263454437256
train gradient:  0.14163143356183433
iteration : 12042
train acc:  0.828125
train loss:  0.36411187052726746
train gradient:  0.11985339851230258
iteration : 12043
train acc:  0.859375
train loss:  0.33147427439689636
train gradient:  0.1359791832905944
iteration : 12044
train acc:  0.8515625
train loss:  0.3223044276237488
train gradient:  0.11163933592272568
iteration : 12045
train acc:  0.875
train loss:  0.3236561715602875
train gradient:  0.09436543023191239
iteration : 12046
train acc:  0.8671875
train loss:  0.33219820261001587
train gradient:  0.11530253599442039
iteration : 12047
train acc:  0.8515625
train loss:  0.29848650097846985
train gradient:  0.11383266259121226
iteration : 12048
train acc:  0.8671875
train loss:  0.25801533460617065
train gradient:  0.10821677597178894
iteration : 12049
train acc:  0.90625
train loss:  0.27474910020828247
train gradient:  0.10291842251089448
iteration : 12050
train acc:  0.84375
train loss:  0.3041688799858093
train gradient:  0.11901330025509817
iteration : 12051
train acc:  0.8515625
train loss:  0.2948903441429138
train gradient:  0.1079551444850328
iteration : 12052
train acc:  0.875
train loss:  0.3741355538368225
train gradient:  0.19451867696696606
iteration : 12053
train acc:  0.84375
train loss:  0.32764947414398193
train gradient:  0.13072063723558158
iteration : 12054
train acc:  0.859375
train loss:  0.3302355408668518
train gradient:  0.1281686133407724
iteration : 12055
train acc:  0.890625
train loss:  0.24618232250213623
train gradient:  0.15201380363374023
iteration : 12056
train acc:  0.875
train loss:  0.29750651121139526
train gradient:  0.09133425400122454
iteration : 12057
train acc:  0.90625
train loss:  0.2187611162662506
train gradient:  0.08002857405706888
iteration : 12058
train acc:  0.8671875
train loss:  0.37988388538360596
train gradient:  0.1352761685752223
iteration : 12059
train acc:  0.8203125
train loss:  0.404083788394928
train gradient:  0.16106714969412644
iteration : 12060
train acc:  0.828125
train loss:  0.36030828952789307
train gradient:  0.18750761345058864
iteration : 12061
train acc:  0.8359375
train loss:  0.31800124049186707
train gradient:  0.10975134006876172
iteration : 12062
train acc:  0.84375
train loss:  0.2972031533718109
train gradient:  0.16614229676617703
iteration : 12063
train acc:  0.859375
train loss:  0.33112138509750366
train gradient:  0.15955042908196493
iteration : 12064
train acc:  0.890625
train loss:  0.2964937090873718
train gradient:  0.1788373200157838
iteration : 12065
train acc:  0.8515625
train loss:  0.40023303031921387
train gradient:  0.1837794167017966
iteration : 12066
train acc:  0.8203125
train loss:  0.38230592012405396
train gradient:  0.2267020542871016
iteration : 12067
train acc:  0.875
train loss:  0.32573202252388
train gradient:  0.10881424630971649
iteration : 12068
train acc:  0.9375
train loss:  0.20704463124275208
train gradient:  0.07546230328992946
iteration : 12069
train acc:  0.890625
train loss:  0.255466490983963
train gradient:  0.09379567232011664
iteration : 12070
train acc:  0.859375
train loss:  0.2968815565109253
train gradient:  0.1313232650270118
iteration : 12071
train acc:  0.890625
train loss:  0.2654004693031311
train gradient:  0.13809536983329598
iteration : 12072
train acc:  0.828125
train loss:  0.358903169631958
train gradient:  0.14544958948756043
iteration : 12073
train acc:  0.796875
train loss:  0.4315754771232605
train gradient:  0.2330409830169823
iteration : 12074
train acc:  0.8203125
train loss:  0.396781325340271
train gradient:  0.21531174148655235
iteration : 12075
train acc:  0.890625
train loss:  0.30289381742477417
train gradient:  0.10353063855388658
iteration : 12076
train acc:  0.8046875
train loss:  0.4610428214073181
train gradient:  0.37268989249610207
iteration : 12077
train acc:  0.875
train loss:  0.32620036602020264
train gradient:  0.12305730076247905
iteration : 12078
train acc:  0.859375
train loss:  0.34116578102111816
train gradient:  0.2643504242646054
iteration : 12079
train acc:  0.9140625
train loss:  0.22853025794029236
train gradient:  0.08799209372970784
iteration : 12080
train acc:  0.875
train loss:  0.26703721284866333
train gradient:  0.08905937107224227
iteration : 12081
train acc:  0.875
train loss:  0.3540304899215698
train gradient:  0.14626689769512025
iteration : 12082
train acc:  0.8359375
train loss:  0.3659011721611023
train gradient:  0.1162500100807476
iteration : 12083
train acc:  0.8828125
train loss:  0.32245099544525146
train gradient:  0.15627716740332215
iteration : 12084
train acc:  0.875
train loss:  0.318620502948761
train gradient:  0.14182431395502776
iteration : 12085
train acc:  0.8671875
train loss:  0.30161356925964355
train gradient:  0.16597134879091924
iteration : 12086
train acc:  0.84375
train loss:  0.3833468556404114
train gradient:  0.1548261236982696
iteration : 12087
train acc:  0.9140625
train loss:  0.2704368233680725
train gradient:  0.12098192360356687
iteration : 12088
train acc:  0.796875
train loss:  0.41101565957069397
train gradient:  0.15315360833545605
iteration : 12089
train acc:  0.859375
train loss:  0.34652814269065857
train gradient:  0.15189713969398405
iteration : 12090
train acc:  0.8203125
train loss:  0.3413728177547455
train gradient:  0.14335953966043416
iteration : 12091
train acc:  0.8671875
train loss:  0.3426746428012848
train gradient:  0.13782681671858246
iteration : 12092
train acc:  0.8359375
train loss:  0.3307819366455078
train gradient:  0.13131158616192867
iteration : 12093
train acc:  0.84375
train loss:  0.3082634210586548
train gradient:  0.14778306252760554
iteration : 12094
train acc:  0.90625
train loss:  0.28775554895401
train gradient:  0.1559391648385099
iteration : 12095
train acc:  0.7890625
train loss:  0.38821011781692505
train gradient:  0.18863582876600793
iteration : 12096
train acc:  0.828125
train loss:  0.3402404487133026
train gradient:  0.12830257109578486
iteration : 12097
train acc:  0.828125
train loss:  0.33066439628601074
train gradient:  0.13021077371599166
iteration : 12098
train acc:  0.84375
train loss:  0.3287159204483032
train gradient:  0.15138971745863636
iteration : 12099
train acc:  0.859375
train loss:  0.260922372341156
train gradient:  0.13975664499148988
iteration : 12100
train acc:  0.859375
train loss:  0.3108815848827362
train gradient:  0.1116986688850588
iteration : 12101
train acc:  0.8828125
train loss:  0.323181688785553
train gradient:  0.18334262010450814
iteration : 12102
train acc:  0.8671875
train loss:  0.3059642016887665
train gradient:  0.14186255024598393
iteration : 12103
train acc:  0.8359375
train loss:  0.3388862609863281
train gradient:  0.3003122806627972
iteration : 12104
train acc:  0.8671875
train loss:  0.29003578424453735
train gradient:  0.12824820433853829
iteration : 12105
train acc:  0.9296875
train loss:  0.19844220578670502
train gradient:  0.05827785551149581
iteration : 12106
train acc:  0.8671875
train loss:  0.3562479019165039
train gradient:  0.13201122174645047
iteration : 12107
train acc:  0.875
train loss:  0.2793726325035095
train gradient:  0.09220398055129306
iteration : 12108
train acc:  0.8203125
train loss:  0.3642160892486572
train gradient:  0.28280876099989377
iteration : 12109
train acc:  0.8359375
train loss:  0.3894992470741272
train gradient:  0.1659756897139965
iteration : 12110
train acc:  0.84375
train loss:  0.3565129041671753
train gradient:  0.13886646378106993
iteration : 12111
train acc:  0.859375
train loss:  0.3387907147407532
train gradient:  0.15445842614670013
iteration : 12112
train acc:  0.859375
train loss:  0.30481481552124023
train gradient:  0.10385729634867348
iteration : 12113
train acc:  0.8203125
train loss:  0.41533204913139343
train gradient:  0.23760308607347555
iteration : 12114
train acc:  0.8515625
train loss:  0.3583345115184784
train gradient:  0.1521170311230682
iteration : 12115
train acc:  0.9140625
train loss:  0.29154714941978455
train gradient:  0.11668782427719263
iteration : 12116
train acc:  0.8515625
train loss:  0.3368387222290039
train gradient:  0.14420669501574102
iteration : 12117
train acc:  0.875
train loss:  0.3192673325538635
train gradient:  0.13143465744285554
iteration : 12118
train acc:  0.84375
train loss:  0.3301735520362854
train gradient:  0.16070730878513004
iteration : 12119
train acc:  0.8671875
train loss:  0.2954636514186859
train gradient:  0.10871543593093386
iteration : 12120
train acc:  0.8359375
train loss:  0.31349557638168335
train gradient:  0.1566131616247568
iteration : 12121
train acc:  0.84375
train loss:  0.3101128339767456
train gradient:  0.11454707687675665
iteration : 12122
train acc:  0.8515625
train loss:  0.3744581937789917
train gradient:  0.24903495967527167
iteration : 12123
train acc:  0.921875
train loss:  0.2859480381011963
train gradient:  0.09867184008188752
iteration : 12124
train acc:  0.8828125
train loss:  0.2684212327003479
train gradient:  0.10121949737018522
iteration : 12125
train acc:  0.8515625
train loss:  0.30090659856796265
train gradient:  0.11568006746653306
iteration : 12126
train acc:  0.7890625
train loss:  0.4765477478504181
train gradient:  0.241195278146209
iteration : 12127
train acc:  0.8046875
train loss:  0.3850800096988678
train gradient:  0.1465780456967516
iteration : 12128
train acc:  0.9453125
train loss:  0.21745556592941284
train gradient:  0.09925598277679258
iteration : 12129
train acc:  0.859375
train loss:  0.3815145492553711
train gradient:  0.12137670883654372
iteration : 12130
train acc:  0.8984375
train loss:  0.24342654645442963
train gradient:  0.084543505005135
iteration : 12131
train acc:  0.8515625
train loss:  0.3205853998661041
train gradient:  0.12321224374553956
iteration : 12132
train acc:  0.859375
train loss:  0.33188870549201965
train gradient:  0.1164466267313192
iteration : 12133
train acc:  0.8671875
train loss:  0.3682954013347626
train gradient:  0.23278574315429595
iteration : 12134
train acc:  0.859375
train loss:  0.3070131540298462
train gradient:  0.1462255769684898
iteration : 12135
train acc:  0.8046875
train loss:  0.3587262034416199
train gradient:  0.18027930677472662
iteration : 12136
train acc:  0.875
train loss:  0.30863314867019653
train gradient:  0.12023216814846083
iteration : 12137
train acc:  0.875
train loss:  0.33143723011016846
train gradient:  0.09057891714333573
iteration : 12138
train acc:  0.8671875
train loss:  0.35918664932250977
train gradient:  0.1944872348413364
iteration : 12139
train acc:  0.859375
train loss:  0.33956313133239746
train gradient:  0.11680565840730821
iteration : 12140
train acc:  0.84375
train loss:  0.35901135206222534
train gradient:  0.15390871457859354
iteration : 12141
train acc:  0.8515625
train loss:  0.35910555720329285
train gradient:  0.18102184684188985
iteration : 12142
train acc:  0.8515625
train loss:  0.31902676820755005
train gradient:  0.20086330133202038
iteration : 12143
train acc:  0.890625
train loss:  0.33098167181015015
train gradient:  0.19081948546206273
iteration : 12144
train acc:  0.8828125
train loss:  0.3395198583602905
train gradient:  0.1359110385644019
iteration : 12145
train acc:  0.859375
train loss:  0.28016918897628784
train gradient:  0.13281644632399953
iteration : 12146
train acc:  0.8515625
train loss:  0.38242629170417786
train gradient:  0.12352253757397869
iteration : 12147
train acc:  0.875
train loss:  0.29941725730895996
train gradient:  0.10081921071126422
iteration : 12148
train acc:  0.8984375
train loss:  0.23239830136299133
train gradient:  0.08779024624822367
iteration : 12149
train acc:  0.828125
train loss:  0.36657851934432983
train gradient:  0.15560063151031384
iteration : 12150
train acc:  0.859375
train loss:  0.3151380717754364
train gradient:  0.20312293961074118
iteration : 12151
train acc:  0.8984375
train loss:  0.29445672035217285
train gradient:  0.09214666451153387
iteration : 12152
train acc:  0.9140625
train loss:  0.2874479293823242
train gradient:  0.07852459383056068
iteration : 12153
train acc:  0.890625
train loss:  0.34519052505493164
train gradient:  0.1235006278270127
iteration : 12154
train acc:  0.84375
train loss:  0.34716981649398804
train gradient:  0.14343118610907635
iteration : 12155
train acc:  0.9375
train loss:  0.23245994746685028
train gradient:  0.09353147389116744
iteration : 12156
train acc:  0.859375
train loss:  0.3663208484649658
train gradient:  0.13453066672146796
iteration : 12157
train acc:  0.875
train loss:  0.30030882358551025
train gradient:  0.10434411529032797
iteration : 12158
train acc:  0.859375
train loss:  0.26153483986854553
train gradient:  0.08660449285308695
iteration : 12159
train acc:  0.8671875
train loss:  0.31374070048332214
train gradient:  0.13533534634081762
iteration : 12160
train acc:  0.8984375
train loss:  0.2532567083835602
train gradient:  0.1185386624840019
iteration : 12161
train acc:  0.8515625
train loss:  0.3381792902946472
train gradient:  0.14015624455925058
iteration : 12162
train acc:  0.8046875
train loss:  0.38120877742767334
train gradient:  0.193777199438174
iteration : 12163
train acc:  0.9140625
train loss:  0.2307816445827484
train gradient:  0.09835302788221356
iteration : 12164
train acc:  0.84375
train loss:  0.29989680647850037
train gradient:  0.14118219406588606
iteration : 12165
train acc:  0.859375
train loss:  0.26869142055511475
train gradient:  0.18812692708288203
iteration : 12166
train acc:  0.8984375
train loss:  0.2624053657054901
train gradient:  0.09149138210486496
iteration : 12167
train acc:  0.875
train loss:  0.2904623746871948
train gradient:  0.13410830760833858
iteration : 12168
train acc:  0.859375
train loss:  0.3233492374420166
train gradient:  0.17874268005849187
iteration : 12169
train acc:  0.8046875
train loss:  0.4553970396518707
train gradient:  0.23852775124482556
iteration : 12170
train acc:  0.84375
train loss:  0.3354497253894806
train gradient:  0.1262380689277447
iteration : 12171
train acc:  0.875
train loss:  0.3425472378730774
train gradient:  0.22478734774830705
iteration : 12172
train acc:  0.84375
train loss:  0.3533930778503418
train gradient:  0.13333119632393278
iteration : 12173
train acc:  0.875
train loss:  0.2929246127605438
train gradient:  0.1385366163303047
iteration : 12174
train acc:  0.8984375
train loss:  0.2648354768753052
train gradient:  0.1298963036957984
iteration : 12175
train acc:  0.828125
train loss:  0.387185275554657
train gradient:  0.20945981505416875
iteration : 12176
train acc:  0.8203125
train loss:  0.4469873309135437
train gradient:  0.43821709522900665
iteration : 12177
train acc:  0.8359375
train loss:  0.3360069990158081
train gradient:  0.15025134485942804
iteration : 12178
train acc:  0.8359375
train loss:  0.3564631938934326
train gradient:  0.15191063962087886
iteration : 12179
train acc:  0.84375
train loss:  0.38657963275909424
train gradient:  0.1607815960416023
iteration : 12180
train acc:  0.84375
train loss:  0.34189552068710327
train gradient:  0.1614746898150959
iteration : 12181
train acc:  0.890625
train loss:  0.2962300181388855
train gradient:  0.1425234918029885
iteration : 12182
train acc:  0.8671875
train loss:  0.30836087465286255
train gradient:  0.1264437448179922
iteration : 12183
train acc:  0.875
train loss:  0.32028353214263916
train gradient:  0.11755108504097574
iteration : 12184
train acc:  0.8984375
train loss:  0.2723942995071411
train gradient:  0.09058848943757193
iteration : 12185
train acc:  0.828125
train loss:  0.34461021423339844
train gradient:  0.18539948184365465
iteration : 12186
train acc:  0.8828125
train loss:  0.2995564341545105
train gradient:  0.12621646437385398
iteration : 12187
train acc:  0.859375
train loss:  0.3018823266029358
train gradient:  0.11339341362507459
iteration : 12188
train acc:  0.8359375
train loss:  0.3147430419921875
train gradient:  0.13101900728224725
iteration : 12189
train acc:  0.828125
train loss:  0.316222220659256
train gradient:  0.12328369607484273
iteration : 12190
train acc:  0.8828125
train loss:  0.26869624853134155
train gradient:  0.0962895389194098
iteration : 12191
train acc:  0.890625
train loss:  0.2929900884628296
train gradient:  0.1361865630532488
iteration : 12192
train acc:  0.8125
train loss:  0.3546503782272339
train gradient:  0.19462973207495157
iteration : 12193
train acc:  0.859375
train loss:  0.3117538392543793
train gradient:  0.10288863073865043
iteration : 12194
train acc:  0.90625
train loss:  0.2859722971916199
train gradient:  0.10064919059909563
iteration : 12195
train acc:  0.8671875
train loss:  0.31158602237701416
train gradient:  0.13478292329779518
iteration : 12196
train acc:  0.890625
train loss:  0.259698748588562
train gradient:  0.11778435807859272
iteration : 12197
train acc:  0.8515625
train loss:  0.36697593331336975
train gradient:  0.13885984872168555
iteration : 12198
train acc:  0.8828125
train loss:  0.2986661195755005
train gradient:  0.11822919992981207
iteration : 12199
train acc:  0.875
train loss:  0.3017859160900116
train gradient:  0.1831043878540672
iteration : 12200
train acc:  0.8515625
train loss:  0.3149949908256531
train gradient:  0.11853586071390669
iteration : 12201
train acc:  0.828125
train loss:  0.3367125988006592
train gradient:  0.1455049379398825
iteration : 12202
train acc:  0.890625
train loss:  0.2908172607421875
train gradient:  0.134637708183975
iteration : 12203
train acc:  0.875
train loss:  0.2775793671607971
train gradient:  0.08857572220544678
iteration : 12204
train acc:  0.859375
train loss:  0.29002419114112854
train gradient:  0.10647300453236727
iteration : 12205
train acc:  0.875
train loss:  0.28439861536026
train gradient:  0.10083590809438325
iteration : 12206
train acc:  0.765625
train loss:  0.45566311478614807
train gradient:  0.24063148353000574
iteration : 12207
train acc:  0.8359375
train loss:  0.32479920983314514
train gradient:  0.13311213278487607
iteration : 12208
train acc:  0.875
train loss:  0.2711527943611145
train gradient:  0.18616443129405433
iteration : 12209
train acc:  0.8203125
train loss:  0.364900678396225
train gradient:  0.21904821521622964
iteration : 12210
train acc:  0.890625
train loss:  0.27307114005088806
train gradient:  0.09612565139455018
iteration : 12211
train acc:  0.859375
train loss:  0.3839903771877289
train gradient:  0.2106577729038853
iteration : 12212
train acc:  0.890625
train loss:  0.3402385413646698
train gradient:  0.17997841624463878
iteration : 12213
train acc:  0.84375
train loss:  0.38650625944137573
train gradient:  0.18733624936494547
iteration : 12214
train acc:  0.8984375
train loss:  0.2343149036169052
train gradient:  0.15035954910981097
iteration : 12215
train acc:  0.84375
train loss:  0.4358464479446411
train gradient:  0.21543903855527563
iteration : 12216
train acc:  0.8828125
train loss:  0.26891225576400757
train gradient:  0.08861078454244747
iteration : 12217
train acc:  0.8359375
train loss:  0.33319348096847534
train gradient:  0.17522895407228284
iteration : 12218
train acc:  0.96875
train loss:  0.17685770988464355
train gradient:  0.06185640546434418
iteration : 12219
train acc:  0.828125
train loss:  0.341770738363266
train gradient:  0.17743324080823386
iteration : 12220
train acc:  0.796875
train loss:  0.41443872451782227
train gradient:  0.1923548789512055
iteration : 12221
train acc:  0.8046875
train loss:  0.3779982328414917
train gradient:  0.17505110569142807
iteration : 12222
train acc:  0.828125
train loss:  0.34645456075668335
train gradient:  0.1616161438152317
iteration : 12223
train acc:  0.8515625
train loss:  0.3351869583129883
train gradient:  0.1365209618498557
iteration : 12224
train acc:  0.8671875
train loss:  0.3075140714645386
train gradient:  0.12228818225623303
iteration : 12225
train acc:  0.8828125
train loss:  0.26930856704711914
train gradient:  0.08613119672129461
iteration : 12226
train acc:  0.796875
train loss:  0.4302026033401489
train gradient:  0.17883066544385473
iteration : 12227
train acc:  0.8671875
train loss:  0.33936411142349243
train gradient:  0.12085963081680143
iteration : 12228
train acc:  0.875
train loss:  0.26855629682540894
train gradient:  0.0741513337074544
iteration : 12229
train acc:  0.875
train loss:  0.2676698863506317
train gradient:  0.10320752815767463
iteration : 12230
train acc:  0.8828125
train loss:  0.30867189168930054
train gradient:  0.13124219778079843
iteration : 12231
train acc:  0.90625
train loss:  0.2457764595746994
train gradient:  0.11056754072686328
iteration : 12232
train acc:  0.875
train loss:  0.24268832802772522
train gradient:  0.08954403059805734
iteration : 12233
train acc:  0.84375
train loss:  0.3551103472709656
train gradient:  0.17638883698045196
iteration : 12234
train acc:  0.8671875
train loss:  0.3002217411994934
train gradient:  0.13068814559611952
iteration : 12235
train acc:  0.8828125
train loss:  0.2483416199684143
train gradient:  0.11261130849104692
iteration : 12236
train acc:  0.8671875
train loss:  0.3271068334579468
train gradient:  0.13005663439788107
iteration : 12237
train acc:  0.8984375
train loss:  0.24736334383487701
train gradient:  0.10190843060674729
iteration : 12238
train acc:  0.875
train loss:  0.26295799016952515
train gradient:  0.11726452517892459
iteration : 12239
train acc:  0.8671875
train loss:  0.3122628331184387
train gradient:  0.1744578110273578
iteration : 12240
train acc:  0.84375
train loss:  0.2977096438407898
train gradient:  0.10604523693279347
iteration : 12241
train acc:  0.8984375
train loss:  0.2354104220867157
train gradient:  0.08853941277847124
iteration : 12242
train acc:  0.84375
train loss:  0.3657188415527344
train gradient:  0.16521379100648578
iteration : 12243
train acc:  0.90625
train loss:  0.2485857903957367
train gradient:  0.10786366780433085
iteration : 12244
train acc:  0.828125
train loss:  0.34993845224380493
train gradient:  0.1919176643186336
iteration : 12245
train acc:  0.8359375
train loss:  0.3590410351753235
train gradient:  0.20220326570517025
iteration : 12246
train acc:  0.8359375
train loss:  0.33978497982025146
train gradient:  0.182630131913945
iteration : 12247
train acc:  0.921875
train loss:  0.2263699173927307
train gradient:  0.1055565568973279
iteration : 12248
train acc:  0.875
train loss:  0.3432735800743103
train gradient:  0.21021255281330517
iteration : 12249
train acc:  0.828125
train loss:  0.34284693002700806
train gradient:  0.1598958306478282
iteration : 12250
train acc:  0.875
train loss:  0.3462705910205841
train gradient:  0.16128229615454243
iteration : 12251
train acc:  0.828125
train loss:  0.39854180812835693
train gradient:  0.2014413723688711
iteration : 12252
train acc:  0.8828125
train loss:  0.2958386540412903
train gradient:  0.11855615638823569
iteration : 12253
train acc:  0.859375
train loss:  0.3795372247695923
train gradient:  0.19150033596605004
iteration : 12254
train acc:  0.8984375
train loss:  0.24791419506072998
train gradient:  0.09439253296710938
iteration : 12255
train acc:  0.890625
train loss:  0.27401822805404663
train gradient:  0.07743393297995448
iteration : 12256
train acc:  0.9140625
train loss:  0.23747017979621887
train gradient:  0.12844572154617412
iteration : 12257
train acc:  0.859375
train loss:  0.2981041669845581
train gradient:  0.12143392551486257
iteration : 12258
train acc:  0.8359375
train loss:  0.37842702865600586
train gradient:  0.18079719128267072
iteration : 12259
train acc:  0.84375
train loss:  0.3342595100402832
train gradient:  0.13119851772195434
iteration : 12260
train acc:  0.921875
train loss:  0.2258070856332779
train gradient:  0.0890203367090539
iteration : 12261
train acc:  0.8828125
train loss:  0.29518672823905945
train gradient:  0.1422060742741307
iteration : 12262
train acc:  0.8359375
train loss:  0.39415523409843445
train gradient:  0.34939498144545794
iteration : 12263
train acc:  0.8359375
train loss:  0.38558730483055115
train gradient:  0.23817730640639434
iteration : 12264
train acc:  0.8359375
train loss:  0.3676885962486267
train gradient:  0.37181891355174435
iteration : 12265
train acc:  0.859375
train loss:  0.36230260133743286
train gradient:  0.12478120268787395
iteration : 12266
train acc:  0.828125
train loss:  0.3875706195831299
train gradient:  0.21028972914627492
iteration : 12267
train acc:  0.8671875
train loss:  0.31111791729927063
train gradient:  0.14017818357411127
iteration : 12268
train acc:  0.875
train loss:  0.3283975124359131
train gradient:  0.14871256884845707
iteration : 12269
train acc:  0.828125
train loss:  0.3657230734825134
train gradient:  0.19085556547099075
iteration : 12270
train acc:  0.8125
train loss:  0.3871864974498749
train gradient:  0.1482268429594214
iteration : 12271
train acc:  0.921875
train loss:  0.25084343552589417
train gradient:  0.07153566268163247
iteration : 12272
train acc:  0.8984375
train loss:  0.23511311411857605
train gradient:  0.10985007064921846
iteration : 12273
train acc:  0.890625
train loss:  0.286774218082428
train gradient:  0.10122023088906044
iteration : 12274
train acc:  0.8671875
train loss:  0.2696874439716339
train gradient:  0.09713453252620416
iteration : 12275
train acc:  0.8515625
train loss:  0.37963420152664185
train gradient:  0.27698305843796883
iteration : 12276
train acc:  0.84375
train loss:  0.3504553437232971
train gradient:  0.17690977113872225
iteration : 12277
train acc:  0.875
train loss:  0.27808231115341187
train gradient:  0.11287564461822312
iteration : 12278
train acc:  0.8515625
train loss:  0.3256866931915283
train gradient:  0.15083113114285052
iteration : 12279
train acc:  0.828125
train loss:  0.37003976106643677
train gradient:  0.1614173433182151
iteration : 12280
train acc:  0.8515625
train loss:  0.3637528717517853
train gradient:  0.13139002632044064
iteration : 12281
train acc:  0.859375
train loss:  0.37488752603530884
train gradient:  0.16219746235609622
iteration : 12282
train acc:  0.828125
train loss:  0.3741949200630188
train gradient:  0.16898753151876075
iteration : 12283
train acc:  0.90625
train loss:  0.2978442907333374
train gradient:  0.14637255161227336
iteration : 12284
train acc:  0.8359375
train loss:  0.386008083820343
train gradient:  0.19857528355170984
iteration : 12285
train acc:  0.828125
train loss:  0.38023942708969116
train gradient:  0.21179600072734245
iteration : 12286
train acc:  0.890625
train loss:  0.2878066301345825
train gradient:  0.1312640430288543
iteration : 12287
train acc:  0.8359375
train loss:  0.3226455748081207
train gradient:  0.23829333047043866
iteration : 12288
train acc:  0.9140625
train loss:  0.2267158329486847
train gradient:  0.0935424766801042
iteration : 12289
train acc:  0.8984375
train loss:  0.30389559268951416
train gradient:  0.14905659300403848
iteration : 12290
train acc:  0.8671875
train loss:  0.33529138565063477
train gradient:  0.11550779752920577
iteration : 12291
train acc:  0.8125
train loss:  0.39231836795806885
train gradient:  0.17611053394327456
iteration : 12292
train acc:  0.90625
train loss:  0.2759193778038025
train gradient:  0.18717075891410198
iteration : 12293
train acc:  0.8125
train loss:  0.3976839482784271
train gradient:  0.19766753547441462
iteration : 12294
train acc:  0.8359375
train loss:  0.3506816327571869
train gradient:  0.1699469655344074
iteration : 12295
train acc:  0.828125
train loss:  0.36216792464256287
train gradient:  0.15947611641421708
iteration : 12296
train acc:  0.8515625
train loss:  0.2966998815536499
train gradient:  0.12890694399221186
iteration : 12297
train acc:  0.859375
train loss:  0.29140403866767883
train gradient:  0.1250377464913945
iteration : 12298
train acc:  0.84375
train loss:  0.3441620469093323
train gradient:  0.17355191908885897
iteration : 12299
train acc:  0.875
train loss:  0.280691921710968
train gradient:  0.10444470018744395
iteration : 12300
train acc:  0.875
train loss:  0.31432652473449707
train gradient:  0.13534701964917048
iteration : 12301
train acc:  0.8828125
train loss:  0.2798561453819275
train gradient:  0.11067136859606612
iteration : 12302
train acc:  0.890625
train loss:  0.32104039192199707
train gradient:  0.11101670695589756
iteration : 12303
train acc:  0.828125
train loss:  0.31504857540130615
train gradient:  0.16273019005229314
iteration : 12304
train acc:  0.8984375
train loss:  0.2756462097167969
train gradient:  0.20922348790680786
iteration : 12305
train acc:  0.828125
train loss:  0.39477574825286865
train gradient:  0.21452606718242825
iteration : 12306
train acc:  0.8203125
train loss:  0.4065970778465271
train gradient:  0.2541059113267233
iteration : 12307
train acc:  0.8671875
train loss:  0.34685832262039185
train gradient:  0.14561104891599413
iteration : 12308
train acc:  0.8984375
train loss:  0.2743189334869385
train gradient:  0.08361469798325026
iteration : 12309
train acc:  0.8359375
train loss:  0.31651222705841064
train gradient:  0.15575666916900444
iteration : 12310
train acc:  0.9453125
train loss:  0.20451296865940094
train gradient:  0.06602723304826151
iteration : 12311
train acc:  0.875
train loss:  0.2615460753440857
train gradient:  0.1073130033435486
iteration : 12312
train acc:  0.8515625
train loss:  0.2833537459373474
train gradient:  0.0951311609702819
iteration : 12313
train acc:  0.8828125
train loss:  0.2742350995540619
train gradient:  0.13064447681766167
iteration : 12314
train acc:  0.8828125
train loss:  0.22992920875549316
train gradient:  0.08816006617709835
iteration : 12315
train acc:  0.875
train loss:  0.2657371759414673
train gradient:  0.099390864062632
iteration : 12316
train acc:  0.8203125
train loss:  0.4054085612297058
train gradient:  0.2110981825898898
iteration : 12317
train acc:  0.8125
train loss:  0.4589267671108246
train gradient:  0.22770922343682937
iteration : 12318
train acc:  0.859375
train loss:  0.28741133213043213
train gradient:  0.12273984504721887
iteration : 12319
train acc:  0.859375
train loss:  0.2846314311027527
train gradient:  0.14249573110484448
iteration : 12320
train acc:  0.875
train loss:  0.27240419387817383
train gradient:  0.11305320468330671
iteration : 12321
train acc:  0.8359375
train loss:  0.3485739529132843
train gradient:  0.13515102144370922
iteration : 12322
train acc:  0.8515625
train loss:  0.3226688504219055
train gradient:  0.1444050291080482
iteration : 12323
train acc:  0.828125
train loss:  0.4110293388366699
train gradient:  0.2289310608547142
iteration : 12324
train acc:  0.875
train loss:  0.2944551408290863
train gradient:  0.15686269334997793
iteration : 12325
train acc:  0.8515625
train loss:  0.41931694746017456
train gradient:  0.27377382043954235
iteration : 12326
train acc:  0.90625
train loss:  0.2297825813293457
train gradient:  0.10150525559534293
iteration : 12327
train acc:  0.875
train loss:  0.31506747007369995
train gradient:  0.1233053413591457
iteration : 12328
train acc:  0.875
train loss:  0.3022034466266632
train gradient:  0.11664774934609252
iteration : 12329
train acc:  0.890625
train loss:  0.310139000415802
train gradient:  0.11820219827578883
iteration : 12330
train acc:  0.890625
train loss:  0.2664869427680969
train gradient:  0.1084165570814855
iteration : 12331
train acc:  0.859375
train loss:  0.3225328326225281
train gradient:  0.14269109035654534
iteration : 12332
train acc:  0.8984375
train loss:  0.27065929770469666
train gradient:  0.0883030945924795
iteration : 12333
train acc:  0.828125
train loss:  0.39092588424682617
train gradient:  0.18869167174877038
iteration : 12334
train acc:  0.921875
train loss:  0.19588781893253326
train gradient:  0.09744719379901812
iteration : 12335
train acc:  0.859375
train loss:  0.3526179790496826
train gradient:  0.1359463860296915
iteration : 12336
train acc:  0.84375
train loss:  0.366019070148468
train gradient:  0.13272668211811067
iteration : 12337
train acc:  0.8828125
train loss:  0.31957119703292847
train gradient:  0.14360094527495887
iteration : 12338
train acc:  0.8984375
train loss:  0.30965352058410645
train gradient:  0.12267490914481351
iteration : 12339
train acc:  0.8828125
train loss:  0.30035603046417236
train gradient:  0.1646995517959148
iteration : 12340
train acc:  0.859375
train loss:  0.3132614493370056
train gradient:  0.10599615705912732
iteration : 12341
train acc:  0.8828125
train loss:  0.30436187982559204
train gradient:  0.09457417964968429
iteration : 12342
train acc:  0.859375
train loss:  0.35449743270874023
train gradient:  0.16709834463096512
iteration : 12343
train acc:  0.90625
train loss:  0.2756979167461395
train gradient:  0.08658352100586265
iteration : 12344
train acc:  0.8984375
train loss:  0.23530878126621246
train gradient:  0.07383885993171296
iteration : 12345
train acc:  0.8515625
train loss:  0.31772834062576294
train gradient:  0.18782894612874904
iteration : 12346
train acc:  0.8515625
train loss:  0.3268929123878479
train gradient:  0.13263273391713734
iteration : 12347
train acc:  0.859375
train loss:  0.3204714059829712
train gradient:  0.13144709873838242
iteration : 12348
train acc:  0.8671875
train loss:  0.30734342336654663
train gradient:  0.15018625980437908
iteration : 12349
train acc:  0.8515625
train loss:  0.3097509443759918
train gradient:  0.12965669560652474
iteration : 12350
train acc:  0.90625
train loss:  0.2577400207519531
train gradient:  0.158040393964065
iteration : 12351
train acc:  0.828125
train loss:  0.31674686074256897
train gradient:  0.09801516293189508
iteration : 12352
train acc:  0.7890625
train loss:  0.40843361616134644
train gradient:  0.24432484275897356
iteration : 12353
train acc:  0.8671875
train loss:  0.2773458659648895
train gradient:  0.12302199433558568
iteration : 12354
train acc:  0.859375
train loss:  0.38385945558547974
train gradient:  0.19048425891002346
iteration : 12355
train acc:  0.8515625
train loss:  0.3392021656036377
train gradient:  0.16222134753346218
iteration : 12356
train acc:  0.8828125
train loss:  0.32978957891464233
train gradient:  0.14623381385585457
iteration : 12357
train acc:  0.859375
train loss:  0.3503738045692444
train gradient:  0.2150623005703252
iteration : 12358
train acc:  0.828125
train loss:  0.37370818853378296
train gradient:  0.15423079448522947
iteration : 12359
train acc:  0.875
train loss:  0.31714165210723877
train gradient:  0.14649866422911034
iteration : 12360
train acc:  0.8984375
train loss:  0.3309343159198761
train gradient:  0.1081897549325454
iteration : 12361
train acc:  0.8359375
train loss:  0.31631919741630554
train gradient:  0.11739798164685657
iteration : 12362
train acc:  0.8828125
train loss:  0.3165303170681
train gradient:  0.12102341144437716
iteration : 12363
train acc:  0.875
train loss:  0.2502249479293823
train gradient:  0.08330328599864753
iteration : 12364
train acc:  0.8828125
train loss:  0.31862643361091614
train gradient:  0.18835221123056134
iteration : 12365
train acc:  0.796875
train loss:  0.4008927345275879
train gradient:  0.21438852452007628
iteration : 12366
train acc:  0.796875
train loss:  0.4146178960800171
train gradient:  0.27336076553911304
iteration : 12367
train acc:  0.8125
train loss:  0.33812832832336426
train gradient:  0.1280792281297652
iteration : 12368
train acc:  0.890625
train loss:  0.2886182367801666
train gradient:  0.13703275269569926
iteration : 12369
train acc:  0.8203125
train loss:  0.42532265186309814
train gradient:  0.24775509146698305
iteration : 12370
train acc:  0.84375
train loss:  0.33377528190612793
train gradient:  0.14913301022131192
iteration : 12371
train acc:  0.828125
train loss:  0.32960236072540283
train gradient:  0.17927064617264704
iteration : 12372
train acc:  0.796875
train loss:  0.3988755941390991
train gradient:  0.18069547798108038
iteration : 12373
train acc:  0.859375
train loss:  0.33474457263946533
train gradient:  0.13093871902972493
iteration : 12374
train acc:  0.84375
train loss:  0.32768529653549194
train gradient:  0.12540870957511463
iteration : 12375
train acc:  0.828125
train loss:  0.34493160247802734
train gradient:  0.1649321847498616
iteration : 12376
train acc:  0.890625
train loss:  0.26090943813323975
train gradient:  0.07851340212328117
iteration : 12377
train acc:  0.8515625
train loss:  0.33431684970855713
train gradient:  0.13249370868530524
iteration : 12378
train acc:  0.9296875
train loss:  0.23553389310836792
train gradient:  0.07301906196357132
iteration : 12379
train acc:  0.8359375
train loss:  0.34845250844955444
train gradient:  0.10715428763142469
iteration : 12380
train acc:  0.8515625
train loss:  0.2841964364051819
train gradient:  0.1028755125756488
iteration : 12381
train acc:  0.875
train loss:  0.2655597925186157
train gradient:  0.16372831364190504
iteration : 12382
train acc:  0.8515625
train loss:  0.3471648097038269
train gradient:  0.15206548967846187
iteration : 12383
train acc:  0.8671875
train loss:  0.30723124742507935
train gradient:  0.125143783200265
iteration : 12384
train acc:  0.859375
train loss:  0.2697676420211792
train gradient:  0.0865327030897682
iteration : 12385
train acc:  0.828125
train loss:  0.3457258939743042
train gradient:  0.16375238872128733
iteration : 12386
train acc:  0.8203125
train loss:  0.3357357084751129
train gradient:  0.18474747233943756
iteration : 12387
train acc:  0.828125
train loss:  0.3452050983905792
train gradient:  0.17323324727517328
iteration : 12388
train acc:  0.7734375
train loss:  0.4102952778339386
train gradient:  0.17918019783665046
iteration : 12389
train acc:  0.890625
train loss:  0.3094026446342468
train gradient:  0.08782016049797445
iteration : 12390
train acc:  0.8984375
train loss:  0.2984832525253296
train gradient:  0.10176899561765039
iteration : 12391
train acc:  0.8671875
train loss:  0.3396304249763489
train gradient:  0.1678926614251528
iteration : 12392
train acc:  0.8203125
train loss:  0.44273635745048523
train gradient:  0.22902248283628507
iteration : 12393
train acc:  0.8515625
train loss:  0.29367920756340027
train gradient:  0.13105421820124175
iteration : 12394
train acc:  0.8515625
train loss:  0.327250212430954
train gradient:  0.15138942090608773
iteration : 12395
train acc:  0.828125
train loss:  0.3297843337059021
train gradient:  0.12242819240787746
iteration : 12396
train acc:  0.8515625
train loss:  0.3111451268196106
train gradient:  0.11948972498071171
iteration : 12397
train acc:  0.875
train loss:  0.2854437828063965
train gradient:  0.0938335153052258
iteration : 12398
train acc:  0.8515625
train loss:  0.3602345585823059
train gradient:  0.1575511168058976
iteration : 12399
train acc:  0.8828125
train loss:  0.301471084356308
train gradient:  0.21567992764130722
iteration : 12400
train acc:  0.90625
train loss:  0.29867109656333923
train gradient:  0.16746567850143218
iteration : 12401
train acc:  0.890625
train loss:  0.2847704291343689
train gradient:  0.12813946096355391
iteration : 12402
train acc:  0.8984375
train loss:  0.2883506417274475
train gradient:  0.12329668276065245
iteration : 12403
train acc:  0.8984375
train loss:  0.2559705078601837
train gradient:  0.12416017983653661
iteration : 12404
train acc:  0.8359375
train loss:  0.36421671509742737
train gradient:  0.17740723725357588
iteration : 12405
train acc:  0.875
train loss:  0.3079739511013031
train gradient:  0.1835545598561419
iteration : 12406
train acc:  0.8984375
train loss:  0.28573113679885864
train gradient:  0.09983637001753966
iteration : 12407
train acc:  0.8671875
train loss:  0.2859542667865753
train gradient:  0.11437029823875895
iteration : 12408
train acc:  0.9296875
train loss:  0.24271219968795776
train gradient:  0.09486791049804957
iteration : 12409
train acc:  0.796875
train loss:  0.36656540632247925
train gradient:  0.13911078223615814
iteration : 12410
train acc:  0.8671875
train loss:  0.291422963142395
train gradient:  0.13123412464543044
iteration : 12411
train acc:  0.90625
train loss:  0.21854379773139954
train gradient:  0.08450471899809073
iteration : 12412
train acc:  0.9140625
train loss:  0.2600308656692505
train gradient:  0.09603550928584818
iteration : 12413
train acc:  0.8671875
train loss:  0.36073195934295654
train gradient:  0.17548053585059242
iteration : 12414
train acc:  0.9140625
train loss:  0.22401836514472961
train gradient:  0.09398655478689544
iteration : 12415
train acc:  0.8828125
train loss:  0.25101563334465027
train gradient:  0.13289209509141772
iteration : 12416
train acc:  0.875
train loss:  0.2652657926082611
train gradient:  0.10446408626088115
iteration : 12417
train acc:  0.8671875
train loss:  0.31365764141082764
train gradient:  0.17156718284181732
iteration : 12418
train acc:  0.859375
train loss:  0.3892110586166382
train gradient:  0.17245731476242582
iteration : 12419
train acc:  0.890625
train loss:  0.316476047039032
train gradient:  0.22834996070704844
iteration : 12420
train acc:  0.8671875
train loss:  0.3046363592147827
train gradient:  0.10655141356393137
iteration : 12421
train acc:  0.828125
train loss:  0.3506816029548645
train gradient:  0.17884883496225557
iteration : 12422
train acc:  0.90625
train loss:  0.25881344079971313
train gradient:  0.07059747839102742
iteration : 12423
train acc:  0.8515625
train loss:  0.3695785701274872
train gradient:  0.1608698487215402
iteration : 12424
train acc:  0.8828125
train loss:  0.28534722328186035
train gradient:  0.15529745906991438
iteration : 12425
train acc:  0.90625
train loss:  0.28566592931747437
train gradient:  0.1496782379770057
iteration : 12426
train acc:  0.9140625
train loss:  0.2207835167646408
train gradient:  0.09226701242742884
iteration : 12427
train acc:  0.8828125
train loss:  0.2774871289730072
train gradient:  0.11984896394759718
iteration : 12428
train acc:  0.90625
train loss:  0.22255343198776245
train gradient:  0.07571658777303843
iteration : 12429
train acc:  0.9453125
train loss:  0.2268628627061844
train gradient:  0.0913071248408317
iteration : 12430
train acc:  0.859375
train loss:  0.323917031288147
train gradient:  0.10221088193890722
iteration : 12431
train acc:  0.8671875
train loss:  0.2676871716976166
train gradient:  0.1251416160465311
iteration : 12432
train acc:  0.8828125
train loss:  0.3002477288246155
train gradient:  0.13700298401950795
iteration : 12433
train acc:  0.8671875
train loss:  0.3098539113998413
train gradient:  0.1385931292955495
iteration : 12434
train acc:  0.875
train loss:  0.24319988489151
train gradient:  0.1565836148997582
iteration : 12435
train acc:  0.8984375
train loss:  0.23633691668510437
train gradient:  0.08862282537561013
iteration : 12436
train acc:  0.859375
train loss:  0.32186341285705566
train gradient:  0.1450316217240709
iteration : 12437
train acc:  0.875
train loss:  0.303440660238266
train gradient:  0.12079353193213666
iteration : 12438
train acc:  0.8046875
train loss:  0.4509921967983246
train gradient:  0.22167819656082516
iteration : 12439
train acc:  0.890625
train loss:  0.24569153785705566
train gradient:  0.13139093251866305
iteration : 12440
train acc:  0.875
train loss:  0.3418920934200287
train gradient:  0.18136592009639502
iteration : 12441
train acc:  0.875
train loss:  0.2896209955215454
train gradient:  0.1188039282606881
iteration : 12442
train acc:  0.84375
train loss:  0.31683605909347534
train gradient:  0.13350670972129935
iteration : 12443
train acc:  0.921875
train loss:  0.2488354742527008
train gradient:  0.10321485376315477
iteration : 12444
train acc:  0.796875
train loss:  0.40201184153556824
train gradient:  0.20013444931920618
iteration : 12445
train acc:  0.890625
train loss:  0.2612672448158264
train gradient:  0.15576578889636766
iteration : 12446
train acc:  0.8671875
train loss:  0.26898133754730225
train gradient:  0.11722962218339128
iteration : 12447
train acc:  0.9296875
train loss:  0.2735474705696106
train gradient:  0.19018008405363263
iteration : 12448
train acc:  0.8671875
train loss:  0.2457343339920044
train gradient:  0.11161007629463703
iteration : 12449
train acc:  0.859375
train loss:  0.3599807918071747
train gradient:  0.1862534781522184
iteration : 12450
train acc:  0.890625
train loss:  0.2824142873287201
train gradient:  0.12939738766474085
iteration : 12451
train acc:  0.8671875
train loss:  0.27670538425445557
train gradient:  0.13516324237835076
iteration : 12452
train acc:  0.921875
train loss:  0.21308265626430511
train gradient:  0.08027974747878601
iteration : 12453
train acc:  0.8359375
train loss:  0.3895885646343231
train gradient:  0.19486926287756517
iteration : 12454
train acc:  0.875
train loss:  0.31961941719055176
train gradient:  0.16028448789055152
iteration : 12455
train acc:  0.8125
train loss:  0.40524324774742126
train gradient:  0.16837686694108023
iteration : 12456
train acc:  0.8046875
train loss:  0.41565245389938354
train gradient:  0.2674743586879964
iteration : 12457
train acc:  0.859375
train loss:  0.3840022683143616
train gradient:  0.16611382182341178
iteration : 12458
train acc:  0.8671875
train loss:  0.31342852115631104
train gradient:  0.1406667380895052
iteration : 12459
train acc:  0.84375
train loss:  0.36537960171699524
train gradient:  0.171047970795124
iteration : 12460
train acc:  0.8671875
train loss:  0.29665955901145935
train gradient:  0.1400149790937973
iteration : 12461
train acc:  0.8984375
train loss:  0.2780812978744507
train gradient:  0.2587964854114616
iteration : 12462
train acc:  0.90625
train loss:  0.24802319705486298
train gradient:  0.09254830479938435
iteration : 12463
train acc:  0.8671875
train loss:  0.333624005317688
train gradient:  0.24697599819058796
iteration : 12464
train acc:  0.875
train loss:  0.26086634397506714
train gradient:  0.10620911321831486
iteration : 12465
train acc:  0.8671875
train loss:  0.30742743611335754
train gradient:  0.12263988053010119
iteration : 12466
train acc:  0.78125
train loss:  0.43610674142837524
train gradient:  0.20954323197638164
iteration : 12467
train acc:  0.8671875
train loss:  0.3228304386138916
train gradient:  0.1552112041233678
iteration : 12468
train acc:  0.8828125
train loss:  0.29515230655670166
train gradient:  0.12258654661512364
iteration : 12469
train acc:  0.828125
train loss:  0.3663400411605835
train gradient:  0.17043330661425743
iteration : 12470
train acc:  0.8203125
train loss:  0.41480693221092224
train gradient:  0.245162297054265
iteration : 12471
train acc:  0.8828125
train loss:  0.3168334662914276
train gradient:  0.16868888048881509
iteration : 12472
train acc:  0.7890625
train loss:  0.42303699254989624
train gradient:  0.6036494454109298
iteration : 12473
train acc:  0.859375
train loss:  0.32296380400657654
train gradient:  0.12741099545039877
iteration : 12474
train acc:  0.8671875
train loss:  0.2934514284133911
train gradient:  0.13701163860802484
iteration : 12475
train acc:  0.8203125
train loss:  0.3593994379043579
train gradient:  0.1706976552683212
iteration : 12476
train acc:  0.8671875
train loss:  0.26613500714302063
train gradient:  0.11998690454486993
iteration : 12477
train acc:  0.875
train loss:  0.2971513867378235
train gradient:  0.1381627207562035
iteration : 12478
train acc:  0.8359375
train loss:  0.32809966802597046
train gradient:  0.16235007303905535
iteration : 12479
train acc:  0.875
train loss:  0.30883273482322693
train gradient:  0.1482997761444018
iteration : 12480
train acc:  0.8671875
train loss:  0.2884015142917633
train gradient:  0.13918465969081456
iteration : 12481
train acc:  0.8984375
train loss:  0.2554419934749603
train gradient:  0.07255278273651178
iteration : 12482
train acc:  0.828125
train loss:  0.32676219940185547
train gradient:  0.12878407375227569
iteration : 12483
train acc:  0.84375
train loss:  0.36133715510368347
train gradient:  0.1844626424769604
iteration : 12484
train acc:  0.859375
train loss:  0.3204434812068939
train gradient:  0.15552930817720365
iteration : 12485
train acc:  0.875
train loss:  0.34370875358581543
train gradient:  0.25686616527856965
iteration : 12486
train acc:  0.8671875
train loss:  0.34116023778915405
train gradient:  0.16237690156586737
iteration : 12487
train acc:  0.828125
train loss:  0.3792582154273987
train gradient:  0.23820641751487648
iteration : 12488
train acc:  0.890625
train loss:  0.26948362588882446
train gradient:  0.11328732952085654
iteration : 12489
train acc:  0.8828125
train loss:  0.32053664326667786
train gradient:  0.16933218092346414
iteration : 12490
train acc:  0.828125
train loss:  0.35761335492134094
train gradient:  0.15074255188288452
iteration : 12491
train acc:  0.84375
train loss:  0.29835301637649536
train gradient:  0.1179832399741018
iteration : 12492
train acc:  0.796875
train loss:  0.4331657588481903
train gradient:  0.31938674309069326
iteration : 12493
train acc:  0.828125
train loss:  0.44968023896217346
train gradient:  0.2384720059804316
iteration : 12494
train acc:  0.875
train loss:  0.3335881233215332
train gradient:  0.1691389232455105
iteration : 12495
train acc:  0.875
train loss:  0.26211342215538025
train gradient:  0.10546353926638966
iteration : 12496
train acc:  0.8515625
train loss:  0.3581608831882477
train gradient:  0.30330647590280585
iteration : 12497
train acc:  0.859375
train loss:  0.343950092792511
train gradient:  0.16532426593554012
iteration : 12498
train acc:  0.8828125
train loss:  0.31337398290634155
train gradient:  0.132667085035804
iteration : 12499
train acc:  0.9140625
train loss:  0.26138004660606384
train gradient:  0.11788826303982868
iteration : 12500
train acc:  0.8515625
train loss:  0.344302237033844
train gradient:  0.13125009430502776
iteration : 12501
train acc:  0.8671875
train loss:  0.31916794180870056
train gradient:  0.16269655755039014
iteration : 12502
train acc:  0.9375
train loss:  0.2545067369937897
train gradient:  0.09991730780860929
iteration : 12503
train acc:  0.890625
train loss:  0.269006609916687
train gradient:  0.15456246482318392
iteration : 12504
train acc:  0.890625
train loss:  0.33878856897354126
train gradient:  0.16146777243954805
iteration : 12505
train acc:  0.890625
train loss:  0.3389241099357605
train gradient:  0.18009500480152044
iteration : 12506
train acc:  0.8671875
train loss:  0.3039705157279968
train gradient:  0.1811000472767978
iteration : 12507
train acc:  0.890625
train loss:  0.28302592039108276
train gradient:  0.12717008715872716
iteration : 12508
train acc:  0.84375
train loss:  0.36853787302970886
train gradient:  0.17801234196827878
iteration : 12509
train acc:  0.90625
train loss:  0.24664413928985596
train gradient:  0.1152867512570783
iteration : 12510
train acc:  0.8828125
train loss:  0.26720285415649414
train gradient:  0.10350905078113519
iteration : 12511
train acc:  0.84375
train loss:  0.40479907393455505
train gradient:  0.240175551579538
iteration : 12512
train acc:  0.84375
train loss:  0.36670172214508057
train gradient:  0.16318153639214195
iteration : 12513
train acc:  0.890625
train loss:  0.2998560667037964
train gradient:  0.13095733307663582
iteration : 12514
train acc:  0.8359375
train loss:  0.4147533178329468
train gradient:  0.20691638226079023
iteration : 12515
train acc:  0.8671875
train loss:  0.32694482803344727
train gradient:  0.25079182431987873
iteration : 12516
train acc:  0.8125
train loss:  0.3567997217178345
train gradient:  0.16126300234915458
iteration : 12517
train acc:  0.8359375
train loss:  0.36088645458221436
train gradient:  0.1344332429312076
iteration : 12518
train acc:  0.84375
train loss:  0.33492472767829895
train gradient:  0.12301961951093594
iteration : 12519
train acc:  0.8671875
train loss:  0.3261658549308777
train gradient:  0.1370606628825747
iteration : 12520
train acc:  0.8515625
train loss:  0.3287641704082489
train gradient:  0.1438671086887138
iteration : 12521
train acc:  0.8671875
train loss:  0.27557826042175293
train gradient:  0.09620343155722152
iteration : 12522
train acc:  0.90625
train loss:  0.34347444772720337
train gradient:  0.15386799096360404
iteration : 12523
train acc:  0.828125
train loss:  0.34824517369270325
train gradient:  0.1424512416608486
iteration : 12524
train acc:  0.8359375
train loss:  0.33725836873054504
train gradient:  0.16986690003820681
iteration : 12525
train acc:  0.8046875
train loss:  0.4150428771972656
train gradient:  0.3034640821132358
iteration : 12526
train acc:  0.8984375
train loss:  0.2756761312484741
train gradient:  0.0945898984240721
iteration : 12527
train acc:  0.859375
train loss:  0.3725549578666687
train gradient:  0.2203729555239661
iteration : 12528
train acc:  0.9140625
train loss:  0.26120930910110474
train gradient:  0.07108737563856507
iteration : 12529
train acc:  0.875
train loss:  0.3684440851211548
train gradient:  0.19752958276438015
iteration : 12530
train acc:  0.8515625
train loss:  0.360668420791626
train gradient:  0.20880893440616372
iteration : 12531
train acc:  0.90625
train loss:  0.2210712730884552
train gradient:  0.06516751635333949
iteration : 12532
train acc:  0.8984375
train loss:  0.28724920749664307
train gradient:  0.15852167551175084
iteration : 12533
train acc:  0.8046875
train loss:  0.4259493947029114
train gradient:  0.24057930047003157
iteration : 12534
train acc:  0.8828125
train loss:  0.2829827070236206
train gradient:  0.1243428867170376
iteration : 12535
train acc:  0.890625
train loss:  0.2616421580314636
train gradient:  0.12659055593562113
iteration : 12536
train acc:  0.8359375
train loss:  0.37201303243637085
train gradient:  0.19328686539227588
iteration : 12537
train acc:  0.8984375
train loss:  0.28219300508499146
train gradient:  0.09675245762213286
iteration : 12538
train acc:  0.90625
train loss:  0.24669939279556274
train gradient:  0.07942942424437824
iteration : 12539
train acc:  0.8125
train loss:  0.43214911222457886
train gradient:  0.31408913237653435
iteration : 12540
train acc:  0.90625
train loss:  0.2698825001716614
train gradient:  0.14486344591275635
iteration : 12541
train acc:  0.875
train loss:  0.26155614852905273
train gradient:  0.1050811149170832
iteration : 12542
train acc:  0.828125
train loss:  0.374066025018692
train gradient:  0.15392881191365326
iteration : 12543
train acc:  0.8515625
train loss:  0.30433979630470276
train gradient:  0.10526619827418845
iteration : 12544
train acc:  0.84375
train loss:  0.3595050573348999
train gradient:  0.14930236971436844
iteration : 12545
train acc:  0.78125
train loss:  0.4570366144180298
train gradient:  0.30638571850798674
iteration : 12546
train acc:  0.8203125
train loss:  0.4126034379005432
train gradient:  0.22027606398999194
iteration : 12547
train acc:  0.828125
train loss:  0.40813395380973816
train gradient:  0.15946846190599437
iteration : 12548
train acc:  0.890625
train loss:  0.26304715871810913
train gradient:  0.11211597978395457
iteration : 12549
train acc:  0.8828125
train loss:  0.3177693486213684
train gradient:  0.11864197178309185
iteration : 12550
train acc:  0.8671875
train loss:  0.34633052349090576
train gradient:  0.1494888744298617
iteration : 12551
train acc:  0.8671875
train loss:  0.31931835412979126
train gradient:  0.14117408010765486
iteration : 12552
train acc:  0.8828125
train loss:  0.2760186493396759
train gradient:  0.07707457176958629
iteration : 12553
train acc:  0.8515625
train loss:  0.302854061126709
train gradient:  0.12261844411808803
iteration : 12554
train acc:  0.890625
train loss:  0.3137072026729584
train gradient:  0.16330219617893604
iteration : 12555
train acc:  0.9140625
train loss:  0.26908189058303833
train gradient:  0.08332740745757114
iteration : 12556
train acc:  0.875
train loss:  0.3332643508911133
train gradient:  0.17103076086802174
iteration : 12557
train acc:  0.875
train loss:  0.3097447156906128
train gradient:  0.09946413541496531
iteration : 12558
train acc:  0.859375
train loss:  0.277932345867157
train gradient:  0.08511857787735116
iteration : 12559
train acc:  0.8671875
train loss:  0.28260958194732666
train gradient:  0.16389215841213953
iteration : 12560
train acc:  0.8515625
train loss:  0.2989778220653534
train gradient:  0.11179847565455577
iteration : 12561
train acc:  0.859375
train loss:  0.32858210802078247
train gradient:  0.1004953099133631
iteration : 12562
train acc:  0.859375
train loss:  0.3362675905227661
train gradient:  0.12735473460263946
iteration : 12563
train acc:  0.875
train loss:  0.2871529459953308
train gradient:  0.11992004535500535
iteration : 12564
train acc:  0.828125
train loss:  0.3646743595600128
train gradient:  0.16954819043881442
iteration : 12565
train acc:  0.859375
train loss:  0.32802048325538635
train gradient:  0.2136586990034079
iteration : 12566
train acc:  0.859375
train loss:  0.27751797437667847
train gradient:  0.13984217538369603
iteration : 12567
train acc:  0.8359375
train loss:  0.340631902217865
train gradient:  0.09398009478537867
iteration : 12568
train acc:  0.890625
train loss:  0.24106615781784058
train gradient:  0.11805615921664438
iteration : 12569
train acc:  0.859375
train loss:  0.3780783414840698
train gradient:  0.19335997799230786
iteration : 12570
train acc:  0.8671875
train loss:  0.3709057867527008
train gradient:  0.13284712328076081
iteration : 12571
train acc:  0.859375
train loss:  0.29566919803619385
train gradient:  0.09527204216481097
iteration : 12572
train acc:  0.9140625
train loss:  0.26511162519454956
train gradient:  0.1067639266596375
iteration : 12573
train acc:  0.8515625
train loss:  0.37106597423553467
train gradient:  0.19034689118208728
iteration : 12574
train acc:  0.8671875
train loss:  0.2894039750099182
train gradient:  0.12043897005884284
iteration : 12575
train acc:  0.828125
train loss:  0.3438641428947449
train gradient:  0.1609518026345703
iteration : 12576
train acc:  0.84375
train loss:  0.39410483837127686
train gradient:  0.22803320985278158
iteration : 12577
train acc:  0.8828125
train loss:  0.2921939790248871
train gradient:  0.12722151592467779
iteration : 12578
train acc:  0.84375
train loss:  0.31727930903434753
train gradient:  0.14872215300382552
iteration : 12579
train acc:  0.8125
train loss:  0.32551276683807373
train gradient:  0.180066095785546
iteration : 12580
train acc:  0.859375
train loss:  0.3117760717868805
train gradient:  0.11154136594343003
iteration : 12581
train acc:  0.8828125
train loss:  0.2709062099456787
train gradient:  0.07076716767107573
iteration : 12582
train acc:  0.8984375
train loss:  0.23043978214263916
train gradient:  0.10892069215223071
iteration : 12583
train acc:  0.828125
train loss:  0.4272690713405609
train gradient:  0.2805444087230542
iteration : 12584
train acc:  0.8515625
train loss:  0.3147682845592499
train gradient:  0.11426095307851084
iteration : 12585
train acc:  0.8828125
train loss:  0.28549331426620483
train gradient:  0.11173225974058523
iteration : 12586
train acc:  0.890625
train loss:  0.29642951488494873
train gradient:  0.09233816381703358
iteration : 12587
train acc:  0.8046875
train loss:  0.3996046185493469
train gradient:  0.24531829610318576
iteration : 12588
train acc:  0.8203125
train loss:  0.4285064935684204
train gradient:  0.20064513317131866
iteration : 12589
train acc:  0.8828125
train loss:  0.2640826106071472
train gradient:  0.11077006225162045
iteration : 12590
train acc:  0.8828125
train loss:  0.28087618947029114
train gradient:  0.1507226071144078
iteration : 12591
train acc:  0.875
train loss:  0.30775851011276245
train gradient:  0.18966435384481958
iteration : 12592
train acc:  0.8828125
train loss:  0.3563001751899719
train gradient:  0.16829483226165384
iteration : 12593
train acc:  0.8828125
train loss:  0.2580946385860443
train gradient:  0.10032332158528977
iteration : 12594
train acc:  0.90625
train loss:  0.2351410984992981
train gradient:  0.12057049003186919
iteration : 12595
train acc:  0.9140625
train loss:  0.26000458002090454
train gradient:  0.12720656508195635
iteration : 12596
train acc:  0.8671875
train loss:  0.3155353367328644
train gradient:  0.1113059221965548
iteration : 12597
train acc:  0.8203125
train loss:  0.38987797498703003
train gradient:  0.16593457160909972
iteration : 12598
train acc:  0.8671875
train loss:  0.3365797996520996
train gradient:  0.12004855025013898
iteration : 12599
train acc:  0.78125
train loss:  0.3671036660671234
train gradient:  0.12930787823052223
iteration : 12600
train acc:  0.796875
train loss:  0.4286787509918213
train gradient:  0.20986132275446195
iteration : 12601
train acc:  0.9140625
train loss:  0.26799631118774414
train gradient:  0.16236383231760237
iteration : 12602
train acc:  0.828125
train loss:  0.376828670501709
train gradient:  0.16234349367914658
iteration : 12603
train acc:  0.8984375
train loss:  0.27739977836608887
train gradient:  0.08481834871104003
iteration : 12604
train acc:  0.8671875
train loss:  0.3272596597671509
train gradient:  0.13334605065102972
iteration : 12605
train acc:  0.8671875
train loss:  0.325991153717041
train gradient:  0.14217787332930176
iteration : 12606
train acc:  0.8671875
train loss:  0.29648447036743164
train gradient:  0.14627548693735803
iteration : 12607
train acc:  0.8828125
train loss:  0.3006783425807953
train gradient:  0.10822448455340157
iteration : 12608
train acc:  0.84375
train loss:  0.3296906352043152
train gradient:  0.16579279033719635
iteration : 12609
train acc:  0.890625
train loss:  0.2611701488494873
train gradient:  0.10193537707525888
iteration : 12610
train acc:  0.8671875
train loss:  0.31928157806396484
train gradient:  0.1734028280043932
iteration : 12611
train acc:  0.875
train loss:  0.327228307723999
train gradient:  0.1549570317900721
iteration : 12612
train acc:  0.8828125
train loss:  0.26754409074783325
train gradient:  0.10109985342457915
iteration : 12613
train acc:  0.8515625
train loss:  0.29989632964134216
train gradient:  0.09791733582877662
iteration : 12614
train acc:  0.796875
train loss:  0.42730438709259033
train gradient:  0.21329254602737652
iteration : 12615
train acc:  0.8359375
train loss:  0.35444605350494385
train gradient:  0.14602543105265625
iteration : 12616
train acc:  0.8203125
train loss:  0.3926696479320526
train gradient:  0.20189539565135303
iteration : 12617
train acc:  0.9140625
train loss:  0.20723272860050201
train gradient:  0.08274594496662249
iteration : 12618
train acc:  0.8203125
train loss:  0.3956807851791382
train gradient:  0.16020035073147915
iteration : 12619
train acc:  0.8671875
train loss:  0.3089839518070221
train gradient:  0.12522227168728423
iteration : 12620
train acc:  0.890625
train loss:  0.26988333463668823
train gradient:  0.0771723209947564
iteration : 12621
train acc:  0.8515625
train loss:  0.3325871229171753
train gradient:  0.16032115499285354
iteration : 12622
train acc:  0.8203125
train loss:  0.3571004867553711
train gradient:  0.15202330963287222
iteration : 12623
train acc:  0.8828125
train loss:  0.2936793267726898
train gradient:  0.11576694426899059
iteration : 12624
train acc:  0.859375
train loss:  0.37218478322029114
train gradient:  0.12001931965085932
iteration : 12625
train acc:  0.8203125
train loss:  0.379486620426178
train gradient:  0.16088265570674704
iteration : 12626
train acc:  0.921875
train loss:  0.24784153699874878
train gradient:  0.0973000438102127
iteration : 12627
train acc:  0.875
train loss:  0.2653980255126953
train gradient:  0.1431730945644011
iteration : 12628
train acc:  0.9140625
train loss:  0.24382847547531128
train gradient:  0.12014089637972468
iteration : 12629
train acc:  0.84375
train loss:  0.3508991301059723
train gradient:  0.1281663610700954
iteration : 12630
train acc:  0.8359375
train loss:  0.35351836681365967
train gradient:  0.16200201799829192
iteration : 12631
train acc:  0.75
train loss:  0.5051799416542053
train gradient:  0.293048167299207
iteration : 12632
train acc:  0.8515625
train loss:  0.3080599009990692
train gradient:  0.1096471123311232
iteration : 12633
train acc:  0.8671875
train loss:  0.2796178460121155
train gradient:  0.10780284341006442
iteration : 12634
train acc:  0.875
train loss:  0.3021179437637329
train gradient:  0.12652491296037957
iteration : 12635
train acc:  0.7578125
train loss:  0.45434242486953735
train gradient:  0.20999101954966382
iteration : 12636
train acc:  0.8984375
train loss:  0.24861562252044678
train gradient:  0.08306483689163302
iteration : 12637
train acc:  0.8671875
train loss:  0.30749940872192383
train gradient:  0.11006652007855597
iteration : 12638
train acc:  0.9140625
train loss:  0.2758563756942749
train gradient:  0.12936950043057105
iteration : 12639
train acc:  0.875
train loss:  0.2616150379180908
train gradient:  0.13433416796512054
iteration : 12640
train acc:  0.8984375
train loss:  0.27461230754852295
train gradient:  0.08672518052489489
iteration : 12641
train acc:  0.875
train loss:  0.26714980602264404
train gradient:  0.09526673497367676
iteration : 12642
train acc:  0.8828125
train loss:  0.3175126910209656
train gradient:  0.1149312115930465
iteration : 12643
train acc:  0.8828125
train loss:  0.2861834764480591
train gradient:  0.08709764458827686
iteration : 12644
train acc:  0.859375
train loss:  0.33143240213394165
train gradient:  0.14307079836359515
iteration : 12645
train acc:  0.8515625
train loss:  0.3114558458328247
train gradient:  0.15124229344678355
iteration : 12646
train acc:  0.828125
train loss:  0.3649805784225464
train gradient:  0.15379840412456697
iteration : 12647
train acc:  0.8828125
train loss:  0.25906842947006226
train gradient:  0.20740939378305856
iteration : 12648
train acc:  0.8984375
train loss:  0.34502869844436646
train gradient:  0.11714227004327138
iteration : 12649
train acc:  0.8984375
train loss:  0.23961973190307617
train gradient:  0.09322660426327797
iteration : 12650
train acc:  0.8671875
train loss:  0.34955981373786926
train gradient:  0.20526806340536513
iteration : 12651
train acc:  0.890625
train loss:  0.22882257401943207
train gradient:  0.07914463322863584
iteration : 12652
train acc:  0.8671875
train loss:  0.3013197183609009
train gradient:  0.1286671050606353
iteration : 12653
train acc:  0.859375
train loss:  0.34744274616241455
train gradient:  0.18420258812017432
iteration : 12654
train acc:  0.84375
train loss:  0.30380547046661377
train gradient:  0.15910711408906403
iteration : 12655
train acc:  0.890625
train loss:  0.35698825120925903
train gradient:  0.1789706866721782
iteration : 12656
train acc:  0.84375
train loss:  0.37252840399742126
train gradient:  0.16074565308129968
iteration : 12657
train acc:  0.8203125
train loss:  0.42748162150382996
train gradient:  0.20993164769991968
iteration : 12658
train acc:  0.828125
train loss:  0.3547465205192566
train gradient:  0.24788221229844382
iteration : 12659
train acc:  0.8359375
train loss:  0.36314743757247925
train gradient:  0.20434532783902715
iteration : 12660
train acc:  0.8203125
train loss:  0.36633121967315674
train gradient:  0.17572293233083008
iteration : 12661
train acc:  0.859375
train loss:  0.3298534154891968
train gradient:  0.1648504378746395
iteration : 12662
train acc:  0.9296875
train loss:  0.1992466151714325
train gradient:  0.07197424974050665
iteration : 12663
train acc:  0.8359375
train loss:  0.3032971918582916
train gradient:  0.1157981891691423
iteration : 12664
train acc:  0.8046875
train loss:  0.31630027294158936
train gradient:  0.12268088721920013
iteration : 12665
train acc:  0.8203125
train loss:  0.36672455072402954
train gradient:  0.138318548286543
iteration : 12666
train acc:  0.8515625
train loss:  0.3094068169593811
train gradient:  0.11834281026109336
iteration : 12667
train acc:  0.84375
train loss:  0.30424463748931885
train gradient:  0.15839578734004853
iteration : 12668
train acc:  0.8125
train loss:  0.3460899591445923
train gradient:  0.20217466331632467
iteration : 12669
train acc:  0.859375
train loss:  0.3170504570007324
train gradient:  0.15858861992738785
iteration : 12670
train acc:  0.8515625
train loss:  0.3465900719165802
train gradient:  0.16343120111323878
iteration : 12671
train acc:  0.8125
train loss:  0.3818594515323639
train gradient:  0.16217874428759319
iteration : 12672
train acc:  0.8671875
train loss:  0.2822569012641907
train gradient:  0.14458721607472236
iteration : 12673
train acc:  0.890625
train loss:  0.2561008930206299
train gradient:  0.08930457624823605
iteration : 12674
train acc:  0.8359375
train loss:  0.33470404148101807
train gradient:  0.13812739070230218
iteration : 12675
train acc:  0.8671875
train loss:  0.2728150188922882
train gradient:  0.09147799300321488
iteration : 12676
train acc:  0.8515625
train loss:  0.31661972403526306
train gradient:  0.1602328760191709
iteration : 12677
train acc:  0.859375
train loss:  0.3599013090133667
train gradient:  0.21924161547181772
iteration : 12678
train acc:  0.84375
train loss:  0.3736729621887207
train gradient:  0.21501150923807422
iteration : 12679
train acc:  0.8359375
train loss:  0.3308160603046417
train gradient:  0.0923091040454459
iteration : 12680
train acc:  0.84375
train loss:  0.36107632517814636
train gradient:  0.1962627697692116
iteration : 12681
train acc:  0.875
train loss:  0.31224942207336426
train gradient:  0.13145189864686968
iteration : 12682
train acc:  0.84375
train loss:  0.35633188486099243
train gradient:  0.33577337019041636
iteration : 12683
train acc:  0.828125
train loss:  0.4002508521080017
train gradient:  0.3325339613554221
iteration : 12684
train acc:  0.8671875
train loss:  0.27623510360717773
train gradient:  0.11110642489178837
iteration : 12685
train acc:  0.8359375
train loss:  0.37405216693878174
train gradient:  0.19437111471852453
iteration : 12686
train acc:  0.8671875
train loss:  0.26577693223953247
train gradient:  0.10895010590600215
iteration : 12687
train acc:  0.828125
train loss:  0.3557243347167969
train gradient:  0.16688383052464803
iteration : 12688
train acc:  0.8515625
train loss:  0.2703537344932556
train gradient:  0.10998252714988553
iteration : 12689
train acc:  0.875
train loss:  0.30827265977859497
train gradient:  0.1545335292988651
iteration : 12690
train acc:  0.859375
train loss:  0.3057069182395935
train gradient:  0.11498333685469869
iteration : 12691
train acc:  0.8203125
train loss:  0.32286107540130615
train gradient:  0.11724585282505129
iteration : 12692
train acc:  0.8046875
train loss:  0.41063180565834045
train gradient:  0.21367697871644764
iteration : 12693
train acc:  0.84375
train loss:  0.27969443798065186
train gradient:  0.13356961781445298
iteration : 12694
train acc:  0.9140625
train loss:  0.23844963312149048
train gradient:  0.139153110496047
iteration : 12695
train acc:  0.8671875
train loss:  0.3005914092063904
train gradient:  0.1595164375586872
iteration : 12696
train acc:  0.875
train loss:  0.2988981008529663
train gradient:  0.13368644903620433
iteration : 12697
train acc:  0.921875
train loss:  0.22821560502052307
train gradient:  0.10348470861310044
iteration : 12698
train acc:  0.859375
train loss:  0.34172675013542175
train gradient:  0.1503802051327863
iteration : 12699
train acc:  0.84375
train loss:  0.3881709575653076
train gradient:  0.21094078662710863
iteration : 12700
train acc:  0.8515625
train loss:  0.327359139919281
train gradient:  0.21423006061025335
iteration : 12701
train acc:  0.8828125
train loss:  0.2694407105445862
train gradient:  0.13092144346894793
iteration : 12702
train acc:  0.890625
train loss:  0.29981133341789246
train gradient:  0.1105296509710395
iteration : 12703
train acc:  0.8203125
train loss:  0.3291134238243103
train gradient:  0.1260569625137807
iteration : 12704
train acc:  0.8046875
train loss:  0.35906776785850525
train gradient:  0.13504212291976847
iteration : 12705
train acc:  0.8515625
train loss:  0.30270981788635254
train gradient:  0.09260802335420122
iteration : 12706
train acc:  0.84375
train loss:  0.3698272705078125
train gradient:  0.20792006672229596
iteration : 12707
train acc:  0.875
train loss:  0.2660096287727356
train gradient:  0.10767729071611827
iteration : 12708
train acc:  0.8515625
train loss:  0.34226635098457336
train gradient:  0.19000167752849823
iteration : 12709
train acc:  0.875
train loss:  0.29622331261634827
train gradient:  0.15358687624705816
iteration : 12710
train acc:  0.875
train loss:  0.32987940311431885
train gradient:  0.25714770818812777
iteration : 12711
train acc:  0.8515625
train loss:  0.33216965198516846
train gradient:  0.19435733611060066
iteration : 12712
train acc:  0.90625
train loss:  0.2190200686454773
train gradient:  0.09054616048037407
iteration : 12713
train acc:  0.859375
train loss:  0.3198692798614502
train gradient:  0.16767502485615038
iteration : 12714
train acc:  0.875
train loss:  0.3168098032474518
train gradient:  0.24666339483720967
iteration : 12715
train acc:  0.8515625
train loss:  0.3387216627597809
train gradient:  0.1448342667288882
iteration : 12716
train acc:  0.828125
train loss:  0.3589795231819153
train gradient:  0.15959629747333587
iteration : 12717
train acc:  0.8828125
train loss:  0.2988399267196655
train gradient:  0.135326648914644
iteration : 12718
train acc:  0.7890625
train loss:  0.3644341230392456
train gradient:  0.19079286361720466
iteration : 12719
train acc:  0.9140625
train loss:  0.21434009075164795
train gradient:  0.08195630563688867
iteration : 12720
train acc:  0.828125
train loss:  0.37091392278671265
train gradient:  0.1538705010527702
iteration : 12721
train acc:  0.8671875
train loss:  0.3054398000240326
train gradient:  0.1748303921087474
iteration : 12722
train acc:  0.828125
train loss:  0.3828188180923462
train gradient:  0.2224369475265428
iteration : 12723
train acc:  0.828125
train loss:  0.409630686044693
train gradient:  0.21444064273451785
iteration : 12724
train acc:  0.875
train loss:  0.3284108340740204
train gradient:  0.13471137253884924
iteration : 12725
train acc:  0.8359375
train loss:  0.34945279359817505
train gradient:  0.13199129687483135
iteration : 12726
train acc:  0.875
train loss:  0.32258203625679016
train gradient:  0.18000289837269626
iteration : 12727
train acc:  0.8984375
train loss:  0.22043147683143616
train gradient:  0.09816027168168538
iteration : 12728
train acc:  0.8359375
train loss:  0.3667370676994324
train gradient:  0.2002632086250521
iteration : 12729
train acc:  0.8828125
train loss:  0.3193800747394562
train gradient:  0.1581411515667547
iteration : 12730
train acc:  0.9296875
train loss:  0.2300599366426468
train gradient:  0.09147471061350103
iteration : 12731
train acc:  0.796875
train loss:  0.376554012298584
train gradient:  0.20241434408816988
iteration : 12732
train acc:  0.890625
train loss:  0.2671293616294861
train gradient:  0.12425522836576194
iteration : 12733
train acc:  0.8125
train loss:  0.4051680266857147
train gradient:  0.15028008870236714
iteration : 12734
train acc:  0.84375
train loss:  0.2714914083480835
train gradient:  0.08054674451990278
iteration : 12735
train acc:  0.859375
train loss:  0.3280513882637024
train gradient:  0.13115634639232093
iteration : 12736
train acc:  0.875
train loss:  0.2732987403869629
train gradient:  0.10499919903115233
iteration : 12737
train acc:  0.8828125
train loss:  0.2740015983581543
train gradient:  0.1367703373803652
iteration : 12738
train acc:  0.8671875
train loss:  0.2868117690086365
train gradient:  0.1292492945630096
iteration : 12739
train acc:  0.890625
train loss:  0.27167099714279175
train gradient:  0.07705630967677538
iteration : 12740
train acc:  0.90625
train loss:  0.23057188093662262
train gradient:  0.09764158367406485
iteration : 12741
train acc:  0.9140625
train loss:  0.253126323223114
train gradient:  0.15209166535358964
iteration : 12742
train acc:  0.859375
train loss:  0.3068123757839203
train gradient:  0.18186943535713007
iteration : 12743
train acc:  0.8671875
train loss:  0.27961674332618713
train gradient:  0.14666735301188727
iteration : 12744
train acc:  0.859375
train loss:  0.3127102255821228
train gradient:  0.13725650384295873
iteration : 12745
train acc:  0.8671875
train loss:  0.24870461225509644
train gradient:  0.09518909722067101
iteration : 12746
train acc:  0.8359375
train loss:  0.3736715614795685
train gradient:  0.21173329185009998
iteration : 12747
train acc:  0.8515625
train loss:  0.28691375255584717
train gradient:  0.12544831030877615
iteration : 12748
train acc:  0.8828125
train loss:  0.27974605560302734
train gradient:  0.12781968802488547
iteration : 12749
train acc:  0.859375
train loss:  0.30887317657470703
train gradient:  0.11148833918922971
iteration : 12750
train acc:  0.875
train loss:  0.3830626308917999
train gradient:  0.1670171784447688
iteration : 12751
train acc:  0.7890625
train loss:  0.47432664036750793
train gradient:  0.25145936029957283
iteration : 12752
train acc:  0.8046875
train loss:  0.33674952387809753
train gradient:  0.11648481308420744
iteration : 12753
train acc:  0.8515625
train loss:  0.31003332138061523
train gradient:  0.16702319102463942
iteration : 12754
train acc:  0.8125
train loss:  0.37449637055397034
train gradient:  0.20568914342034067
iteration : 12755
train acc:  0.828125
train loss:  0.37527239322662354
train gradient:  0.2158170269466735
iteration : 12756
train acc:  0.8203125
train loss:  0.35704582929611206
train gradient:  0.1289333645787244
iteration : 12757
train acc:  0.8671875
train loss:  0.2798137366771698
train gradient:  0.1501710801511281
iteration : 12758
train acc:  0.8046875
train loss:  0.4226377010345459
train gradient:  0.19747245474418398
iteration : 12759
train acc:  0.8359375
train loss:  0.3820948600769043
train gradient:  0.14588531566761895
iteration : 12760
train acc:  0.890625
train loss:  0.272807240486145
train gradient:  0.11370828532487243
iteration : 12761
train acc:  0.859375
train loss:  0.28277862071990967
train gradient:  0.0784435689571941
iteration : 12762
train acc:  0.90625
train loss:  0.27320772409439087
train gradient:  0.14674379266510837
iteration : 12763
train acc:  0.859375
train loss:  0.33360570669174194
train gradient:  0.11577826576196698
iteration : 12764
train acc:  0.9140625
train loss:  0.20255810022354126
train gradient:  0.06023431820206654
iteration : 12765
train acc:  0.8984375
train loss:  0.26364821195602417
train gradient:  0.07924434386139227
iteration : 12766
train acc:  0.8671875
train loss:  0.3334782123565674
train gradient:  0.1439324029337141
iteration : 12767
train acc:  0.90625
train loss:  0.26934173703193665
train gradient:  0.11790532933187226
iteration : 12768
train acc:  0.8828125
train loss:  0.24665910005569458
train gradient:  0.08461025586727071
iteration : 12769
train acc:  0.859375
train loss:  0.2766569256782532
train gradient:  0.09194868539829965
iteration : 12770
train acc:  0.8671875
train loss:  0.34772658348083496
train gradient:  0.13350944500536677
iteration : 12771
train acc:  0.921875
train loss:  0.22540539503097534
train gradient:  0.08237525567112242
iteration : 12772
train acc:  0.8125
train loss:  0.35586339235305786
train gradient:  0.12879064867757617
iteration : 12773
train acc:  0.796875
train loss:  0.4410983920097351
train gradient:  0.30236222141092195
iteration : 12774
train acc:  0.8984375
train loss:  0.2709498405456543
train gradient:  0.08696071699245281
iteration : 12775
train acc:  0.8515625
train loss:  0.3105265498161316
train gradient:  0.13062793528769334
iteration : 12776
train acc:  0.875
train loss:  0.28910234570503235
train gradient:  0.139851125761837
iteration : 12777
train acc:  0.8984375
train loss:  0.24013611674308777
train gradient:  0.0765062435912415
iteration : 12778
train acc:  0.8515625
train loss:  0.3429068922996521
train gradient:  0.16394472212467975
iteration : 12779
train acc:  0.875
train loss:  0.32748761773109436
train gradient:  0.15141268569778665
iteration : 12780
train acc:  0.859375
train loss:  0.3192436397075653
train gradient:  0.1520118789743734
iteration : 12781
train acc:  0.859375
train loss:  0.31739285588264465
train gradient:  0.12675734718039317
iteration : 12782
train acc:  0.8515625
train loss:  0.3386642634868622
train gradient:  0.1327235517031681
iteration : 12783
train acc:  0.8671875
train loss:  0.2980855107307434
train gradient:  0.10538561133717905
iteration : 12784
train acc:  0.875
train loss:  0.33127081394195557
train gradient:  0.1259720543477561
iteration : 12785
train acc:  0.8203125
train loss:  0.43263953924179077
train gradient:  0.2063415503412317
iteration : 12786
train acc:  0.8671875
train loss:  0.3174424469470978
train gradient:  0.16448158451699946
iteration : 12787
train acc:  0.8828125
train loss:  0.29309457540512085
train gradient:  0.11958968657945938
iteration : 12788
train acc:  0.859375
train loss:  0.29718315601348877
train gradient:  0.09792301591557144
iteration : 12789
train acc:  0.875
train loss:  0.3178933560848236
train gradient:  0.1366358240788173
iteration : 12790
train acc:  0.9140625
train loss:  0.2442658245563507
train gradient:  0.09839856642902566
iteration : 12791
train acc:  0.8671875
train loss:  0.28894805908203125
train gradient:  0.11264650826119088
iteration : 12792
train acc:  0.890625
train loss:  0.2577509582042694
train gradient:  0.09769297087527488
iteration : 12793
train acc:  0.90625
train loss:  0.24034523963928223
train gradient:  0.07319840471285777
iteration : 12794
train acc:  0.8203125
train loss:  0.35126107931137085
train gradient:  0.10674063066844898
iteration : 12795
train acc:  0.828125
train loss:  0.3660738468170166
train gradient:  0.15357057247498784
iteration : 12796
train acc:  0.8671875
train loss:  0.2973746359348297
train gradient:  0.10610378819434015
iteration : 12797
train acc:  0.8828125
train loss:  0.24917981028556824
train gradient:  0.07816280552262019
iteration : 12798
train acc:  0.8828125
train loss:  0.27323198318481445
train gradient:  0.09260517620200352
iteration : 12799
train acc:  0.8828125
train loss:  0.29371604323387146
train gradient:  0.15144850278446764
iteration : 12800
train acc:  0.8828125
train loss:  0.2714341878890991
train gradient:  0.08618766012218752
iteration : 12801
train acc:  0.84375
train loss:  0.3855254650115967
train gradient:  0.16188541082159838
iteration : 12802
train acc:  0.890625
train loss:  0.2421524077653885
train gradient:  0.15736220791947653
iteration : 12803
train acc:  0.8828125
train loss:  0.23691782355308533
train gradient:  0.09012896214340506
iteration : 12804
train acc:  0.8671875
train loss:  0.3513833284378052
train gradient:  0.25265371137315035
iteration : 12805
train acc:  0.8671875
train loss:  0.39752644300460815
train gradient:  0.21910182509213913
iteration : 12806
train acc:  0.875
train loss:  0.3240852355957031
train gradient:  0.12161511589882361
iteration : 12807
train acc:  0.8671875
train loss:  0.3494162857532501
train gradient:  0.14641285514550817
iteration : 12808
train acc:  0.8984375
train loss:  0.2599503993988037
train gradient:  0.08442434760730708
iteration : 12809
train acc:  0.828125
train loss:  0.3667910099029541
train gradient:  0.1818685946199221
iteration : 12810
train acc:  0.859375
train loss:  0.3060312271118164
train gradient:  0.1087025361621126
iteration : 12811
train acc:  0.828125
train loss:  0.35661113262176514
train gradient:  0.15916062992209906
iteration : 12812
train acc:  0.8359375
train loss:  0.4222462773323059
train gradient:  0.1981838116076146
iteration : 12813
train acc:  0.8671875
train loss:  0.286905437707901
train gradient:  0.11623846182093707
iteration : 12814
train acc:  0.8671875
train loss:  0.295989066362381
train gradient:  0.14137146748008567
iteration : 12815
train acc:  0.8125
train loss:  0.42230308055877686
train gradient:  0.2054809172332167
iteration : 12816
train acc:  0.875
train loss:  0.2655189037322998
train gradient:  0.10466485488339149
iteration : 12817
train acc:  0.8984375
train loss:  0.28406375646591187
train gradient:  0.12777552853272073
iteration : 12818
train acc:  0.8671875
train loss:  0.28079700469970703
train gradient:  0.15296646798709168
iteration : 12819
train acc:  0.8203125
train loss:  0.34968844056129456
train gradient:  0.12738790604888625
iteration : 12820
train acc:  0.828125
train loss:  0.34404391050338745
train gradient:  0.15045908766402813
iteration : 12821
train acc:  0.90625
train loss:  0.2661024034023285
train gradient:  0.11627135800060179
iteration : 12822
train acc:  0.84375
train loss:  0.36038750410079956
train gradient:  0.15702171435166726
iteration : 12823
train acc:  0.875
train loss:  0.3371666967868805
train gradient:  0.15106343049510948
iteration : 12824
train acc:  0.859375
train loss:  0.2909643352031708
train gradient:  0.13907624417623116
iteration : 12825
train acc:  0.8828125
train loss:  0.28099048137664795
train gradient:  0.11297732430725836
iteration : 12826
train acc:  0.84375
train loss:  0.36286765336990356
train gradient:  0.13279811818408074
iteration : 12827
train acc:  0.921875
train loss:  0.22190232574939728
train gradient:  0.09711927543139279
iteration : 12828
train acc:  0.8203125
train loss:  0.487870454788208
train gradient:  0.2814180001914462
iteration : 12829
train acc:  0.90625
train loss:  0.30021369457244873
train gradient:  0.1344513269530178
iteration : 12830
train acc:  0.8359375
train loss:  0.34935423731803894
train gradient:  0.20067306919990882
iteration : 12831
train acc:  0.828125
train loss:  0.34609270095825195
train gradient:  0.19296829532560825
iteration : 12832
train acc:  0.890625
train loss:  0.26158177852630615
train gradient:  0.10607292999961737
iteration : 12833
train acc:  0.8984375
train loss:  0.25108179450035095
train gradient:  0.08291672163628572
iteration : 12834
train acc:  0.8515625
train loss:  0.3224947154521942
train gradient:  0.1309944969076675
iteration : 12835
train acc:  0.859375
train loss:  0.2949790358543396
train gradient:  0.15210222833547596
iteration : 12836
train acc:  0.8359375
train loss:  0.3513821065425873
train gradient:  0.11770661315681726
iteration : 12837
train acc:  0.8203125
train loss:  0.32514286041259766
train gradient:  0.13582898140974975
iteration : 12838
train acc:  0.8359375
train loss:  0.3068673312664032
train gradient:  0.11242812657260794
iteration : 12839
train acc:  0.9296875
train loss:  0.26110249757766724
train gradient:  0.09578990423775835
iteration : 12840
train acc:  0.8671875
train loss:  0.32175618410110474
train gradient:  0.1148566179756059
iteration : 12841
train acc:  0.875
train loss:  0.28899145126342773
train gradient:  0.11135223123743647
iteration : 12842
train acc:  0.8515625
train loss:  0.35938772559165955
train gradient:  0.17272659074448932
iteration : 12843
train acc:  0.8203125
train loss:  0.39762425422668457
train gradient:  0.20854369157151034
iteration : 12844
train acc:  0.8046875
train loss:  0.49957162141799927
train gradient:  0.32547011663331865
iteration : 12845
train acc:  0.875
train loss:  0.3424842357635498
train gradient:  0.15858939908729225
iteration : 12846
train acc:  0.8828125
train loss:  0.25048741698265076
train gradient:  0.10284583197709826
iteration : 12847
train acc:  0.921875
train loss:  0.2747419476509094
train gradient:  0.08628424163118699
iteration : 12848
train acc:  0.8984375
train loss:  0.2284657508134842
train gradient:  0.08510354904041756
iteration : 12849
train acc:  0.8671875
train loss:  0.34943753480911255
train gradient:  0.23772518571303938
iteration : 12850
train acc:  0.828125
train loss:  0.4130326211452484
train gradient:  0.24396337577244098
iteration : 12851
train acc:  0.828125
train loss:  0.3368619382381439
train gradient:  0.14550227354682876
iteration : 12852
train acc:  0.8671875
train loss:  0.3575201630592346
train gradient:  0.5165825404933961
iteration : 12853
train acc:  0.8359375
train loss:  0.4022759795188904
train gradient:  0.28621667399534567
iteration : 12854
train acc:  0.828125
train loss:  0.3431072533130646
train gradient:  0.14065962425055117
iteration : 12855
train acc:  0.8359375
train loss:  0.4004442095756531
train gradient:  0.23718837516011176
iteration : 12856
train acc:  0.828125
train loss:  0.3320678472518921
train gradient:  0.17537546320172537
iteration : 12857
train acc:  0.8515625
train loss:  0.3127228915691376
train gradient:  0.12037883210679669
iteration : 12858
train acc:  0.875
train loss:  0.28508463501930237
train gradient:  0.12937051307931963
iteration : 12859
train acc:  0.8828125
train loss:  0.3121411204338074
train gradient:  0.1351641656226496
iteration : 12860
train acc:  0.8671875
train loss:  0.2699407935142517
train gradient:  0.10105412328001087
iteration : 12861
train acc:  0.84375
train loss:  0.3501458168029785
train gradient:  0.17785470668800862
iteration : 12862
train acc:  0.9296875
train loss:  0.23335662484169006
train gradient:  0.07971142234693465
iteration : 12863
train acc:  0.8984375
train loss:  0.26681801676750183
train gradient:  0.12011179244129216
iteration : 12864
train acc:  0.8359375
train loss:  0.3381781578063965
train gradient:  0.1764381239215263
iteration : 12865
train acc:  0.8828125
train loss:  0.2579624056816101
train gradient:  0.10471375738881046
iteration : 12866
train acc:  0.8828125
train loss:  0.25306057929992676
train gradient:  0.09469877990248471
iteration : 12867
train acc:  0.8828125
train loss:  0.3009486794471741
train gradient:  0.11411660982807326
iteration : 12868
train acc:  0.875
train loss:  0.36299294233322144
train gradient:  0.16927843200405063
iteration : 12869
train acc:  0.84375
train loss:  0.3251386880874634
train gradient:  0.09728524117862003
iteration : 12870
train acc:  0.8671875
train loss:  0.27124595642089844
train gradient:  0.14362021316558157
iteration : 12871
train acc:  0.8359375
train loss:  0.34945914149284363
train gradient:  0.11810583109626228
iteration : 12872
train acc:  0.828125
train loss:  0.3311280310153961
train gradient:  0.16679370976016877
iteration : 12873
train acc:  0.8203125
train loss:  0.374875009059906
train gradient:  0.3029464410431929
iteration : 12874
train acc:  0.8828125
train loss:  0.30085593461990356
train gradient:  0.1195650909305337
iteration : 12875
train acc:  0.8984375
train loss:  0.2693352699279785
train gradient:  0.12128919728149712
iteration : 12876
train acc:  0.875
train loss:  0.32746055722236633
train gradient:  0.17576721061929027
iteration : 12877
train acc:  0.8203125
train loss:  0.3368309736251831
train gradient:  0.17286147754781606
iteration : 12878
train acc:  0.8203125
train loss:  0.35876476764678955
train gradient:  0.15212244055668814
iteration : 12879
train acc:  0.84375
train loss:  0.32657939195632935
train gradient:  0.15170103349690334
iteration : 12880
train acc:  0.875
train loss:  0.35000574588775635
train gradient:  0.19752514536374854
iteration : 12881
train acc:  0.84375
train loss:  0.3142855763435364
train gradient:  0.13137329910969145
iteration : 12882
train acc:  0.8515625
train loss:  0.3874300718307495
train gradient:  0.18083807790541975
iteration : 12883
train acc:  0.9296875
train loss:  0.2485625147819519
train gradient:  0.10202921121810261
iteration : 12884
train acc:  0.8359375
train loss:  0.29553908109664917
train gradient:  0.12112211722686173
iteration : 12885
train acc:  0.875
train loss:  0.32986366748809814
train gradient:  0.13327006363831262
iteration : 12886
train acc:  0.9140625
train loss:  0.23750090599060059
train gradient:  0.0759591612469082
iteration : 12887
train acc:  0.8984375
train loss:  0.25911951065063477
train gradient:  0.08970781640995433
iteration : 12888
train acc:  0.890625
train loss:  0.267394483089447
train gradient:  0.12494384577903395
iteration : 12889
train acc:  0.859375
train loss:  0.3253759443759918
train gradient:  0.2315899141005971
iteration : 12890
train acc:  0.84375
train loss:  0.4078437089920044
train gradient:  0.1950471866994345
iteration : 12891
train acc:  0.9140625
train loss:  0.25540754199028015
train gradient:  0.11311463181475377
iteration : 12892
train acc:  0.859375
train loss:  0.32175254821777344
train gradient:  0.15107577683438178
iteration : 12893
train acc:  0.859375
train loss:  0.3254145383834839
train gradient:  0.16586290383880747
iteration : 12894
train acc:  0.84375
train loss:  0.34520605206489563
train gradient:  0.1827172954244229
iteration : 12895
train acc:  0.8671875
train loss:  0.31147128343582153
train gradient:  0.1809552954636709
iteration : 12896
train acc:  0.9140625
train loss:  0.24068036675453186
train gradient:  0.09660069370890247
iteration : 12897
train acc:  0.8359375
train loss:  0.35599279403686523
train gradient:  0.18183679841921194
iteration : 12898
train acc:  0.8671875
train loss:  0.3086799085140228
train gradient:  0.1438664523721584
iteration : 12899
train acc:  0.8671875
train loss:  0.29782527685165405
train gradient:  0.11616579658287125
iteration : 12900
train acc:  0.9140625
train loss:  0.2845434844493866
train gradient:  0.11352873605413083
iteration : 12901
train acc:  0.921875
train loss:  0.21417564153671265
train gradient:  0.10413286356086073
iteration : 12902
train acc:  0.8203125
train loss:  0.3555981516838074
train gradient:  0.26797501854626343
iteration : 12903
train acc:  0.90625
train loss:  0.22832588851451874
train gradient:  0.07192790378287892
iteration : 12904
train acc:  0.890625
train loss:  0.2887554168701172
train gradient:  0.10272699097465218
iteration : 12905
train acc:  0.859375
train loss:  0.34012937545776367
train gradient:  0.17678834840950808
iteration : 12906
train acc:  0.8984375
train loss:  0.3118523359298706
train gradient:  0.15944020290697652
iteration : 12907
train acc:  0.84375
train loss:  0.311808317899704
train gradient:  0.10874442602340616
iteration : 12908
train acc:  0.8359375
train loss:  0.3312839865684509
train gradient:  0.16246765383386458
iteration : 12909
train acc:  0.8515625
train loss:  0.3253871202468872
train gradient:  0.12735271230456668
iteration : 12910
train acc:  0.875
train loss:  0.3009854555130005
train gradient:  0.1148947689622829
iteration : 12911
train acc:  0.8671875
train loss:  0.2843018174171448
train gradient:  0.09268702886908807
iteration : 12912
train acc:  0.8671875
train loss:  0.31725117564201355
train gradient:  0.13701581046666572
iteration : 12913
train acc:  0.8828125
train loss:  0.3748232424259186
train gradient:  0.2211729492406098
iteration : 12914
train acc:  0.84375
train loss:  0.36419108510017395
train gradient:  0.19606658265613025
iteration : 12915
train acc:  0.859375
train loss:  0.40421292185783386
train gradient:  0.19518039057646625
iteration : 12916
train acc:  0.875
train loss:  0.27247127890586853
train gradient:  0.09236283858127622
iteration : 12917
train acc:  0.8359375
train loss:  0.35214197635650635
train gradient:  0.17733184324600842
iteration : 12918
train acc:  0.8984375
train loss:  0.2788879871368408
train gradient:  0.12564786496456856
iteration : 12919
train acc:  0.921875
train loss:  0.25433340668678284
train gradient:  0.09683652730395985
iteration : 12920
train acc:  0.8671875
train loss:  0.24352309107780457
train gradient:  0.10516276187141833
iteration : 12921
train acc:  0.8359375
train loss:  0.29054850339889526
train gradient:  0.09811612521011402
iteration : 12922
train acc:  0.90625
train loss:  0.28350651264190674
train gradient:  0.08566071808897506
iteration : 12923
train acc:  0.8984375
train loss:  0.2572038471698761
train gradient:  0.09883744539446661
iteration : 12924
train acc:  0.875
train loss:  0.3198332190513611
train gradient:  0.18904179827259388
iteration : 12925
train acc:  0.8515625
train loss:  0.37592291831970215
train gradient:  0.17184196763796789
iteration : 12926
train acc:  0.890625
train loss:  0.2820216417312622
train gradient:  0.14349382320977855
iteration : 12927
train acc:  0.859375
train loss:  0.3014342188835144
train gradient:  0.12833639303391972
iteration : 12928
train acc:  0.875
train loss:  0.2817072570323944
train gradient:  0.15928899356004078
iteration : 12929
train acc:  0.8828125
train loss:  0.2624818682670593
train gradient:  0.12561314693247527
iteration : 12930
train acc:  0.8203125
train loss:  0.40215784311294556
train gradient:  0.16953574750289485
iteration : 12931
train acc:  0.921875
train loss:  0.21312767267227173
train gradient:  0.09051831445522768
iteration : 12932
train acc:  0.90625
train loss:  0.22995707392692566
train gradient:  0.09177689832873792
iteration : 12933
train acc:  0.8515625
train loss:  0.32527703046798706
train gradient:  0.14260886970384382
iteration : 12934
train acc:  0.859375
train loss:  0.28342974185943604
train gradient:  0.1077136096041727
iteration : 12935
train acc:  0.875
train loss:  0.2751244306564331
train gradient:  0.10490950753012042
iteration : 12936
train acc:  0.8359375
train loss:  0.33873066306114197
train gradient:  0.18507731939068106
iteration : 12937
train acc:  0.8984375
train loss:  0.22838108241558075
train gradient:  0.07725365323257996
iteration : 12938
train acc:  0.890625
train loss:  0.2769199013710022
train gradient:  0.08820881224991345
iteration : 12939
train acc:  0.84375
train loss:  0.37887173891067505
train gradient:  0.18043339535044825
iteration : 12940
train acc:  0.875
train loss:  0.2391466498374939
train gradient:  0.18722551654256855
iteration : 12941
train acc:  0.90625
train loss:  0.20842212438583374
train gradient:  0.0794987776459736
iteration : 12942
train acc:  0.8828125
train loss:  0.2741340696811676
train gradient:  0.11442937064155277
iteration : 12943
train acc:  0.828125
train loss:  0.3439244031906128
train gradient:  0.1548441968459057
iteration : 12944
train acc:  0.875
train loss:  0.2753251791000366
train gradient:  0.1260366327179429
iteration : 12945
train acc:  0.8828125
train loss:  0.2797430157661438
train gradient:  0.16778774685060854
iteration : 12946
train acc:  0.890625
train loss:  0.2779633104801178
train gradient:  0.213063296356377
iteration : 12947
train acc:  0.875
train loss:  0.3126247525215149
train gradient:  0.18797214978688892
iteration : 12948
train acc:  0.859375
train loss:  0.3436998724937439
train gradient:  0.1884283830403771
iteration : 12949
train acc:  0.875
train loss:  0.29712387919425964
train gradient:  0.11941140947876742
iteration : 12950
train acc:  0.859375
train loss:  0.33433032035827637
train gradient:  0.23942108713447885
iteration : 12951
train acc:  0.875
train loss:  0.32654500007629395
train gradient:  0.1478596732426455
iteration : 12952
train acc:  0.8359375
train loss:  0.3438687026500702
train gradient:  0.166886539307881
iteration : 12953
train acc:  0.859375
train loss:  0.3261420726776123
train gradient:  0.16252769157962046
iteration : 12954
train acc:  0.90625
train loss:  0.24725037813186646
train gradient:  0.11867611582506246
iteration : 12955
train acc:  0.8671875
train loss:  0.3382588028907776
train gradient:  0.16187249708734408
iteration : 12956
train acc:  0.8671875
train loss:  0.29523032903671265
train gradient:  0.1899012154615743
iteration : 12957
train acc:  0.8125
train loss:  0.3599599301815033
train gradient:  0.1592390578745967
iteration : 12958
train acc:  0.8828125
train loss:  0.3133350610733032
train gradient:  0.12421533807117303
iteration : 12959
train acc:  0.890625
train loss:  0.29203736782073975
train gradient:  0.13569085690435867
iteration : 12960
train acc:  0.859375
train loss:  0.33409273624420166
train gradient:  0.14902862739780134
iteration : 12961
train acc:  0.8515625
train loss:  0.3028814196586609
train gradient:  0.13015850632264786
iteration : 12962
train acc:  0.8671875
train loss:  0.340584933757782
train gradient:  0.14664791642733155
iteration : 12963
train acc:  0.8359375
train loss:  0.3538753390312195
train gradient:  0.17700386552323438
iteration : 12964
train acc:  0.859375
train loss:  0.3248666524887085
train gradient:  0.1334215005627324
iteration : 12965
train acc:  0.859375
train loss:  0.3460266590118408
train gradient:  0.14956230898843348
iteration : 12966
train acc:  0.828125
train loss:  0.3948768973350525
train gradient:  0.1826795880210847
iteration : 12967
train acc:  0.875
train loss:  0.29002517461776733
train gradient:  0.16652321455342756
iteration : 12968
train acc:  0.8046875
train loss:  0.3982122540473938
train gradient:  0.24429719847862136
iteration : 12969
train acc:  0.84375
train loss:  0.3923049569129944
train gradient:  0.18706109702634743
iteration : 12970
train acc:  0.8046875
train loss:  0.40808671712875366
train gradient:  0.24788397448303076
iteration : 12971
train acc:  0.875
train loss:  0.34561359882354736
train gradient:  0.1409146110462855
iteration : 12972
train acc:  0.8515625
train loss:  0.3260698616504669
train gradient:  0.11418702921421067
iteration : 12973
train acc:  0.890625
train loss:  0.27457550168037415
train gradient:  0.11489166201011643
iteration : 12974
train acc:  0.8046875
train loss:  0.3678794503211975
train gradient:  0.1384947328560973
iteration : 12975
train acc:  0.8359375
train loss:  0.3320247530937195
train gradient:  0.12887591524918374
iteration : 12976
train acc:  0.8125
train loss:  0.4405234456062317
train gradient:  0.21348502282994158
iteration : 12977
train acc:  0.84375
train loss:  0.39459165930747986
train gradient:  0.16766095798315925
iteration : 12978
train acc:  0.875
train loss:  0.29589998722076416
train gradient:  0.1117462075295473
iteration : 12979
train acc:  0.828125
train loss:  0.3741632103919983
train gradient:  0.18706473110619398
iteration : 12980
train acc:  0.859375
train loss:  0.33803820610046387
train gradient:  0.17345961116278252
iteration : 12981
train acc:  0.9140625
train loss:  0.2503913640975952
train gradient:  0.09295739756196825
iteration : 12982
train acc:  0.8515625
train loss:  0.31287211179733276
train gradient:  0.10930698269927706
iteration : 12983
train acc:  0.859375
train loss:  0.3165011405944824
train gradient:  0.17488292000685696
iteration : 12984
train acc:  0.859375
train loss:  0.3196817636489868
train gradient:  0.12344412319539107
iteration : 12985
train acc:  0.859375
train loss:  0.33728882670402527
train gradient:  0.14051801374786763
iteration : 12986
train acc:  0.84375
train loss:  0.37213969230651855
train gradient:  0.2051705251845053
iteration : 12987
train acc:  0.875
train loss:  0.26544708013534546
train gradient:  0.09200366644978786
iteration : 12988
train acc:  0.8828125
train loss:  0.2757057249546051
train gradient:  0.0815393164866713
iteration : 12989
train acc:  0.828125
train loss:  0.34927260875701904
train gradient:  0.13313032092935387
iteration : 12990
train acc:  0.90625
train loss:  0.2837914526462555
train gradient:  0.11110676040686433
iteration : 12991
train acc:  0.8671875
train loss:  0.3056623339653015
train gradient:  0.10857349754736476
iteration : 12992
train acc:  0.875
train loss:  0.28267568349838257
train gradient:  0.07837861237308298
iteration : 12993
train acc:  0.8359375
train loss:  0.3429482579231262
train gradient:  0.15529674534240373
iteration : 12994
train acc:  0.8828125
train loss:  0.28657039999961853
train gradient:  0.09231409736795233
iteration : 12995
train acc:  0.828125
train loss:  0.3803723156452179
train gradient:  0.13462187606521261
iteration : 12996
train acc:  0.8828125
train loss:  0.3128376603126526
train gradient:  0.10839453424592188
iteration : 12997
train acc:  0.9296875
train loss:  0.25911760330200195
train gradient:  0.11484600740402591
iteration : 12998
train acc:  0.859375
train loss:  0.3316710591316223
train gradient:  0.1181742337317956
iteration : 12999
train acc:  0.875
train loss:  0.3174166679382324
train gradient:  0.19192144326956131
iteration : 13000
train acc:  0.859375
train loss:  0.2854728400707245
train gradient:  0.0979765534914934
iteration : 13001
train acc:  0.890625
train loss:  0.29478919506073
train gradient:  0.1177819755188742
iteration : 13002
train acc:  0.828125
train loss:  0.370403915643692
train gradient:  0.17887044749757192
iteration : 13003
train acc:  0.84375
train loss:  0.3165557086467743
train gradient:  0.13149226829195987
iteration : 13004
train acc:  0.859375
train loss:  0.3084561228752136
train gradient:  0.10259656456039554
iteration : 13005
train acc:  0.8359375
train loss:  0.30093348026275635
train gradient:  0.1410840644228537
iteration : 13006
train acc:  0.8671875
train loss:  0.2928371727466583
train gradient:  0.10354631113231164
iteration : 13007
train acc:  0.8671875
train loss:  0.29712915420532227
train gradient:  0.10879967809652313
iteration : 13008
train acc:  0.8671875
train loss:  0.3057032823562622
train gradient:  0.13050913090620198
iteration : 13009
train acc:  0.8828125
train loss:  0.3189021348953247
train gradient:  0.10028439822234565
iteration : 13010
train acc:  0.859375
train loss:  0.29336830973625183
train gradient:  0.09224410925952667
iteration : 13011
train acc:  0.890625
train loss:  0.30458229780197144
train gradient:  0.14497538510230218
iteration : 13012
train acc:  0.875
train loss:  0.3443824052810669
train gradient:  0.1289551018844562
iteration : 13013
train acc:  0.8515625
train loss:  0.2971452474594116
train gradient:  0.16642408768019473
iteration : 13014
train acc:  0.8828125
train loss:  0.3018374741077423
train gradient:  0.11673990867399718
iteration : 13015
train acc:  0.8515625
train loss:  0.32825350761413574
train gradient:  0.2061998960262202
iteration : 13016
train acc:  0.84375
train loss:  0.36043581366539
train gradient:  0.16299083957120294
iteration : 13017
train acc:  0.859375
train loss:  0.33304840326309204
train gradient:  0.10102543046393235
iteration : 13018
train acc:  0.8359375
train loss:  0.30267763137817383
train gradient:  0.10739499098455868
iteration : 13019
train acc:  0.8203125
train loss:  0.36839866638183594
train gradient:  0.19170196269764692
iteration : 13020
train acc:  0.859375
train loss:  0.3371579051017761
train gradient:  0.1168265953289357
iteration : 13021
train acc:  0.84375
train loss:  0.3100367784500122
train gradient:  0.10815065119115781
iteration : 13022
train acc:  0.9140625
train loss:  0.22073133289813995
train gradient:  0.11166952645591033
iteration : 13023
train acc:  0.859375
train loss:  0.3005630075931549
train gradient:  0.14014402314849028
iteration : 13024
train acc:  0.8984375
train loss:  0.26008522510528564
train gradient:  0.1140389600114039
iteration : 13025
train acc:  0.8359375
train loss:  0.3999439477920532
train gradient:  0.2472561423939238
iteration : 13026
train acc:  0.859375
train loss:  0.3282434344291687
train gradient:  0.18187412642920386
iteration : 13027
train acc:  0.8671875
train loss:  0.27723777294158936
train gradient:  0.08043488120806877
iteration : 13028
train acc:  0.84375
train loss:  0.28078997135162354
train gradient:  0.11185056072088591
iteration : 13029
train acc:  0.8359375
train loss:  0.31128060817718506
train gradient:  0.08840302573645492
iteration : 13030
train acc:  0.8125
train loss:  0.40353628993034363
train gradient:  0.19451212491073464
iteration : 13031
train acc:  0.890625
train loss:  0.30992990732192993
train gradient:  0.14180282600246524
iteration : 13032
train acc:  0.8671875
train loss:  0.35401076078414917
train gradient:  0.13456749822747455
iteration : 13033
train acc:  0.875
train loss:  0.30063867568969727
train gradient:  0.15198689743835533
iteration : 13034
train acc:  0.90625
train loss:  0.25130772590637207
train gradient:  0.10483214751679058
iteration : 13035
train acc:  0.8671875
train loss:  0.2864929735660553
train gradient:  0.1288089894818365
iteration : 13036
train acc:  0.84375
train loss:  0.3750326633453369
train gradient:  0.19799653179978172
iteration : 13037
train acc:  0.921875
train loss:  0.2251283824443817
train gradient:  0.06396789781222173
iteration : 13038
train acc:  0.8515625
train loss:  0.3011969029903412
train gradient:  0.11729669637196855
iteration : 13039
train acc:  0.8984375
train loss:  0.2646440267562866
train gradient:  0.0831154777796715
iteration : 13040
train acc:  0.8359375
train loss:  0.34460926055908203
train gradient:  0.1319256952418581
iteration : 13041
train acc:  0.8515625
train loss:  0.32494598627090454
train gradient:  0.1282370734772882
iteration : 13042
train acc:  0.8828125
train loss:  0.26426663994789124
train gradient:  0.10695567502400241
iteration : 13043
train acc:  0.859375
train loss:  0.3284411132335663
train gradient:  0.11032676605868072
iteration : 13044
train acc:  0.921875
train loss:  0.23168106377124786
train gradient:  0.10866500533743365
iteration : 13045
train acc:  0.8828125
train loss:  0.3556692600250244
train gradient:  0.20140374244923115
iteration : 13046
train acc:  0.8984375
train loss:  0.2707624137401581
train gradient:  0.08691881878454087
iteration : 13047
train acc:  0.8515625
train loss:  0.3113871216773987
train gradient:  0.11626255426506456
iteration : 13048
train acc:  0.828125
train loss:  0.3070411682128906
train gradient:  0.1380837474511083
iteration : 13049
train acc:  0.84375
train loss:  0.34214237332344055
train gradient:  0.13746624502116744
iteration : 13050
train acc:  0.90625
train loss:  0.2349957823753357
train gradient:  0.10079272422930599
iteration : 13051
train acc:  0.8828125
train loss:  0.2751868963241577
train gradient:  0.09377065766266
iteration : 13052
train acc:  0.875
train loss:  0.2720317840576172
train gradient:  0.14981756704043825
iteration : 13053
train acc:  0.8515625
train loss:  0.28332579135894775
train gradient:  0.16843787419405146
iteration : 13054
train acc:  0.8671875
train loss:  0.278584748506546
train gradient:  0.12856614000620298
iteration : 13055
train acc:  0.875
train loss:  0.26560091972351074
train gradient:  0.0883743357579948
iteration : 13056
train acc:  0.8515625
train loss:  0.35472050309181213
train gradient:  0.23705077882720377
iteration : 13057
train acc:  0.8515625
train loss:  0.3581242561340332
train gradient:  0.140096660991916
iteration : 13058
train acc:  0.828125
train loss:  0.32589203119277954
train gradient:  0.13784693924931007
iteration : 13059
train acc:  0.828125
train loss:  0.39121031761169434
train gradient:  0.18732335843730474
iteration : 13060
train acc:  0.90625
train loss:  0.2801460027694702
train gradient:  0.13153361063813568
iteration : 13061
train acc:  0.84375
train loss:  0.3630867004394531
train gradient:  0.14622182367687098
iteration : 13062
train acc:  0.84375
train loss:  0.33021825551986694
train gradient:  0.12259049671223934
iteration : 13063
train acc:  0.859375
train loss:  0.30521780252456665
train gradient:  0.11279476045043071
iteration : 13064
train acc:  0.875
train loss:  0.3002970218658447
train gradient:  0.1324009771586765
iteration : 13065
train acc:  0.859375
train loss:  0.29853230714797974
train gradient:  0.13788760120395033
iteration : 13066
train acc:  0.890625
train loss:  0.3054618537425995
train gradient:  0.09224959812181964
iteration : 13067
train acc:  0.875
train loss:  0.26223084330558777
train gradient:  0.08492524888081943
iteration : 13068
train acc:  0.828125
train loss:  0.3342057466506958
train gradient:  0.08955860518599222
iteration : 13069
train acc:  0.78125
train loss:  0.4058206081390381
train gradient:  0.2582909153550669
iteration : 13070
train acc:  0.8515625
train loss:  0.288789302110672
train gradient:  0.09528439012974974
iteration : 13071
train acc:  0.8359375
train loss:  0.3373253345489502
train gradient:  0.15328870327697763
iteration : 13072
train acc:  0.8359375
train loss:  0.3289230465888977
train gradient:  0.18822476813222028
iteration : 13073
train acc:  0.8515625
train loss:  0.300293505191803
train gradient:  0.11454917431770341
iteration : 13074
train acc:  0.8984375
train loss:  0.27221739292144775
train gradient:  0.09548590269204213
iteration : 13075
train acc:  0.8828125
train loss:  0.26042109727859497
train gradient:  0.10603228006774668
iteration : 13076
train acc:  0.8359375
train loss:  0.34625446796417236
train gradient:  0.12728155982734712
iteration : 13077
train acc:  0.8359375
train loss:  0.2867479920387268
train gradient:  0.12758597579238895
iteration : 13078
train acc:  0.890625
train loss:  0.3446730971336365
train gradient:  0.15024820667569191
iteration : 13079
train acc:  0.8984375
train loss:  0.2725027799606323
train gradient:  0.14982688247504092
iteration : 13080
train acc:  0.8515625
train loss:  0.358537882566452
train gradient:  0.15815375735850923
iteration : 13081
train acc:  0.8984375
train loss:  0.26612651348114014
train gradient:  0.09337709479461853
iteration : 13082
train acc:  0.859375
train loss:  0.34794485569000244
train gradient:  0.1740637440844165
iteration : 13083
train acc:  0.84375
train loss:  0.3253793716430664
train gradient:  0.14588370731527844
iteration : 13084
train acc:  0.875
train loss:  0.26001396775245667
train gradient:  0.10966617646002513
iteration : 13085
train acc:  0.90625
train loss:  0.238460510969162
train gradient:  0.09011423212733127
iteration : 13086
train acc:  0.8671875
train loss:  0.31909599900245667
train gradient:  0.11721098487080978
iteration : 13087
train acc:  0.828125
train loss:  0.3465159237384796
train gradient:  0.14149760547444734
iteration : 13088
train acc:  0.90625
train loss:  0.24597427248954773
train gradient:  0.10832341332325576
iteration : 13089
train acc:  0.875
train loss:  0.2823482155799866
train gradient:  0.10400999319512856
iteration : 13090
train acc:  0.8984375
train loss:  0.23219749331474304
train gradient:  0.07590527724471094
iteration : 13091
train acc:  0.8515625
train loss:  0.3715123236179352
train gradient:  0.20011544307907003
iteration : 13092
train acc:  0.859375
train loss:  0.31700438261032104
train gradient:  0.14283919314549964
iteration : 13093
train acc:  0.890625
train loss:  0.30446308851242065
train gradient:  0.10914863100518887
iteration : 13094
train acc:  0.828125
train loss:  0.3346862196922302
train gradient:  0.14792339624943868
iteration : 13095
train acc:  0.8671875
train loss:  0.31408393383026123
train gradient:  0.12941666443111605
iteration : 13096
train acc:  0.90625
train loss:  0.25746792554855347
train gradient:  0.10471599470645927
iteration : 13097
train acc:  0.859375
train loss:  0.3230338394641876
train gradient:  0.1675805348701585
iteration : 13098
train acc:  0.8359375
train loss:  0.3646767735481262
train gradient:  0.23793023373145922
iteration : 13099
train acc:  0.8671875
train loss:  0.30047205090522766
train gradient:  0.2329221046411611
iteration : 13100
train acc:  0.828125
train loss:  0.3964109420776367
train gradient:  0.235600414024505
iteration : 13101
train acc:  0.890625
train loss:  0.26056206226348877
train gradient:  0.07477206099694261
iteration : 13102
train acc:  0.9140625
train loss:  0.24392586946487427
train gradient:  0.09517521835155456
iteration : 13103
train acc:  0.890625
train loss:  0.28189918398857117
train gradient:  0.13890270883083977
iteration : 13104
train acc:  0.875
train loss:  0.3037891387939453
train gradient:  0.13478010065635626
iteration : 13105
train acc:  0.8671875
train loss:  0.3432531952857971
train gradient:  0.1367611885197352
iteration : 13106
train acc:  0.8828125
train loss:  0.3170081377029419
train gradient:  0.17271646536687157
iteration : 13107
train acc:  0.890625
train loss:  0.29415416717529297
train gradient:  0.12193756969883807
iteration : 13108
train acc:  0.8125
train loss:  0.4037672281265259
train gradient:  0.23972001518672809
iteration : 13109
train acc:  0.8515625
train loss:  0.4115729331970215
train gradient:  0.21007937293164747
iteration : 13110
train acc:  0.8515625
train loss:  0.338955819606781
train gradient:  0.15097070981096194
iteration : 13111
train acc:  0.8046875
train loss:  0.3740735948085785
train gradient:  0.20612167280788268
iteration : 13112
train acc:  0.8203125
train loss:  0.3689832091331482
train gradient:  0.18322990588173915
iteration : 13113
train acc:  0.9375
train loss:  0.20694023370742798
train gradient:  0.08556040542422277
iteration : 13114
train acc:  0.828125
train loss:  0.3726288676261902
train gradient:  0.16901560226039675
iteration : 13115
train acc:  0.84375
train loss:  0.27414098381996155
train gradient:  0.10804738000676131
iteration : 13116
train acc:  0.890625
train loss:  0.33771347999572754
train gradient:  0.1853229971215464
iteration : 13117
train acc:  0.890625
train loss:  0.26498740911483765
train gradient:  0.11338775138369786
iteration : 13118
train acc:  0.8515625
train loss:  0.2845299541950226
train gradient:  0.11459774336188935
iteration : 13119
train acc:  0.8671875
train loss:  0.3788963258266449
train gradient:  0.17734341461825043
iteration : 13120
train acc:  0.890625
train loss:  0.2747628092765808
train gradient:  0.10474085467964249
iteration : 13121
train acc:  0.8671875
train loss:  0.24453718960285187
train gradient:  0.10627340260501086
iteration : 13122
train acc:  0.875
train loss:  0.33450520038604736
train gradient:  0.2220856812907929
iteration : 13123
train acc:  0.890625
train loss:  0.3018231987953186
train gradient:  0.09447920218545207
iteration : 13124
train acc:  0.859375
train loss:  0.2984490394592285
train gradient:  0.10926149482676412
iteration : 13125
train acc:  0.84375
train loss:  0.3034292459487915
train gradient:  0.14043355620493345
iteration : 13126
train acc:  0.875
train loss:  0.2840437889099121
train gradient:  0.12379895450807991
iteration : 13127
train acc:  0.8984375
train loss:  0.31775233149528503
train gradient:  0.13975488762078678
iteration : 13128
train acc:  0.8515625
train loss:  0.324440598487854
train gradient:  0.10918102827572455
iteration : 13129
train acc:  0.8203125
train loss:  0.43385863304138184
train gradient:  0.24312283806063456
iteration : 13130
train acc:  0.859375
train loss:  0.33695003390312195
train gradient:  0.19669111328882327
iteration : 13131
train acc:  0.8984375
train loss:  0.22808992862701416
train gradient:  0.07312514241076803
iteration : 13132
train acc:  0.859375
train loss:  0.28666257858276367
train gradient:  0.10285628330523965
iteration : 13133
train acc:  0.8125
train loss:  0.3268090486526489
train gradient:  0.1361106265990104
iteration : 13134
train acc:  0.859375
train loss:  0.3033360540866852
train gradient:  0.12201718718671109
iteration : 13135
train acc:  0.84375
train loss:  0.3549767732620239
train gradient:  0.13147809784440093
iteration : 13136
train acc:  0.8828125
train loss:  0.32448679208755493
train gradient:  0.16449451018060401
iteration : 13137
train acc:  0.875
train loss:  0.33423930406570435
train gradient:  0.18337657335265348
iteration : 13138
train acc:  0.8515625
train loss:  0.4155693054199219
train gradient:  0.13247486880063802
iteration : 13139
train acc:  0.8515625
train loss:  0.309052437543869
train gradient:  0.13680526781593763
iteration : 13140
train acc:  0.8984375
train loss:  0.26842057704925537
train gradient:  0.09713588129828751
iteration : 13141
train acc:  0.875
train loss:  0.2645541727542877
train gradient:  0.11610867994508331
iteration : 13142
train acc:  0.84375
train loss:  0.28908488154411316
train gradient:  0.10087968605108787
iteration : 13143
train acc:  0.8359375
train loss:  0.32533589005470276
train gradient:  0.1572518209384568
iteration : 13144
train acc:  0.84375
train loss:  0.3572250008583069
train gradient:  0.18279411682600097
iteration : 13145
train acc:  0.8359375
train loss:  0.30748194456100464
train gradient:  0.11498992857340276
iteration : 13146
train acc:  0.875
train loss:  0.2965925335884094
train gradient:  0.12226469028045168
iteration : 13147
train acc:  0.8359375
train loss:  0.3367842137813568
train gradient:  0.14596181902867522
iteration : 13148
train acc:  0.8828125
train loss:  0.2748830318450928
train gradient:  0.1036573762773834
iteration : 13149
train acc:  0.921875
train loss:  0.24720415472984314
train gradient:  0.10472563719964496
iteration : 13150
train acc:  0.875
train loss:  0.30511218309402466
train gradient:  0.14136000130242768
iteration : 13151
train acc:  0.90625
train loss:  0.2678280472755432
train gradient:  0.143310452401549
iteration : 13152
train acc:  0.84375
train loss:  0.34569647908210754
train gradient:  0.1747483813491818
iteration : 13153
train acc:  0.8828125
train loss:  0.35310328006744385
train gradient:  0.15293234478237433
iteration : 13154
train acc:  0.859375
train loss:  0.2938981056213379
train gradient:  0.131014952466457
iteration : 13155
train acc:  0.8203125
train loss:  0.3620665371417999
train gradient:  0.2133310243092329
iteration : 13156
train acc:  0.828125
train loss:  0.40672165155410767
train gradient:  0.22505539788382606
iteration : 13157
train acc:  0.8359375
train loss:  0.30612263083457947
train gradient:  0.14173808142794295
iteration : 13158
train acc:  0.8671875
train loss:  0.2996046543121338
train gradient:  0.14127676943708806
iteration : 13159
train acc:  0.859375
train loss:  0.32631319761276245
train gradient:  0.13547153393859396
iteration : 13160
train acc:  0.8515625
train loss:  0.3404819369316101
train gradient:  0.1591687398497169
iteration : 13161
train acc:  0.796875
train loss:  0.33779415488243103
train gradient:  0.20619527063309084
iteration : 13162
train acc:  0.796875
train loss:  0.45399653911590576
train gradient:  0.27919688859096675
iteration : 13163
train acc:  0.8515625
train loss:  0.3820396661758423
train gradient:  0.22911864485770553
iteration : 13164
train acc:  0.8515625
train loss:  0.363482803106308
train gradient:  0.16098082084507004
iteration : 13165
train acc:  0.875
train loss:  0.29268717765808105
train gradient:  0.12336867590583162
iteration : 13166
train acc:  0.875
train loss:  0.3090883493423462
train gradient:  0.1416553306504575
iteration : 13167
train acc:  0.8125
train loss:  0.3770987093448639
train gradient:  0.15496812523244588
iteration : 13168
train acc:  0.890625
train loss:  0.27220097184181213
train gradient:  0.09302422461709742
iteration : 13169
train acc:  0.8828125
train loss:  0.306312620639801
train gradient:  0.13875528102467444
iteration : 13170
train acc:  0.875
train loss:  0.33221498131752014
train gradient:  0.14220363725501478
iteration : 13171
train acc:  0.8984375
train loss:  0.28330156207084656
train gradient:  0.12191152995739483
iteration : 13172
train acc:  0.859375
train loss:  0.2991541922092438
train gradient:  0.13371364792013732
iteration : 13173
train acc:  0.8828125
train loss:  0.3244621455669403
train gradient:  0.18112627287421434
iteration : 13174
train acc:  0.8671875
train loss:  0.3543309271335602
train gradient:  0.16118953677829098
iteration : 13175
train acc:  0.8671875
train loss:  0.2803190350532532
train gradient:  0.1508348571800636
iteration : 13176
train acc:  0.8828125
train loss:  0.33194535970687866
train gradient:  0.13092983179089895
iteration : 13177
train acc:  0.8203125
train loss:  0.4173765480518341
train gradient:  0.32026563530338015
iteration : 13178
train acc:  0.84375
train loss:  0.3284332752227783
train gradient:  0.16350431070492696
iteration : 13179
train acc:  0.8671875
train loss:  0.3171182870864868
train gradient:  0.13437679474105146
iteration : 13180
train acc:  0.8828125
train loss:  0.2839612364768982
train gradient:  0.09545492244441332
iteration : 13181
train acc:  0.8671875
train loss:  0.29479724168777466
train gradient:  0.1249975022812415
iteration : 13182
train acc:  0.8671875
train loss:  0.3264702558517456
train gradient:  0.13361139495797142
iteration : 13183
train acc:  0.8671875
train loss:  0.38830822706222534
train gradient:  0.16893866529335844
iteration : 13184
train acc:  0.8203125
train loss:  0.3283863067626953
train gradient:  0.2221531885498022
iteration : 13185
train acc:  0.8828125
train loss:  0.3138633668422699
train gradient:  0.1725390973516035
iteration : 13186
train acc:  0.8515625
train loss:  0.2705892026424408
train gradient:  0.09393264841392662
iteration : 13187
train acc:  0.8671875
train loss:  0.2916027903556824
train gradient:  0.10540238395230271
iteration : 13188
train acc:  0.8828125
train loss:  0.30166423320770264
train gradient:  0.09464184928702654
iteration : 13189
train acc:  0.8515625
train loss:  0.3498081862926483
train gradient:  0.2307962268896628
iteration : 13190
train acc:  0.90625
train loss:  0.25068753957748413
train gradient:  0.08601415112519567
iteration : 13191
train acc:  0.8359375
train loss:  0.27469921112060547
train gradient:  0.10278067694385297
iteration : 13192
train acc:  0.921875
train loss:  0.24892374873161316
train gradient:  0.11138512370756044
iteration : 13193
train acc:  0.890625
train loss:  0.2742428183555603
train gradient:  0.09957286718379652
iteration : 13194
train acc:  0.8671875
train loss:  0.3511934280395508
train gradient:  0.196211930140151
iteration : 13195
train acc:  0.8515625
train loss:  0.3531554937362671
train gradient:  0.19230903916960201
iteration : 13196
train acc:  0.8671875
train loss:  0.30032142996788025
train gradient:  0.11631986384893453
iteration : 13197
train acc:  0.921875
train loss:  0.22097650170326233
train gradient:  0.08772833013626953
iteration : 13198
train acc:  0.90625
train loss:  0.2629584074020386
train gradient:  0.10170195380040309
iteration : 13199
train acc:  0.875
train loss:  0.307557612657547
train gradient:  0.11494255306169532
iteration : 13200
train acc:  0.8515625
train loss:  0.33307957649230957
train gradient:  0.1329750297762849
iteration : 13201
train acc:  0.890625
train loss:  0.27286869287490845
train gradient:  0.11364982073444985
iteration : 13202
train acc:  0.90625
train loss:  0.26509207487106323
train gradient:  0.07423452786003407
iteration : 13203
train acc:  0.796875
train loss:  0.3736534118652344
train gradient:  0.1396672429361827
iteration : 13204
train acc:  0.875
train loss:  0.3169289529323578
train gradient:  0.1337444199974573
iteration : 13205
train acc:  0.8828125
train loss:  0.2637057304382324
train gradient:  0.11924151767458083
iteration : 13206
train acc:  0.875
train loss:  0.2435196042060852
train gradient:  0.07611472371319447
iteration : 13207
train acc:  0.875
train loss:  0.2748517394065857
train gradient:  0.11023653733611444
iteration : 13208
train acc:  0.8515625
train loss:  0.30408525466918945
train gradient:  0.13250140387490117
iteration : 13209
train acc:  0.859375
train loss:  0.38624119758605957
train gradient:  0.18604445422150004
iteration : 13210
train acc:  0.875
train loss:  0.2990235984325409
train gradient:  0.11786712610795658
iteration : 13211
train acc:  0.875
train loss:  0.30271124839782715
train gradient:  0.11284434434011897
iteration : 13212
train acc:  0.8984375
train loss:  0.23775242269039154
train gradient:  0.07318109201271457
iteration : 13213
train acc:  0.890625
train loss:  0.25559693574905396
train gradient:  0.10293003166698027
iteration : 13214
train acc:  0.875
train loss:  0.3755708336830139
train gradient:  0.17330092028996816
iteration : 13215
train acc:  0.8515625
train loss:  0.277234822511673
train gradient:  0.186288241027122
iteration : 13216
train acc:  0.8828125
train loss:  0.2786467671394348
train gradient:  0.14067503155310773
iteration : 13217
train acc:  0.8828125
train loss:  0.33148178458213806
train gradient:  0.17778620939492973
iteration : 13218
train acc:  0.84375
train loss:  0.3272660970687866
train gradient:  0.13494373493980638
iteration : 13219
train acc:  0.875
train loss:  0.3291795253753662
train gradient:  0.11433273870077462
iteration : 13220
train acc:  0.9296875
train loss:  0.19742539525032043
train gradient:  0.07366211827653002
iteration : 13221
train acc:  0.8515625
train loss:  0.3009992837905884
train gradient:  0.11868614462325236
iteration : 13222
train acc:  0.859375
train loss:  0.26566028594970703
train gradient:  0.12496726510815791
iteration : 13223
train acc:  0.8828125
train loss:  0.2667953372001648
train gradient:  0.10775946195002557
iteration : 13224
train acc:  0.9140625
train loss:  0.28087496757507324
train gradient:  0.12775187877186117
iteration : 13225
train acc:  0.8671875
train loss:  0.28668802976608276
train gradient:  0.1524635121685658
iteration : 13226
train acc:  0.890625
train loss:  0.2928861975669861
train gradient:  0.15100747251440738
iteration : 13227
train acc:  0.859375
train loss:  0.3033221960067749
train gradient:  0.1298340769153165
iteration : 13228
train acc:  0.84375
train loss:  0.3757937252521515
train gradient:  0.20920229897238618
iteration : 13229
train acc:  0.828125
train loss:  0.351595938205719
train gradient:  0.13812460290561418
iteration : 13230
train acc:  0.8828125
train loss:  0.23288966715335846
train gradient:  0.099906808965345
iteration : 13231
train acc:  0.890625
train loss:  0.23694726824760437
train gradient:  0.09081323189830487
iteration : 13232
train acc:  0.8828125
train loss:  0.28704309463500977
train gradient:  0.13508591808541345
iteration : 13233
train acc:  0.8203125
train loss:  0.3959099054336548
train gradient:  0.19835892013047518
iteration : 13234
train acc:  0.8203125
train loss:  0.36123740673065186
train gradient:  0.1659490520626852
iteration : 13235
train acc:  0.84375
train loss:  0.3199656307697296
train gradient:  0.1367330210722959
iteration : 13236
train acc:  0.8359375
train loss:  0.371194452047348
train gradient:  0.16754570617540138
iteration : 13237
train acc:  0.8359375
train loss:  0.41780033707618713
train gradient:  0.2431858000739377
iteration : 13238
train acc:  0.828125
train loss:  0.32621321082115173
train gradient:  0.17022880727773976
iteration : 13239
train acc:  0.90625
train loss:  0.20369629561901093
train gradient:  0.0734137381515312
iteration : 13240
train acc:  0.875
train loss:  0.2920406460762024
train gradient:  0.16861859297662862
iteration : 13241
train acc:  0.9140625
train loss:  0.24792949855327606
train gradient:  0.1900979067952369
iteration : 13242
train acc:  0.90625
train loss:  0.24232158064842224
train gradient:  0.10380693861573617
iteration : 13243
train acc:  0.8515625
train loss:  0.3311997056007385
train gradient:  0.17754047161477712
iteration : 13244
train acc:  0.8671875
train loss:  0.34708812832832336
train gradient:  0.20614467861864477
iteration : 13245
train acc:  0.90625
train loss:  0.27255386114120483
train gradient:  0.083891361691443
iteration : 13246
train acc:  0.8515625
train loss:  0.36271965503692627
train gradient:  0.1353732197921546
iteration : 13247
train acc:  0.890625
train loss:  0.26250648498535156
train gradient:  0.08597614317384598
iteration : 13248
train acc:  0.8515625
train loss:  0.30356159806251526
train gradient:  0.15740878997463853
iteration : 13249
train acc:  0.84375
train loss:  0.3386493921279907
train gradient:  0.12597636387105954
iteration : 13250
train acc:  0.859375
train loss:  0.3079243004322052
train gradient:  0.19203750942264186
iteration : 13251
train acc:  0.8359375
train loss:  0.38900309801101685
train gradient:  0.22695631870986574
iteration : 13252
train acc:  0.8984375
train loss:  0.3049170970916748
train gradient:  0.12469553848655854
iteration : 13253
train acc:  0.8828125
train loss:  0.2989377975463867
train gradient:  0.10777364566840586
iteration : 13254
train acc:  0.875
train loss:  0.2601269483566284
train gradient:  0.12196580913422224
iteration : 13255
train acc:  0.8984375
train loss:  0.26815664768218994
train gradient:  0.1578000896098645
iteration : 13256
train acc:  0.8515625
train loss:  0.3217991590499878
train gradient:  0.16625589761969894
iteration : 13257
train acc:  0.8203125
train loss:  0.37913864850997925
train gradient:  0.12387133817425204
iteration : 13258
train acc:  0.8515625
train loss:  0.2539467513561249
train gradient:  0.08794897560543628
iteration : 13259
train acc:  0.8671875
train loss:  0.26502692699432373
train gradient:  0.11680096032536443
iteration : 13260
train acc:  0.828125
train loss:  0.3948403298854828
train gradient:  0.18116050016014063
iteration : 13261
train acc:  0.828125
train loss:  0.3183234930038452
train gradient:  0.1300499529284906
iteration : 13262
train acc:  0.828125
train loss:  0.3549133837223053
train gradient:  0.16479368249541282
iteration : 13263
train acc:  0.8671875
train loss:  0.3370339572429657
train gradient:  0.1684902946892632
iteration : 13264
train acc:  0.8515625
train loss:  0.3459666967391968
train gradient:  0.19201503907663311
iteration : 13265
train acc:  0.8359375
train loss:  0.3158762753009796
train gradient:  0.15619410037382395
iteration : 13266
train acc:  0.890625
train loss:  0.30211174488067627
train gradient:  0.13251627415596498
iteration : 13267
train acc:  0.8125
train loss:  0.42255571484565735
train gradient:  0.288068676370725
iteration : 13268
train acc:  0.8671875
train loss:  0.3044653534889221
train gradient:  0.09616338559785824
iteration : 13269
train acc:  0.8515625
train loss:  0.32847467064857483
train gradient:  0.10424242270416534
iteration : 13270
train acc:  0.8671875
train loss:  0.32723021507263184
train gradient:  0.14782250092163907
iteration : 13271
train acc:  0.9296875
train loss:  0.21016718447208405
train gradient:  0.0873986096237817
iteration : 13272
train acc:  0.859375
train loss:  0.3706338405609131
train gradient:  0.19990392935686951
iteration : 13273
train acc:  0.875
train loss:  0.23446062207221985
train gradient:  0.07280031462528876
iteration : 13274
train acc:  0.796875
train loss:  0.38474076986312866
train gradient:  0.21775051405723062
iteration : 13275
train acc:  0.890625
train loss:  0.25847023725509644
train gradient:  0.09443757907267072
iteration : 13276
train acc:  0.890625
train loss:  0.2707555890083313
train gradient:  0.08646248654258029
iteration : 13277
train acc:  0.8515625
train loss:  0.3103465735912323
train gradient:  0.1258056597248205
iteration : 13278
train acc:  0.8671875
train loss:  0.25144079327583313
train gradient:  0.11156764564348864
iteration : 13279
train acc:  0.7890625
train loss:  0.4227581024169922
train gradient:  0.22468844042214184
iteration : 13280
train acc:  0.890625
train loss:  0.3094564974308014
train gradient:  0.1318809529912376
iteration : 13281
train acc:  0.90625
train loss:  0.2757524847984314
train gradient:  0.1459609412306947
iteration : 13282
train acc:  0.8046875
train loss:  0.35964563488960266
train gradient:  0.23192469129794657
iteration : 13283
train acc:  0.8671875
train loss:  0.3273172080516815
train gradient:  0.14704863718220254
iteration : 13284
train acc:  0.890625
train loss:  0.31563276052474976
train gradient:  0.11284239216246487
iteration : 13285
train acc:  0.796875
train loss:  0.4011456370353699
train gradient:  0.28331559893183256
iteration : 13286
train acc:  0.859375
train loss:  0.3179416358470917
train gradient:  0.12143898270169592
iteration : 13287
train acc:  0.8984375
train loss:  0.25685644149780273
train gradient:  0.08198422113594422
iteration : 13288
train acc:  0.9140625
train loss:  0.27279379963874817
train gradient:  0.128694848112807
iteration : 13289
train acc:  0.8671875
train loss:  0.37601083517074585
train gradient:  0.14421023315300494
iteration : 13290
train acc:  0.84375
train loss:  0.3524901866912842
train gradient:  0.21995627956838512
iteration : 13291
train acc:  0.8125
train loss:  0.5201249718666077
train gradient:  0.3008727606987462
iteration : 13292
train acc:  0.8828125
train loss:  0.3094375729560852
train gradient:  0.12375822233264092
iteration : 13293
train acc:  0.8515625
train loss:  0.2913687229156494
train gradient:  0.12451496114478426
iteration : 13294
train acc:  0.875
train loss:  0.2988343834877014
train gradient:  0.13397134731198337
iteration : 13295
train acc:  0.8515625
train loss:  0.2742433547973633
train gradient:  0.1234060300578429
iteration : 13296
train acc:  0.90625
train loss:  0.2501797080039978
train gradient:  0.1303469535389314
iteration : 13297
train acc:  0.828125
train loss:  0.3953980505466461
train gradient:  0.17172435650996065
iteration : 13298
train acc:  0.859375
train loss:  0.3343364894390106
train gradient:  0.13752582989859735
iteration : 13299
train acc:  0.8203125
train loss:  0.42589905858039856
train gradient:  0.22760651472703197
iteration : 13300
train acc:  0.9375
train loss:  0.24585279822349548
train gradient:  0.09348798996455265
iteration : 13301
train acc:  0.8671875
train loss:  0.2938900589942932
train gradient:  0.11924077506632241
iteration : 13302
train acc:  0.7734375
train loss:  0.4005371332168579
train gradient:  0.19272093672016855
iteration : 13303
train acc:  0.890625
train loss:  0.25128602981567383
train gradient:  0.09664219952646272
iteration : 13304
train acc:  0.8671875
train loss:  0.37076717615127563
train gradient:  0.15034611531854622
iteration : 13305
train acc:  0.90625
train loss:  0.3333250880241394
train gradient:  0.1254397818209697
iteration : 13306
train acc:  0.8828125
train loss:  0.25888651609420776
train gradient:  0.20869675513714564
iteration : 13307
train acc:  0.8671875
train loss:  0.3679197430610657
train gradient:  0.21355715313471907
iteration : 13308
train acc:  0.828125
train loss:  0.3741980791091919
train gradient:  0.1483266949665975
iteration : 13309
train acc:  0.859375
train loss:  0.2946690320968628
train gradient:  0.14382093538187612
iteration : 13310
train acc:  0.8515625
train loss:  0.29925331473350525
train gradient:  0.14349502572156075
iteration : 13311
train acc:  0.859375
train loss:  0.3230363726615906
train gradient:  0.23334809043282073
iteration : 13312
train acc:  0.875
train loss:  0.26233017444610596
train gradient:  0.0933139900098593
iteration : 13313
train acc:  0.875
train loss:  0.2957468032836914
train gradient:  0.1358355381496939
iteration : 13314
train acc:  0.84375
train loss:  0.3264518976211548
train gradient:  0.09429018869279494
iteration : 13315
train acc:  0.890625
train loss:  0.295221745967865
train gradient:  0.14037575103895716
iteration : 13316
train acc:  0.921875
train loss:  0.24825167655944824
train gradient:  0.07550457645448407
iteration : 13317
train acc:  0.890625
train loss:  0.28068456053733826
train gradient:  0.1024775682018542
iteration : 13318
train acc:  0.8828125
train loss:  0.2744654417037964
train gradient:  0.094072553315476
iteration : 13319
train acc:  0.875
train loss:  0.312639057636261
train gradient:  0.1700858639965408
iteration : 13320
train acc:  0.84375
train loss:  0.31521520018577576
train gradient:  0.20263100205739526
iteration : 13321
train acc:  0.8671875
train loss:  0.3185015320777893
train gradient:  0.16526407585976818
iteration : 13322
train acc:  0.9296875
train loss:  0.21825681626796722
train gradient:  0.10343993144756632
iteration : 13323
train acc:  0.875
train loss:  0.35237574577331543
train gradient:  0.17474238112689205
iteration : 13324
train acc:  0.8515625
train loss:  0.3568260669708252
train gradient:  0.17317897352769285
iteration : 13325
train acc:  0.8203125
train loss:  0.41805148124694824
train gradient:  0.22028933705257914
iteration : 13326
train acc:  0.8671875
train loss:  0.3578833043575287
train gradient:  0.18387894975744934
iteration : 13327
train acc:  0.8203125
train loss:  0.3871803879737854
train gradient:  0.14210840315961645
iteration : 13328
train acc:  0.8203125
train loss:  0.3091445565223694
train gradient:  0.14366228901268477
iteration : 13329
train acc:  0.859375
train loss:  0.3442257344722748
train gradient:  0.16663640639303695
iteration : 13330
train acc:  0.8515625
train loss:  0.3629493713378906
train gradient:  0.20021510701561746
iteration : 13331
train acc:  0.828125
train loss:  0.3979592025279999
train gradient:  0.2061401341023913
iteration : 13332
train acc:  0.90625
train loss:  0.26868191361427307
train gradient:  0.0983080829226577
iteration : 13333
train acc:  0.828125
train loss:  0.35951298475265503
train gradient:  0.15155741060882272
iteration : 13334
train acc:  0.8203125
train loss:  0.3763561248779297
train gradient:  0.16021023373689425
iteration : 13335
train acc:  0.875
train loss:  0.341105192899704
train gradient:  0.1449061110872346
iteration : 13336
train acc:  0.859375
train loss:  0.33425211906433105
train gradient:  0.12387999828753113
iteration : 13337
train acc:  0.890625
train loss:  0.24402496218681335
train gradient:  0.08861813225452798
iteration : 13338
train acc:  0.8828125
train loss:  0.2645655870437622
train gradient:  0.10768223546291834
iteration : 13339
train acc:  0.8515625
train loss:  0.3625691533088684
train gradient:  0.14056378658642293
iteration : 13340
train acc:  0.859375
train loss:  0.3233015537261963
train gradient:  0.13067619514209983
iteration : 13341
train acc:  0.84375
train loss:  0.3961724042892456
train gradient:  0.20527469591526587
iteration : 13342
train acc:  0.859375
train loss:  0.273552805185318
train gradient:  0.11330267677930221
iteration : 13343
train acc:  0.8671875
train loss:  0.3136894404888153
train gradient:  0.13788065831591467
iteration : 13344
train acc:  0.796875
train loss:  0.3700951635837555
train gradient:  0.14235350603758942
iteration : 13345
train acc:  0.9296875
train loss:  0.2341676652431488
train gradient:  0.09202026922200449
iteration : 13346
train acc:  0.9140625
train loss:  0.22723212838172913
train gradient:  0.10953673016732209
iteration : 13347
train acc:  0.859375
train loss:  0.2869287431240082
train gradient:  0.09056397765355709
iteration : 13348
train acc:  0.859375
train loss:  0.27660679817199707
train gradient:  0.1698475169110794
iteration : 13349
train acc:  0.828125
train loss:  0.36607348918914795
train gradient:  0.15825825496344997
iteration : 13350
train acc:  0.828125
train loss:  0.3224145174026489
train gradient:  0.11136605816594175
iteration : 13351
train acc:  0.84375
train loss:  0.34699052572250366
train gradient:  0.15093086865378647
iteration : 13352
train acc:  0.859375
train loss:  0.29816892743110657
train gradient:  0.14623853367352058
iteration : 13353
train acc:  0.8515625
train loss:  0.3054981827735901
train gradient:  0.15351898929889457
iteration : 13354
train acc:  0.8828125
train loss:  0.294890820980072
train gradient:  0.15364469901818076
iteration : 13355
train acc:  0.8671875
train loss:  0.3320581018924713
train gradient:  0.14779214141859676
iteration : 13356
train acc:  0.8515625
train loss:  0.3100128173828125
train gradient:  0.13862847678178908
iteration : 13357
train acc:  0.8671875
train loss:  0.2826327681541443
train gradient:  0.11500471101218333
iteration : 13358
train acc:  0.84375
train loss:  0.31807824969291687
train gradient:  0.1736548017644115
iteration : 13359
train acc:  0.875
train loss:  0.38024094700813293
train gradient:  0.17623901321376217
iteration : 13360
train acc:  0.84375
train loss:  0.32558438181877136
train gradient:  0.14593569472419105
iteration : 13361
train acc:  0.890625
train loss:  0.29811495542526245
train gradient:  0.11688128202870339
iteration : 13362
train acc:  0.84375
train loss:  0.34521520137786865
train gradient:  0.16324740305612434
iteration : 13363
train acc:  0.828125
train loss:  0.3996344208717346
train gradient:  0.24797052103127543
iteration : 13364
train acc:  0.84375
train loss:  0.3391839861869812
train gradient:  0.12271895730766967
iteration : 13365
train acc:  0.875
train loss:  0.2937178611755371
train gradient:  0.12404884998074538
iteration : 13366
train acc:  0.859375
train loss:  0.3490266501903534
train gradient:  0.15430173682832057
iteration : 13367
train acc:  0.8359375
train loss:  0.3600395619869232
train gradient:  0.16053565766524608
iteration : 13368
train acc:  0.8671875
train loss:  0.3652117848396301
train gradient:  0.14847400010368586
iteration : 13369
train acc:  0.8359375
train loss:  0.33157384395599365
train gradient:  0.11320351143809103
iteration : 13370
train acc:  0.8046875
train loss:  0.40908849239349365
train gradient:  0.16541329037876074
iteration : 13371
train acc:  0.8671875
train loss:  0.2678098678588867
train gradient:  0.09971519116669321
iteration : 13372
train acc:  0.8359375
train loss:  0.38004910945892334
train gradient:  0.2228718880716234
iteration : 13373
train acc:  0.875
train loss:  0.30409708619117737
train gradient:  0.08910312211269514
iteration : 13374
train acc:  0.8828125
train loss:  0.3231758177280426
train gradient:  0.1410564812899156
iteration : 13375
train acc:  0.828125
train loss:  0.3748593330383301
train gradient:  0.15661916643776708
iteration : 13376
train acc:  0.8671875
train loss:  0.2983682155609131
train gradient:  0.12030130683828892
iteration : 13377
train acc:  0.890625
train loss:  0.2776133716106415
train gradient:  0.10045344489657432
iteration : 13378
train acc:  0.8203125
train loss:  0.32100552320480347
train gradient:  0.1082818236047816
iteration : 13379
train acc:  0.8671875
train loss:  0.2906143367290497
train gradient:  0.11472518517247066
iteration : 13380
train acc:  0.8515625
train loss:  0.2930816113948822
train gradient:  0.13403385188331665
iteration : 13381
train acc:  0.875
train loss:  0.262226939201355
train gradient:  0.08879881603877277
iteration : 13382
train acc:  0.8046875
train loss:  0.3484352231025696
train gradient:  0.13396698680177277
iteration : 13383
train acc:  0.890625
train loss:  0.2515825033187866
train gradient:  0.13387862964112007
iteration : 13384
train acc:  0.84375
train loss:  0.34706854820251465
train gradient:  0.19884519178829663
iteration : 13385
train acc:  0.8828125
train loss:  0.2566611170768738
train gradient:  0.1045523269805965
iteration : 13386
train acc:  0.8671875
train loss:  0.3385700583457947
train gradient:  0.12506267164081908
iteration : 13387
train acc:  0.90625
train loss:  0.29167211055755615
train gradient:  0.10353605902953293
iteration : 13388
train acc:  0.8828125
train loss:  0.35994377732276917
train gradient:  0.17920318596442908
iteration : 13389
train acc:  0.890625
train loss:  0.25037360191345215
train gradient:  0.16778239631540667
iteration : 13390
train acc:  0.8671875
train loss:  0.3454267978668213
train gradient:  0.16245429263370884
iteration : 13391
train acc:  0.859375
train loss:  0.3406655192375183
train gradient:  0.20291220628275172
iteration : 13392
train acc:  0.8515625
train loss:  0.3128100335597992
train gradient:  0.1452885158390253
iteration : 13393
train acc:  0.8359375
train loss:  0.3624776303768158
train gradient:  0.15370864145946478
iteration : 13394
train acc:  0.859375
train loss:  0.31237852573394775
train gradient:  0.14468538989209329
iteration : 13395
train acc:  0.8984375
train loss:  0.2551462948322296
train gradient:  0.07440739432818329
iteration : 13396
train acc:  0.90625
train loss:  0.29773402214050293
train gradient:  0.14365439440019317
iteration : 13397
train acc:  0.875
train loss:  0.2981124520301819
train gradient:  0.11891791393473375
iteration : 13398
train acc:  0.8203125
train loss:  0.3574225902557373
train gradient:  0.23699068462091805
iteration : 13399
train acc:  0.8359375
train loss:  0.332655131816864
train gradient:  0.18046618783828194
iteration : 13400
train acc:  0.8828125
train loss:  0.28158634901046753
train gradient:  0.10980116473690703
iteration : 13401
train acc:  0.875
train loss:  0.2839827239513397
train gradient:  0.08637802916612498
iteration : 13402
train acc:  0.8359375
train loss:  0.39320868253707886
train gradient:  0.23142307405054682
iteration : 13403
train acc:  0.84375
train loss:  0.38580983877182007
train gradient:  0.14331864720951926
iteration : 13404
train acc:  0.8359375
train loss:  0.3771352171897888
train gradient:  0.16151644686552785
iteration : 13405
train acc:  0.8515625
train loss:  0.3300240635871887
train gradient:  0.1896028817385681
iteration : 13406
train acc:  0.8671875
train loss:  0.3346284329891205
train gradient:  0.13665102920044503
iteration : 13407
train acc:  0.8671875
train loss:  0.4085690379142761
train gradient:  0.1906479909540848
iteration : 13408
train acc:  0.8359375
train loss:  0.32214808464050293
train gradient:  0.2110161994612759
iteration : 13409
train acc:  0.859375
train loss:  0.31617313623428345
train gradient:  0.11883450939621729
iteration : 13410
train acc:  0.921875
train loss:  0.2729361355304718
train gradient:  0.11020288147122834
iteration : 13411
train acc:  0.828125
train loss:  0.2944611608982086
train gradient:  0.1740419453306128
iteration : 13412
train acc:  0.8515625
train loss:  0.3981148898601532
train gradient:  0.19205342506298867
iteration : 13413
train acc:  0.8515625
train loss:  0.30910855531692505
train gradient:  0.12192124203552628
iteration : 13414
train acc:  0.875
train loss:  0.3225671052932739
train gradient:  0.15171694545340667
iteration : 13415
train acc:  0.8828125
train loss:  0.2820271849632263
train gradient:  0.17274493783126627
iteration : 13416
train acc:  0.828125
train loss:  0.3956477642059326
train gradient:  0.2209496285026999
iteration : 13417
train acc:  0.828125
train loss:  0.36142104864120483
train gradient:  0.15346016274247448
iteration : 13418
train acc:  0.8359375
train loss:  0.3221983313560486
train gradient:  0.1439885331937712
iteration : 13419
train acc:  0.8125
train loss:  0.4230210781097412
train gradient:  0.20542443353792375
iteration : 13420
train acc:  0.8984375
train loss:  0.2698381245136261
train gradient:  0.1067530233525373
iteration : 13421
train acc:  0.90625
train loss:  0.22695085406303406
train gradient:  0.08577537404104171
iteration : 13422
train acc:  0.8359375
train loss:  0.3770764172077179
train gradient:  0.21934266965731097
iteration : 13423
train acc:  0.921875
train loss:  0.24255651235580444
train gradient:  0.12586694558637357
iteration : 13424
train acc:  0.8203125
train loss:  0.36772146821022034
train gradient:  0.11772089194216172
iteration : 13425
train acc:  0.828125
train loss:  0.377840518951416
train gradient:  0.1983972076134597
iteration : 13426
train acc:  0.84375
train loss:  0.34547320008277893
train gradient:  0.16146052330104665
iteration : 13427
train acc:  0.8828125
train loss:  0.308474063873291
train gradient:  0.16408944621101945
iteration : 13428
train acc:  0.890625
train loss:  0.3433963656425476
train gradient:  0.18833822406218753
iteration : 13429
train acc:  0.875
train loss:  0.31799203157424927
train gradient:  0.1345265168577855
iteration : 13430
train acc:  0.8828125
train loss:  0.3135565221309662
train gradient:  0.08842891789485816
iteration : 13431
train acc:  0.8359375
train loss:  0.358124703168869
train gradient:  0.13912073832054694
iteration : 13432
train acc:  0.875
train loss:  0.36385488510131836
train gradient:  0.18376631242955807
iteration : 13433
train acc:  0.8828125
train loss:  0.29735666513442993
train gradient:  0.12432850643071636
iteration : 13434
train acc:  0.859375
train loss:  0.3306872248649597
train gradient:  0.10323628744233018
iteration : 13435
train acc:  0.859375
train loss:  0.30215078592300415
train gradient:  0.1442612856925068
iteration : 13436
train acc:  0.8515625
train loss:  0.32024136185646057
train gradient:  0.16645025387371995
iteration : 13437
train acc:  0.875
train loss:  0.29565519094467163
train gradient:  0.1665390054972925
iteration : 13438
train acc:  0.875
train loss:  0.31365811824798584
train gradient:  0.119045324636975
iteration : 13439
train acc:  0.84375
train loss:  0.29498714208602905
train gradient:  0.09394802460985523
iteration : 13440
train acc:  0.8828125
train loss:  0.29319730401039124
train gradient:  0.10360642135381885
iteration : 13441
train acc:  0.8671875
train loss:  0.28485047817230225
train gradient:  0.15385420966753202
iteration : 13442
train acc:  0.875
train loss:  0.29109987616539
train gradient:  0.14128539841368098
iteration : 13443
train acc:  0.90625
train loss:  0.2644727826118469
train gradient:  0.12059693480181012
iteration : 13444
train acc:  0.828125
train loss:  0.3972264528274536
train gradient:  0.22534713783546767
iteration : 13445
train acc:  0.8515625
train loss:  0.33661025762557983
train gradient:  0.10930039373921559
iteration : 13446
train acc:  0.8828125
train loss:  0.2685811519622803
train gradient:  0.14946700177278582
iteration : 13447
train acc:  0.8671875
train loss:  0.296306848526001
train gradient:  0.13855015583818364
iteration : 13448
train acc:  0.8203125
train loss:  0.39008960127830505
train gradient:  0.17386280236095597
iteration : 13449
train acc:  0.8828125
train loss:  0.3119511902332306
train gradient:  0.22192706739566553
iteration : 13450
train acc:  0.84375
train loss:  0.3131502866744995
train gradient:  0.16519828306010909
iteration : 13451
train acc:  0.875
train loss:  0.37658900022506714
train gradient:  0.16211255441245698
iteration : 13452
train acc:  0.8515625
train loss:  0.30782967805862427
train gradient:  0.13286633816651716
iteration : 13453
train acc:  0.9140625
train loss:  0.27441221475601196
train gradient:  0.10190695401242275
iteration : 13454
train acc:  0.8203125
train loss:  0.3767189681529999
train gradient:  0.16442680037077811
iteration : 13455
train acc:  0.890625
train loss:  0.295756459236145
train gradient:  0.14919504846251247
iteration : 13456
train acc:  0.8984375
train loss:  0.24670743942260742
train gradient:  0.1074030440954996
iteration : 13457
train acc:  0.890625
train loss:  0.2811318635940552
train gradient:  0.09029893007830264
iteration : 13458
train acc:  0.8828125
train loss:  0.2832365036010742
train gradient:  0.10029931584358744
iteration : 13459
train acc:  0.8359375
train loss:  0.3101367950439453
train gradient:  0.14653333604676713
iteration : 13460
train acc:  0.796875
train loss:  0.382832407951355
train gradient:  0.22138948356441213
iteration : 13461
train acc:  0.890625
train loss:  0.27521732449531555
train gradient:  0.09035014195889556
iteration : 13462
train acc:  0.890625
train loss:  0.23805458843708038
train gradient:  0.06460768504810895
iteration : 13463
train acc:  0.859375
train loss:  0.2969503402709961
train gradient:  0.11355356665238978
iteration : 13464
train acc:  0.84375
train loss:  0.312483012676239
train gradient:  0.12698349612132084
iteration : 13465
train acc:  0.84375
train loss:  0.3236390948295593
train gradient:  0.17378531367530953
iteration : 13466
train acc:  0.828125
train loss:  0.42907023429870605
train gradient:  0.224558971618364
iteration : 13467
train acc:  0.8515625
train loss:  0.26838409900665283
train gradient:  0.0834351177303659
iteration : 13468
train acc:  0.859375
train loss:  0.31234222650527954
train gradient:  0.10424120109756613
iteration : 13469
train acc:  0.875
train loss:  0.40406912565231323
train gradient:  0.21207105445273744
iteration : 13470
train acc:  0.859375
train loss:  0.28713157773017883
train gradient:  0.10007490426933224
iteration : 13471
train acc:  0.8515625
train loss:  0.4045197367668152
train gradient:  0.19259641751250844
iteration : 13472
train acc:  0.921875
train loss:  0.2566561698913574
train gradient:  0.1417743152894999
iteration : 13473
train acc:  0.90625
train loss:  0.266956627368927
train gradient:  0.14978620216965355
iteration : 13474
train acc:  0.8359375
train loss:  0.3230023980140686
train gradient:  0.12557533806920695
iteration : 13475
train acc:  0.8671875
train loss:  0.272644579410553
train gradient:  0.1313962991892378
iteration : 13476
train acc:  0.890625
train loss:  0.22860008478164673
train gradient:  0.08138667304374589
iteration : 13477
train acc:  0.890625
train loss:  0.2581535279750824
train gradient:  0.07853584305126132
iteration : 13478
train acc:  0.8828125
train loss:  0.2431000918149948
train gradient:  0.12226976016060656
iteration : 13479
train acc:  0.859375
train loss:  0.28199052810668945
train gradient:  0.0792517900552991
iteration : 13480
train acc:  0.828125
train loss:  0.40461111068725586
train gradient:  0.19571335194181205
iteration : 13481
train acc:  0.828125
train loss:  0.4610740542411804
train gradient:  0.23944404999905572
iteration : 13482
train acc:  0.8359375
train loss:  0.3252076506614685
train gradient:  0.13163305986201598
iteration : 13483
train acc:  0.828125
train loss:  0.4397745132446289
train gradient:  0.3105473367970278
iteration : 13484
train acc:  0.890625
train loss:  0.2958908677101135
train gradient:  0.1110439885127347
iteration : 13485
train acc:  0.890625
train loss:  0.32150423526763916
train gradient:  0.21523536896686912
iteration : 13486
train acc:  0.84375
train loss:  0.302903413772583
train gradient:  0.13529536916731488
iteration : 13487
train acc:  0.8203125
train loss:  0.3594644069671631
train gradient:  0.12843951702762138
iteration : 13488
train acc:  0.875
train loss:  0.250865638256073
train gradient:  0.09406483481690384
iteration : 13489
train acc:  0.7890625
train loss:  0.4188551902770996
train gradient:  0.18021828528655842
iteration : 13490
train acc:  0.8828125
train loss:  0.2833525538444519
train gradient:  0.11452471770899474
iteration : 13491
train acc:  0.828125
train loss:  0.32810038328170776
train gradient:  0.15808777743351538
iteration : 13492
train acc:  0.875
train loss:  0.3121500015258789
train gradient:  0.12481244575189707
iteration : 13493
train acc:  0.90625
train loss:  0.25477659702301025
train gradient:  0.15297305280143006
iteration : 13494
train acc:  0.8203125
train loss:  0.3716147541999817
train gradient:  0.19256007995785332
iteration : 13495
train acc:  0.875
train loss:  0.2776407301425934
train gradient:  0.11542713204760484
iteration : 13496
train acc:  0.8671875
train loss:  0.3295954763889313
train gradient:  0.1282891974942439
iteration : 13497
train acc:  0.84375
train loss:  0.36734142899513245
train gradient:  0.15341963931830294
iteration : 13498
train acc:  0.875
train loss:  0.33455580472946167
train gradient:  0.11945637540277085
iteration : 13499
train acc:  0.859375
train loss:  0.3388044834136963
train gradient:  0.1273637810772349
iteration : 13500
train acc:  0.84375
train loss:  0.3025694191455841
train gradient:  0.11128886715448272
iteration : 13501
train acc:  0.8515625
train loss:  0.3260357975959778
train gradient:  0.13050688564530646
iteration : 13502
train acc:  0.8359375
train loss:  0.31315815448760986
train gradient:  0.1323199428025085
iteration : 13503
train acc:  0.796875
train loss:  0.4488009214401245
train gradient:  0.18543030239365588
iteration : 13504
train acc:  0.8359375
train loss:  0.335477739572525
train gradient:  0.15334540354670823
iteration : 13505
train acc:  0.8671875
train loss:  0.3393091559410095
train gradient:  0.1035962684286667
iteration : 13506
train acc:  0.90625
train loss:  0.30947989225387573
train gradient:  0.08365641787596412
iteration : 13507
train acc:  0.828125
train loss:  0.3575558662414551
train gradient:  0.15944959382788954
iteration : 13508
train acc:  0.890625
train loss:  0.3407615125179291
train gradient:  0.13987098520081392
iteration : 13509
train acc:  0.828125
train loss:  0.4246295690536499
train gradient:  0.18451244353779034
iteration : 13510
train acc:  0.84375
train loss:  0.38806092739105225
train gradient:  0.1690566651756102
iteration : 13511
train acc:  0.8671875
train loss:  0.3637509047985077
train gradient:  0.1333937459947524
iteration : 13512
train acc:  0.875
train loss:  0.3165397047996521
train gradient:  0.14535508247244103
iteration : 13513
train acc:  0.890625
train loss:  0.2916738986968994
train gradient:  0.10036384948412287
iteration : 13514
train acc:  0.8515625
train loss:  0.37005776166915894
train gradient:  0.12916907149099194
iteration : 13515
train acc:  0.875
train loss:  0.33300691843032837
train gradient:  0.25190636840332614
iteration : 13516
train acc:  0.8671875
train loss:  0.2991184592247009
train gradient:  0.11619103440720233
iteration : 13517
train acc:  0.8671875
train loss:  0.34870797395706177
train gradient:  0.1031694731609304
iteration : 13518
train acc:  0.8203125
train loss:  0.37345772981643677
train gradient:  0.17625005469536026
iteration : 13519
train acc:  0.8359375
train loss:  0.3140140771865845
train gradient:  0.11231561266072783
iteration : 13520
train acc:  0.8828125
train loss:  0.314208060503006
train gradient:  0.10696093778022371
iteration : 13521
train acc:  0.8828125
train loss:  0.2285270392894745
train gradient:  0.13729620073168142
iteration : 13522
train acc:  0.8828125
train loss:  0.31811076402664185
train gradient:  0.10596650055966593
iteration : 13523
train acc:  0.8984375
train loss:  0.282075971364975
train gradient:  0.10567079843449365
iteration : 13524
train acc:  0.875
train loss:  0.3219216763973236
train gradient:  0.13726221026839472
iteration : 13525
train acc:  0.90625
train loss:  0.2636635899543762
train gradient:  0.10469955579981775
iteration : 13526
train acc:  0.8828125
train loss:  0.2731998562812805
train gradient:  0.13545418304303686
iteration : 13527
train acc:  0.875
train loss:  0.3222850561141968
train gradient:  0.12153382550514685
iteration : 13528
train acc:  0.84375
train loss:  0.27078357338905334
train gradient:  0.10656917329452026
iteration : 13529
train acc:  0.84375
train loss:  0.33988720178604126
train gradient:  0.1471795809000857
iteration : 13530
train acc:  0.8828125
train loss:  0.2891085147857666
train gradient:  0.10443791365944859
iteration : 13531
train acc:  0.8515625
train loss:  0.31309983134269714
train gradient:  0.12693726563545357
iteration : 13532
train acc:  0.8515625
train loss:  0.32178765535354614
train gradient:  0.14659743730490132
iteration : 13533
train acc:  0.859375
train loss:  0.321353554725647
train gradient:  0.12545890469103216
iteration : 13534
train acc:  0.8515625
train loss:  0.3645324110984802
train gradient:  0.12264111603845743
iteration : 13535
train acc:  0.8125
train loss:  0.35798120498657227
train gradient:  0.12496164609796093
iteration : 13536
train acc:  0.8828125
train loss:  0.3775465488433838
train gradient:  0.20785434368835037
iteration : 13537
train acc:  0.9140625
train loss:  0.24240435659885406
train gradient:  0.13606232666523582
iteration : 13538
train acc:  0.90625
train loss:  0.2713963985443115
train gradient:  0.08587608607335967
iteration : 13539
train acc:  0.8984375
train loss:  0.2783154547214508
train gradient:  0.09005652926226856
iteration : 13540
train acc:  0.8828125
train loss:  0.3267788887023926
train gradient:  0.11565895593653365
iteration : 13541
train acc:  0.859375
train loss:  0.2919729948043823
train gradient:  0.14232997177184756
iteration : 13542
train acc:  0.8984375
train loss:  0.30363160371780396
train gradient:  0.08823723754668415
iteration : 13543
train acc:  0.8515625
train loss:  0.32271406054496765
train gradient:  0.1721506044790665
iteration : 13544
train acc:  0.859375
train loss:  0.29376205801963806
train gradient:  0.0937946165888263
iteration : 13545
train acc:  0.8125
train loss:  0.41330546140670776
train gradient:  0.17511676908071674
iteration : 13546
train acc:  0.859375
train loss:  0.3359139561653137
train gradient:  0.12757113959784136
iteration : 13547
train acc:  0.875
train loss:  0.35946667194366455
train gradient:  0.15935583908231643
iteration : 13548
train acc:  0.8359375
train loss:  0.37048059701919556
train gradient:  0.17739868524737343
iteration : 13549
train acc:  0.8671875
train loss:  0.3040505051612854
train gradient:  0.11576655240959532
iteration : 13550
train acc:  0.8828125
train loss:  0.285036563873291
train gradient:  0.10173578729508327
iteration : 13551
train acc:  0.8828125
train loss:  0.2875514030456543
train gradient:  0.139572610837238
iteration : 13552
train acc:  0.828125
train loss:  0.3730599284172058
train gradient:  0.1273630675639652
iteration : 13553
train acc:  0.84375
train loss:  0.3260078728199005
train gradient:  0.19117220815762426
iteration : 13554
train acc:  0.8828125
train loss:  0.2563574016094208
train gradient:  0.1408254457018501
iteration : 13555
train acc:  0.8046875
train loss:  0.3720352053642273
train gradient:  0.18159129105273458
iteration : 13556
train acc:  0.890625
train loss:  0.2568563222885132
train gradient:  0.1594339162815494
iteration : 13557
train acc:  0.9140625
train loss:  0.28063952922821045
train gradient:  0.11380398522527468
iteration : 13558
train acc:  0.84375
train loss:  0.386933296918869
train gradient:  0.18468056049096127
iteration : 13559
train acc:  0.8515625
train loss:  0.3359069228172302
train gradient:  0.13715903725894285
iteration : 13560
train acc:  0.796875
train loss:  0.32129454612731934
train gradient:  0.18615815037334996
iteration : 13561
train acc:  0.84375
train loss:  0.29807960987091064
train gradient:  0.15075475393389298
iteration : 13562
train acc:  0.859375
train loss:  0.33020466566085815
train gradient:  0.11736434541915211
iteration : 13563
train acc:  0.84375
train loss:  0.36891040205955505
train gradient:  0.12250954567982053
iteration : 13564
train acc:  0.890625
train loss:  0.31400632858276367
train gradient:  0.11078438246286908
iteration : 13565
train acc:  0.8671875
train loss:  0.33900755643844604
train gradient:  0.14271471623619328
iteration : 13566
train acc:  0.84375
train loss:  0.36421316862106323
train gradient:  0.12792001114375812
iteration : 13567
train acc:  0.875
train loss:  0.31941425800323486
train gradient:  0.09675752293659738
iteration : 13568
train acc:  0.921875
train loss:  0.24747754633426666
train gradient:  0.07851001514317178
iteration : 13569
train acc:  0.84375
train loss:  0.3837849497795105
train gradient:  0.12590471905222322
iteration : 13570
train acc:  0.828125
train loss:  0.3219057321548462
train gradient:  0.13127345806946628
iteration : 13571
train acc:  0.8515625
train loss:  0.3648393154144287
train gradient:  0.11172062369032494
iteration : 13572
train acc:  0.8203125
train loss:  0.361044317483902
train gradient:  0.1367031531977872
iteration : 13573
train acc:  0.796875
train loss:  0.3550910949707031
train gradient:  0.1471452582356147
iteration : 13574
train acc:  0.8515625
train loss:  0.3479679524898529
train gradient:  0.19239149876890127
iteration : 13575
train acc:  0.828125
train loss:  0.3465445339679718
train gradient:  0.10175855527950889
iteration : 13576
train acc:  0.8046875
train loss:  0.37284472584724426
train gradient:  0.16307608075295565
iteration : 13577
train acc:  0.8671875
train loss:  0.29802554845809937
train gradient:  0.12133368019999019
iteration : 13578
train acc:  0.84375
train loss:  0.34794917702674866
train gradient:  0.11611753898287668
iteration : 13579
train acc:  0.9140625
train loss:  0.24575133621692657
train gradient:  0.09101955752750635
iteration : 13580
train acc:  0.8828125
train loss:  0.3048521876335144
train gradient:  0.14490144198184535
iteration : 13581
train acc:  0.859375
train loss:  0.31520792841911316
train gradient:  0.1167864791662011
iteration : 13582
train acc:  0.859375
train loss:  0.3013903498649597
train gradient:  0.14044857223447443
iteration : 13583
train acc:  0.84375
train loss:  0.30670273303985596
train gradient:  0.12518451160481453
iteration : 13584
train acc:  0.8515625
train loss:  0.3204750418663025
train gradient:  0.14167695624707405
iteration : 13585
train acc:  0.8984375
train loss:  0.2854114770889282
train gradient:  0.08723940330323145
iteration : 13586
train acc:  0.8515625
train loss:  0.29733070731163025
train gradient:  0.1296878019323195
iteration : 13587
train acc:  0.84375
train loss:  0.3811165690422058
train gradient:  0.17333216383986286
iteration : 13588
train acc:  0.875
train loss:  0.3153771460056305
train gradient:  0.14926990742535867
iteration : 13589
train acc:  0.859375
train loss:  0.31511926651000977
train gradient:  0.11004377176966451
iteration : 13590
train acc:  0.875
train loss:  0.33372074365615845
train gradient:  0.13440170263195036
iteration : 13591
train acc:  0.8046875
train loss:  0.41299939155578613
train gradient:  0.24865178788327666
iteration : 13592
train acc:  0.8515625
train loss:  0.36518028378486633
train gradient:  0.13515103104764886
iteration : 13593
train acc:  0.859375
train loss:  0.28419381380081177
train gradient:  0.13653166982063197
iteration : 13594
train acc:  0.921875
train loss:  0.2889820337295532
train gradient:  0.14314692900573656
iteration : 13595
train acc:  0.8828125
train loss:  0.28314828872680664
train gradient:  0.10497299773768222
iteration : 13596
train acc:  0.8515625
train loss:  0.36462628841400146
train gradient:  0.15815287464893418
iteration : 13597
train acc:  0.8359375
train loss:  0.3342905640602112
train gradient:  0.11248845240585548
iteration : 13598
train acc:  0.8828125
train loss:  0.2535628378391266
train gradient:  0.07551870849233522
iteration : 13599
train acc:  0.8671875
train loss:  0.3443867266178131
train gradient:  0.1658965001833388
iteration : 13600
train acc:  0.828125
train loss:  0.3891485929489136
train gradient:  0.1454796600376504
iteration : 13601
train acc:  0.875
train loss:  0.3004688024520874
train gradient:  0.17305594041863792
iteration : 13602
train acc:  0.875
train loss:  0.28854942321777344
train gradient:  0.09188466434521905
iteration : 13603
train acc:  0.8828125
train loss:  0.2701094150543213
train gradient:  0.12009005760730039
iteration : 13604
train acc:  0.8671875
train loss:  0.3178339898586273
train gradient:  0.11996265795842899
iteration : 13605
train acc:  0.8984375
train loss:  0.2414691299200058
train gradient:  0.06163175044103461
iteration : 13606
train acc:  0.8515625
train loss:  0.3006740212440491
train gradient:  0.16893424891029374
iteration : 13607
train acc:  0.875
train loss:  0.3117469847202301
train gradient:  0.1149895659711752
iteration : 13608
train acc:  0.875
train loss:  0.25788742303848267
train gradient:  0.08356225784038256
iteration : 13609
train acc:  0.890625
train loss:  0.29499542713165283
train gradient:  0.1263051892846313
iteration : 13610
train acc:  0.9375
train loss:  0.2321072518825531
train gradient:  0.06915747354096549
iteration : 13611
train acc:  0.8671875
train loss:  0.2890666127204895
train gradient:  0.09924567900056784
iteration : 13612
train acc:  0.8828125
train loss:  0.2992885112762451
train gradient:  0.1033599131148123
iteration : 13613
train acc:  0.8515625
train loss:  0.28045618534088135
train gradient:  0.0978552374958365
iteration : 13614
train acc:  0.78125
train loss:  0.3706966042518616
train gradient:  0.12684406469610457
iteration : 13615
train acc:  0.9140625
train loss:  0.23690977692604065
train gradient:  0.10777195546263783
iteration : 13616
train acc:  0.8828125
train loss:  0.3049737513065338
train gradient:  0.10172363690443334
iteration : 13617
train acc:  0.921875
train loss:  0.22947536408901215
train gradient:  0.09165383896578957
iteration : 13618
train acc:  0.8671875
train loss:  0.2964859902858734
train gradient:  0.14413149498065408
iteration : 13619
train acc:  0.8671875
train loss:  0.2754940688610077
train gradient:  0.13711184941256518
iteration : 13620
train acc:  0.8515625
train loss:  0.3413408696651459
train gradient:  0.18873218615560527
iteration : 13621
train acc:  0.859375
train loss:  0.31378406286239624
train gradient:  0.13111858813366933
iteration : 13622
train acc:  0.8515625
train loss:  0.3176332116127014
train gradient:  0.16588911924890645
iteration : 13623
train acc:  0.8515625
train loss:  0.2871157228946686
train gradient:  0.08080716001002539
iteration : 13624
train acc:  0.8515625
train loss:  0.33653467893600464
train gradient:  0.17828690544922202
iteration : 13625
train acc:  0.8671875
train loss:  0.3661175072193146
train gradient:  0.1546663990189744
iteration : 13626
train acc:  0.890625
train loss:  0.24192124605178833
train gradient:  0.09962377105248085
iteration : 13627
train acc:  0.828125
train loss:  0.3260772228240967
train gradient:  0.12415934045154134
iteration : 13628
train acc:  0.875
train loss:  0.2833542227745056
train gradient:  0.12889306484338167
iteration : 13629
train acc:  0.875
train loss:  0.33890262246131897
train gradient:  0.13859072687332957
iteration : 13630
train acc:  0.8359375
train loss:  0.335847407579422
train gradient:  0.21192383294269412
iteration : 13631
train acc:  0.8984375
train loss:  0.2937471866607666
train gradient:  0.11204036869744194
iteration : 13632
train acc:  0.8828125
train loss:  0.30175769329071045
train gradient:  0.14738449540909054
iteration : 13633
train acc:  0.8828125
train loss:  0.26460063457489014
train gradient:  0.14646880174075685
iteration : 13634
train acc:  0.875
train loss:  0.2906225323677063
train gradient:  0.11909988829182526
iteration : 13635
train acc:  0.890625
train loss:  0.3182756304740906
train gradient:  0.1167904420531236
iteration : 13636
train acc:  0.890625
train loss:  0.2677598297595978
train gradient:  0.1147332889641917
iteration : 13637
train acc:  0.90625
train loss:  0.21907025575637817
train gradient:  0.09924422112331577
iteration : 13638
train acc:  0.890625
train loss:  0.26168036460876465
train gradient:  0.12274938140074906
iteration : 13639
train acc:  0.8359375
train loss:  0.33035504817962646
train gradient:  0.17676541894627656
iteration : 13640
train acc:  0.828125
train loss:  0.3742702007293701
train gradient:  0.15729376749112853
iteration : 13641
train acc:  0.90625
train loss:  0.22407926619052887
train gradient:  0.08359708354891458
iteration : 13642
train acc:  0.921875
train loss:  0.24534565210342407
train gradient:  0.11550684655364395
iteration : 13643
train acc:  0.875
train loss:  0.33732086420059204
train gradient:  0.23360314393127285
iteration : 13644
train acc:  0.8359375
train loss:  0.286854088306427
train gradient:  0.1522387566607049
iteration : 13645
train acc:  0.890625
train loss:  0.27332448959350586
train gradient:  0.12712907846427368
iteration : 13646
train acc:  0.8359375
train loss:  0.3805607259273529
train gradient:  0.15266059565277001
iteration : 13647
train acc:  0.859375
train loss:  0.3510515093803406
train gradient:  0.22726080314970792
iteration : 13648
train acc:  0.84375
train loss:  0.3437725305557251
train gradient:  0.1800686045759083
iteration : 13649
train acc:  0.84375
train loss:  0.38530707359313965
train gradient:  0.15881293546375835
iteration : 13650
train acc:  0.9140625
train loss:  0.240182563662529
train gradient:  0.08361490168217353
iteration : 13651
train acc:  0.8828125
train loss:  0.28248709440231323
train gradient:  0.0972807938919418
iteration : 13652
train acc:  0.7890625
train loss:  0.41334110498428345
train gradient:  0.22524968843922244
iteration : 13653
train acc:  0.8125
train loss:  0.4522230625152588
train gradient:  0.2692969040346344
iteration : 13654
train acc:  0.859375
train loss:  0.32073283195495605
train gradient:  0.172100976446897
iteration : 13655
train acc:  0.875
train loss:  0.3483443856239319
train gradient:  0.16889770242388127
iteration : 13656
train acc:  0.8984375
train loss:  0.23480868339538574
train gradient:  0.11586717663866575
iteration : 13657
train acc:  0.890625
train loss:  0.27884525060653687
train gradient:  0.12190335559598717
iteration : 13658
train acc:  0.8984375
train loss:  0.2867409884929657
train gradient:  0.11192756510955618
iteration : 13659
train acc:  0.8515625
train loss:  0.26334670186042786
train gradient:  0.10970803645636401
iteration : 13660
train acc:  0.828125
train loss:  0.40340951085090637
train gradient:  0.1580205729249913
iteration : 13661
train acc:  0.8828125
train loss:  0.29374217987060547
train gradient:  0.1287460253509929
iteration : 13662
train acc:  0.8515625
train loss:  0.39899271726608276
train gradient:  0.19401835580925042
iteration : 13663
train acc:  0.8671875
train loss:  0.30938804149627686
train gradient:  0.11385308738264145
iteration : 13664
train acc:  0.90625
train loss:  0.2522214651107788
train gradient:  0.11184954222974607
iteration : 13665
train acc:  0.8984375
train loss:  0.3015967607498169
train gradient:  0.15542400735379663
iteration : 13666
train acc:  0.8359375
train loss:  0.41628772020339966
train gradient:  0.1948453864546842
iteration : 13667
train acc:  0.828125
train loss:  0.3717891573905945
train gradient:  0.13215315296665137
iteration : 13668
train acc:  0.90625
train loss:  0.24014465510845184
train gradient:  0.10373811155758378
iteration : 13669
train acc:  0.8203125
train loss:  0.40152400732040405
train gradient:  0.20872080453332284
iteration : 13670
train acc:  0.84375
train loss:  0.3173111081123352
train gradient:  0.16569939131972727
iteration : 13671
train acc:  0.84375
train loss:  0.37167036533355713
train gradient:  0.1355944954339649
iteration : 13672
train acc:  0.8125
train loss:  0.3669382631778717
train gradient:  0.25651281968609646
iteration : 13673
train acc:  0.828125
train loss:  0.33291763067245483
train gradient:  0.09960190247186824
iteration : 13674
train acc:  0.875
train loss:  0.34149813652038574
train gradient:  0.13674665925445034
iteration : 13675
train acc:  0.84375
train loss:  0.4002659320831299
train gradient:  0.21785346428743296
iteration : 13676
train acc:  0.8125
train loss:  0.39442068338394165
train gradient:  0.22637209447837675
iteration : 13677
train acc:  0.859375
train loss:  0.3765438497066498
train gradient:  0.14267669388785525
iteration : 13678
train acc:  0.8984375
train loss:  0.2616386413574219
train gradient:  0.07741046779229001
iteration : 13679
train acc:  0.90625
train loss:  0.23079752922058105
train gradient:  0.08872113246231533
iteration : 13680
train acc:  0.828125
train loss:  0.35659801959991455
train gradient:  0.15914736648205718
iteration : 13681
train acc:  0.90625
train loss:  0.21971017122268677
train gradient:  0.08542622499216794
iteration : 13682
train acc:  0.890625
train loss:  0.2647448778152466
train gradient:  0.10144714070609522
iteration : 13683
train acc:  0.8515625
train loss:  0.29829341173171997
train gradient:  0.1336371415651457
iteration : 13684
train acc:  0.78125
train loss:  0.4730513095855713
train gradient:  0.27555618861380826
iteration : 13685
train acc:  0.8515625
train loss:  0.3418045938014984
train gradient:  0.15845800260236664
iteration : 13686
train acc:  0.8359375
train loss:  0.38689231872558594
train gradient:  0.28865309841511366
iteration : 13687
train acc:  0.828125
train loss:  0.329248309135437
train gradient:  0.11344065108639315
iteration : 13688
train acc:  0.8203125
train loss:  0.40278130769729614
train gradient:  0.1734051776434097
iteration : 13689
train acc:  0.8828125
train loss:  0.2628840208053589
train gradient:  0.12803882629447816
iteration : 13690
train acc:  0.90625
train loss:  0.288568913936615
train gradient:  0.11364272804900084
iteration : 13691
train acc:  0.8828125
train loss:  0.27094727754592896
train gradient:  0.15441042178386383
iteration : 13692
train acc:  0.8359375
train loss:  0.3532930612564087
train gradient:  0.10485190110196375
iteration : 13693
train acc:  0.8671875
train loss:  0.3114919662475586
train gradient:  0.1833851850756652
iteration : 13694
train acc:  0.875
train loss:  0.27760225534439087
train gradient:  0.10248079562052652
iteration : 13695
train acc:  0.875
train loss:  0.30731499195098877
train gradient:  0.13138859585772905
iteration : 13696
train acc:  0.8828125
train loss:  0.2850990295410156
train gradient:  0.1605591527466543
iteration : 13697
train acc:  0.8671875
train loss:  0.2809280753135681
train gradient:  0.09951189791645429
iteration : 13698
train acc:  0.8359375
train loss:  0.3696002662181854
train gradient:  0.18166471349703997
iteration : 13699
train acc:  0.8515625
train loss:  0.33798590302467346
train gradient:  0.1302978406585542
iteration : 13700
train acc:  0.8671875
train loss:  0.29899531602859497
train gradient:  0.1232381802831289
iteration : 13701
train acc:  0.90625
train loss:  0.22152608633041382
train gradient:  0.09263084329285601
iteration : 13702
train acc:  0.828125
train loss:  0.36733758449554443
train gradient:  0.2676524757432334
iteration : 13703
train acc:  0.8828125
train loss:  0.3040362000465393
train gradient:  0.12035588418608538
iteration : 13704
train acc:  0.8359375
train loss:  0.3885759115219116
train gradient:  0.24738642828378818
iteration : 13705
train acc:  0.890625
train loss:  0.29747965931892395
train gradient:  0.10384037984247488
iteration : 13706
train acc:  0.859375
train loss:  0.2999812364578247
train gradient:  0.12168258799792075
iteration : 13707
train acc:  0.828125
train loss:  0.31365838646888733
train gradient:  0.22207326302524905
iteration : 13708
train acc:  0.859375
train loss:  0.2923766076564789
train gradient:  0.15311445346538852
iteration : 13709
train acc:  0.875
train loss:  0.3058438003063202
train gradient:  0.12185524340661748
iteration : 13710
train acc:  0.796875
train loss:  0.4354686141014099
train gradient:  0.24908238578155967
iteration : 13711
train acc:  0.8203125
train loss:  0.4028301239013672
train gradient:  0.17211068535424567
iteration : 13712
train acc:  0.875
train loss:  0.2987444996833801
train gradient:  0.09644336978913026
iteration : 13713
train acc:  0.828125
train loss:  0.4094368517398834
train gradient:  0.1842895002993999
iteration : 13714
train acc:  0.8828125
train loss:  0.3259589672088623
train gradient:  0.14200190901588555
iteration : 13715
train acc:  0.8359375
train loss:  0.3511199951171875
train gradient:  0.1300945105001777
iteration : 13716
train acc:  0.875
train loss:  0.3131444454193115
train gradient:  0.11235070111165757
iteration : 13717
train acc:  0.8359375
train loss:  0.31703299283981323
train gradient:  0.15568406751070324
iteration : 13718
train acc:  0.8671875
train loss:  0.2933671176433563
train gradient:  0.09991713266814417
iteration : 13719
train acc:  0.859375
train loss:  0.33332884311676025
train gradient:  0.1262845906660493
iteration : 13720
train acc:  0.8671875
train loss:  0.32244300842285156
train gradient:  0.11283953387214544
iteration : 13721
train acc:  0.859375
train loss:  0.308552086353302
train gradient:  0.17735328485315138
iteration : 13722
train acc:  0.8984375
train loss:  0.27387604117393494
train gradient:  0.09685723838677368
iteration : 13723
train acc:  0.9140625
train loss:  0.2694546580314636
train gradient:  0.12647440633217785
iteration : 13724
train acc:  0.8984375
train loss:  0.2320886254310608
train gradient:  0.12172543394692999
iteration : 13725
train acc:  0.8125
train loss:  0.3687005639076233
train gradient:  0.18526005863611908
iteration : 13726
train acc:  0.875
train loss:  0.2723071873188019
train gradient:  0.10964627653930117
iteration : 13727
train acc:  0.8515625
train loss:  0.32934170961380005
train gradient:  0.13135002762843975
iteration : 13728
train acc:  0.8125
train loss:  0.376258909702301
train gradient:  0.14024246057555778
iteration : 13729
train acc:  0.84375
train loss:  0.28374749422073364
train gradient:  0.11838180222735245
iteration : 13730
train acc:  0.8515625
train loss:  0.35490015149116516
train gradient:  0.13965550961273776
iteration : 13731
train acc:  0.890625
train loss:  0.2776104211807251
train gradient:  0.2520709710884089
iteration : 13732
train acc:  0.90625
train loss:  0.27047765254974365
train gradient:  0.09829122480100334
iteration : 13733
train acc:  0.8125
train loss:  0.3778315782546997
train gradient:  0.17564670942545194
iteration : 13734
train acc:  0.859375
train loss:  0.3298392593860626
train gradient:  0.1439887240889309
iteration : 13735
train acc:  0.8515625
train loss:  0.3086332380771637
train gradient:  0.21556063142394777
iteration : 13736
train acc:  0.8828125
train loss:  0.34142547845840454
train gradient:  0.18293228354207602
iteration : 13737
train acc:  0.9140625
train loss:  0.23807471990585327
train gradient:  0.07936036213242639
iteration : 13738
train acc:  0.8515625
train loss:  0.33242639899253845
train gradient:  0.11925943014287599
iteration : 13739
train acc:  0.8046875
train loss:  0.4078713655471802
train gradient:  0.16148189497304188
iteration : 13740
train acc:  0.890625
train loss:  0.25701969861984253
train gradient:  0.08595511244228844
iteration : 13741
train acc:  0.921875
train loss:  0.2497941553592682
train gradient:  0.18168623245155896
iteration : 13742
train acc:  0.8515625
train loss:  0.33529308438301086
train gradient:  0.1297599733004571
iteration : 13743
train acc:  0.8359375
train loss:  0.34946829080581665
train gradient:  0.12902174732073163
iteration : 13744
train acc:  0.8359375
train loss:  0.33510228991508484
train gradient:  0.11582357365746329
iteration : 13745
train acc:  0.8203125
train loss:  0.36947914958000183
train gradient:  0.15489325882628507
iteration : 13746
train acc:  0.875
train loss:  0.26657921075820923
train gradient:  0.11507198259045222
iteration : 13747
train acc:  0.828125
train loss:  0.3804306983947754
train gradient:  0.1806958525410093
iteration : 13748
train acc:  0.8359375
train loss:  0.37192198634147644
train gradient:  0.18384956415419945
iteration : 13749
train acc:  0.90625
train loss:  0.26431798934936523
train gradient:  0.09391854244012915
iteration : 13750
train acc:  0.859375
train loss:  0.31213313341140747
train gradient:  0.167431537081761
iteration : 13751
train acc:  0.8671875
train loss:  0.26170796155929565
train gradient:  0.0890444996081724
iteration : 13752
train acc:  0.875
train loss:  0.33208736777305603
train gradient:  0.10273955571408545
iteration : 13753
train acc:  0.8359375
train loss:  0.3290163278579712
train gradient:  0.15847999821576408
iteration : 13754
train acc:  0.859375
train loss:  0.3132762312889099
train gradient:  0.11483522914067262
iteration : 13755
train acc:  0.8984375
train loss:  0.2770141363143921
train gradient:  0.07246230536756439
iteration : 13756
train acc:  0.84375
train loss:  0.32797589898109436
train gradient:  0.15475900085812486
iteration : 13757
train acc:  0.84375
train loss:  0.35075080394744873
train gradient:  0.151000385899074
iteration : 13758
train acc:  0.8984375
train loss:  0.3262995481491089
train gradient:  0.08588282016023306
iteration : 13759
train acc:  0.8515625
train loss:  0.35554081201553345
train gradient:  0.25103479434743514
iteration : 13760
train acc:  0.828125
train loss:  0.2976987659931183
train gradient:  0.11916966047593355
iteration : 13761
train acc:  0.890625
train loss:  0.2652350962162018
train gradient:  0.09029276079576576
iteration : 13762
train acc:  0.9140625
train loss:  0.29368436336517334
train gradient:  0.09370877834756253
iteration : 13763
train acc:  0.90625
train loss:  0.24618175625801086
train gradient:  0.06563121549861405
iteration : 13764
train acc:  0.8828125
train loss:  0.299704909324646
train gradient:  0.12458900359904852
iteration : 13765
train acc:  0.8828125
train loss:  0.262184202671051
train gradient:  0.09718320648084466
iteration : 13766
train acc:  0.8046875
train loss:  0.4073556363582611
train gradient:  0.18010681068329165
iteration : 13767
train acc:  0.8984375
train loss:  0.2861224412918091
train gradient:  0.11558133135843009
iteration : 13768
train acc:  0.859375
train loss:  0.2914124131202698
train gradient:  0.2396217094022023
iteration : 13769
train acc:  0.8671875
train loss:  0.30888909101486206
train gradient:  0.1598023763983516
iteration : 13770
train acc:  0.890625
train loss:  0.32616642117500305
train gradient:  0.11536898098921795
iteration : 13771
train acc:  0.890625
train loss:  0.2459428906440735
train gradient:  0.07449597634539717
iteration : 13772
train acc:  0.921875
train loss:  0.2520115077495575
train gradient:  0.11987616399941775
iteration : 13773
train acc:  0.8671875
train loss:  0.3200891613960266
train gradient:  0.13951231515615103
iteration : 13774
train acc:  0.8671875
train loss:  0.33352601528167725
train gradient:  0.11400238273256937
iteration : 13775
train acc:  0.8828125
train loss:  0.2724376618862152
train gradient:  0.11182348066110762
iteration : 13776
train acc:  0.90625
train loss:  0.23085558414459229
train gradient:  0.08040701896680322
iteration : 13777
train acc:  0.7578125
train loss:  0.4226820468902588
train gradient:  0.17516562588954554
iteration : 13778
train acc:  0.8515625
train loss:  0.31953179836273193
train gradient:  0.11064636158155809
iteration : 13779
train acc:  0.875
train loss:  0.2745492160320282
train gradient:  0.1175419516337059
iteration : 13780
train acc:  0.8515625
train loss:  0.31256505846977234
train gradient:  0.1271157827302769
iteration : 13781
train acc:  0.84375
train loss:  0.3386154770851135
train gradient:  0.1421278521608142
iteration : 13782
train acc:  0.84375
train loss:  0.3454435169696808
train gradient:  0.14270741003873816
iteration : 13783
train acc:  0.8203125
train loss:  0.39644524455070496
train gradient:  0.19619304702211376
iteration : 13784
train acc:  0.859375
train loss:  0.283027321100235
train gradient:  0.1311515351375481
iteration : 13785
train acc:  0.8671875
train loss:  0.2957417368888855
train gradient:  0.10191227613223686
iteration : 13786
train acc:  0.8359375
train loss:  0.3322244882583618
train gradient:  0.1609994175112863
iteration : 13787
train acc:  0.875
train loss:  0.24489271640777588
train gradient:  0.09324474243380702
iteration : 13788
train acc:  0.828125
train loss:  0.3336045742034912
train gradient:  0.15130324471101103
iteration : 13789
train acc:  0.7890625
train loss:  0.41923418641090393
train gradient:  0.24924960128827217
iteration : 13790
train acc:  0.8671875
train loss:  0.2912927269935608
train gradient:  0.11614406670870425
iteration : 13791
train acc:  0.859375
train loss:  0.2854926586151123
train gradient:  0.10627826729646082
iteration : 13792
train acc:  0.8359375
train loss:  0.31733834743499756
train gradient:  0.18684465191430677
iteration : 13793
train acc:  0.8671875
train loss:  0.30456554889678955
train gradient:  0.09365245938400643
iteration : 13794
train acc:  0.875
train loss:  0.2928432822227478
train gradient:  0.09141274828873824
iteration : 13795
train acc:  0.875
train loss:  0.3040177822113037
train gradient:  0.10785879302608499
iteration : 13796
train acc:  0.8359375
train loss:  0.32551124691963196
train gradient:  0.16629692840207447
iteration : 13797
train acc:  0.8671875
train loss:  0.2629867494106293
train gradient:  0.1083186385616905
iteration : 13798
train acc:  0.84375
train loss:  0.38849785923957825
train gradient:  0.20332518497593663
iteration : 13799
train acc:  0.8984375
train loss:  0.2570401728153229
train gradient:  0.09799632987719277
iteration : 13800
train acc:  0.890625
train loss:  0.2982373535633087
train gradient:  0.10577618086190525
iteration : 13801
train acc:  0.8515625
train loss:  0.32882726192474365
train gradient:  0.1003758168004238
iteration : 13802
train acc:  0.890625
train loss:  0.30134546756744385
train gradient:  0.11528259181498686
iteration : 13803
train acc:  0.8515625
train loss:  0.34422624111175537
train gradient:  0.19507365797721524
iteration : 13804
train acc:  0.8671875
train loss:  0.2866174280643463
train gradient:  0.08273014097380611
iteration : 13805
train acc:  0.84375
train loss:  0.3638492822647095
train gradient:  0.18844422184461807
iteration : 13806
train acc:  0.8671875
train loss:  0.35388100147247314
train gradient:  0.20264183316699272
iteration : 13807
train acc:  0.8359375
train loss:  0.4036324620246887
train gradient:  0.15188571192388034
iteration : 13808
train acc:  0.90625
train loss:  0.22226080298423767
train gradient:  0.05671816380812827
iteration : 13809
train acc:  0.8515625
train loss:  0.3392697274684906
train gradient:  0.09981997250326448
iteration : 13810
train acc:  0.921875
train loss:  0.2354319989681244
train gradient:  0.07305170582359855
iteration : 13811
train acc:  0.921875
train loss:  0.22144511342048645
train gradient:  0.0961137420169832
iteration : 13812
train acc:  0.859375
train loss:  0.335662305355072
train gradient:  0.11499146051686132
iteration : 13813
train acc:  0.8671875
train loss:  0.3831751346588135
train gradient:  0.1478381700192945
iteration : 13814
train acc:  0.8671875
train loss:  0.30430832505226135
train gradient:  0.09004496016133794
iteration : 13815
train acc:  0.8671875
train loss:  0.2898362874984741
train gradient:  0.12120861773179513
iteration : 13816
train acc:  0.921875
train loss:  0.23942908644676208
train gradient:  0.09959064212377793
iteration : 13817
train acc:  0.8671875
train loss:  0.33073264360427856
train gradient:  0.1400707506452094
iteration : 13818
train acc:  0.859375
train loss:  0.35946932435035706
train gradient:  0.15817032715564938
iteration : 13819
train acc:  0.84375
train loss:  0.36419016122817993
train gradient:  0.23439284766340318
iteration : 13820
train acc:  0.8515625
train loss:  0.3431140184402466
train gradient:  0.18901746996379354
iteration : 13821
train acc:  0.8828125
train loss:  0.2906785011291504
train gradient:  0.10937987679181081
iteration : 13822
train acc:  0.8984375
train loss:  0.2661312222480774
train gradient:  0.10078075981957386
iteration : 13823
train acc:  0.8671875
train loss:  0.29732781648635864
train gradient:  0.09775421001209537
iteration : 13824
train acc:  0.859375
train loss:  0.3147210478782654
train gradient:  0.11594243196330416
iteration : 13825
train acc:  0.8828125
train loss:  0.26131367683410645
train gradient:  0.10109664270642064
iteration : 13826
train acc:  0.875
train loss:  0.33401554822921753
train gradient:  0.1296398305924594
iteration : 13827
train acc:  0.8046875
train loss:  0.39777034521102905
train gradient:  0.17519329227087202
iteration : 13828
train acc:  0.859375
train loss:  0.349479615688324
train gradient:  0.17722694191441024
iteration : 13829
train acc:  0.8671875
train loss:  0.3455546498298645
train gradient:  0.16509410964852966
iteration : 13830
train acc:  0.859375
train loss:  0.3036172688007355
train gradient:  0.12801601950673988
iteration : 13831
train acc:  0.859375
train loss:  0.2766159772872925
train gradient:  0.1343189310254905
iteration : 13832
train acc:  0.8515625
train loss:  0.29817086458206177
train gradient:  0.11512146284199096
iteration : 13833
train acc:  0.84375
train loss:  0.3794322609901428
train gradient:  0.17745856661030418
iteration : 13834
train acc:  0.828125
train loss:  0.3566451072692871
train gradient:  0.18445031573891155
iteration : 13835
train acc:  0.875
train loss:  0.2974611520767212
train gradient:  0.1270881475529791
iteration : 13836
train acc:  0.8671875
train loss:  0.3294755220413208
train gradient:  0.14512530252954822
iteration : 13837
train acc:  0.8359375
train loss:  0.3486614525318146
train gradient:  0.15517507905510233
iteration : 13838
train acc:  0.875
train loss:  0.2998109459877014
train gradient:  0.1286429795895152
iteration : 13839
train acc:  0.8359375
train loss:  0.38081294298171997
train gradient:  0.17048490891325557
iteration : 13840
train acc:  0.8515625
train loss:  0.3778007924556732
train gradient:  0.16305671758636464
iteration : 13841
train acc:  0.84375
train loss:  0.31927669048309326
train gradient:  0.11031030979137747
iteration : 13842
train acc:  0.8359375
train loss:  0.3391544222831726
train gradient:  0.12452015092398884
iteration : 13843
train acc:  0.8203125
train loss:  0.4565942883491516
train gradient:  0.262694920181912
iteration : 13844
train acc:  0.859375
train loss:  0.3283565044403076
train gradient:  0.10648522143532003
iteration : 13845
train acc:  0.828125
train loss:  0.36568596959114075
train gradient:  0.15782118357373526
iteration : 13846
train acc:  0.8984375
train loss:  0.2520586848258972
train gradient:  0.1121240766273529
iteration : 13847
train acc:  0.859375
train loss:  0.323833703994751
train gradient:  0.20824834829781955
iteration : 13848
train acc:  0.8125
train loss:  0.38549625873565674
train gradient:  0.1637764230999213
iteration : 13849
train acc:  0.828125
train loss:  0.4339033365249634
train gradient:  0.28289021280706356
iteration : 13850
train acc:  0.8515625
train loss:  0.3477784991264343
train gradient:  0.1339257632457732
iteration : 13851
train acc:  0.8671875
train loss:  0.34612637758255005
train gradient:  0.18117857379045188
iteration : 13852
train acc:  0.84375
train loss:  0.343617707490921
train gradient:  0.1578087860845021
iteration : 13853
train acc:  0.8671875
train loss:  0.33336037397384644
train gradient:  0.28788732886001056
iteration : 13854
train acc:  0.828125
train loss:  0.3636581003665924
train gradient:  0.16657232941883718
iteration : 13855
train acc:  0.890625
train loss:  0.2601461708545685
train gradient:  0.09292963574636312
iteration : 13856
train acc:  0.8984375
train loss:  0.28206729888916016
train gradient:  0.05936709036722421
iteration : 13857
train acc:  0.890625
train loss:  0.31288060545921326
train gradient:  0.11858247819899156
iteration : 13858
train acc:  0.8359375
train loss:  0.3583836555480957
train gradient:  0.12064357150933358
iteration : 13859
train acc:  0.8203125
train loss:  0.37305915355682373
train gradient:  0.2919473655414145
iteration : 13860
train acc:  0.859375
train loss:  0.3562452495098114
train gradient:  0.1303546761079389
iteration : 13861
train acc:  0.8984375
train loss:  0.24585238099098206
train gradient:  0.08381922558970184
iteration : 13862
train acc:  0.8828125
train loss:  0.3145267963409424
train gradient:  0.13215008419843244
iteration : 13863
train acc:  0.84375
train loss:  0.31698766350746155
train gradient:  0.15159936950384423
iteration : 13864
train acc:  0.875
train loss:  0.254946768283844
train gradient:  0.08514899115668637
iteration : 13865
train acc:  0.90625
train loss:  0.22910268604755402
train gradient:  0.13442769440548286
iteration : 13866
train acc:  0.8515625
train loss:  0.30271559953689575
train gradient:  0.10890047147367193
iteration : 13867
train acc:  0.8828125
train loss:  0.3342549800872803
train gradient:  0.14666527155750303
iteration : 13868
train acc:  0.8671875
train loss:  0.3091355562210083
train gradient:  0.11703017714840405
iteration : 13869
train acc:  0.8671875
train loss:  0.27377840876579285
train gradient:  0.10169635123227928
iteration : 13870
train acc:  0.875
train loss:  0.30495190620422363
train gradient:  0.09314619049789442
iteration : 13871
train acc:  0.875
train loss:  0.3273371458053589
train gradient:  0.09374818146862295
iteration : 13872
train acc:  0.8359375
train loss:  0.3823097348213196
train gradient:  0.1518478890539185
iteration : 13873
train acc:  0.8671875
train loss:  0.32595962285995483
train gradient:  0.11779151300936057
iteration : 13874
train acc:  0.8359375
train loss:  0.33505779504776
train gradient:  0.1714733082357595
iteration : 13875
train acc:  0.8671875
train loss:  0.30451712012290955
train gradient:  0.09085348405654352
iteration : 13876
train acc:  0.8125
train loss:  0.3853919506072998
train gradient:  0.2138837981595641
iteration : 13877
train acc:  0.859375
train loss:  0.26928895711898804
train gradient:  0.1305568797877219
iteration : 13878
train acc:  0.8359375
train loss:  0.3621203303337097
train gradient:  0.1519367558662158
iteration : 13879
train acc:  0.890625
train loss:  0.27837073802948
train gradient:  0.1240712755971931
iteration : 13880
train acc:  0.890625
train loss:  0.267137348651886
train gradient:  0.09821532647967506
iteration : 13881
train acc:  0.84375
train loss:  0.3890898525714874
train gradient:  0.2508920342749627
iteration : 13882
train acc:  0.8359375
train loss:  0.33542555570602417
train gradient:  0.11988268381644031
iteration : 13883
train acc:  0.8203125
train loss:  0.36330586671829224
train gradient:  0.1327218672121745
iteration : 13884
train acc:  0.875
train loss:  0.33983081579208374
train gradient:  0.13564213910733186
iteration : 13885
train acc:  0.8671875
train loss:  0.32618799805641174
train gradient:  0.19068192031593142
iteration : 13886
train acc:  0.828125
train loss:  0.35458889603614807
train gradient:  0.13768438102630803
iteration : 13887
train acc:  0.84375
train loss:  0.36539649963378906
train gradient:  0.1609583354676812
iteration : 13888
train acc:  0.8828125
train loss:  0.2879536747932434
train gradient:  0.09981210530438593
iteration : 13889
train acc:  0.8515625
train loss:  0.3263726532459259
train gradient:  0.13110359726379778
iteration : 13890
train acc:  0.8515625
train loss:  0.3127354085445404
train gradient:  0.1160237919421383
iteration : 13891
train acc:  0.8515625
train loss:  0.39122307300567627
train gradient:  0.1892085213420259
iteration : 13892
train acc:  0.8671875
train loss:  0.31803661584854126
train gradient:  0.1372588718457576
iteration : 13893
train acc:  0.8515625
train loss:  0.2955086827278137
train gradient:  0.08360693975747784
iteration : 13894
train acc:  0.8203125
train loss:  0.3894997239112854
train gradient:  0.2341490272934692
iteration : 13895
train acc:  0.859375
train loss:  0.3465272784233093
train gradient:  0.10726549735570894
iteration : 13896
train acc:  0.890625
train loss:  0.23165932297706604
train gradient:  0.10719554904293493
iteration : 13897
train acc:  0.828125
train loss:  0.33693888783454895
train gradient:  0.15600024813505708
iteration : 13898
train acc:  0.84375
train loss:  0.3474041223526001
train gradient:  0.127035143750337
iteration : 13899
train acc:  0.8828125
train loss:  0.31347501277923584
train gradient:  0.12714787351372064
iteration : 13900
train acc:  0.8671875
train loss:  0.30053722858428955
train gradient:  0.0978989653016164
iteration : 13901
train acc:  0.875
train loss:  0.37164029479026794
train gradient:  0.17552119473398742
iteration : 13902
train acc:  0.8984375
train loss:  0.27925676107406616
train gradient:  0.10074492850503526
iteration : 13903
train acc:  0.8359375
train loss:  0.3786921203136444
train gradient:  0.23826425607150867
iteration : 13904
train acc:  0.8828125
train loss:  0.3192991614341736
train gradient:  0.15701611725083747
iteration : 13905
train acc:  0.8515625
train loss:  0.31822049617767334
train gradient:  0.11392846281777234
iteration : 13906
train acc:  0.9140625
train loss:  0.2450733482837677
train gradient:  0.09179352683477807
iteration : 13907
train acc:  0.8671875
train loss:  0.3348339796066284
train gradient:  0.14925642162342911
iteration : 13908
train acc:  0.859375
train loss:  0.3783840537071228
train gradient:  0.15909582865767308
iteration : 13909
train acc:  0.8515625
train loss:  0.3035099506378174
train gradient:  0.10977367234485073
iteration : 13910
train acc:  0.8671875
train loss:  0.2969263195991516
train gradient:  0.1054368598188737
iteration : 13911
train acc:  0.890625
train loss:  0.2881520390510559
train gradient:  0.10845551946703001
iteration : 13912
train acc:  0.8203125
train loss:  0.3825222849845886
train gradient:  0.19872975387228997
iteration : 13913
train acc:  0.828125
train loss:  0.3628309369087219
train gradient:  0.1681863733355666
iteration : 13914
train acc:  0.875
train loss:  0.32089319825172424
train gradient:  0.1335208362673093
iteration : 13915
train acc:  0.8671875
train loss:  0.27031853795051575
train gradient:  0.09547743063152242
iteration : 13916
train acc:  0.875
train loss:  0.3125187158584595
train gradient:  0.11784290079386889
iteration : 13917
train acc:  0.828125
train loss:  0.36282482743263245
train gradient:  0.1711596332382585
iteration : 13918
train acc:  0.8359375
train loss:  0.3730449676513672
train gradient:  0.15223142043617682
iteration : 13919
train acc:  0.8671875
train loss:  0.31246304512023926
train gradient:  0.12293434092328713
iteration : 13920
train acc:  0.8671875
train loss:  0.2900955080986023
train gradient:  0.10954435621798536
iteration : 13921
train acc:  0.8515625
train loss:  0.38689595460891724
train gradient:  0.15792937156160103
iteration : 13922
train acc:  0.8125
train loss:  0.4136163592338562
train gradient:  0.1862313697863967
iteration : 13923
train acc:  0.8671875
train loss:  0.3078111410140991
train gradient:  0.09793934999604151
iteration : 13924
train acc:  0.84375
train loss:  0.29462313652038574
train gradient:  0.14296182957031123
iteration : 13925
train acc:  0.84375
train loss:  0.4349830150604248
train gradient:  0.21854592646923277
iteration : 13926
train acc:  0.8359375
train loss:  0.3571048974990845
train gradient:  0.1337617635333534
iteration : 13927
train acc:  0.8828125
train loss:  0.33257895708084106
train gradient:  0.15254785308385668
iteration : 13928
train acc:  0.875
train loss:  0.3235872685909271
train gradient:  0.1425643769340626
iteration : 13929
train acc:  0.859375
train loss:  0.34276437759399414
train gradient:  0.13335872708616453
iteration : 13930
train acc:  0.8984375
train loss:  0.28270232677459717
train gradient:  0.11269073789121695
iteration : 13931
train acc:  0.8671875
train loss:  0.27711546421051025
train gradient:  0.124416524898881
iteration : 13932
train acc:  0.84375
train loss:  0.4227008521556854
train gradient:  0.28348003400770855
iteration : 13933
train acc:  0.84375
train loss:  0.3852476179599762
train gradient:  0.18873138030830203
iteration : 13934
train acc:  0.8203125
train loss:  0.37368881702423096
train gradient:  0.14528921840656212
iteration : 13935
train acc:  0.8359375
train loss:  0.3358064591884613
train gradient:  0.12972307788917006
iteration : 13936
train acc:  0.8671875
train loss:  0.36311107873916626
train gradient:  0.17645056049239247
iteration : 13937
train acc:  0.890625
train loss:  0.25628772377967834
train gradient:  0.0740694206421073
iteration : 13938
train acc:  0.8828125
train loss:  0.2759050726890564
train gradient:  0.11964051604903715
iteration : 13939
train acc:  0.828125
train loss:  0.3822162449359894
train gradient:  0.17047303380316958
iteration : 13940
train acc:  0.8671875
train loss:  0.31233054399490356
train gradient:  0.12312736475281466
iteration : 13941
train acc:  0.78125
train loss:  0.39807218313217163
train gradient:  0.2161586793288997
iteration : 13942
train acc:  0.828125
train loss:  0.3651111125946045
train gradient:  0.2068943769798349
iteration : 13943
train acc:  0.8359375
train loss:  0.36440396308898926
train gradient:  0.17369014903304567
iteration : 13944
train acc:  0.8046875
train loss:  0.37221822142601013
train gradient:  0.1414003931769171
iteration : 13945
train acc:  0.8828125
train loss:  0.3271183371543884
train gradient:  0.11808949822998137
iteration : 13946
train acc:  0.8359375
train loss:  0.3861582577228546
train gradient:  0.18779709749928775
iteration : 13947
train acc:  0.859375
train loss:  0.33396416902542114
train gradient:  0.14987504883333413
iteration : 13948
train acc:  0.90625
train loss:  0.3115823268890381
train gradient:  0.11035688423340761
iteration : 13949
train acc:  0.8671875
train loss:  0.3077490031719208
train gradient:  0.10865980504524964
iteration : 13950
train acc:  0.8984375
train loss:  0.27952343225479126
train gradient:  0.07924405813288393
iteration : 13951
train acc:  0.875
train loss:  0.3005087375640869
train gradient:  0.09659506226146601
iteration : 13952
train acc:  0.84375
train loss:  0.3340521454811096
train gradient:  0.1285178430281571
iteration : 13953
train acc:  0.859375
train loss:  0.30820387601852417
train gradient:  0.09795412952302036
iteration : 13954
train acc:  0.875
train loss:  0.26654869318008423
train gradient:  0.13386589205166832
iteration : 13955
train acc:  0.8828125
train loss:  0.2823489010334015
train gradient:  0.10410230826974579
iteration : 13956
train acc:  0.859375
train loss:  0.36093252897262573
train gradient:  0.1404753468446573
iteration : 13957
train acc:  0.875
train loss:  0.33278316259384155
train gradient:  0.11603970735500432
iteration : 13958
train acc:  0.8515625
train loss:  0.2623789310455322
train gradient:  0.08842750204547481
iteration : 13959
train acc:  0.8828125
train loss:  0.2754163146018982
train gradient:  0.08532472889883581
iteration : 13960
train acc:  0.8984375
train loss:  0.25231799483299255
train gradient:  0.0845071116812388
iteration : 13961
train acc:  0.875
train loss:  0.31230735778808594
train gradient:  0.15507800666356392
iteration : 13962
train acc:  0.8828125
train loss:  0.3086848556995392
train gradient:  0.09651211953617252
iteration : 13963
train acc:  0.875
train loss:  0.30196988582611084
train gradient:  0.13954278063559242
iteration : 13964
train acc:  0.875
train loss:  0.3081657886505127
train gradient:  0.11308322039787067
iteration : 13965
train acc:  0.8515625
train loss:  0.32787108421325684
train gradient:  0.12044525071620561
iteration : 13966
train acc:  0.859375
train loss:  0.3478533923625946
train gradient:  0.13279055041152338
iteration : 13967
train acc:  0.8203125
train loss:  0.40605199337005615
train gradient:  0.21759375568037873
iteration : 13968
train acc:  0.828125
train loss:  0.39133572578430176
train gradient:  0.1821402879421017
iteration : 13969
train acc:  0.9140625
train loss:  0.19660684466362
train gradient:  0.06476728748187174
iteration : 13970
train acc:  0.8515625
train loss:  0.3669407367706299
train gradient:  0.1397995367065888
iteration : 13971
train acc:  0.8515625
train loss:  0.322198748588562
train gradient:  0.17919723051930708
iteration : 13972
train acc:  0.875
train loss:  0.3157552182674408
train gradient:  0.11557801528945812
iteration : 13973
train acc:  0.8203125
train loss:  0.31860846281051636
train gradient:  0.10966774155415461
iteration : 13974
train acc:  0.8828125
train loss:  0.33453017473220825
train gradient:  0.14695817791697013
iteration : 13975
train acc:  0.84375
train loss:  0.33919772505760193
train gradient:  0.14177189060284776
iteration : 13976
train acc:  0.828125
train loss:  0.4077719748020172
train gradient:  0.2480524058769869
iteration : 13977
train acc:  0.84375
train loss:  0.3198329210281372
train gradient:  0.11737456782213909
iteration : 13978
train acc:  0.8671875
train loss:  0.26961103081703186
train gradient:  0.07366892104851151
iteration : 13979
train acc:  0.90625
train loss:  0.26560238003730774
train gradient:  0.13621271890936087
iteration : 13980
train acc:  0.8984375
train loss:  0.24978986382484436
train gradient:  0.09865042339966781
iteration : 13981
train acc:  0.875
train loss:  0.30123987793922424
train gradient:  0.1015779695467167
iteration : 13982
train acc:  0.90625
train loss:  0.2183689922094345
train gradient:  0.08236736531911126
iteration : 13983
train acc:  0.8828125
train loss:  0.2536006569862366
train gradient:  0.06942875380306505
iteration : 13984
train acc:  0.84375
train loss:  0.32457786798477173
train gradient:  0.17903761795288708
iteration : 13985
train acc:  0.8828125
train loss:  0.308028906583786
train gradient:  0.10861000705625162
iteration : 13986
train acc:  0.8046875
train loss:  0.3617950677871704
train gradient:  0.18098361015367104
iteration : 13987
train acc:  0.8984375
train loss:  0.2598983943462372
train gradient:  0.09163628565240275
iteration : 13988
train acc:  0.828125
train loss:  0.33514511585235596
train gradient:  0.12021115661837671
iteration : 13989
train acc:  0.859375
train loss:  0.28045108914375305
train gradient:  0.10340146376519174
iteration : 13990
train acc:  0.859375
train loss:  0.3852293789386749
train gradient:  0.19399478271584747
iteration : 13991
train acc:  0.8125
train loss:  0.3540925979614258
train gradient:  0.1294193363210016
iteration : 13992
train acc:  0.84375
train loss:  0.3631111979484558
train gradient:  0.14596568899294576
iteration : 13993
train acc:  0.875
train loss:  0.3593108355998993
train gradient:  0.1633340894889569
iteration : 13994
train acc:  0.8828125
train loss:  0.2885706424713135
train gradient:  0.116802462981863
iteration : 13995
train acc:  0.8984375
train loss:  0.329395055770874
train gradient:  0.14773539613587872
iteration : 13996
train acc:  0.84375
train loss:  0.36422109603881836
train gradient:  0.14584835736982818
iteration : 13997
train acc:  0.8828125
train loss:  0.31238871812820435
train gradient:  0.1636801494450435
iteration : 13998
train acc:  0.8359375
train loss:  0.3286389708518982
train gradient:  0.10437488825213205
iteration : 13999
train acc:  0.9140625
train loss:  0.243425190448761
train gradient:  0.08765401676841802
iteration : 14000
train acc:  0.8671875
train loss:  0.30663031339645386
train gradient:  0.1427615129114187
iteration : 14001
train acc:  0.8828125
train loss:  0.3462238907814026
train gradient:  0.11905019024817245
iteration : 14002
train acc:  0.8359375
train loss:  0.3163805603981018
train gradient:  0.12145606856664011
iteration : 14003
train acc:  0.84375
train loss:  0.3307126760482788
train gradient:  0.12954578398930017
iteration : 14004
train acc:  0.8828125
train loss:  0.2552151083946228
train gradient:  0.11439075920521796
iteration : 14005
train acc:  0.90625
train loss:  0.2458961009979248
train gradient:  0.09818875130300007
iteration : 14006
train acc:  0.84375
train loss:  0.3170481324195862
train gradient:  0.12330300107836967
iteration : 14007
train acc:  0.8828125
train loss:  0.29288148880004883
train gradient:  0.10155827022412793
iteration : 14008
train acc:  0.84375
train loss:  0.32811784744262695
train gradient:  0.17720161140327434
iteration : 14009
train acc:  0.890625
train loss:  0.2613033056259155
train gradient:  0.07713475079263563
iteration : 14010
train acc:  0.78125
train loss:  0.4487125277519226
train gradient:  0.19455628646772538
iteration : 14011
train acc:  0.8984375
train loss:  0.23454219102859497
train gradient:  0.07832141140341525
iteration : 14012
train acc:  0.828125
train loss:  0.337907612323761
train gradient:  0.14464479421928522
iteration : 14013
train acc:  0.8203125
train loss:  0.3586176633834839
train gradient:  0.15341926056205235
iteration : 14014
train acc:  0.796875
train loss:  0.4242702126502991
train gradient:  0.24586010389668156
iteration : 14015
train acc:  0.859375
train loss:  0.31740903854370117
train gradient:  0.11820490982075348
iteration : 14016
train acc:  0.875
train loss:  0.30428656935691833
train gradient:  0.10709602534945008
iteration : 14017
train acc:  0.8671875
train loss:  0.3208869695663452
train gradient:  0.13289155209979348
iteration : 14018
train acc:  0.9140625
train loss:  0.21330808103084564
train gradient:  0.07233196816849707
iteration : 14019
train acc:  0.828125
train loss:  0.3454224765300751
train gradient:  0.1691986611320394
iteration : 14020
train acc:  0.859375
train loss:  0.3349316716194153
train gradient:  0.13240498143328774
iteration : 14021
train acc:  0.9140625
train loss:  0.20930807292461395
train gradient:  0.0792756625776025
iteration : 14022
train acc:  0.8671875
train loss:  0.2858999967575073
train gradient:  0.09153180406017787
iteration : 14023
train acc:  0.921875
train loss:  0.2398325502872467
train gradient:  0.08305631189144368
iteration : 14024
train acc:  0.8203125
train loss:  0.4134044647216797
train gradient:  0.19833206993736163
iteration : 14025
train acc:  0.921875
train loss:  0.21760442852973938
train gradient:  0.08057278370230449
iteration : 14026
train acc:  0.8671875
train loss:  0.28484973311424255
train gradient:  0.15491593441745075
iteration : 14027
train acc:  0.890625
train loss:  0.28191661834716797
train gradient:  0.09337009010826762
iteration : 14028
train acc:  0.875
train loss:  0.28607475757598877
train gradient:  0.15791347946170758
iteration : 14029
train acc:  0.78125
train loss:  0.3645036816596985
train gradient:  0.2047633174197325
iteration : 14030
train acc:  0.8203125
train loss:  0.3583218455314636
train gradient:  0.1578540848364211
iteration : 14031
train acc:  0.890625
train loss:  0.25119107961654663
train gradient:  0.09998552000643403
iteration : 14032
train acc:  0.875
train loss:  0.2785802483558655
train gradient:  0.17053856777156468
iteration : 14033
train acc:  0.890625
train loss:  0.26632896065711975
train gradient:  0.11006801901090368
iteration : 14034
train acc:  0.84375
train loss:  0.3939393162727356
train gradient:  0.22951827998130875
iteration : 14035
train acc:  0.8671875
train loss:  0.2915101647377014
train gradient:  0.11553695605666911
iteration : 14036
train acc:  0.8671875
train loss:  0.3559107780456543
train gradient:  0.1361129512602212
iteration : 14037
train acc:  0.8671875
train loss:  0.3067607581615448
train gradient:  0.14624274933491926
iteration : 14038
train acc:  0.859375
train loss:  0.2854699492454529
train gradient:  0.13974313260126642
iteration : 14039
train acc:  0.875
train loss:  0.3549783527851105
train gradient:  0.14714658500356315
iteration : 14040
train acc:  0.859375
train loss:  0.30560219287872314
train gradient:  0.13700578770174537
iteration : 14041
train acc:  0.859375
train loss:  0.3127575218677521
train gradient:  0.13286961391612895
iteration : 14042
train acc:  0.8828125
train loss:  0.2903119921684265
train gradient:  0.09446360744730649
iteration : 14043
train acc:  0.8125
train loss:  0.5058770775794983
train gradient:  0.287454491096324
iteration : 14044
train acc:  0.796875
train loss:  0.4798280894756317
train gradient:  0.29486866197594475
iteration : 14045
train acc:  0.8671875
train loss:  0.3446234464645386
train gradient:  0.12614166611320715
iteration : 14046
train acc:  0.8671875
train loss:  0.27508649230003357
train gradient:  0.11816014655560836
iteration : 14047
train acc:  0.828125
train loss:  0.318469375371933
train gradient:  0.10443251596992689
iteration : 14048
train acc:  0.8359375
train loss:  0.3291914463043213
train gradient:  0.12496676878945236
iteration : 14049
train acc:  0.875
train loss:  0.2947242259979248
train gradient:  0.10596637186918206
iteration : 14050
train acc:  0.8828125
train loss:  0.3212512731552124
train gradient:  0.1260130221093351
iteration : 14051
train acc:  0.8828125
train loss:  0.275001585483551
train gradient:  0.08808980607551625
iteration : 14052
train acc:  0.8828125
train loss:  0.2686445713043213
train gradient:  0.13958884761595558
iteration : 14053
train acc:  0.90625
train loss:  0.2528826892375946
train gradient:  0.09369776374870778
iteration : 14054
train acc:  0.859375
train loss:  0.3606671690940857
train gradient:  0.11089361560488513
iteration : 14055
train acc:  0.859375
train loss:  0.25523173809051514
train gradient:  0.08395033444632669
iteration : 14056
train acc:  0.8359375
train loss:  0.3937946557998657
train gradient:  0.18540653168101678
iteration : 14057
train acc:  0.8828125
train loss:  0.2573501467704773
train gradient:  0.1256155310692093
iteration : 14058
train acc:  0.828125
train loss:  0.36402082443237305
train gradient:  0.180769957835624
iteration : 14059
train acc:  0.9296875
train loss:  0.26339536905288696
train gradient:  0.15592756870844643
iteration : 14060
train acc:  0.828125
train loss:  0.4412686824798584
train gradient:  0.23929341039540175
iteration : 14061
train acc:  0.890625
train loss:  0.28834033012390137
train gradient:  0.10910695149937989
iteration : 14062
train acc:  0.84375
train loss:  0.3510686159133911
train gradient:  0.20141041223235606
iteration : 14063
train acc:  0.9296875
train loss:  0.21727696061134338
train gradient:  0.09654542012980025
iteration : 14064
train acc:  0.875
train loss:  0.36829882860183716
train gradient:  0.130289203315599
iteration : 14065
train acc:  0.8984375
train loss:  0.29775333404541016
train gradient:  0.08231489088622747
iteration : 14066
train acc:  0.8671875
train loss:  0.34969258308410645
train gradient:  0.17392454746160346
iteration : 14067
train acc:  0.890625
train loss:  0.26865994930267334
train gradient:  0.07706940287864704
iteration : 14068
train acc:  0.84375
train loss:  0.31820300221443176
train gradient:  0.17294063400909704
iteration : 14069
train acc:  0.8359375
train loss:  0.3807324767112732
train gradient:  0.1558017521103136
iteration : 14070
train acc:  0.8984375
train loss:  0.30814260244369507
train gradient:  0.13368505334324718
iteration : 14071
train acc:  0.890625
train loss:  0.29946765303611755
train gradient:  0.1848587748212249
iteration : 14072
train acc:  0.8359375
train loss:  0.4130050539970398
train gradient:  0.18935665392428724
iteration : 14073
train acc:  0.9140625
train loss:  0.24847203493118286
train gradient:  0.08208448933850611
iteration : 14074
train acc:  0.8828125
train loss:  0.26695141196250916
train gradient:  0.08407329587098254
iteration : 14075
train acc:  0.875
train loss:  0.30283504724502563
train gradient:  0.1159130997108117
iteration : 14076
train acc:  0.84375
train loss:  0.3256034255027771
train gradient:  0.13262055876336534
iteration : 14077
train acc:  0.828125
train loss:  0.31871241331100464
train gradient:  0.13439916308873165
iteration : 14078
train acc:  0.859375
train loss:  0.29377076029777527
train gradient:  0.1307387574360671
iteration : 14079
train acc:  0.8671875
train loss:  0.30388686060905457
train gradient:  0.13520157988663267
iteration : 14080
train acc:  0.859375
train loss:  0.2948302626609802
train gradient:  0.11162309451914482
iteration : 14081
train acc:  0.8359375
train loss:  0.3911336362361908
train gradient:  0.17055297348125745
iteration : 14082
train acc:  0.875
train loss:  0.3492584228515625
train gradient:  0.213328869145744
iteration : 14083
train acc:  0.8515625
train loss:  0.3232700824737549
train gradient:  0.12758468130930753
iteration : 14084
train acc:  0.84375
train loss:  0.32851576805114746
train gradient:  0.17749456572032235
iteration : 14085
train acc:  0.8359375
train loss:  0.3810761570930481
train gradient:  0.1177525148825608
iteration : 14086
train acc:  0.859375
train loss:  0.2644376754760742
train gradient:  0.0813769753125333
iteration : 14087
train acc:  0.8359375
train loss:  0.33963465690612793
train gradient:  0.19275139849637513
iteration : 14088
train acc:  0.84375
train loss:  0.3536216616630554
train gradient:  0.18809654397061654
iteration : 14089
train acc:  0.84375
train loss:  0.3526648283004761
train gradient:  0.1453345492368654
iteration : 14090
train acc:  0.875
train loss:  0.3109363913536072
train gradient:  0.1258419670368561
iteration : 14091
train acc:  0.8515625
train loss:  0.3297751545906067
train gradient:  0.15970108209026096
iteration : 14092
train acc:  0.875
train loss:  0.29401516914367676
train gradient:  0.098045089261283
iteration : 14093
train acc:  0.84375
train loss:  0.3273907005786896
train gradient:  0.1863091255984216
iteration : 14094
train acc:  0.8671875
train loss:  0.38208359479904175
train gradient:  0.1762199313851826
iteration : 14095
train acc:  0.84375
train loss:  0.3760294020175934
train gradient:  0.18976279036580906
iteration : 14096
train acc:  0.875
train loss:  0.29614686965942383
train gradient:  0.12802424802146334
iteration : 14097
train acc:  0.8828125
train loss:  0.2744705379009247
train gradient:  0.10268267362721162
iteration : 14098
train acc:  0.9140625
train loss:  0.22366179525852203
train gradient:  0.0721443791767161
iteration : 14099
train acc:  0.8359375
train loss:  0.35835739970207214
train gradient:  0.12640275583885557
iteration : 14100
train acc:  0.8828125
train loss:  0.269655704498291
train gradient:  0.09105608436913996
iteration : 14101
train acc:  0.8125
train loss:  0.32397258281707764
train gradient:  0.18808176512792196
iteration : 14102
train acc:  0.84375
train loss:  0.3503805994987488
train gradient:  0.14819840624542485
iteration : 14103
train acc:  0.8828125
train loss:  0.23959937691688538
train gradient:  0.08121924434532057
iteration : 14104
train acc:  0.8828125
train loss:  0.2611139416694641
train gradient:  0.08422839729129133
iteration : 14105
train acc:  0.828125
train loss:  0.3929712772369385
train gradient:  0.18768903838959167
iteration : 14106
train acc:  0.8828125
train loss:  0.2840515375137329
train gradient:  0.08798694777071014
iteration : 14107
train acc:  0.8359375
train loss:  0.37283068895339966
train gradient:  0.18954377355207525
iteration : 14108
train acc:  0.84375
train loss:  0.286800742149353
train gradient:  0.11327085579536171
iteration : 14109
train acc:  0.875
train loss:  0.2715297341346741
train gradient:  0.10964187036778764
iteration : 14110
train acc:  0.8515625
train loss:  0.33469271659851074
train gradient:  0.15944344467014415
iteration : 14111
train acc:  0.8515625
train loss:  0.3284013569355011
train gradient:  0.16386201622657634
iteration : 14112
train acc:  0.859375
train loss:  0.3515324592590332
train gradient:  0.12379457997904918
iteration : 14113
train acc:  0.84375
train loss:  0.32023847103118896
train gradient:  0.10627796998669664
iteration : 14114
train acc:  0.8359375
train loss:  0.37351763248443604
train gradient:  0.1739984007038426
iteration : 14115
train acc:  0.8359375
train loss:  0.30048030614852905
train gradient:  0.11212757241758654
iteration : 14116
train acc:  0.8828125
train loss:  0.27194783091545105
train gradient:  0.11836626305209814
iteration : 14117
train acc:  0.859375
train loss:  0.367916077375412
train gradient:  0.14800183179307122
iteration : 14118
train acc:  0.8515625
train loss:  0.3036229610443115
train gradient:  0.12704961520068445
iteration : 14119
train acc:  0.8671875
train loss:  0.2866814136505127
train gradient:  0.10674614556675904
iteration : 14120
train acc:  0.8203125
train loss:  0.3201906681060791
train gradient:  0.1368931507506814
iteration : 14121
train acc:  0.8671875
train loss:  0.3532581031322479
train gradient:  0.1850085744204692
iteration : 14122
train acc:  0.8671875
train loss:  0.30081212520599365
train gradient:  0.13942325488471452
iteration : 14123
train acc:  0.859375
train loss:  0.27251189947128296
train gradient:  0.10199100915013373
iteration : 14124
train acc:  0.875
train loss:  0.31316077709198
train gradient:  0.135681222380697
iteration : 14125
train acc:  0.828125
train loss:  0.40328481793403625
train gradient:  0.23177223767192995
iteration : 14126
train acc:  0.8515625
train loss:  0.2915409207344055
train gradient:  0.12480751687063278
iteration : 14127
train acc:  0.84375
train loss:  0.31724780797958374
train gradient:  0.15358322605967095
iteration : 14128
train acc:  0.84375
train loss:  0.32902786135673523
train gradient:  0.14491662234477132
iteration : 14129
train acc:  0.859375
train loss:  0.35810786485671997
train gradient:  0.19065456234497474
iteration : 14130
train acc:  0.8828125
train loss:  0.27357542514801025
train gradient:  0.09301919432338654
iteration : 14131
train acc:  0.84375
train loss:  0.33167678117752075
train gradient:  0.13905433612125243
iteration : 14132
train acc:  0.8671875
train loss:  0.302343487739563
train gradient:  0.09048864151436849
iteration : 14133
train acc:  0.890625
train loss:  0.27035367488861084
train gradient:  0.11496709990591815
iteration : 14134
train acc:  0.8359375
train loss:  0.29268312454223633
train gradient:  0.10208530822590756
iteration : 14135
train acc:  0.84375
train loss:  0.394508957862854
train gradient:  0.2024377131949554
iteration : 14136
train acc:  0.890625
train loss:  0.30794137716293335
train gradient:  0.12030345440658259
iteration : 14137
train acc:  0.875
train loss:  0.2968217134475708
train gradient:  0.09244178020349318
iteration : 14138
train acc:  0.859375
train loss:  0.34967905282974243
train gradient:  0.11555065211669198
iteration : 14139
train acc:  0.84375
train loss:  0.29122647643089294
train gradient:  0.11243201600225985
iteration : 14140
train acc:  0.8828125
train loss:  0.3408162593841553
train gradient:  0.15882909891363584
iteration : 14141
train acc:  0.8984375
train loss:  0.27354735136032104
train gradient:  0.0833266602506378
iteration : 14142
train acc:  0.859375
train loss:  0.3328590989112854
train gradient:  0.14657595136829032
iteration : 14143
train acc:  0.9140625
train loss:  0.2557215988636017
train gradient:  0.09238668183218056
iteration : 14144
train acc:  0.8203125
train loss:  0.350311279296875
train gradient:  0.12214560932773076
iteration : 14145
train acc:  0.8359375
train loss:  0.32510459423065186
train gradient:  0.15571128891426794
iteration : 14146
train acc:  0.890625
train loss:  0.2589576840400696
train gradient:  0.09558293999349292
iteration : 14147
train acc:  0.890625
train loss:  0.325381338596344
train gradient:  0.13875835400474773
iteration : 14148
train acc:  0.8359375
train loss:  0.32231569290161133
train gradient:  0.10078838606888325
iteration : 14149
train acc:  0.90625
train loss:  0.23138807713985443
train gradient:  0.0763345973049277
iteration : 14150
train acc:  0.84375
train loss:  0.3089091181755066
train gradient:  0.10897983387198305
iteration : 14151
train acc:  0.8828125
train loss:  0.32413262128829956
train gradient:  0.17753089381132364
iteration : 14152
train acc:  0.890625
train loss:  0.37543851137161255
train gradient:  0.16710198544193997
iteration : 14153
train acc:  0.8359375
train loss:  0.3523179888725281
train gradient:  0.11990924376393636
iteration : 14154
train acc:  0.875
train loss:  0.2940172553062439
train gradient:  0.1480031007598434
iteration : 14155
train acc:  0.8515625
train loss:  0.36586135625839233
train gradient:  0.1521597060352529
iteration : 14156
train acc:  0.875
train loss:  0.2967836558818817
train gradient:  0.0872022700792297
iteration : 14157
train acc:  0.859375
train loss:  0.3300986588001251
train gradient:  0.1074282707071423
iteration : 14158
train acc:  0.921875
train loss:  0.24093365669250488
train gradient:  0.1069806040420321
iteration : 14159
train acc:  0.8125
train loss:  0.4037649631500244
train gradient:  0.24573504155233444
iteration : 14160
train acc:  0.8671875
train loss:  0.3051745891571045
train gradient:  0.11241364786890187
iteration : 14161
train acc:  0.8515625
train loss:  0.32543671131134033
train gradient:  0.1277592767410511
iteration : 14162
train acc:  0.828125
train loss:  0.40140780806541443
train gradient:  0.19704144304305643
iteration : 14163
train acc:  0.875
train loss:  0.3081081509590149
train gradient:  0.13397630132969832
iteration : 14164
train acc:  0.875
train loss:  0.2712813913822174
train gradient:  0.06990803326707022
iteration : 14165
train acc:  0.90625
train loss:  0.2494778335094452
train gradient:  0.09721100690653309
iteration : 14166
train acc:  0.8125
train loss:  0.37194836139678955
train gradient:  0.16905443897808767
iteration : 14167
train acc:  0.859375
train loss:  0.3274902105331421
train gradient:  0.1123619334678338
iteration : 14168
train acc:  0.8671875
train loss:  0.2870182991027832
train gradient:  0.1013129896769141
iteration : 14169
train acc:  0.84375
train loss:  0.3240688741207123
train gradient:  0.1497450987571627
iteration : 14170
train acc:  0.875
train loss:  0.284442663192749
train gradient:  0.1102908217255325
iteration : 14171
train acc:  0.859375
train loss:  0.2959991693496704
train gradient:  0.14233976393485911
iteration : 14172
train acc:  0.875
train loss:  0.2966409921646118
train gradient:  0.14383010935120544
iteration : 14173
train acc:  0.8984375
train loss:  0.24242714047431946
train gradient:  0.0827664387147541
iteration : 14174
train acc:  0.8671875
train loss:  0.27500540018081665
train gradient:  0.1264630234282419
iteration : 14175
train acc:  0.859375
train loss:  0.2928934693336487
train gradient:  0.11053098719996639
iteration : 14176
train acc:  0.8203125
train loss:  0.403251051902771
train gradient:  0.21699718116020444
iteration : 14177
train acc:  0.8515625
train loss:  0.2901006042957306
train gradient:  0.08839825561795966
iteration : 14178
train acc:  0.8671875
train loss:  0.2645246982574463
train gradient:  0.07631364090050992
iteration : 14179
train acc:  0.8515625
train loss:  0.3482055366039276
train gradient:  0.206072723421109
iteration : 14180
train acc:  0.8671875
train loss:  0.2698853015899658
train gradient:  0.12974958904467265
iteration : 14181
train acc:  0.859375
train loss:  0.32246649265289307
train gradient:  0.17402766019801474
iteration : 14182
train acc:  0.8515625
train loss:  0.3225679397583008
train gradient:  0.15904821409019532
iteration : 14183
train acc:  0.890625
train loss:  0.2990017533302307
train gradient:  0.07921673566317401
iteration : 14184
train acc:  0.8515625
train loss:  0.3707987666130066
train gradient:  0.2757428251164781
iteration : 14185
train acc:  0.8828125
train loss:  0.2986687421798706
train gradient:  0.1648473953532716
iteration : 14186
train acc:  1.0
train loss:  0.13213376700878143
train gradient:  0.18972214664009326
val acc:  0.867964709461515
val f1:  0.8685578718893544
val confusion matrix:  [[85145 13465]
 [12575 86035]]

----------------------------------------new_epoch--------------------------------------

epoch:  1
iteration : 0
train acc:  0.875
train loss:  0.29661422967910767
train gradient:  0.14326034354884837
iteration : 1
train acc:  0.8671875
train loss:  0.3155648708343506
train gradient:  0.1331696668837419
iteration : 2
train acc:  0.859375
train loss:  0.3225040137767792
train gradient:  0.16323600582906583
iteration : 3
train acc:  0.875
train loss:  0.3268461525440216
train gradient:  0.12146746432144011
iteration : 4
train acc:  0.8671875
train loss:  0.2795701026916504
train gradient:  0.09492617575710179
iteration : 5
train acc:  0.859375
train loss:  0.22638094425201416
train gradient:  0.08616987147111177
iteration : 6
train acc:  0.8671875
train loss:  0.30987194180488586
train gradient:  0.1307038008710996
iteration : 7
train acc:  0.875
train loss:  0.303455650806427
train gradient:  0.16125315414925306
iteration : 8
train acc:  0.890625
train loss:  0.20600640773773193
train gradient:  0.07692372030534626
iteration : 9
train acc:  0.8984375
train loss:  0.2924390733242035
train gradient:  0.09550718561246632
iteration : 10
train acc:  0.8984375
train loss:  0.31264162063598633
train gradient:  0.2528736621136366
iteration : 11
train acc:  0.890625
train loss:  0.2390335500240326
train gradient:  0.11809696242518522
iteration : 12
train acc:  0.875
train loss:  0.2881687581539154
train gradient:  0.15751746010484224
iteration : 13
train acc:  0.890625
train loss:  0.2721728980541229
train gradient:  0.1243738930698457
iteration : 14
train acc:  0.859375
train loss:  0.3163861632347107
train gradient:  0.24437323367495156
iteration : 15
train acc:  0.8359375
train loss:  0.3489192724227905
train gradient:  0.17183230254767184
iteration : 16
train acc:  0.890625
train loss:  0.27858954668045044
train gradient:  0.09770029555954152
iteration : 17
train acc:  0.890625
train loss:  0.33465489745140076
train gradient:  0.18083916872687783
iteration : 18
train acc:  0.8671875
train loss:  0.2873891294002533
train gradient:  0.1719585383111957
iteration : 19
train acc:  0.84375
train loss:  0.3295953571796417
train gradient:  0.13518804806017132
iteration : 20
train acc:  0.875
train loss:  0.30190569162368774
train gradient:  0.12076216623831137
iteration : 21
train acc:  0.8359375
train loss:  0.38795965909957886
train gradient:  0.16844920741836256
iteration : 22
train acc:  0.859375
train loss:  0.336955189704895
train gradient:  0.14023751736634477
iteration : 23
train acc:  0.8359375
train loss:  0.3509120047092438
train gradient:  0.12220996189930802
iteration : 24
train acc:  0.8671875
train loss:  0.3337549567222595
train gradient:  0.1363588631063425
iteration : 25
train acc:  0.84375
train loss:  0.2994576692581177
train gradient:  0.12775732602479156
iteration : 26
train acc:  0.875
train loss:  0.34435713291168213
train gradient:  0.17684127821251386
iteration : 27
train acc:  0.890625
train loss:  0.27146729826927185
train gradient:  0.07875832298624585
iteration : 28
train acc:  0.828125
train loss:  0.3604172170162201
train gradient:  0.1918911640128684
iteration : 29
train acc:  0.890625
train loss:  0.2511334717273712
train gradient:  0.10636875650329561
iteration : 30
train acc:  0.875
train loss:  0.3064480423927307
train gradient:  0.09392493735756467
iteration : 31
train acc:  0.8515625
train loss:  0.332725465297699
train gradient:  0.20511108717565965
iteration : 32
train acc:  0.859375
train loss:  0.3199743628501892
train gradient:  0.13971283170456195
iteration : 33
train acc:  0.890625
train loss:  0.2926023602485657
train gradient:  0.1342413384093376
iteration : 34
train acc:  0.8671875
train loss:  0.291975736618042
train gradient:  0.1496763946254741
iteration : 35
train acc:  0.8671875
train loss:  0.3234758973121643
train gradient:  0.1274517320311363
iteration : 36
train acc:  0.890625
train loss:  0.2798154950141907
train gradient:  0.12477609145963825
iteration : 37
train acc:  0.875
train loss:  0.25054749846458435
train gradient:  0.073566553663771
iteration : 38
train acc:  0.828125
train loss:  0.372539222240448
train gradient:  0.2349812981894449
iteration : 39
train acc:  0.8359375
train loss:  0.37365126609802246
train gradient:  0.19325020648561392
iteration : 40
train acc:  0.8828125
train loss:  0.25130486488342285
train gradient:  0.10448461098470539
iteration : 41
train acc:  0.8359375
train loss:  0.3756996989250183
train gradient:  0.23139170968016054
iteration : 42
train acc:  0.8359375
train loss:  0.35848402976989746
train gradient:  0.17958190405085495
iteration : 43
train acc:  0.84375
train loss:  0.298073410987854
train gradient:  0.11003511510198287
iteration : 44
train acc:  0.8515625
train loss:  0.3330432176589966
train gradient:  0.13893839912341985
iteration : 45
train acc:  0.8671875
train loss:  0.3312300443649292
train gradient:  0.18215973026202448
iteration : 46
train acc:  0.859375
train loss:  0.3469126522541046
train gradient:  0.11331651308020284
iteration : 47
train acc:  0.8828125
train loss:  0.2544748783111572
train gradient:  0.1284164157278338
iteration : 48
train acc:  0.875
train loss:  0.26953864097595215
train gradient:  0.12331198725511415
iteration : 49
train acc:  0.890625
train loss:  0.2502107620239258
train gradient:  0.0829467873887093
iteration : 50
train acc:  0.8828125
train loss:  0.3112596869468689
train gradient:  0.13598826721684443
iteration : 51
train acc:  0.890625
train loss:  0.2815224826335907
train gradient:  0.10828616199496778
iteration : 52
train acc:  0.9140625
train loss:  0.24956543743610382
train gradient:  0.07441384408661009
iteration : 53
train acc:  0.8671875
train loss:  0.3147946000099182
train gradient:  0.1367312030334119
iteration : 54
train acc:  0.828125
train loss:  0.3560538589954376
train gradient:  0.14847264480171057
iteration : 55
train acc:  0.84375
train loss:  0.33546775579452515
train gradient:  0.12616371549399297
iteration : 56
train acc:  0.875
train loss:  0.3686257004737854
train gradient:  0.2479750251206529
iteration : 57
train acc:  0.8515625
train loss:  0.30548083782196045
train gradient:  0.09725028498084494
iteration : 58
train acc:  0.875
train loss:  0.2834339141845703
train gradient:  0.1174733154694677
iteration : 59
train acc:  0.890625
train loss:  0.26920604705810547
train gradient:  0.08602332894010008
iteration : 60
train acc:  0.875
train loss:  0.3065580725669861
train gradient:  0.12448469501530562
iteration : 61
train acc:  0.8671875
train loss:  0.31423500180244446
train gradient:  0.09168927087069843
iteration : 62
train acc:  0.8203125
train loss:  0.40485092997550964
train gradient:  0.295437622909406
iteration : 63
train acc:  0.8984375
train loss:  0.2505786418914795
train gradient:  0.13250597235566605
iteration : 64
train acc:  0.8671875
train loss:  0.29468047618865967
train gradient:  0.09081766255460151
iteration : 65
train acc:  0.859375
train loss:  0.31940847635269165
train gradient:  0.17293259931085464
iteration : 66
train acc:  0.8828125
train loss:  0.28699713945388794
train gradient:  0.12646838996122653
iteration : 67
train acc:  0.8359375
train loss:  0.35536789894104004
train gradient:  0.14312105299944014
iteration : 68
train acc:  0.8359375
train loss:  0.358312726020813
train gradient:  0.17703415335313735
iteration : 69
train acc:  0.8515625
train loss:  0.30106663703918457
train gradient:  0.1245138575363554
iteration : 70
train acc:  0.84375
train loss:  0.32554715871810913
train gradient:  0.1544899577562281
iteration : 71
train acc:  0.859375
train loss:  0.3003814220428467
train gradient:  0.1567119418298833
iteration : 72
train acc:  0.875
train loss:  0.2814914584159851
train gradient:  0.13369863669736026
iteration : 73
train acc:  0.8984375
train loss:  0.2797902822494507
train gradient:  0.11191401125113624
iteration : 74
train acc:  0.890625
train loss:  0.261432945728302
train gradient:  0.10742636930668616
iteration : 75
train acc:  0.8359375
train loss:  0.3412064015865326
train gradient:  0.15065041474015364
iteration : 76
train acc:  0.90625
train loss:  0.20124408602714539
train gradient:  0.06640881627568979
iteration : 77
train acc:  0.8359375
train loss:  0.31702160835266113
train gradient:  0.1320588782953234
iteration : 78
train acc:  0.875
train loss:  0.29415667057037354
train gradient:  0.12299029408394316
iteration : 79
train acc:  0.921875
train loss:  0.2516349256038666
train gradient:  0.1331909129856772
iteration : 80
train acc:  0.8671875
train loss:  0.31050539016723633
train gradient:  0.1349528441426871
iteration : 81
train acc:  0.890625
train loss:  0.2754475176334381
train gradient:  0.08993004179333489
iteration : 82
train acc:  0.890625
train loss:  0.2588059902191162
train gradient:  0.13551136257178598
iteration : 83
train acc:  0.875
train loss:  0.3048761188983917
train gradient:  0.1760141816980211
iteration : 84
train acc:  0.9140625
train loss:  0.2382785528898239
train gradient:  0.08101445641632704
iteration : 85
train acc:  0.9140625
train loss:  0.21078245341777802
train gradient:  0.06930046933699772
iteration : 86
train acc:  0.921875
train loss:  0.22876060009002686
train gradient:  0.05861643349501288
iteration : 87
train acc:  0.921875
train loss:  0.2342965006828308
train gradient:  0.09658018381800684
iteration : 88
train acc:  0.84375
train loss:  0.36794313788414
train gradient:  0.2656415260874904
iteration : 89
train acc:  0.8671875
train loss:  0.3149121403694153
train gradient:  0.1560463238936533
iteration : 90
train acc:  0.875
train loss:  0.2928566336631775
train gradient:  0.1268500847896891
iteration : 91
train acc:  0.84375
train loss:  0.37832584977149963
train gradient:  0.17817625135336257
iteration : 92
train acc:  0.921875
train loss:  0.24369318783283234
train gradient:  0.07431379213041284
iteration : 93
train acc:  0.9140625
train loss:  0.26697736978530884
train gradient:  0.1528673748882262
iteration : 94
train acc:  0.859375
train loss:  0.3212219774723053
train gradient:  0.14437698310479735
iteration : 95
train acc:  0.8515625
train loss:  0.3395877480506897
train gradient:  0.1440949087152674
iteration : 96
train acc:  0.8984375
train loss:  0.3146147131919861
train gradient:  0.11013205174163233
iteration : 97
train acc:  0.875
train loss:  0.30617961287498474
train gradient:  0.12095119157719632
iteration : 98
train acc:  0.875
train loss:  0.34245842695236206
train gradient:  0.20035812956261345
iteration : 99
train acc:  0.84375
train loss:  0.35887712240219116
train gradient:  0.14024530487534148
iteration : 100
train acc:  0.890625
train loss:  0.2793533504009247
train gradient:  0.0954746589921144
iteration : 101
train acc:  0.8359375
train loss:  0.3277924954891205
train gradient:  0.10298967257849313
iteration : 102
train acc:  0.890625
train loss:  0.2795378565788269
train gradient:  0.10147087058156724
iteration : 103
train acc:  0.8828125
train loss:  0.31528180837631226
train gradient:  0.09613177172923182
iteration : 104
train acc:  0.8828125
train loss:  0.30714407563209534
train gradient:  0.12403714191698018
iteration : 105
train acc:  0.8828125
train loss:  0.27087727189064026
train gradient:  0.12297298896945823
iteration : 106
train acc:  0.8515625
train loss:  0.30200666189193726
train gradient:  0.12911605859423994
iteration : 107
train acc:  0.9140625
train loss:  0.23667608201503754
train gradient:  0.06425474208417095
iteration : 108
train acc:  0.9296875
train loss:  0.20754708349704742
train gradient:  0.08481955447829471
iteration : 109
train acc:  0.859375
train loss:  0.27835506200790405
train gradient:  0.09563562261271594
iteration : 110
train acc:  0.7890625
train loss:  0.38293617963790894
train gradient:  0.2227158334426222
iteration : 111
train acc:  0.921875
train loss:  0.18283897638320923
train gradient:  0.07547104222987637
iteration : 112
train acc:  0.9296875
train loss:  0.1868358850479126
train gradient:  0.0766042231265344
iteration : 113
train acc:  0.84375
train loss:  0.30518752336502075
train gradient:  0.15229060384446985
iteration : 114
train acc:  0.8515625
train loss:  0.3479872941970825
train gradient:  0.17270120910877837
iteration : 115
train acc:  0.84375
train loss:  0.2900959253311157
train gradient:  0.15591507029652837
iteration : 116
train acc:  0.8984375
train loss:  0.2518964409828186
train gradient:  0.1141584271388775
iteration : 117
train acc:  0.84375
train loss:  0.40958869457244873
train gradient:  0.3283576159614224
iteration : 118
train acc:  0.7890625
train loss:  0.4096733331680298
train gradient:  0.18841111409122901
iteration : 119
train acc:  0.90625
train loss:  0.24407729506492615
train gradient:  0.09906009026294742
iteration : 120
train acc:  0.8046875
train loss:  0.45863571763038635
train gradient:  0.2905143607913706
iteration : 121
train acc:  0.8984375
train loss:  0.29926323890686035
train gradient:  0.12399574962825119
iteration : 122
train acc:  0.8359375
train loss:  0.36115437746047974
train gradient:  0.1342402964642247
iteration : 123
train acc:  0.8984375
train loss:  0.2688235640525818
train gradient:  0.1195496241791323
iteration : 124
train acc:  0.8515625
train loss:  0.4170679450035095
train gradient:  0.206018090738606
iteration : 125
train acc:  0.859375
train loss:  0.28235745429992676
train gradient:  0.10229401694565492
iteration : 126
train acc:  0.8359375
train loss:  0.3230184316635132
train gradient:  0.12141690649377519
iteration : 127
train acc:  0.875
train loss:  0.23125675320625305
train gradient:  0.10365095664094978
iteration : 128
train acc:  0.8359375
train loss:  0.384103000164032
train gradient:  0.20627309461915466
iteration : 129
train acc:  0.8125
train loss:  0.39177143573760986
train gradient:  0.28999291650798975
iteration : 130
train acc:  0.875
train loss:  0.2962462902069092
train gradient:  0.13626945864134823
iteration : 131
train acc:  0.8125
train loss:  0.33598846197128296
train gradient:  0.13563643605982215
iteration : 132
train acc:  0.8984375
train loss:  0.23995153605937958
train gradient:  0.13371806466554734
iteration : 133
train acc:  0.859375
train loss:  0.3120475709438324
train gradient:  0.11385472629744456
iteration : 134
train acc:  0.8984375
train loss:  0.2710381746292114
train gradient:  0.13291124689154982
iteration : 135
train acc:  0.9296875
train loss:  0.26105600595474243
train gradient:  0.1349841095393827
iteration : 136
train acc:  0.8515625
train loss:  0.3344728648662567
train gradient:  0.25245662911830347
iteration : 137
train acc:  0.8828125
train loss:  0.3061848282814026
train gradient:  0.14294502227220118
iteration : 138
train acc:  0.7890625
train loss:  0.4959728419780731
train gradient:  0.2754541893827068
iteration : 139
train acc:  0.828125
train loss:  0.36570578813552856
train gradient:  0.21464970594765043
iteration : 140
train acc:  0.890625
train loss:  0.28396469354629517
train gradient:  0.10650239781262573
iteration : 141
train acc:  0.8125
train loss:  0.34852778911590576
train gradient:  0.1943583665272159
iteration : 142
train acc:  0.828125
train loss:  0.30941343307495117
train gradient:  0.14636048492701426
iteration : 143
train acc:  0.8515625
train loss:  0.33445343375205994
train gradient:  0.10597630574813993
iteration : 144
train acc:  0.890625
train loss:  0.2403799295425415
train gradient:  0.07842098142778692
iteration : 145
train acc:  0.859375
train loss:  0.34802892804145813
train gradient:  0.20878311828017726
iteration : 146
train acc:  0.8359375
train loss:  0.30523625016212463
train gradient:  0.1512824190147774
iteration : 147
train acc:  0.8671875
train loss:  0.27919042110443115
train gradient:  0.10297918535383375
iteration : 148
train acc:  0.8984375
train loss:  0.2495494931936264
train gradient:  0.08642722677029431
iteration : 149
train acc:  0.84375
train loss:  0.3354506492614746
train gradient:  0.19391841640470175
iteration : 150
train acc:  0.8671875
train loss:  0.2689164876937866
train gradient:  0.12350305734628532
iteration : 151
train acc:  0.84375
train loss:  0.3715958595275879
train gradient:  0.16936710509280245
iteration : 152
train acc:  0.84375
train loss:  0.32863807678222656
train gradient:  0.161103670053233
iteration : 153
train acc:  0.8125
train loss:  0.34450531005859375
train gradient:  0.13547112105267334
iteration : 154
train acc:  0.8515625
train loss:  0.3449529707431793
train gradient:  0.2101876592894323
iteration : 155
train acc:  0.8671875
train loss:  0.28199470043182373
train gradient:  0.11214068206177534
iteration : 156
train acc:  0.8671875
train loss:  0.32588350772857666
train gradient:  0.13694535719583645
iteration : 157
train acc:  0.90625
train loss:  0.30598923563957214
train gradient:  0.25891301452067655
iteration : 158
train acc:  0.859375
train loss:  0.30499541759490967
train gradient:  0.1445522186497884
iteration : 159
train acc:  0.875
train loss:  0.3365815281867981
train gradient:  0.1325222325664968
iteration : 160
train acc:  0.859375
train loss:  0.3236212134361267
train gradient:  0.14263993737689926
iteration : 161
train acc:  0.859375
train loss:  0.3527956008911133
train gradient:  0.1602985816198798
iteration : 162
train acc:  0.8984375
train loss:  0.2777046263217926
train gradient:  0.11675603238838582
iteration : 163
train acc:  0.8671875
train loss:  0.302031934261322
train gradient:  0.14742707542268774
iteration : 164
train acc:  0.875
train loss:  0.2628833055496216
train gradient:  0.09562476340480264
iteration : 165
train acc:  0.8359375
train loss:  0.3346409201622009
train gradient:  0.158867697461664
iteration : 166
train acc:  0.859375
train loss:  0.30247703194618225
train gradient:  0.12993852645827253
iteration : 167
train acc:  0.875
train loss:  0.35915347933769226
train gradient:  0.16840006472858307
iteration : 168
train acc:  0.90625
train loss:  0.2651383876800537
train gradient:  0.0969182004026584
iteration : 169
train acc:  0.859375
train loss:  0.3100033402442932
train gradient:  0.17723537436337655
iteration : 170
train acc:  0.8515625
train loss:  0.34537094831466675
train gradient:  0.18509722342869425
iteration : 171
train acc:  0.84375
train loss:  0.3114584684371948
train gradient:  0.14726924144710082
iteration : 172
train acc:  0.8671875
train loss:  0.286903440952301
train gradient:  0.11600776220715786
iteration : 173
train acc:  0.875
train loss:  0.3506573438644409
train gradient:  0.15062014548641373
iteration : 174
train acc:  0.828125
train loss:  0.48444899916648865
train gradient:  0.21616183929285546
iteration : 175
train acc:  0.828125
train loss:  0.3562580347061157
train gradient:  0.16268080201218435
iteration : 176
train acc:  0.8671875
train loss:  0.33102110028266907
train gradient:  0.17520649313998093
iteration : 177
train acc:  0.875
train loss:  0.2860548496246338
train gradient:  0.0986268455959985
iteration : 178
train acc:  0.8828125
train loss:  0.3471603989601135
train gradient:  0.16312629394957157
iteration : 179
train acc:  0.8359375
train loss:  0.3103870153427124
train gradient:  0.1373784858699729
iteration : 180
train acc:  0.84375
train loss:  0.41288137435913086
train gradient:  0.22138785076173267
iteration : 181
train acc:  0.8515625
train loss:  0.3397185802459717
train gradient:  0.144155621751075
iteration : 182
train acc:  0.8515625
train loss:  0.30222952365875244
train gradient:  0.11571176767892503
iteration : 183
train acc:  0.84375
train loss:  0.37742340564727783
train gradient:  0.21327386937778267
iteration : 184
train acc:  0.84375
train loss:  0.39632919430732727
train gradient:  0.17928732576327633
iteration : 185
train acc:  0.859375
train loss:  0.30991509556770325
train gradient:  0.15406805940537677
iteration : 186
train acc:  0.8984375
train loss:  0.28265777230262756
train gradient:  0.11762722918357996
iteration : 187
train acc:  0.90625
train loss:  0.2487797737121582
train gradient:  0.10418917196680914
iteration : 188
train acc:  0.890625
train loss:  0.25366806983947754
train gradient:  0.08553076165102362
iteration : 189
train acc:  0.8359375
train loss:  0.31413978338241577
train gradient:  0.1811091185434957
iteration : 190
train acc:  0.8359375
train loss:  0.42355257272720337
train gradient:  0.22402780948288864
iteration : 191
train acc:  0.890625
train loss:  0.2425832599401474
train gradient:  0.08596905119895605
iteration : 192
train acc:  0.875
train loss:  0.3201233148574829
train gradient:  0.0975622778433679
iteration : 193
train acc:  0.875
train loss:  0.33279815316200256
train gradient:  0.18228550021755208
iteration : 194
train acc:  0.859375
train loss:  0.2834495007991791
train gradient:  0.14469056318087076
iteration : 195
train acc:  0.8828125
train loss:  0.2433728128671646
train gradient:  0.13155329527698262
iteration : 196
train acc:  0.90625
train loss:  0.312439501285553
train gradient:  0.12126895341379898
iteration : 197
train acc:  0.8828125
train loss:  0.26275765895843506
train gradient:  0.14957450116081789
iteration : 198
train acc:  0.8515625
train loss:  0.299789160490036
train gradient:  0.10041930291708574
iteration : 199
train acc:  0.7890625
train loss:  0.3961745500564575
train gradient:  0.19404377774632509
iteration : 200
train acc:  0.8515625
train loss:  0.3458022475242615
train gradient:  0.1976383067597003
iteration : 201
train acc:  0.8515625
train loss:  0.35241472721099854
train gradient:  0.17145744863376555
iteration : 202
train acc:  0.859375
train loss:  0.30095088481903076
train gradient:  0.14449638613142324
iteration : 203
train acc:  0.8828125
train loss:  0.27467870712280273
train gradient:  0.07705943851812154
iteration : 204
train acc:  0.8359375
train loss:  0.31656545400619507
train gradient:  0.22182229358741165
iteration : 205
train acc:  0.8671875
train loss:  0.29321911931037903
train gradient:  0.11149612008294357
iteration : 206
train acc:  0.8671875
train loss:  0.33621278405189514
train gradient:  0.12223135557412639
iteration : 207
train acc:  0.8984375
train loss:  0.25559332966804504
train gradient:  0.08317182697365962
iteration : 208
train acc:  0.8359375
train loss:  0.37184736132621765
train gradient:  0.1388615952113567
iteration : 209
train acc:  0.8515625
train loss:  0.2661234140396118
train gradient:  0.13670671765244388
iteration : 210
train acc:  0.8046875
train loss:  0.409776508808136
train gradient:  0.23751655167589042
iteration : 211
train acc:  0.859375
train loss:  0.28084731101989746
train gradient:  0.07049385366244587
iteration : 212
train acc:  0.8984375
train loss:  0.28573212027549744
train gradient:  0.12387492977606814
iteration : 213
train acc:  0.9140625
train loss:  0.24971096217632294
train gradient:  0.08294665759859153
iteration : 214
train acc:  0.859375
train loss:  0.29770874977111816
train gradient:  0.12774403908753285
iteration : 215
train acc:  0.828125
train loss:  0.3530994951725006
train gradient:  0.15088363733629462
iteration : 216
train acc:  0.8515625
train loss:  0.3051890730857849
train gradient:  0.12697562866441653
iteration : 217
train acc:  0.84375
train loss:  0.3447684049606323
train gradient:  0.17057558704694747
iteration : 218
train acc:  0.8828125
train loss:  0.27170687913894653
train gradient:  0.11954445528203712
iteration : 219
train acc:  0.84375
train loss:  0.3264782428741455
train gradient:  0.12738236585180812
iteration : 220
train acc:  0.875
train loss:  0.3642313480377197
train gradient:  0.19249910084280442
iteration : 221
train acc:  0.890625
train loss:  0.29045963287353516
train gradient:  0.1338276772466625
iteration : 222
train acc:  0.859375
train loss:  0.2982165813446045
train gradient:  0.1070703948678207
iteration : 223
train acc:  0.8515625
train loss:  0.3265930414199829
train gradient:  0.11213280868481143
iteration : 224
train acc:  0.8359375
train loss:  0.41000476479530334
train gradient:  0.2886462535574724
iteration : 225
train acc:  0.8515625
train loss:  0.2982010841369629
train gradient:  0.10984014384781932
iteration : 226
train acc:  0.8515625
train loss:  0.346570760011673
train gradient:  0.13645265111864832
iteration : 227
train acc:  0.890625
train loss:  0.3147963881492615
train gradient:  0.13759899321943891
iteration : 228
train acc:  0.84375
train loss:  0.3269795775413513
train gradient:  0.1103714804448784
iteration : 229
train acc:  0.8828125
train loss:  0.3119238018989563
train gradient:  0.14761402944100832
iteration : 230
train acc:  0.84375
train loss:  0.3833964169025421
train gradient:  0.18352743508857067
iteration : 231
train acc:  0.8828125
train loss:  0.3461204171180725
train gradient:  0.129760160404503
iteration : 232
train acc:  0.7890625
train loss:  0.44450342655181885
train gradient:  0.18152027150517225
iteration : 233
train acc:  0.8828125
train loss:  0.2805526852607727
train gradient:  0.09836741193898743
iteration : 234
train acc:  0.8359375
train loss:  0.3178325295448303
train gradient:  0.10477866091056932
iteration : 235
train acc:  0.8125
train loss:  0.3531745374202728
train gradient:  0.12759219918391226
iteration : 236
train acc:  0.8515625
train loss:  0.32982349395751953
train gradient:  0.18033245897406694
iteration : 237
train acc:  0.8359375
train loss:  0.38506191968917847
train gradient:  0.21428790614682586
iteration : 238
train acc:  0.890625
train loss:  0.2603793144226074
train gradient:  0.11833451581420008
iteration : 239
train acc:  0.8203125
train loss:  0.35326457023620605
train gradient:  0.14337995296968514
iteration : 240
train acc:  0.890625
train loss:  0.2905968427658081
train gradient:  0.13190722571893893
iteration : 241
train acc:  0.9375
train loss:  0.21221831440925598
train gradient:  0.06009693989958283
iteration : 242
train acc:  0.8125
train loss:  0.3707025349140167
train gradient:  0.15458185450110812
iteration : 243
train acc:  0.8984375
train loss:  0.25971680879592896
train gradient:  0.0942750770018457
iteration : 244
train acc:  0.8984375
train loss:  0.25006532669067383
train gradient:  0.09056323063979922
iteration : 245
train acc:  0.828125
train loss:  0.4537523090839386
train gradient:  0.23071906882345222
iteration : 246
train acc:  0.8515625
train loss:  0.37134939432144165
train gradient:  0.17699122567642062
iteration : 247
train acc:  0.8671875
train loss:  0.3297070860862732
train gradient:  0.11719041729708286
iteration : 248
train acc:  0.859375
train loss:  0.25804227590560913
train gradient:  0.13943691813348047
iteration : 249
train acc:  0.90625
train loss:  0.3039565682411194
train gradient:  0.1304219716162121
iteration : 250
train acc:  0.84375
train loss:  0.3753471076488495
train gradient:  0.16664209645947572
iteration : 251
train acc:  0.84375
train loss:  0.3381801247596741
train gradient:  0.12328492531188762
iteration : 252
train acc:  0.859375
train loss:  0.34100112318992615
train gradient:  0.15624321906520847
iteration : 253
train acc:  0.8671875
train loss:  0.3275321125984192
train gradient:  0.1544926957178271
iteration : 254
train acc:  0.875
train loss:  0.3153408169746399
train gradient:  0.12424207528601965
iteration : 255
train acc:  0.84375
train loss:  0.34913885593414307
train gradient:  0.2547953148324293
iteration : 256
train acc:  0.8828125
train loss:  0.31539690494537354
train gradient:  0.08431404647833642
iteration : 257
train acc:  0.875
train loss:  0.34648197889328003
train gradient:  0.14087513613002112
iteration : 258
train acc:  0.9140625
train loss:  0.27380239963531494
train gradient:  0.08129229131484292
iteration : 259
train acc:  0.859375
train loss:  0.34634122252464294
train gradient:  0.16108503351182923
iteration : 260
train acc:  0.8359375
train loss:  0.39778169989585876
train gradient:  0.19770166280636178
iteration : 261
train acc:  0.8203125
train loss:  0.3799642324447632
train gradient:  0.17361876227753031
iteration : 262
train acc:  0.9296875
train loss:  0.25249263644218445
train gradient:  0.16650484447747704
iteration : 263
train acc:  0.875
train loss:  0.26767200231552124
train gradient:  0.10168625620305329
iteration : 264
train acc:  0.9140625
train loss:  0.22290930151939392
train gradient:  0.07851068966364243
iteration : 265
train acc:  0.890625
train loss:  0.27442115545272827
train gradient:  0.10577720563935125
iteration : 266
train acc:  0.9140625
train loss:  0.252328097820282
train gradient:  0.0818303258685165
iteration : 267
train acc:  0.890625
train loss:  0.255393385887146
train gradient:  0.15796192592127853
iteration : 268
train acc:  0.8671875
train loss:  0.33372175693511963
train gradient:  0.1420626136281205
iteration : 269
train acc:  0.84375
train loss:  0.3550379276275635
train gradient:  0.15499433999624707
iteration : 270
train acc:  0.8515625
train loss:  0.3674299716949463
train gradient:  0.1365122716963149
iteration : 271
train acc:  0.8125
train loss:  0.37610188126564026
train gradient:  0.1915895691606252
iteration : 272
train acc:  0.875
train loss:  0.27020639181137085
train gradient:  0.20625395288087642
iteration : 273
train acc:  0.8359375
train loss:  0.35437822341918945
train gradient:  0.1246830839247628
iteration : 274
train acc:  0.84375
train loss:  0.33857351541519165
train gradient:  0.17321905057155884
iteration : 275
train acc:  0.859375
train loss:  0.28954005241394043
train gradient:  0.11548397088057781
iteration : 276
train acc:  0.8828125
train loss:  0.2541791796684265
train gradient:  0.11433630310649079
iteration : 277
train acc:  0.84375
train loss:  0.3205581307411194
train gradient:  0.14958180466724091
iteration : 278
train acc:  0.875
train loss:  0.27710437774658203
train gradient:  0.1354557029585329
iteration : 279
train acc:  0.90625
train loss:  0.2557339072227478
train gradient:  0.10685559248286378
iteration : 280
train acc:  0.9140625
train loss:  0.24301907420158386
train gradient:  0.0748075752726235
iteration : 281
train acc:  0.859375
train loss:  0.3322635293006897
train gradient:  0.12473076054887075
iteration : 282
train acc:  0.8671875
train loss:  0.291390597820282
train gradient:  0.11834399889004933
iteration : 283
train acc:  0.8515625
train loss:  0.3299034833908081
train gradient:  0.201315077945465
iteration : 284
train acc:  0.9296875
train loss:  0.19832774996757507
train gradient:  0.07561926434359702
iteration : 285
train acc:  0.890625
train loss:  0.2770234942436218
train gradient:  0.10191049921546048
iteration : 286
train acc:  0.84375
train loss:  0.2646070122718811
train gradient:  0.1020397803920292
iteration : 287
train acc:  0.8828125
train loss:  0.2584353983402252
train gradient:  0.10506703478787062
iteration : 288
train acc:  0.9140625
train loss:  0.25315770506858826
train gradient:  0.0927882608226287
iteration : 289
train acc:  0.8515625
train loss:  0.294305682182312
train gradient:  0.17501026347902482
iteration : 290
train acc:  0.8984375
train loss:  0.3010903000831604
train gradient:  0.1409165704690403
iteration : 291
train acc:  0.9140625
train loss:  0.23735499382019043
train gradient:  0.072539699377525
iteration : 292
train acc:  0.8359375
train loss:  0.2893853485584259
train gradient:  0.14216647757943818
iteration : 293
train acc:  0.8671875
train loss:  0.3499000072479248
train gradient:  0.15978474224115996
iteration : 294
train acc:  0.8828125
train loss:  0.2995222806930542
train gradient:  0.13348288906505848
iteration : 295
train acc:  0.9140625
train loss:  0.23387600481510162
train gradient:  0.07459986147263906
iteration : 296
train acc:  0.84375
train loss:  0.3569200336933136
train gradient:  0.1677352435612482
iteration : 297
train acc:  0.8828125
train loss:  0.24846896529197693
train gradient:  0.08699568813784608
iteration : 298
train acc:  0.859375
train loss:  0.3029997944831848
train gradient:  0.14781338642675135
iteration : 299
train acc:  0.859375
train loss:  0.32504168152809143
train gradient:  0.1642438263584604
iteration : 300
train acc:  0.84375
train loss:  0.37880221009254456
train gradient:  0.18627830179040722
iteration : 301
train acc:  0.8359375
train loss:  0.3816581666469574
train gradient:  0.22866624239461308
iteration : 302
train acc:  0.84375
train loss:  0.3522467613220215
train gradient:  0.14752628761814487
iteration : 303
train acc:  0.8671875
train loss:  0.23769313097000122
train gradient:  0.0843589593177621
iteration : 304
train acc:  0.859375
train loss:  0.2474510371685028
train gradient:  0.11180849110021616
iteration : 305
train acc:  0.8671875
train loss:  0.36058056354522705
train gradient:  0.17614514433979278
iteration : 306
train acc:  0.859375
train loss:  0.35580700635910034
train gradient:  0.2236296961186579
iteration : 307
train acc:  0.8359375
train loss:  0.38942229747772217
train gradient:  0.26012069779056046
iteration : 308
train acc:  0.8671875
train loss:  0.3240278959274292
train gradient:  0.16440944709787825
iteration : 309
train acc:  0.8359375
train loss:  0.4233448803424835
train gradient:  0.25331558009317257
iteration : 310
train acc:  0.8515625
train loss:  0.28745031356811523
train gradient:  0.10298238123661639
iteration : 311
train acc:  0.890625
train loss:  0.28813812136650085
train gradient:  0.12284072696106203
iteration : 312
train acc:  0.890625
train loss:  0.31334230303764343
train gradient:  0.12439018956579945
iteration : 313
train acc:  0.8671875
train loss:  0.35118359327316284
train gradient:  0.2252558406581143
iteration : 314
train acc:  0.8671875
train loss:  0.2679060101509094
train gradient:  0.0804757521472658
iteration : 315
train acc:  0.8359375
train loss:  0.3663942813873291
train gradient:  0.185530692078152
iteration : 316
train acc:  0.8828125
train loss:  0.32320448756217957
train gradient:  0.09812858957307503
iteration : 317
train acc:  0.8203125
train loss:  0.39621755480766296
train gradient:  0.287812919750358
iteration : 318
train acc:  0.90625
train loss:  0.24604666233062744
train gradient:  0.09275378977929395
iteration : 319
train acc:  0.890625
train loss:  0.29319193959236145
train gradient:  0.1044554114666709
iteration : 320
train acc:  0.9375
train loss:  0.21766766905784607
train gradient:  0.06720572868571367
iteration : 321
train acc:  0.8828125
train loss:  0.28619104623794556
train gradient:  0.12740025703833532
iteration : 322
train acc:  0.90625
train loss:  0.27739202976226807
train gradient:  0.10704817587408848
iteration : 323
train acc:  0.9140625
train loss:  0.24526798725128174
train gradient:  0.13175032611434537
iteration : 324
train acc:  0.859375
train loss:  0.3106480538845062
train gradient:  0.10267276937001465
iteration : 325
train acc:  0.890625
train loss:  0.2332727164030075
train gradient:  0.11132628469927423
iteration : 326
train acc:  0.8984375
train loss:  0.26887452602386475
train gradient:  0.14175658709768169
iteration : 327
train acc:  0.859375
train loss:  0.3781391382217407
train gradient:  0.20154567475886187
iteration : 328
train acc:  0.9140625
train loss:  0.23165906965732574
train gradient:  0.06890165221934967
iteration : 329
train acc:  0.8671875
train loss:  0.31227368116378784
train gradient:  0.1987257633032029
iteration : 330
train acc:  0.890625
train loss:  0.3458540439605713
train gradient:  0.17332316283034066
iteration : 331
train acc:  0.8671875
train loss:  0.2925322651863098
train gradient:  0.1383821329620626
iteration : 332
train acc:  0.84375
train loss:  0.32096531987190247
train gradient:  0.14211969474938022
iteration : 333
train acc:  0.8515625
train loss:  0.3459855020046234
train gradient:  0.13268340592725192
iteration : 334
train acc:  0.8828125
train loss:  0.2563358545303345
train gradient:  0.19401553049404122
iteration : 335
train acc:  0.890625
train loss:  0.26366865634918213
train gradient:  0.09306996338845296
iteration : 336
train acc:  0.8828125
train loss:  0.2951149344444275
train gradient:  0.10295064355786379
iteration : 337
train acc:  0.8359375
train loss:  0.34206119179725647
train gradient:  0.13970866301400117
iteration : 338
train acc:  0.828125
train loss:  0.36382928490638733
train gradient:  0.2010834682726292
iteration : 339
train acc:  0.890625
train loss:  0.24668678641319275
train gradient:  0.12506874859085196
iteration : 340
train acc:  0.8046875
train loss:  0.4588189125061035
train gradient:  0.4055082614911573
iteration : 341
train acc:  0.8984375
train loss:  0.25058513879776
train gradient:  0.11373624861959798
iteration : 342
train acc:  0.90625
train loss:  0.2571651339530945
train gradient:  0.11404064722687973
iteration : 343
train acc:  0.7890625
train loss:  0.3949022591114044
train gradient:  0.20369159084337007
iteration : 344
train acc:  0.859375
train loss:  0.34085389971733093
train gradient:  0.1697481216912286
iteration : 345
train acc:  0.8671875
train loss:  0.33382993936538696
train gradient:  0.12825085271515463
iteration : 346
train acc:  0.84375
train loss:  0.3438910245895386
train gradient:  0.14360593389927082
iteration : 347
train acc:  0.921875
train loss:  0.27809011936187744
train gradient:  0.12391863407764558
iteration : 348
train acc:  0.875
train loss:  0.2756540775299072
train gradient:  0.10715353113670134
iteration : 349
train acc:  0.859375
train loss:  0.3273636996746063
train gradient:  0.18049589893293508
iteration : 350
train acc:  0.859375
train loss:  0.33098411560058594
train gradient:  0.1400984668548798
iteration : 351
train acc:  0.8984375
train loss:  0.2833446264266968
train gradient:  0.09804710055489096
iteration : 352
train acc:  0.8828125
train loss:  0.2698368728160858
train gradient:  0.08983795032245626
iteration : 353
train acc:  0.8203125
train loss:  0.3580511808395386
train gradient:  0.13142539027606837
iteration : 354
train acc:  0.875
train loss:  0.2931693196296692
train gradient:  0.0934281829532948
iteration : 355
train acc:  0.8359375
train loss:  0.37669456005096436
train gradient:  0.19253386996102256
iteration : 356
train acc:  0.8046875
train loss:  0.34376251697540283
train gradient:  0.1466173198054822
iteration : 357
train acc:  0.8671875
train loss:  0.35114598274230957
train gradient:  0.19480850157370966
iteration : 358
train acc:  0.84375
train loss:  0.3226388096809387
train gradient:  0.10693874657748043
iteration : 359
train acc:  0.8984375
train loss:  0.24406245350837708
train gradient:  0.09509625307479262
iteration : 360
train acc:  0.890625
train loss:  0.2796619236469269
train gradient:  0.08757362800615685
iteration : 361
train acc:  0.84375
train loss:  0.3740759491920471
train gradient:  0.155136501608852
iteration : 362
train acc:  0.875
train loss:  0.287863552570343
train gradient:  0.13887584461659
iteration : 363
train acc:  0.84375
train loss:  0.34999218583106995
train gradient:  0.13058792900429853
iteration : 364
train acc:  0.90625
train loss:  0.2554742097854614
train gradient:  0.09483388724311692
iteration : 365
train acc:  0.8671875
train loss:  0.28304195404052734
train gradient:  0.12353000371276847
iteration : 366
train acc:  0.890625
train loss:  0.26534581184387207
train gradient:  0.09417821814400824
iteration : 367
train acc:  0.8515625
train loss:  0.3029954135417938
train gradient:  0.10044904392611988
iteration : 368
train acc:  0.8984375
train loss:  0.259297639131546
train gradient:  0.1809520118812144
iteration : 369
train acc:  0.8984375
train loss:  0.26854395866394043
train gradient:  0.11509959552718363
iteration : 370
train acc:  0.8515625
train loss:  0.32666313648223877
train gradient:  0.13502437278556517
iteration : 371
train acc:  0.84375
train loss:  0.35386550426483154
train gradient:  0.1179958091139043
iteration : 372
train acc:  0.859375
train loss:  0.31299906969070435
train gradient:  0.10000959031075539
iteration : 373
train acc:  0.84375
train loss:  0.33694154024124146
train gradient:  0.13694423419743385
iteration : 374
train acc:  0.859375
train loss:  0.2984917461872101
train gradient:  0.09868470498172548
iteration : 375
train acc:  0.890625
train loss:  0.2655388414859772
train gradient:  0.14910770107295723
iteration : 376
train acc:  0.8359375
train loss:  0.3685719668865204
train gradient:  0.24870923166328826
iteration : 377
train acc:  0.875
train loss:  0.3046446144580841
train gradient:  0.09059295446560414
iteration : 378
train acc:  0.9296875
train loss:  0.2470102608203888
train gradient:  0.07866202406314642
iteration : 379
train acc:  0.8515625
train loss:  0.3536774814128876
train gradient:  0.1397209862393473
iteration : 380
train acc:  0.8203125
train loss:  0.39968210458755493
train gradient:  0.24444401507917002
iteration : 381
train acc:  0.8828125
train loss:  0.25462549924850464
train gradient:  0.13526373072562234
iteration : 382
train acc:  0.84375
train loss:  0.29948827624320984
train gradient:  0.13904495462840377
iteration : 383
train acc:  0.84375
train loss:  0.38389715552330017
train gradient:  0.16530529731741206
iteration : 384
train acc:  0.8984375
train loss:  0.3045934736728668
train gradient:  0.10900538304450634
iteration : 385
train acc:  0.90625
train loss:  0.24010370671749115
train gradient:  0.06573168038598245
iteration : 386
train acc:  0.859375
train loss:  0.33520951867103577
train gradient:  0.14835658352645953
iteration : 387
train acc:  0.8828125
train loss:  0.2572072446346283
train gradient:  0.07392282439366998
iteration : 388
train acc:  0.828125
train loss:  0.36362576484680176
train gradient:  0.13056800819891531
iteration : 389
train acc:  0.84375
train loss:  0.3134884834289551
train gradient:  0.158748297833803
iteration : 390
train acc:  0.8359375
train loss:  0.3805115222930908
train gradient:  0.20031011418583855
iteration : 391
train acc:  0.828125
train loss:  0.3764668107032776
train gradient:  0.19959837251696416
iteration : 392
train acc:  0.828125
train loss:  0.3999015688896179
train gradient:  0.18669261541273136
iteration : 393
train acc:  0.84375
train loss:  0.30550703406333923
train gradient:  0.1277774264196413
iteration : 394
train acc:  0.828125
train loss:  0.31044137477874756
train gradient:  0.11556560890045713
iteration : 395
train acc:  0.8984375
train loss:  0.3243253827095032
train gradient:  0.1168615644127398
iteration : 396
train acc:  0.8671875
train loss:  0.262343168258667
train gradient:  0.1142095983232606
iteration : 397
train acc:  0.9140625
train loss:  0.23373766243457794
train gradient:  0.08563903191281352
iteration : 398
train acc:  0.8984375
train loss:  0.2653595209121704
train gradient:  0.10804867492304551
iteration : 399
train acc:  0.859375
train loss:  0.33114564418792725
train gradient:  0.20657268987255584
iteration : 400
train acc:  0.875
train loss:  0.3269432783126831
train gradient:  0.11478310096335453
iteration : 401
train acc:  0.859375
train loss:  0.3538082242012024
train gradient:  0.13353134006190198
iteration : 402
train acc:  0.8671875
train loss:  0.3735218942165375
train gradient:  0.1429709484124487
iteration : 403
train acc:  0.890625
train loss:  0.2695469260215759
train gradient:  0.12037297632345617
iteration : 404
train acc:  0.796875
train loss:  0.4638637900352478
train gradient:  0.22606931273292455
iteration : 405
train acc:  0.8828125
train loss:  0.260517418384552
train gradient:  0.09684264903553012
iteration : 406
train acc:  0.8359375
train loss:  0.3474743366241455
train gradient:  0.15846208710103846
iteration : 407
train acc:  0.859375
train loss:  0.2909587323665619
train gradient:  0.1256797050000709
iteration : 408
train acc:  0.8359375
train loss:  0.3881438374519348
train gradient:  0.20075932622568682
iteration : 409
train acc:  0.890625
train loss:  0.29766109585762024
train gradient:  0.1386611774378223
iteration : 410
train acc:  0.8828125
train loss:  0.278184175491333
train gradient:  0.08591672420069925
iteration : 411
train acc:  0.859375
train loss:  0.31310558319091797
train gradient:  0.12256515872877449
iteration : 412
train acc:  0.8515625
train loss:  0.3463939428329468
train gradient:  0.18227509134764813
iteration : 413
train acc:  0.859375
train loss:  0.3042807877063751
train gradient:  0.12431762125573913
iteration : 414
train acc:  0.8515625
train loss:  0.3457832932472229
train gradient:  0.1499968880021823
iteration : 415
train acc:  0.8515625
train loss:  0.29930242896080017
train gradient:  0.11306631746187522
iteration : 416
train acc:  0.8671875
train loss:  0.3465830683708191
train gradient:  0.11450827313124722
iteration : 417
train acc:  0.8359375
train loss:  0.41672804951667786
train gradient:  0.19517742418588202
iteration : 418
train acc:  0.890625
train loss:  0.2843393087387085
train gradient:  0.13815675045342732
iteration : 419
train acc:  0.921875
train loss:  0.21359112858772278
train gradient:  0.08768243134326331
iteration : 420
train acc:  0.9140625
train loss:  0.250210702419281
train gradient:  0.11289315038468471
iteration : 421
train acc:  0.8828125
train loss:  0.28939735889434814
train gradient:  0.09416343559238224
iteration : 422
train acc:  0.859375
train loss:  0.3952670395374298
train gradient:  0.16977840991517862
iteration : 423
train acc:  0.8515625
train loss:  0.357516884803772
train gradient:  0.12038859841182996
iteration : 424
train acc:  0.8515625
train loss:  0.33925777673721313
train gradient:  0.11380342776856235
iteration : 425
train acc:  0.8359375
train loss:  0.3489021062850952
train gradient:  0.20215313990951164
iteration : 426
train acc:  0.8671875
train loss:  0.3231452703475952
train gradient:  0.11046609935362052
iteration : 427
train acc:  0.8359375
train loss:  0.3300771713256836
train gradient:  0.14160680487442576
iteration : 428
train acc:  0.8046875
train loss:  0.35986316204071045
train gradient:  0.22490653490703633
iteration : 429
train acc:  0.8515625
train loss:  0.2965291440486908
train gradient:  0.1436194360298863
iteration : 430
train acc:  0.8203125
train loss:  0.3505757451057434
train gradient:  0.194852917484039
iteration : 431
train acc:  0.84375
train loss:  0.3578103482723236
train gradient:  0.20072884586447373
iteration : 432
train acc:  0.921875
train loss:  0.25118809938430786
train gradient:  0.06711224279691078
iteration : 433
train acc:  0.8828125
train loss:  0.3337763547897339
train gradient:  0.1847548726770349
iteration : 434
train acc:  0.8984375
train loss:  0.2628750205039978
train gradient:  0.1031966115852069
iteration : 435
train acc:  0.8671875
train loss:  0.31629741191864014
train gradient:  0.156620431639603
iteration : 436
train acc:  0.8671875
train loss:  0.3063104450702667
train gradient:  0.12563113446703775
iteration : 437
train acc:  0.875
train loss:  0.2896694540977478
train gradient:  0.10931756937290311
iteration : 438
train acc:  0.8984375
train loss:  0.21833431720733643
train gradient:  0.08432458756594513
iteration : 439
train acc:  0.859375
train loss:  0.3275209069252014
train gradient:  0.13095115786409728
iteration : 440
train acc:  0.890625
train loss:  0.27028191089630127
train gradient:  0.1366871221069107
iteration : 441
train acc:  0.8203125
train loss:  0.366129994392395
train gradient:  0.21312795917254473
iteration : 442
train acc:  0.8984375
train loss:  0.3065764009952545
train gradient:  0.10358751276676376
iteration : 443
train acc:  0.90625
train loss:  0.27521830797195435
train gradient:  0.10548333807711249
iteration : 444
train acc:  0.8828125
train loss:  0.27231791615486145
train gradient:  0.11587338475978129
iteration : 445
train acc:  0.875
train loss:  0.33235543966293335
train gradient:  0.12701910079834738
iteration : 446
train acc:  0.875
train loss:  0.32729387283325195
train gradient:  0.11656444184936228
iteration : 447
train acc:  0.8046875
train loss:  0.4218137264251709
train gradient:  0.16577109596331305
iteration : 448
train acc:  0.8125
train loss:  0.41127827763557434
train gradient:  0.19836172454957454
iteration : 449
train acc:  0.84375
train loss:  0.32276013493537903
train gradient:  0.11951617802324249
iteration : 450
train acc:  0.890625
train loss:  0.3337550461292267
train gradient:  0.12207093154910209
iteration : 451
train acc:  0.84375
train loss:  0.3740304708480835
train gradient:  0.15854706859052295
iteration : 452
train acc:  0.8984375
train loss:  0.2785509526729584
train gradient:  0.1006436831879037
iteration : 453
train acc:  0.890625
train loss:  0.2970578074455261
train gradient:  0.11723040604343472
iteration : 454
train acc:  0.8203125
train loss:  0.3517751395702362
train gradient:  0.18305884602356332
iteration : 455
train acc:  0.828125
train loss:  0.3225736618041992
train gradient:  0.14726175190204643
iteration : 456
train acc:  0.84375
train loss:  0.3527275621891022
train gradient:  0.14461705330323943
iteration : 457
train acc:  0.875
train loss:  0.3202664256095886
train gradient:  0.11729726082037717
iteration : 458
train acc:  0.859375
train loss:  0.3188532292842865
train gradient:  0.1184974922788972
iteration : 459
train acc:  0.890625
train loss:  0.2551441788673401
train gradient:  0.12355720638383615
iteration : 460
train acc:  0.9140625
train loss:  0.27338314056396484
train gradient:  0.11642444442876289
iteration : 461
train acc:  0.8671875
train loss:  0.2726181745529175
train gradient:  0.11187160442982969
iteration : 462
train acc:  0.859375
train loss:  0.34204816818237305
train gradient:  0.14400764329201998
iteration : 463
train acc:  0.8984375
train loss:  0.28225910663604736
train gradient:  0.11056160888956881
iteration : 464
train acc:  0.921875
train loss:  0.21407243609428406
train gradient:  0.07670332399362254
iteration : 465
train acc:  0.8671875
train loss:  0.3168659806251526
train gradient:  0.10477877632380489
iteration : 466
train acc:  0.859375
train loss:  0.2985687851905823
train gradient:  0.08792301346792264
iteration : 467
train acc:  0.84375
train loss:  0.3398257791996002
train gradient:  0.10660095008809896
iteration : 468
train acc:  0.890625
train loss:  0.2765527665615082
train gradient:  0.10248324270615759
iteration : 469
train acc:  0.8359375
train loss:  0.40638890862464905
train gradient:  0.18530277080091961
iteration : 470
train acc:  0.8515625
train loss:  0.27679508924484253
train gradient:  0.09104392911720596
iteration : 471
train acc:  0.8359375
train loss:  0.32833433151245117
train gradient:  0.17826452383616204
iteration : 472
train acc:  0.8671875
train loss:  0.3828769028186798
train gradient:  0.1683183920405265
iteration : 473
train acc:  0.8671875
train loss:  0.3595898747444153
train gradient:  0.15021299950985584
iteration : 474
train acc:  0.859375
train loss:  0.2767917513847351
train gradient:  0.11408990807526712
iteration : 475
train acc:  0.875
train loss:  0.3385155200958252
train gradient:  0.20101563813297457
iteration : 476
train acc:  0.8984375
train loss:  0.20572273433208466
train gradient:  0.11342125167975674
iteration : 477
train acc:  0.8359375
train loss:  0.37612658739089966
train gradient:  0.15326036521969302
iteration : 478
train acc:  0.875
train loss:  0.33272311091423035
train gradient:  0.17419767514866483
iteration : 479
train acc:  0.8671875
train loss:  0.26665931940078735
train gradient:  0.09746296131255754
iteration : 480
train acc:  0.8984375
train loss:  0.24710841476917267
train gradient:  0.09348025946828703
iteration : 481
train acc:  0.8671875
train loss:  0.28240370750427246
train gradient:  0.112260736871736
iteration : 482
train acc:  0.875
train loss:  0.259628027677536
train gradient:  0.13147552228586873
iteration : 483
train acc:  0.9140625
train loss:  0.21681590378284454
train gradient:  0.08170299277247806
iteration : 484
train acc:  0.875
train loss:  0.276289701461792
train gradient:  0.09313954817473623
iteration : 485
train acc:  0.84375
train loss:  0.3487841486930847
train gradient:  0.13066450010705938
iteration : 486
train acc:  0.84375
train loss:  0.36266106367111206
train gradient:  0.14304930199084326
iteration : 487
train acc:  0.8671875
train loss:  0.3016316890716553
train gradient:  0.12418427390850453
iteration : 488
train acc:  0.8203125
train loss:  0.3650769591331482
train gradient:  0.2680307307206428
iteration : 489
train acc:  0.8359375
train loss:  0.3305332064628601
train gradient:  0.13627887731293004
iteration : 490
train acc:  0.8359375
train loss:  0.35536032915115356
train gradient:  0.18194288514151266
iteration : 491
train acc:  0.859375
train loss:  0.31792473793029785
train gradient:  0.11996935202236057
iteration : 492
train acc:  0.8828125
train loss:  0.30412453413009644
train gradient:  0.10602703629746456
iteration : 493
train acc:  0.921875
train loss:  0.22707903385162354
train gradient:  0.08077164354609961
iteration : 494
train acc:  0.84375
train loss:  0.33634865283966064
train gradient:  0.1404131419145959
iteration : 495
train acc:  0.921875
train loss:  0.20655271410942078
train gradient:  0.0755903774804795
iteration : 496
train acc:  0.890625
train loss:  0.25573647022247314
train gradient:  0.09280635734099847
iteration : 497
train acc:  0.8671875
train loss:  0.2611979842185974
train gradient:  0.09254181233765081
iteration : 498
train acc:  0.859375
train loss:  0.3356640040874481
train gradient:  0.18055449904811244
iteration : 499
train acc:  0.8828125
train loss:  0.25362569093704224
train gradient:  0.07198704051753356
iteration : 500
train acc:  0.8828125
train loss:  0.2650330662727356
train gradient:  0.09550141662123487
iteration : 501
train acc:  0.8671875
train loss:  0.28187137842178345
train gradient:  0.10160982624516146
iteration : 502
train acc:  0.8828125
train loss:  0.3296651840209961
train gradient:  0.13791487546252285
iteration : 503
train acc:  0.8515625
train loss:  0.36986246705055237
train gradient:  0.26270604136708264
iteration : 504
train acc:  0.890625
train loss:  0.2665768265724182
train gradient:  0.09011746864348252
iteration : 505
train acc:  0.859375
train loss:  0.3606571555137634
train gradient:  0.1498264843947907
iteration : 506
train acc:  0.8984375
train loss:  0.27026546001434326
train gradient:  0.11083211921531204
iteration : 507
train acc:  0.828125
train loss:  0.3594895303249359
train gradient:  0.24114625683070523
iteration : 508
train acc:  0.8671875
train loss:  0.29574108123779297
train gradient:  0.09952535084210133
iteration : 509
train acc:  0.90625
train loss:  0.3167848587036133
train gradient:  0.1918241015800776
iteration : 510
train acc:  0.8828125
train loss:  0.23035061359405518
train gradient:  0.07457711112278861
iteration : 511
train acc:  0.8046875
train loss:  0.3381204605102539
train gradient:  0.15838304104590722
iteration : 512
train acc:  0.8359375
train loss:  0.37188827991485596
train gradient:  0.14358845815155802
iteration : 513
train acc:  0.8671875
train loss:  0.31408774852752686
train gradient:  0.11510141887976605
iteration : 514
train acc:  0.859375
train loss:  0.2744210958480835
train gradient:  0.11769155748029274
iteration : 515
train acc:  0.8671875
train loss:  0.29044097661972046
train gradient:  0.1235552533336107
iteration : 516
train acc:  0.9140625
train loss:  0.23795044422149658
train gradient:  0.08788174621128661
iteration : 517
train acc:  0.828125
train loss:  0.3421705365180969
train gradient:  0.1996036101828949
iteration : 518
train acc:  0.828125
train loss:  0.33179771900177
train gradient:  0.13353516812675886
iteration : 519
train acc:  0.84375
train loss:  0.3032999634742737
train gradient:  0.11716479657203904
iteration : 520
train acc:  0.8671875
train loss:  0.3011764883995056
train gradient:  0.14802972130391184
iteration : 521
train acc:  0.8984375
train loss:  0.3003716766834259
train gradient:  0.09018114219741863
iteration : 522
train acc:  0.84375
train loss:  0.2915157079696655
train gradient:  0.11698018970356285
iteration : 523
train acc:  0.8046875
train loss:  0.4122176170349121
train gradient:  0.2638119756830001
iteration : 524
train acc:  0.875
train loss:  0.30884426832199097
train gradient:  0.13977943195656506
iteration : 525
train acc:  0.859375
train loss:  0.3257659673690796
train gradient:  0.14667341688752442
iteration : 526
train acc:  0.828125
train loss:  0.30204707384109497
train gradient:  0.16390174673582075
iteration : 527
train acc:  0.8671875
train loss:  0.29010045528411865
train gradient:  0.09553420506687647
iteration : 528
train acc:  0.921875
train loss:  0.2202570140361786
train gradient:  0.06589157495550313
iteration : 529
train acc:  0.8828125
train loss:  0.35994285345077515
train gradient:  0.19657504752743785
iteration : 530
train acc:  0.8515625
train loss:  0.35817837715148926
train gradient:  0.1684646986800315
iteration : 531
train acc:  0.90625
train loss:  0.2500128746032715
train gradient:  0.08638590554059794
iteration : 532
train acc:  0.8671875
train loss:  0.3230167031288147
train gradient:  0.09916065908019783
iteration : 533
train acc:  0.9140625
train loss:  0.2036331743001938
train gradient:  0.07535820086377695
iteration : 534
train acc:  0.8359375
train loss:  0.3646223545074463
train gradient:  0.16035436099841216
iteration : 535
train acc:  0.875
train loss:  0.31810295581817627
train gradient:  0.15290324388824328
iteration : 536
train acc:  0.8984375
train loss:  0.2827603220939636
train gradient:  0.10877970439528988
iteration : 537
train acc:  0.8359375
train loss:  0.3206228017807007
train gradient:  0.17401944701201066
iteration : 538
train acc:  0.8203125
train loss:  0.37044164538383484
train gradient:  0.1271703621048081
iteration : 539
train acc:  0.8828125
train loss:  0.34787610173225403
train gradient:  0.18315295910239388
iteration : 540
train acc:  0.8828125
train loss:  0.25279098749160767
train gradient:  0.07263594725080363
iteration : 541
train acc:  0.8515625
train loss:  0.32588261365890503
train gradient:  0.15187998726967783
iteration : 542
train acc:  0.8671875
train loss:  0.2935154438018799
train gradient:  0.12092589138698445
iteration : 543
train acc:  0.8671875
train loss:  0.36154991388320923
train gradient:  0.17310470968501432
iteration : 544
train acc:  0.875
train loss:  0.3044847846031189
train gradient:  0.09075226292119801
iteration : 545
train acc:  0.8828125
train loss:  0.269477903842926
train gradient:  0.11614856215269694
iteration : 546
train acc:  0.8984375
train loss:  0.2965978682041168
train gradient:  0.1085027795192598
iteration : 547
train acc:  0.9140625
train loss:  0.2948560118675232
train gradient:  0.09661540349785666
iteration : 548
train acc:  0.9140625
train loss:  0.22077462077140808
train gradient:  0.08996638684417062
iteration : 549
train acc:  0.84375
train loss:  0.38185548782348633
train gradient:  0.17350289034624955
iteration : 550
train acc:  0.875
train loss:  0.308405339717865
train gradient:  0.12343102307025651
iteration : 551
train acc:  0.859375
train loss:  0.31320133805274963
train gradient:  0.18981690176624105
iteration : 552
train acc:  0.8203125
train loss:  0.3661677837371826
train gradient:  0.1461772419671047
iteration : 553
train acc:  0.84375
train loss:  0.327201783657074
train gradient:  0.1351580663474563
iteration : 554
train acc:  0.8515625
train loss:  0.3009597063064575
train gradient:  0.16813243351033966
iteration : 555
train acc:  0.890625
train loss:  0.2949317991733551
train gradient:  0.0708537184361377
iteration : 556
train acc:  0.84375
train loss:  0.34271931648254395
train gradient:  0.14608820500860423
iteration : 557
train acc:  0.8828125
train loss:  0.26859205961227417
train gradient:  0.07318974626539965
iteration : 558
train acc:  0.8046875
train loss:  0.35851597785949707
train gradient:  0.12283807136868147
iteration : 559
train acc:  0.921875
train loss:  0.27365052700042725
train gradient:  0.11010546382080724
iteration : 560
train acc:  0.90625
train loss:  0.24441814422607422
train gradient:  0.06815672774082515
iteration : 561
train acc:  0.828125
train loss:  0.37774282693862915
train gradient:  0.14839706051936927
iteration : 562
train acc:  0.9375
train loss:  0.20211206376552582
train gradient:  0.06120200436820306
iteration : 563
train acc:  0.84375
train loss:  0.34418970346450806
train gradient:  0.1493192060181055
iteration : 564
train acc:  0.8828125
train loss:  0.2719070315361023
train gradient:  0.09239973320661798
iteration : 565
train acc:  0.8203125
train loss:  0.32595378160476685
train gradient:  0.12926096031539908
iteration : 566
train acc:  0.828125
train loss:  0.3632091283798218
train gradient:  0.569837866999359
iteration : 567
train acc:  0.84375
train loss:  0.36284059286117554
train gradient:  0.17902299377755093
iteration : 568
train acc:  0.875
train loss:  0.2959204912185669
train gradient:  0.09851855611607739
iteration : 569
train acc:  0.890625
train loss:  0.2596161365509033
train gradient:  0.09756979268841084
iteration : 570
train acc:  0.84375
train loss:  0.35928279161453247
train gradient:  0.11460023758090447
iteration : 571
train acc:  0.8203125
train loss:  0.3661889433860779
train gradient:  0.16756334209313084
iteration : 572
train acc:  0.90625
train loss:  0.2937086522579193
train gradient:  0.1277855916471887
iteration : 573
train acc:  0.8515625
train loss:  0.31033384799957275
train gradient:  0.13634861820825733
iteration : 574
train acc:  0.875
train loss:  0.2879539728164673
train gradient:  0.12718748705779373
iteration : 575
train acc:  0.8359375
train loss:  0.3961262106895447
train gradient:  0.16214818408550874
iteration : 576
train acc:  0.8515625
train loss:  0.36859896779060364
train gradient:  0.14299302305313025
iteration : 577
train acc:  0.875
train loss:  0.25853049755096436
train gradient:  0.0834979169162276
iteration : 578
train acc:  0.8515625
train loss:  0.32042744755744934
train gradient:  0.2054093551860512
iteration : 579
train acc:  0.875
train loss:  0.34715574979782104
train gradient:  0.17792245879945923
iteration : 580
train acc:  0.84375
train loss:  0.305593341588974
train gradient:  0.14411107125427747
iteration : 581
train acc:  0.859375
train loss:  0.28939953446388245
train gradient:  0.23558720347888698
iteration : 582
train acc:  0.890625
train loss:  0.2626650333404541
train gradient:  0.09538424780488855
iteration : 583
train acc:  0.875
train loss:  0.31476062536239624
train gradient:  0.1419476830447558
iteration : 584
train acc:  0.890625
train loss:  0.3096664547920227
train gradient:  0.16977383655085665
iteration : 585
train acc:  0.8828125
train loss:  0.28338152170181274
train gradient:  0.11084155895053316
iteration : 586
train acc:  0.859375
train loss:  0.32579854130744934
train gradient:  0.11749701756256986
iteration : 587
train acc:  0.8671875
train loss:  0.3047564625740051
train gradient:  0.12053447542855988
iteration : 588
train acc:  0.8203125
train loss:  0.3795277178287506
train gradient:  0.17661320488599636
iteration : 589
train acc:  0.828125
train loss:  0.3276190757751465
train gradient:  0.16478514238432035
iteration : 590
train acc:  0.8515625
train loss:  0.3255127966403961
train gradient:  0.0982563939698604
iteration : 591
train acc:  0.8359375
train loss:  0.33291345834732056
train gradient:  0.12797401952515935
iteration : 592
train acc:  0.859375
train loss:  0.37521594762802124
train gradient:  0.1747115660936058
iteration : 593
train acc:  0.8203125
train loss:  0.38494163751602173
train gradient:  0.18364304032363626
iteration : 594
train acc:  0.8359375
train loss:  0.35436421632766724
train gradient:  0.14164265551008282
iteration : 595
train acc:  0.875
train loss:  0.27194643020629883
train gradient:  0.09972506415339695
iteration : 596
train acc:  0.90625
train loss:  0.23276439309120178
train gradient:  0.13613424503396787
iteration : 597
train acc:  0.828125
train loss:  0.2812047600746155
train gradient:  0.11054361024283856
iteration : 598
train acc:  0.890625
train loss:  0.2617458403110504
train gradient:  0.1336622249527059
iteration : 599
train acc:  0.859375
train loss:  0.33885395526885986
train gradient:  0.1107318900881381
iteration : 600
train acc:  0.8671875
train loss:  0.290758341550827
train gradient:  0.10986357207209849
iteration : 601
train acc:  0.90625
train loss:  0.2640019655227661
train gradient:  0.10702387200939484
iteration : 602
train acc:  0.8828125
train loss:  0.2937512993812561
train gradient:  0.12684368632057336
iteration : 603
train acc:  0.84375
train loss:  0.34735918045043945
train gradient:  0.13998717029823826
iteration : 604
train acc:  0.90625
train loss:  0.25651007890701294
train gradient:  0.10273337233499424
iteration : 605
train acc:  0.8203125
train loss:  0.32634204626083374
train gradient:  0.1467097252903833
iteration : 606
train acc:  0.859375
train loss:  0.3754359483718872
train gradient:  0.16434756207382187
iteration : 607
train acc:  0.828125
train loss:  0.35477888584136963
train gradient:  0.18061857168503015
iteration : 608
train acc:  0.875
train loss:  0.32674166560173035
train gradient:  0.11327516499183068
iteration : 609
train acc:  0.8671875
train loss:  0.3183850049972534
train gradient:  0.14789862878342475
iteration : 610
train acc:  0.828125
train loss:  0.3735561966896057
train gradient:  0.18127962794329852
iteration : 611
train acc:  0.828125
train loss:  0.3627872169017792
train gradient:  0.1973216532206114
iteration : 612
train acc:  0.890625
train loss:  0.24871481955051422
train gradient:  0.12101663023590653
iteration : 613
train acc:  0.8515625
train loss:  0.3113465905189514
train gradient:  0.09258134717514925
iteration : 614
train acc:  0.8203125
train loss:  0.38681668043136597
train gradient:  0.15823417271165846
iteration : 615
train acc:  0.8359375
train loss:  0.3946566581726074
train gradient:  0.15085185912836843
iteration : 616
train acc:  0.875
train loss:  0.2810649871826172
train gradient:  0.1114633824380594
iteration : 617
train acc:  0.8515625
train loss:  0.2971823513507843
train gradient:  0.11035018652984309
iteration : 618
train acc:  0.8671875
train loss:  0.2904563248157501
train gradient:  0.1282959294232131
iteration : 619
train acc:  0.859375
train loss:  0.37589043378829956
train gradient:  0.16462607375545557
iteration : 620
train acc:  0.890625
train loss:  0.28994235396385193
train gradient:  0.14060790583088595
iteration : 621
train acc:  0.8125
train loss:  0.38165730237960815
train gradient:  0.18863889781931734
iteration : 622
train acc:  0.8515625
train loss:  0.36427199840545654
train gradient:  0.20657626893346637
iteration : 623
train acc:  0.890625
train loss:  0.2762686014175415
train gradient:  0.08325710035323328
iteration : 624
train acc:  0.90625
train loss:  0.22245118021965027
train gradient:  0.06890136116455163
iteration : 625
train acc:  0.8515625
train loss:  0.32509106397628784
train gradient:  0.0977997890223557
iteration : 626
train acc:  0.921875
train loss:  0.2870118021965027
train gradient:  0.09213769922011034
iteration : 627
train acc:  0.8671875
train loss:  0.28857362270355225
train gradient:  0.11942243038510156
iteration : 628
train acc:  0.828125
train loss:  0.3200856149196625
train gradient:  0.17489408737285284
iteration : 629
train acc:  0.859375
train loss:  0.37122979760169983
train gradient:  0.13969984015384615
iteration : 630
train acc:  0.9140625
train loss:  0.23005467653274536
train gradient:  0.08342547400751704
iteration : 631
train acc:  0.890625
train loss:  0.25815337896347046
train gradient:  0.10167411673067628
iteration : 632
train acc:  0.8359375
train loss:  0.38170433044433594
train gradient:  0.16775936955127213
iteration : 633
train acc:  0.890625
train loss:  0.25913411378860474
train gradient:  0.08455435560978246
iteration : 634
train acc:  0.828125
train loss:  0.38049307465553284
train gradient:  0.18024509277507675
iteration : 635
train acc:  0.890625
train loss:  0.28497958183288574
train gradient:  0.13331532760469672
iteration : 636
train acc:  0.8125
train loss:  0.3959258794784546
train gradient:  0.22010618168744017
iteration : 637
train acc:  0.9375
train loss:  0.22182412445545197
train gradient:  0.06766419287853363
iteration : 638
train acc:  0.890625
train loss:  0.2810790538787842
train gradient:  0.10979869581711982
iteration : 639
train acc:  0.8671875
train loss:  0.3152740001678467
train gradient:  0.08835306029928908
iteration : 640
train acc:  0.8671875
train loss:  0.27318286895751953
train gradient:  0.11794321476891662
iteration : 641
train acc:  0.8984375
train loss:  0.23460082709789276
train gradient:  0.08899838548715137
iteration : 642
train acc:  0.8984375
train loss:  0.27896517515182495
train gradient:  0.1117305234985987
iteration : 643
train acc:  0.921875
train loss:  0.3123164772987366
train gradient:  0.1377117776013249
iteration : 644
train acc:  0.8828125
train loss:  0.27187827229499817
train gradient:  0.15976550693717073
iteration : 645
train acc:  0.859375
train loss:  0.3720155358314514
train gradient:  0.154296815459409
iteration : 646
train acc:  0.921875
train loss:  0.20933468639850616
train gradient:  0.059107908226992065
iteration : 647
train acc:  0.8203125
train loss:  0.3261341452598572
train gradient:  0.10798492766179875
iteration : 648
train acc:  0.8671875
train loss:  0.30625471472740173
train gradient:  0.10795233701066381
iteration : 649
train acc:  0.859375
train loss:  0.290045827627182
train gradient:  0.15090614168233582
iteration : 650
train acc:  0.8515625
train loss:  0.2994006276130676
train gradient:  0.08756515999701915
iteration : 651
train acc:  0.90625
train loss:  0.24822986125946045
train gradient:  0.09980010054870998
iteration : 652
train acc:  0.8203125
train loss:  0.41040685772895813
train gradient:  0.20486475686741584
iteration : 653
train acc:  0.8203125
train loss:  0.3377486765384674
train gradient:  0.12327817395045405
iteration : 654
train acc:  0.8984375
train loss:  0.25286051630973816
train gradient:  0.10571277198112004
iteration : 655
train acc:  0.8515625
train loss:  0.3843235969543457
train gradient:  0.14628514110770485
iteration : 656
train acc:  0.828125
train loss:  0.33500194549560547
train gradient:  0.18148089113948698
iteration : 657
train acc:  0.7890625
train loss:  0.3813358247280121
train gradient:  0.22650634394235108
iteration : 658
train acc:  0.875
train loss:  0.3052729666233063
train gradient:  0.13121894330360245
iteration : 659
train acc:  0.90625
train loss:  0.22410207986831665
train gradient:  0.0646044413524255
iteration : 660
train acc:  0.8359375
train loss:  0.30625468492507935
train gradient:  0.1142033313300084
iteration : 661
train acc:  0.8046875
train loss:  0.421276330947876
train gradient:  0.1677899453749081
iteration : 662
train acc:  0.875
train loss:  0.27488476037979126
train gradient:  0.11076512930430658
iteration : 663
train acc:  0.8359375
train loss:  0.4119565486907959
train gradient:  0.1931415168702913
iteration : 664
train acc:  0.859375
train loss:  0.33876872062683105
train gradient:  0.17718931419323303
iteration : 665
train acc:  0.875
train loss:  0.30832377076148987
train gradient:  0.10654630048632173
iteration : 666
train acc:  0.8359375
train loss:  0.38537436723709106
train gradient:  0.18983261926301798
iteration : 667
train acc:  0.9296875
train loss:  0.21434539556503296
train gradient:  0.10770812793533661
iteration : 668
train acc:  0.8125
train loss:  0.431746244430542
train gradient:  0.19034498776967568
iteration : 669
train acc:  0.875
train loss:  0.31026190519332886
train gradient:  0.1400447569665356
iteration : 670
train acc:  0.90625
train loss:  0.25969409942626953
train gradient:  0.06449653484723791
iteration : 671
train acc:  0.8828125
train loss:  0.3139733672142029
train gradient:  0.142492391986601
iteration : 672
train acc:  0.9140625
train loss:  0.25577014684677124
train gradient:  0.09883944557848481
iteration : 673
train acc:  0.8671875
train loss:  0.26789185404777527
train gradient:  0.08407145519168777
iteration : 674
train acc:  0.8515625
train loss:  0.35513049364089966
train gradient:  0.21840238892718655
iteration : 675
train acc:  0.875
train loss:  0.3023490905761719
train gradient:  0.16423263653962
iteration : 676
train acc:  0.890625
train loss:  0.2544938921928406
train gradient:  0.10548257546634579
iteration : 677
train acc:  0.828125
train loss:  0.3640712797641754
train gradient:  0.1635583381682938
iteration : 678
train acc:  0.859375
train loss:  0.3280504047870636
train gradient:  0.12444685016826955
iteration : 679
train acc:  0.890625
train loss:  0.3029862344264984
train gradient:  0.1000166926939087
iteration : 680
train acc:  0.84375
train loss:  0.32991695404052734
train gradient:  0.13605450490898924
iteration : 681
train acc:  0.8125
train loss:  0.41694197058677673
train gradient:  0.19136525406247012
iteration : 682
train acc:  0.8671875
train loss:  0.3412383794784546
train gradient:  0.11959473287662589
iteration : 683
train acc:  0.859375
train loss:  0.35376277565956116
train gradient:  0.1312549464812086
iteration : 684
train acc:  0.8671875
train loss:  0.29368501901626587
train gradient:  0.140667817317577
iteration : 685
train acc:  0.8515625
train loss:  0.2803325057029724
train gradient:  0.08957465756504578
iteration : 686
train acc:  0.84375
train loss:  0.3511313796043396
train gradient:  0.17855853130321864
iteration : 687
train acc:  0.8359375
train loss:  0.30724167823791504
train gradient:  0.13199808159530463
iteration : 688
train acc:  0.828125
train loss:  0.3750869631767273
train gradient:  0.17983594120347945
iteration : 689
train acc:  0.875
train loss:  0.2968922257423401
train gradient:  0.09181946390920473
iteration : 690
train acc:  0.84375
train loss:  0.38598546385765076
train gradient:  0.17106005743441374
iteration : 691
train acc:  0.8984375
train loss:  0.24820999801158905
train gradient:  0.09662647327510202
iteration : 692
train acc:  0.859375
train loss:  0.29466378688812256
train gradient:  0.08842689324491618
iteration : 693
train acc:  0.8671875
train loss:  0.29284095764160156
train gradient:  0.09864505320323734
iteration : 694
train acc:  0.8671875
train loss:  0.29919660091400146
train gradient:  0.09750338270333758
iteration : 695
train acc:  0.9296875
train loss:  0.2559967339038849
train gradient:  0.06320040701380414
iteration : 696
train acc:  0.8984375
train loss:  0.2693740725517273
train gradient:  0.08430226396535322
iteration : 697
train acc:  0.8515625
train loss:  0.29911965131759644
train gradient:  0.1775673227028185
iteration : 698
train acc:  0.8515625
train loss:  0.29641395807266235
train gradient:  0.08784399883264335
iteration : 699
train acc:  0.84375
train loss:  0.312998503446579
train gradient:  0.1710073391818665
iteration : 700
train acc:  0.921875
train loss:  0.2649463415145874
train gradient:  0.09596613840871537
iteration : 701
train acc:  0.9140625
train loss:  0.24098902940750122
train gradient:  0.11140574734006814
iteration : 702
train acc:  0.8828125
train loss:  0.33460187911987305
train gradient:  0.12891348947455256
iteration : 703
train acc:  0.828125
train loss:  0.3310093283653259
train gradient:  0.14048518791329442
iteration : 704
train acc:  0.84375
train loss:  0.3767284154891968
train gradient:  0.21977669641081998
iteration : 705
train acc:  0.8359375
train loss:  0.34793704748153687
train gradient:  0.19402194187563057
iteration : 706
train acc:  0.8828125
train loss:  0.30710190534591675
train gradient:  0.131209935239541
iteration : 707
train acc:  0.8359375
train loss:  0.3336687982082367
train gradient:  0.13844251467234509
iteration : 708
train acc:  0.8828125
train loss:  0.28889626264572144
train gradient:  0.10788612676285114
iteration : 709
train acc:  0.8125
train loss:  0.3690791130065918
train gradient:  0.19538906483207308
iteration : 710
train acc:  0.9140625
train loss:  0.27005916833877563
train gradient:  0.09005613247669879
iteration : 711
train acc:  0.9296875
train loss:  0.26292458176612854
train gradient:  0.10555453034229374
iteration : 712
train acc:  0.8671875
train loss:  0.32809245586395264
train gradient:  0.1840211229767859
iteration : 713
train acc:  0.859375
train loss:  0.3383609652519226
train gradient:  0.13692457202383312
iteration : 714
train acc:  0.8671875
train loss:  0.3307984471321106
train gradient:  0.12613587474547738
iteration : 715
train acc:  0.8515625
train loss:  0.30580955743789673
train gradient:  0.0799734473692769
iteration : 716
train acc:  0.8984375
train loss:  0.2835545539855957
train gradient:  0.10818078396100769
iteration : 717
train acc:  0.8515625
train loss:  0.3741292357444763
train gradient:  0.153373893504057
iteration : 718
train acc:  0.8515625
train loss:  0.3973448872566223
train gradient:  0.22660767032640738
iteration : 719
train acc:  0.859375
train loss:  0.30060064792633057
train gradient:  0.15804773507954561
iteration : 720
train acc:  0.8515625
train loss:  0.33524376153945923
train gradient:  0.12609398816083714
iteration : 721
train acc:  0.8359375
train loss:  0.35900187492370605
train gradient:  0.1510192476989246
iteration : 722
train acc:  0.8984375
train loss:  0.24557369947433472
train gradient:  0.07677871835117972
iteration : 723
train acc:  0.890625
train loss:  0.2500712275505066
train gradient:  0.07222828169599588
iteration : 724
train acc:  0.8515625
train loss:  0.33403050899505615
train gradient:  0.11032555276948001
iteration : 725
train acc:  0.8515625
train loss:  0.30832135677337646
train gradient:  0.13386644810003412
iteration : 726
train acc:  0.8828125
train loss:  0.3019958436489105
train gradient:  0.19506394441390273
iteration : 727
train acc:  0.84375
train loss:  0.3412818908691406
train gradient:  0.1802524966571134
iteration : 728
train acc:  0.890625
train loss:  0.24975532293319702
train gradient:  0.07558605492687541
iteration : 729
train acc:  0.8515625
train loss:  0.30370861291885376
train gradient:  0.1516345492850145
iteration : 730
train acc:  0.859375
train loss:  0.30231016874313354
train gradient:  0.11538553450196497
iteration : 731
train acc:  0.8828125
train loss:  0.28272294998168945
train gradient:  0.16631318559026403
iteration : 732
train acc:  0.8359375
train loss:  0.38503220677375793
train gradient:  0.1750052757106842
iteration : 733
train acc:  0.84375
train loss:  0.36926865577697754
train gradient:  0.16972424228435112
iteration : 734
train acc:  0.90625
train loss:  0.2420113980770111
train gradient:  0.09912238934645114
iteration : 735
train acc:  0.84375
train loss:  0.358175128698349
train gradient:  0.1368561043323094
iteration : 736
train acc:  0.8359375
train loss:  0.33170586824417114
train gradient:  0.1410269680144371
iteration : 737
train acc:  0.875
train loss:  0.26238858699798584
train gradient:  0.11748659378134181
iteration : 738
train acc:  0.8984375
train loss:  0.22470644116401672
train gradient:  0.08809840671296892
iteration : 739
train acc:  0.8515625
train loss:  0.332817018032074
train gradient:  0.1122421307628182
iteration : 740
train acc:  0.75
train loss:  0.5192055106163025
train gradient:  0.3859762235640226
iteration : 741
train acc:  0.8828125
train loss:  0.29780495166778564
train gradient:  0.13536653922500064
iteration : 742
train acc:  0.890625
train loss:  0.2782958745956421
train gradient:  0.1140031684974202
iteration : 743
train acc:  0.8359375
train loss:  0.33916616439819336
train gradient:  0.10839320113882932
iteration : 744
train acc:  0.859375
train loss:  0.2897447943687439
train gradient:  0.12370104834800817
iteration : 745
train acc:  0.8984375
train loss:  0.27804216742515564
train gradient:  0.13263123932508825
iteration : 746
train acc:  0.84375
train loss:  0.36303314566612244
train gradient:  0.16794719788355525
iteration : 747
train acc:  0.828125
train loss:  0.37216562032699585
train gradient:  0.1245405235152342
iteration : 748
train acc:  0.890625
train loss:  0.2979503870010376
train gradient:  0.15642238308788797
iteration : 749
train acc:  0.8203125
train loss:  0.3743523061275482
train gradient:  0.233568174726027
iteration : 750
train acc:  0.84375
train loss:  0.31905585527420044
train gradient:  0.18818745999596576
iteration : 751
train acc:  0.8828125
train loss:  0.2811318039894104
train gradient:  0.09122238410582047
iteration : 752
train acc:  0.84375
train loss:  0.3494798541069031
train gradient:  0.13756221870968366
iteration : 753
train acc:  0.921875
train loss:  0.26546794176101685
train gradient:  0.08252894872163288
iteration : 754
train acc:  0.8515625
train loss:  0.3208865523338318
train gradient:  0.12761477575741986
iteration : 755
train acc:  0.8515625
train loss:  0.3111549913883209
train gradient:  0.09284066188954723
iteration : 756
train acc:  0.890625
train loss:  0.2630084455013275
train gradient:  0.12543228360553402
iteration : 757
train acc:  0.875
train loss:  0.30042535066604614
train gradient:  0.14182455784497477
iteration : 758
train acc:  0.8984375
train loss:  0.28574228286743164
train gradient:  0.11336585269154117
iteration : 759
train acc:  0.8828125
train loss:  0.31073296070098877
train gradient:  0.1003405598995354
iteration : 760
train acc:  0.921875
train loss:  0.2259785383939743
train gradient:  0.08992459097624801
iteration : 761
train acc:  0.828125
train loss:  0.333783894777298
train gradient:  0.4890628721331323
iteration : 762
train acc:  0.875
train loss:  0.2814430594444275
train gradient:  0.10297556262100083
iteration : 763
train acc:  0.875
train loss:  0.29575592279434204
train gradient:  0.09233713406497489
iteration : 764
train acc:  0.8203125
train loss:  0.37398993968963623
train gradient:  0.15926702630764855
iteration : 765
train acc:  0.890625
train loss:  0.24045643210411072
train gradient:  0.10579299594695978
iteration : 766
train acc:  0.8359375
train loss:  0.33539971709251404
train gradient:  0.14378747406251513
iteration : 767
train acc:  0.9140625
train loss:  0.2408796101808548
train gradient:  0.0917622443105644
iteration : 768
train acc:  0.890625
train loss:  0.2783374488353729
train gradient:  0.10795310829035361
iteration : 769
train acc:  0.8515625
train loss:  0.36516329646110535
train gradient:  0.19521072977412224
iteration : 770
train acc:  0.859375
train loss:  0.34027594327926636
train gradient:  0.10672946515295477
iteration : 771
train acc:  0.8828125
train loss:  0.2383016049861908
train gradient:  0.07494687493532257
iteration : 772
train acc:  0.9296875
train loss:  0.2395404577255249
train gradient:  0.11883774807448784
iteration : 773
train acc:  0.8125
train loss:  0.3706379532814026
train gradient:  0.20911563413310183
iteration : 774
train acc:  0.8359375
train loss:  0.3665200471878052
train gradient:  0.2891270808036975
iteration : 775
train acc:  0.875
train loss:  0.25188910961151123
train gradient:  0.11210497370211477
iteration : 776
train acc:  0.84375
train loss:  0.35973864793777466
train gradient:  0.11408868303400638
iteration : 777
train acc:  0.8828125
train loss:  0.24453437328338623
train gradient:  0.1009361647137861
iteration : 778
train acc:  0.859375
train loss:  0.34485504031181335
train gradient:  0.18830531874526543
iteration : 779
train acc:  0.8359375
train loss:  0.3326825201511383
train gradient:  0.11705428890208987
iteration : 780
train acc:  0.8359375
train loss:  0.3607391119003296
train gradient:  0.16878709845105788
iteration : 781
train acc:  0.875
train loss:  0.27740544080734253
train gradient:  0.12874112837260288
iteration : 782
train acc:  0.8046875
train loss:  0.36281663179397583
train gradient:  0.13510853538596382
iteration : 783
train acc:  0.859375
train loss:  0.3146705627441406
train gradient:  0.11235719339812562
iteration : 784
train acc:  0.875
train loss:  0.31906580924987793
train gradient:  0.15634644472696307
iteration : 785
train acc:  0.8359375
train loss:  0.3079999089241028
train gradient:  0.1970371428452615
iteration : 786
train acc:  0.9140625
train loss:  0.2308422029018402
train gradient:  0.0634629544081681
iteration : 787
train acc:  0.8828125
train loss:  0.2847806215286255
train gradient:  0.09937095720134397
iteration : 788
train acc:  0.8359375
train loss:  0.3665078282356262
train gradient:  0.1330057084284093
iteration : 789
train acc:  0.8359375
train loss:  0.3389880061149597
train gradient:  0.1417042620128272
iteration : 790
train acc:  0.875
train loss:  0.3117968440055847
train gradient:  0.13437371004811233
iteration : 791
train acc:  0.859375
train loss:  0.3209340572357178
train gradient:  0.16208551225869086
iteration : 792
train acc:  0.8828125
train loss:  0.2476080358028412
train gradient:  0.08229332439627954
iteration : 793
train acc:  0.8828125
train loss:  0.37494146823883057
train gradient:  0.15539956892426335
iteration : 794
train acc:  0.84375
train loss:  0.3309716582298279
train gradient:  0.13406118527994815
iteration : 795
train acc:  0.8828125
train loss:  0.2360813170671463
train gradient:  0.09626897195489102
iteration : 796
train acc:  0.8359375
train loss:  0.45454853773117065
train gradient:  0.2351446195297161
iteration : 797
train acc:  0.78125
train loss:  0.43617188930511475
train gradient:  0.22724288554157968
iteration : 798
train acc:  0.8671875
train loss:  0.31751465797424316
train gradient:  0.11184701833928698
iteration : 799
train acc:  0.8671875
train loss:  0.29487985372543335
train gradient:  0.09622094958863513
iteration : 800
train acc:  0.8515625
train loss:  0.3156276345252991
train gradient:  0.10568919057338895
iteration : 801
train acc:  0.8828125
train loss:  0.3868067264556885
train gradient:  0.1383125534808164
iteration : 802
train acc:  0.8515625
train loss:  0.27210748195648193
train gradient:  0.12041155192030095
iteration : 803
train acc:  0.859375
train loss:  0.33856484293937683
train gradient:  0.10269836454053434
iteration : 804
train acc:  0.8828125
train loss:  0.30938035249710083
train gradient:  0.1531353507933695
iteration : 805
train acc:  0.859375
train loss:  0.299296498298645
train gradient:  0.10203142399937275
iteration : 806
train acc:  0.796875
train loss:  0.4003427028656006
train gradient:  0.1771862005295746
iteration : 807
train acc:  0.828125
train loss:  0.36565738916397095
train gradient:  0.15616303071489504
iteration : 808
train acc:  0.8515625
train loss:  0.317707359790802
train gradient:  0.10790916353165991
iteration : 809
train acc:  0.828125
train loss:  0.34139907360076904
train gradient:  0.13681751130090458
iteration : 810
train acc:  0.875
train loss:  0.28269392251968384
train gradient:  0.08597871690693132
iteration : 811
train acc:  0.890625
train loss:  0.3006981313228607
train gradient:  0.1183917958927646
iteration : 812
train acc:  0.921875
train loss:  0.25021085143089294
train gradient:  0.08509847081316609
iteration : 813
train acc:  0.9140625
train loss:  0.2621702551841736
train gradient:  0.11414220827659154
iteration : 814
train acc:  0.8515625
train loss:  0.3410490155220032
train gradient:  0.13256993333963712
iteration : 815
train acc:  0.8359375
train loss:  0.3472059369087219
train gradient:  0.13020683159015634
iteration : 816
train acc:  0.8359375
train loss:  0.36112505197525024
train gradient:  0.18740430347694598
iteration : 817
train acc:  0.859375
train loss:  0.32259804010391235
train gradient:  0.15533031797975266
iteration : 818
train acc:  0.84375
train loss:  0.38707083463668823
train gradient:  0.19698876834957713
iteration : 819
train acc:  0.8671875
train loss:  0.2972390651702881
train gradient:  0.1418124866787469
iteration : 820
train acc:  0.859375
train loss:  0.3295438289642334
train gradient:  0.12073132131435532
iteration : 821
train acc:  0.890625
train loss:  0.26555997133255005
train gradient:  0.11756597697231778
iteration : 822
train acc:  0.8515625
train loss:  0.31222087144851685
train gradient:  0.11823108693408446
iteration : 823
train acc:  0.890625
train loss:  0.2872779369354248
train gradient:  0.1342724270393713
iteration : 824
train acc:  0.859375
train loss:  0.3344689607620239
train gradient:  0.11973812483375297
iteration : 825
train acc:  0.8671875
train loss:  0.287922739982605
train gradient:  0.07139961421430074
iteration : 826
train acc:  0.9453125
train loss:  0.21180939674377441
train gradient:  0.08922375749981276
iteration : 827
train acc:  0.859375
train loss:  0.35824206471443176
train gradient:  0.36910053412794913
iteration : 828
train acc:  0.8828125
train loss:  0.27674388885498047
train gradient:  0.07984850991489176
iteration : 829
train acc:  0.8671875
train loss:  0.2995116710662842
train gradient:  0.16274698505781127
iteration : 830
train acc:  0.859375
train loss:  0.3413712978363037
train gradient:  0.14302957464814312
iteration : 831
train acc:  0.8203125
train loss:  0.381319135427475
train gradient:  0.14102112454857096
iteration : 832
train acc:  0.875
train loss:  0.24866726994514465
train gradient:  0.07955227291309416
iteration : 833
train acc:  0.8671875
train loss:  0.29316771030426025
train gradient:  0.10497214402162129
iteration : 834
train acc:  0.8984375
train loss:  0.3233536183834076
train gradient:  0.11641957478871276
iteration : 835
train acc:  0.8515625
train loss:  0.3146634101867676
train gradient:  0.09999140532372201
iteration : 836
train acc:  0.84375
train loss:  0.3412341773509979
train gradient:  0.1475763331752794
iteration : 837
train acc:  0.9140625
train loss:  0.23419421911239624
train gradient:  0.07842983012882071
iteration : 838
train acc:  0.8125
train loss:  0.4240124523639679
train gradient:  0.2235005338323515
iteration : 839
train acc:  0.859375
train loss:  0.28318604826927185
train gradient:  0.0846378670680728
iteration : 840
train acc:  0.8984375
train loss:  0.2932775914669037
train gradient:  0.08892748803859545
iteration : 841
train acc:  0.859375
train loss:  0.3854803442955017
train gradient:  0.16901447940828696
iteration : 842
train acc:  0.890625
train loss:  0.24654412269592285
train gradient:  0.06943540632108022
iteration : 843
train acc:  0.875
train loss:  0.25877878069877625
train gradient:  0.10126718722728921
iteration : 844
train acc:  0.8671875
train loss:  0.2816750109195709
train gradient:  0.0933325155219156
iteration : 845
train acc:  0.84375
train loss:  0.30944299697875977
train gradient:  0.10978441593969768
iteration : 846
train acc:  0.8828125
train loss:  0.25758782029151917
train gradient:  0.12484859884055266
iteration : 847
train acc:  0.8671875
train loss:  0.2925301194190979
train gradient:  0.2341731550105865
iteration : 848
train acc:  0.84375
train loss:  0.3691072463989258
train gradient:  0.1574709235685146
iteration : 849
train acc:  0.8828125
train loss:  0.32737812399864197
train gradient:  0.14314724830990777
iteration : 850
train acc:  0.890625
train loss:  0.2895359694957733
train gradient:  0.11515076641962976
iteration : 851
train acc:  0.9140625
train loss:  0.26743122935295105
train gradient:  0.09376301189979314
iteration : 852
train acc:  0.8671875
train loss:  0.3083597421646118
train gradient:  0.11295553232269338
iteration : 853
train acc:  0.90625
train loss:  0.25121957063674927
train gradient:  0.10571694395927543
iteration : 854
train acc:  0.859375
train loss:  0.32475900650024414
train gradient:  0.1452276096483473
iteration : 855
train acc:  0.859375
train loss:  0.36826813220977783
train gradient:  0.21621626810514616
iteration : 856
train acc:  0.890625
train loss:  0.26750683784484863
train gradient:  0.10016379976884197
iteration : 857
train acc:  0.8515625
train loss:  0.35539641976356506
train gradient:  0.1409706617377256
iteration : 858
train acc:  0.8671875
train loss:  0.37064042687416077
train gradient:  0.15428972143567748
iteration : 859
train acc:  0.9140625
train loss:  0.25448763370513916
train gradient:  0.10512557173151348
iteration : 860
train acc:  0.8828125
train loss:  0.26300349831581116
train gradient:  0.09586578294460156
iteration : 861
train acc:  0.8359375
train loss:  0.2756429612636566
train gradient:  0.1532605306647128
iteration : 862
train acc:  0.8671875
train loss:  0.3001607060432434
train gradient:  0.11919777077622888
iteration : 863
train acc:  0.8671875
train loss:  0.28110820055007935
train gradient:  0.15254773336544894
iteration : 864
train acc:  0.90625
train loss:  0.2423485964536667
train gradient:  0.1003213431446004
iteration : 865
train acc:  0.828125
train loss:  0.301277220249176
train gradient:  0.09850646922790031
iteration : 866
train acc:  0.8125
train loss:  0.3520734906196594
train gradient:  0.22602958172224297
iteration : 867
train acc:  0.9140625
train loss:  0.23566055297851562
train gradient:  0.09684832746891767
iteration : 868
train acc:  0.921875
train loss:  0.23800471425056458
train gradient:  0.1113814439261085
iteration : 869
train acc:  0.859375
train loss:  0.34353193640708923
train gradient:  0.129319575754274
iteration : 870
train acc:  0.84375
train loss:  0.3021143972873688
train gradient:  0.1863415393201378
iteration : 871
train acc:  0.859375
train loss:  0.3286752700805664
train gradient:  0.1720799384391909
iteration : 872
train acc:  0.8359375
train loss:  0.37318605184555054
train gradient:  0.1763894334396892
iteration : 873
train acc:  0.8359375
train loss:  0.3441356420516968
train gradient:  0.19379361867820288
iteration : 874
train acc:  0.9140625
train loss:  0.31468161940574646
train gradient:  0.15217331665081243
iteration : 875
train acc:  0.8203125
train loss:  0.36572110652923584
train gradient:  0.2104343182540859
iteration : 876
train acc:  0.8203125
train loss:  0.36989057064056396
train gradient:  0.1635676839311348
iteration : 877
train acc:  0.8828125
train loss:  0.34042495489120483
train gradient:  0.15380704426625147
iteration : 878
train acc:  0.84375
train loss:  0.29626530408859253
train gradient:  0.1116760982453594
iteration : 879
train acc:  0.90625
train loss:  0.24706952273845673
train gradient:  0.13825259642300797
iteration : 880
train acc:  0.828125
train loss:  0.3994811773300171
train gradient:  0.23072764971464738
iteration : 881
train acc:  0.890625
train loss:  0.30726075172424316
train gradient:  0.18440607597736242
iteration : 882
train acc:  0.890625
train loss:  0.2724973261356354
train gradient:  0.08857601594451239
iteration : 883
train acc:  0.8828125
train loss:  0.27994099259376526
train gradient:  0.14507854527472808
iteration : 884
train acc:  0.859375
train loss:  0.31423327326774597
train gradient:  0.15174300646010236
iteration : 885
train acc:  0.8984375
train loss:  0.22986763715744019
train gradient:  0.09720143183944861
iteration : 886
train acc:  0.8515625
train loss:  0.30790066719055176
train gradient:  0.13633923530652053
iteration : 887
train acc:  0.859375
train loss:  0.2853003144264221
train gradient:  0.1107513454954205
iteration : 888
train acc:  0.828125
train loss:  0.36414259672164917
train gradient:  0.15942857538322008
iteration : 889
train acc:  0.8984375
train loss:  0.23759010434150696
train gradient:  0.07973929872716017
iteration : 890
train acc:  0.8203125
train loss:  0.36820828914642334
train gradient:  0.18650213299907217
iteration : 891
train acc:  0.8671875
train loss:  0.31154751777648926
train gradient:  0.10342956498718119
iteration : 892
train acc:  0.875
train loss:  0.2718174457550049
train gradient:  0.11852776410463314
iteration : 893
train acc:  0.859375
train loss:  0.2563888728618622
train gradient:  0.08955716221151637
iteration : 894
train acc:  0.9296875
train loss:  0.21205106377601624
train gradient:  0.07194435095241695
iteration : 895
train acc:  0.9140625
train loss:  0.22985392808914185
train gradient:  0.06770031965592399
iteration : 896
train acc:  0.8515625
train loss:  0.28453636169433594
train gradient:  0.07346367378283722
iteration : 897
train acc:  0.8671875
train loss:  0.3320416510105133
train gradient:  0.14483693952917068
iteration : 898
train acc:  0.8359375
train loss:  0.33875221014022827
train gradient:  0.12537914594900385
iteration : 899
train acc:  0.875
train loss:  0.2763805389404297
train gradient:  0.11942042996993958
iteration : 900
train acc:  0.8515625
train loss:  0.35078030824661255
train gradient:  0.1597788366915426
iteration : 901
train acc:  0.8515625
train loss:  0.3500893712043762
train gradient:  0.16188593892335637
iteration : 902
train acc:  0.90625
train loss:  0.2805851697921753
train gradient:  0.10769604965086234
iteration : 903
train acc:  0.8515625
train loss:  0.317918062210083
train gradient:  0.12100329462969989
iteration : 904
train acc:  0.8828125
train loss:  0.29924535751342773
train gradient:  0.11371479397149638
iteration : 905
train acc:  0.875
train loss:  0.25505009293556213
train gradient:  0.11725741259850245
iteration : 906
train acc:  0.8515625
train loss:  0.38122430443763733
train gradient:  0.19244583505308538
iteration : 907
train acc:  0.8671875
train loss:  0.28507328033447266
train gradient:  0.11236731759537885
iteration : 908
train acc:  0.8359375
train loss:  0.3625468611717224
train gradient:  0.14665821698639198
iteration : 909
train acc:  0.9140625
train loss:  0.2585471570491791
train gradient:  0.09745373453399704
iteration : 910
train acc:  0.8359375
train loss:  0.37756818532943726
train gradient:  0.17928585608061953
iteration : 911
train acc:  0.890625
train loss:  0.3035374879837036
train gradient:  0.1379105799749869
iteration : 912
train acc:  0.875
train loss:  0.26705873012542725
train gradient:  0.13722174841486418
iteration : 913
train acc:  0.875
train loss:  0.32919007539749146
train gradient:  0.15566165850176084
iteration : 914
train acc:  0.859375
train loss:  0.3121556341648102
train gradient:  0.13495052753809444
iteration : 915
train acc:  0.8828125
train loss:  0.2228001356124878
train gradient:  0.0809047094994219
iteration : 916
train acc:  0.8203125
train loss:  0.3350265622138977
train gradient:  0.13114234489605087
iteration : 917
train acc:  0.8828125
train loss:  0.3246084749698639
train gradient:  0.1292259998122954
iteration : 918
train acc:  0.859375
train loss:  0.342775821685791
train gradient:  0.18909774097923004
iteration : 919
train acc:  0.890625
train loss:  0.28517186641693115
train gradient:  0.11923128418653844
iteration : 920
train acc:  0.875
train loss:  0.31359779834747314
train gradient:  0.14859236472907505
iteration : 921
train acc:  0.8515625
train loss:  0.3139316439628601
train gradient:  0.1063710081474463
iteration : 922
train acc:  0.8125
train loss:  0.4241167902946472
train gradient:  0.1928093200003334
iteration : 923
train acc:  0.8984375
train loss:  0.27410003542900085
train gradient:  0.09741877847781359
iteration : 924
train acc:  0.84375
train loss:  0.304962158203125
train gradient:  0.1471979608210102
iteration : 925
train acc:  0.890625
train loss:  0.27651485800743103
train gradient:  0.1353930722719338
iteration : 926
train acc:  0.90625
train loss:  0.2129770666360855
train gradient:  0.07560477337812377
iteration : 927
train acc:  0.9296875
train loss:  0.22729246318340302
train gradient:  0.11396893901110025
iteration : 928
train acc:  0.84375
train loss:  0.30954068899154663
train gradient:  0.1257100708475048
iteration : 929
train acc:  0.8671875
train loss:  0.3086366057395935
train gradient:  0.13835288438799437
iteration : 930
train acc:  0.84375
train loss:  0.3525598645210266
train gradient:  0.19026396512340674
iteration : 931
train acc:  0.8828125
train loss:  0.2914663553237915
train gradient:  0.28156297183890194
iteration : 932
train acc:  0.8359375
train loss:  0.34709635376930237
train gradient:  0.1687989978903639
iteration : 933
train acc:  0.9375
train loss:  0.21713688969612122
train gradient:  0.12690070326012265
iteration : 934
train acc:  0.84375
train loss:  0.3248158097267151
train gradient:  0.15358336274201206
iteration : 935
train acc:  0.8203125
train loss:  0.3622037172317505
train gradient:  0.19677016279512222
iteration : 936
train acc:  0.8515625
train loss:  0.3152581751346588
train gradient:  0.15347876879469136
iteration : 937
train acc:  0.875
train loss:  0.31444358825683594
train gradient:  0.17507119654001296
iteration : 938
train acc:  0.859375
train loss:  0.312676340341568
train gradient:  0.135957562869288
iteration : 939
train acc:  0.8203125
train loss:  0.42890968918800354
train gradient:  0.24371801963485681
iteration : 940
train acc:  0.8984375
train loss:  0.2773871421813965
train gradient:  0.09086970706665551
iteration : 941
train acc:  0.8203125
train loss:  0.4247187376022339
train gradient:  0.3080118435960218
iteration : 942
train acc:  0.8046875
train loss:  0.38561707735061646
train gradient:  0.15500170374043704
iteration : 943
train acc:  0.890625
train loss:  0.3822830617427826
train gradient:  0.1943488663734456
iteration : 944
train acc:  0.890625
train loss:  0.2912757396697998
train gradient:  0.09993355318378201
iteration : 945
train acc:  0.890625
train loss:  0.24459567666053772
train gradient:  0.10585855301910099
iteration : 946
train acc:  0.8515625
train loss:  0.28201502561569214
train gradient:  0.11206559019900857
iteration : 947
train acc:  0.828125
train loss:  0.4019893705844879
train gradient:  0.24062732210996862
iteration : 948
train acc:  0.8515625
train loss:  0.37286466360092163
train gradient:  0.19086275849500195
iteration : 949
train acc:  0.8828125
train loss:  0.2903650999069214
train gradient:  0.09666324642422613
iteration : 950
train acc:  0.8359375
train loss:  0.3198264241218567
train gradient:  0.1609319958842288
iteration : 951
train acc:  0.859375
train loss:  0.3231186866760254
train gradient:  0.12109521762415978
iteration : 952
train acc:  0.8671875
train loss:  0.3538252115249634
train gradient:  0.14418370105000594
iteration : 953
train acc:  0.8515625
train loss:  0.313158243894577
train gradient:  0.1271983876975012
iteration : 954
train acc:  0.8203125
train loss:  0.36267781257629395
train gradient:  0.2690774528726558
iteration : 955
train acc:  0.875
train loss:  0.28108757734298706
train gradient:  0.10552369478391752
iteration : 956
train acc:  0.8671875
train loss:  0.32617151737213135
train gradient:  0.13460394788241545
iteration : 957
train acc:  0.9296875
train loss:  0.22445788979530334
train gradient:  0.09500355909216342
iteration : 958
train acc:  0.8984375
train loss:  0.26162195205688477
train gradient:  0.13264392688259682
iteration : 959
train acc:  0.8828125
train loss:  0.2898556590080261
train gradient:  0.09806537381271972
iteration : 960
train acc:  0.8671875
train loss:  0.27204567193984985
train gradient:  0.1173692321997336
iteration : 961
train acc:  0.828125
train loss:  0.31481608748435974
train gradient:  0.13623296266262064
iteration : 962
train acc:  0.8359375
train loss:  0.35120487213134766
train gradient:  0.17915013177216738
iteration : 963
train acc:  0.859375
train loss:  0.31091704964637756
train gradient:  0.09794525322543716
iteration : 964
train acc:  0.8046875
train loss:  0.38231128454208374
train gradient:  0.22277474697415123
iteration : 965
train acc:  0.8671875
train loss:  0.28377020359039307
train gradient:  0.1114222464921496
iteration : 966
train acc:  0.859375
train loss:  0.32724666595458984
train gradient:  0.16528376523017238
iteration : 967
train acc:  0.890625
train loss:  0.2652878165245056
train gradient:  0.11131559180084605
iteration : 968
train acc:  0.875
train loss:  0.2935890257358551
train gradient:  0.07997111459113979
iteration : 969
train acc:  0.875
train loss:  0.2996775209903717
train gradient:  0.11448182698908008
iteration : 970
train acc:  0.8671875
train loss:  0.3073504567146301
train gradient:  0.11502441930006951
iteration : 971
train acc:  0.8515625
train loss:  0.3914727568626404
train gradient:  0.16028473368621843
iteration : 972
train acc:  0.890625
train loss:  0.28960850834846497
train gradient:  0.13787109366774902
iteration : 973
train acc:  0.84375
train loss:  0.35602909326553345
train gradient:  0.14614499591018573
iteration : 974
train acc:  0.875
train loss:  0.3082343339920044
train gradient:  0.12633472818445707
iteration : 975
train acc:  0.8046875
train loss:  0.416820228099823
train gradient:  0.18044213405595821
iteration : 976
train acc:  0.890625
train loss:  0.2315577268600464
train gradient:  0.08671959404662287
iteration : 977
train acc:  0.8828125
train loss:  0.23981329798698425
train gradient:  0.08250603553217756
iteration : 978
train acc:  0.8359375
train loss:  0.3700110614299774
train gradient:  0.15491579171251518
iteration : 979
train acc:  0.890625
train loss:  0.28005164861679077
train gradient:  0.13466492344929126
iteration : 980
train acc:  0.84375
train loss:  0.3481791615486145
train gradient:  0.13699232798360844
iteration : 981
train acc:  0.84375
train loss:  0.2904495596885681
train gradient:  0.12975236907596283
iteration : 982
train acc:  0.875
train loss:  0.29505079984664917
train gradient:  0.08861381070884294
iteration : 983
train acc:  0.8828125
train loss:  0.28999653458595276
train gradient:  0.11030931020022451
iteration : 984
train acc:  0.875
train loss:  0.2699512839317322
train gradient:  0.09581930024538271
iteration : 985
train acc:  0.828125
train loss:  0.3560943603515625
train gradient:  0.12929045051584936
iteration : 986
train acc:  0.8984375
train loss:  0.25880753993988037
train gradient:  0.10861257694166727
iteration : 987
train acc:  0.859375
train loss:  0.31953275203704834
train gradient:  0.11723469853637779
iteration : 988
train acc:  0.8515625
train loss:  0.31370314955711365
train gradient:  0.11797185232162617
iteration : 989
train acc:  0.875
train loss:  0.35431909561157227
train gradient:  0.16432359911309863
iteration : 990
train acc:  0.859375
train loss:  0.3376721143722534
train gradient:  0.1578224715311403
iteration : 991
train acc:  0.8828125
train loss:  0.27059847116470337
train gradient:  0.1162526848647554
iteration : 992
train acc:  0.875
train loss:  0.317659467458725
train gradient:  0.1536292020941364
iteration : 993
train acc:  0.8671875
train loss:  0.3119829595088959
train gradient:  0.19427280786645884
iteration : 994
train acc:  0.8984375
train loss:  0.281970739364624
train gradient:  0.09592746423438027
iteration : 995
train acc:  0.8671875
train loss:  0.30949240922927856
train gradient:  0.13718348427896032
iteration : 996
train acc:  0.859375
train loss:  0.35807740688323975
train gradient:  0.14346766087785684
iteration : 997
train acc:  0.875
train loss:  0.3060506284236908
train gradient:  0.10547557367153218
iteration : 998
train acc:  0.8515625
train loss:  0.3258882462978363
train gradient:  0.1501978090158349
iteration : 999
train acc:  0.84375
train loss:  0.3771830201148987
train gradient:  0.16707442688910118
iteration : 1000
train acc:  0.84375
train loss:  0.34397315979003906
train gradient:  0.15111350059661768
iteration : 1001
train acc:  0.828125
train loss:  0.32564371824264526
train gradient:  0.20466054516858825
iteration : 1002
train acc:  0.8671875
train loss:  0.3472706079483032
train gradient:  0.1351224432310287
iteration : 1003
train acc:  0.8671875
train loss:  0.3208036422729492
train gradient:  0.14237166033894183
iteration : 1004
train acc:  0.859375
train loss:  0.3621281683444977
train gradient:  0.142101018031121
iteration : 1005
train acc:  0.890625
train loss:  0.3169053792953491
train gradient:  0.151117322496709
iteration : 1006
train acc:  0.8515625
train loss:  0.35618072748184204
train gradient:  0.19661621838235283
iteration : 1007
train acc:  0.8984375
train loss:  0.32414790987968445
train gradient:  0.12174374438951127
iteration : 1008
train acc:  0.8359375
train loss:  0.36945831775665283
train gradient:  0.1559945419165473
iteration : 1009
train acc:  0.8671875
train loss:  0.26342281699180603
train gradient:  0.10507623633416309
iteration : 1010
train acc:  0.8515625
train loss:  0.3348720669746399
train gradient:  0.1480106349237258
iteration : 1011
train acc:  0.8203125
train loss:  0.40489456057548523
train gradient:  0.2211483391287531
iteration : 1012
train acc:  0.8515625
train loss:  0.2967139184474945
train gradient:  0.12983442944308915
iteration : 1013
train acc:  0.8203125
train loss:  0.3317807614803314
train gradient:  0.1376206277917087
iteration : 1014
train acc:  0.8828125
train loss:  0.26846444606781006
train gradient:  0.12360318485899974
iteration : 1015
train acc:  0.8671875
train loss:  0.29197022318840027
train gradient:  0.1621092994345405
iteration : 1016
train acc:  0.875
train loss:  0.37708479166030884
train gradient:  0.1575669077795638
iteration : 1017
train acc:  0.8046875
train loss:  0.3334011137485504
train gradient:  0.15981241502109728
iteration : 1018
train acc:  0.8125
train loss:  0.3754490613937378
train gradient:  0.11402990105336466
iteration : 1019
train acc:  0.9140625
train loss:  0.2475799322128296
train gradient:  0.09115261323692239
iteration : 1020
train acc:  0.90625
train loss:  0.24212084710597992
train gradient:  0.10860080825844008
iteration : 1021
train acc:  0.8828125
train loss:  0.35392171144485474
train gradient:  0.15228662767239026
iteration : 1022
train acc:  0.859375
train loss:  0.31748297810554504
train gradient:  0.14285680833346254
iteration : 1023
train acc:  0.875
train loss:  0.27802035212516785
train gradient:  0.11962170824407467
iteration : 1024
train acc:  0.84375
train loss:  0.30993640422821045
train gradient:  0.0804996647447864
iteration : 1025
train acc:  0.828125
train loss:  0.37328261137008667
train gradient:  0.14070941419609542
iteration : 1026
train acc:  0.8046875
train loss:  0.3306935429573059
train gradient:  0.14565072541947782
iteration : 1027
train acc:  0.890625
train loss:  0.27066636085510254
train gradient:  0.09038380691756948
iteration : 1028
train acc:  0.8515625
train loss:  0.3018244504928589
train gradient:  0.15488445016016766
iteration : 1029
train acc:  0.84375
train loss:  0.3617040812969208
train gradient:  0.13234813125963935
iteration : 1030
train acc:  0.859375
train loss:  0.32539552450180054
train gradient:  0.1429640375924518
iteration : 1031
train acc:  0.8671875
train loss:  0.3123462200164795
train gradient:  0.0897593195019142
iteration : 1032
train acc:  0.828125
train loss:  0.3445587754249573
train gradient:  0.11799688916027978
iteration : 1033
train acc:  0.8515625
train loss:  0.3346322476863861
train gradient:  0.11066230781640853
iteration : 1034
train acc:  0.8671875
train loss:  0.3045571446418762
train gradient:  0.10604727856195578
iteration : 1035
train acc:  0.8984375
train loss:  0.26637154817581177
train gradient:  0.08566092405442947
iteration : 1036
train acc:  0.8125
train loss:  0.39395052194595337
train gradient:  0.14759359509824882
iteration : 1037
train acc:  0.828125
train loss:  0.3831828236579895
train gradient:  0.1392681114100943
iteration : 1038
train acc:  0.859375
train loss:  0.34797903895378113
train gradient:  0.16594492246611234
iteration : 1039
train acc:  0.8671875
train loss:  0.32198914885520935
train gradient:  0.1390581651862169
iteration : 1040
train acc:  0.890625
train loss:  0.22982099652290344
train gradient:  0.09026963176917625
iteration : 1041
train acc:  0.921875
train loss:  0.2512659728527069
train gradient:  0.07913680257647802
iteration : 1042
train acc:  0.796875
train loss:  0.38403964042663574
train gradient:  0.17958046134602285
iteration : 1043
train acc:  0.828125
train loss:  0.3266943097114563
train gradient:  0.09249443994942837
iteration : 1044
train acc:  0.8828125
train loss:  0.30214935541152954
train gradient:  0.133408468057154
iteration : 1045
train acc:  0.859375
train loss:  0.3854750990867615
train gradient:  0.15807230824912408
iteration : 1046
train acc:  0.84375
train loss:  0.40075719356536865
train gradient:  0.1490490057149083
iteration : 1047
train acc:  0.875
train loss:  0.3004905581474304
train gradient:  0.09050194754738361
iteration : 1048
train acc:  0.890625
train loss:  0.3196284770965576
train gradient:  0.17246394707610835
iteration : 1049
train acc:  0.8125
train loss:  0.3507830500602722
train gradient:  0.14758704666209166
iteration : 1050
train acc:  0.90625
train loss:  0.2636226713657379
train gradient:  0.06704370229930585
iteration : 1051
train acc:  0.90625
train loss:  0.24906635284423828
train gradient:  0.09342885742751335
iteration : 1052
train acc:  0.7890625
train loss:  0.45647644996643066
train gradient:  0.20594285140213964
iteration : 1053
train acc:  0.8984375
train loss:  0.22971805930137634
train gradient:  0.07072551765530748
iteration : 1054
train acc:  0.8671875
train loss:  0.33620911836624146
train gradient:  0.1399163271500066
iteration : 1055
train acc:  0.8828125
train loss:  0.26413917541503906
train gradient:  0.08223939312189409
iteration : 1056
train acc:  0.84375
train loss:  0.34566423296928406
train gradient:  0.11311210536795469
iteration : 1057
train acc:  0.8984375
train loss:  0.21777111291885376
train gradient:  0.04661074438393338
iteration : 1058
train acc:  0.8828125
train loss:  0.2605310082435608
train gradient:  0.07302317137120867
iteration : 1059
train acc:  0.828125
train loss:  0.437778115272522
train gradient:  0.3051959807587593
iteration : 1060
train acc:  0.8671875
train loss:  0.2751356363296509
train gradient:  0.08748875187400622
iteration : 1061
train acc:  0.84375
train loss:  0.3172321319580078
train gradient:  0.11367593801590624
iteration : 1062
train acc:  0.859375
train loss:  0.3629756569862366
train gradient:  0.14414778781769008
iteration : 1063
train acc:  0.8359375
train loss:  0.34786856174468994
train gradient:  0.1494290636612931
iteration : 1064
train acc:  0.8671875
train loss:  0.33942386507987976
train gradient:  0.09938217865389608
iteration : 1065
train acc:  0.8984375
train loss:  0.2794022858142853
train gradient:  0.13360472719842437
iteration : 1066
train acc:  0.828125
train loss:  0.34975337982177734
train gradient:  0.1845569953359491
iteration : 1067
train acc:  0.8828125
train loss:  0.3062761425971985
train gradient:  0.169070685011943
iteration : 1068
train acc:  0.859375
train loss:  0.3210403323173523
train gradient:  0.10082835269114375
iteration : 1069
train acc:  0.8125
train loss:  0.3372111916542053
train gradient:  0.11205094746579175
iteration : 1070
train acc:  0.890625
train loss:  0.24756887555122375
train gradient:  0.11490236249100685
iteration : 1071
train acc:  0.90625
train loss:  0.2335229218006134
train gradient:  0.0960153355903499
iteration : 1072
train acc:  0.8984375
train loss:  0.29443594813346863
train gradient:  0.08802357650330112
iteration : 1073
train acc:  0.859375
train loss:  0.25055205821990967
train gradient:  0.07864497029632193
iteration : 1074
train acc:  0.859375
train loss:  0.28391364216804504
train gradient:  0.07847490576507696
iteration : 1075
train acc:  0.8515625
train loss:  0.32951846718788147
train gradient:  0.17317758574272757
iteration : 1076
train acc:  0.8984375
train loss:  0.2533821165561676
train gradient:  0.1005649933149466
iteration : 1077
train acc:  0.8828125
train loss:  0.30289503931999207
train gradient:  0.11986151634815004
iteration : 1078
train acc:  0.90625
train loss:  0.24956269562244415
train gradient:  0.08423717743313208
iteration : 1079
train acc:  0.8671875
train loss:  0.35600656270980835
train gradient:  0.1333881051867169
iteration : 1080
train acc:  0.875
train loss:  0.30975964665412903
train gradient:  0.15813571046026775
iteration : 1081
train acc:  0.8828125
train loss:  0.2642773389816284
train gradient:  0.1308104753262399
iteration : 1082
train acc:  0.875
train loss:  0.2543215751647949
train gradient:  0.11463184927362427
iteration : 1083
train acc:  0.90625
train loss:  0.26565903425216675
train gradient:  0.1275619100397879
iteration : 1084
train acc:  0.8671875
train loss:  0.34436213970184326
train gradient:  0.10796778679392248
iteration : 1085
train acc:  0.8828125
train loss:  0.25101545453071594
train gradient:  0.08790385938439314
iteration : 1086
train acc:  0.8828125
train loss:  0.2765057682991028
train gradient:  0.07942908170715837
iteration : 1087
train acc:  0.828125
train loss:  0.371552050113678
train gradient:  0.11651763885085462
iteration : 1088
train acc:  0.859375
train loss:  0.33346372842788696
train gradient:  0.10609507614944591
iteration : 1089
train acc:  0.8515625
train loss:  0.2833000123500824
train gradient:  0.16378950062960979
iteration : 1090
train acc:  0.875
train loss:  0.28936269879341125
train gradient:  0.10948290747437346
iteration : 1091
train acc:  0.828125
train loss:  0.3673055171966553
train gradient:  0.2959289255418515
iteration : 1092
train acc:  0.8671875
train loss:  0.30185526609420776
train gradient:  0.11520687942439019
iteration : 1093
train acc:  0.9296875
train loss:  0.1896439790725708
train gradient:  0.06396920375773944
iteration : 1094
train acc:  0.90625
train loss:  0.27193909883499146
train gradient:  0.09740097523094089
iteration : 1095
train acc:  0.8984375
train loss:  0.27401942014694214
train gradient:  0.0827066933298938
iteration : 1096
train acc:  0.8203125
train loss:  0.3325098156929016
train gradient:  0.14465200501132214
iteration : 1097
train acc:  0.875
train loss:  0.31760454177856445
train gradient:  0.14432166143684622
iteration : 1098
train acc:  0.8515625
train loss:  0.3211621046066284
train gradient:  0.12297373100351003
iteration : 1099
train acc:  0.8359375
train loss:  0.31803154945373535
train gradient:  0.20307828997671748
iteration : 1100
train acc:  0.8828125
train loss:  0.3415029048919678
train gradient:  0.12025177011052758
iteration : 1101
train acc:  0.90625
train loss:  0.25000113248825073
train gradient:  0.09448765210813105
iteration : 1102
train acc:  0.8984375
train loss:  0.2881651818752289
train gradient:  0.1121205264539184
iteration : 1103
train acc:  0.8515625
train loss:  0.3335748314857483
train gradient:  0.17071488577042493
iteration : 1104
train acc:  0.8203125
train loss:  0.4060276746749878
train gradient:  0.23313129723956738
iteration : 1105
train acc:  0.828125
train loss:  0.3126615881919861
train gradient:  0.1165310343625997
iteration : 1106
train acc:  0.9140625
train loss:  0.23242411017417908
train gradient:  0.07876569285069526
iteration : 1107
train acc:  0.8984375
train loss:  0.2486169934272766
train gradient:  0.14284157080793264
iteration : 1108
train acc:  0.859375
train loss:  0.33468085527420044
train gradient:  0.1525168445876764
iteration : 1109
train acc:  0.875
train loss:  0.338762491941452
train gradient:  0.15871168605820574
iteration : 1110
train acc:  0.828125
train loss:  0.355304092168808
train gradient:  0.17796675252746563
iteration : 1111
train acc:  0.8203125
train loss:  0.40800464153289795
train gradient:  0.12646513088974148
iteration : 1112
train acc:  0.875
train loss:  0.30633774399757385
train gradient:  0.13055285113361112
iteration : 1113
train acc:  0.8984375
train loss:  0.27492332458496094
train gradient:  0.08256708074307254
iteration : 1114
train acc:  0.828125
train loss:  0.32336804270744324
train gradient:  0.08732864643765136
iteration : 1115
train acc:  0.859375
train loss:  0.3447417914867401
train gradient:  0.15430104624793725
iteration : 1116
train acc:  0.859375
train loss:  0.343698114156723
train gradient:  0.13016371748830297
iteration : 1117
train acc:  0.890625
train loss:  0.2544771432876587
train gradient:  0.09865069442448604
iteration : 1118
train acc:  0.875
train loss:  0.40155065059661865
train gradient:  0.23328468743342998
iteration : 1119
train acc:  0.859375
train loss:  0.34177058935165405
train gradient:  0.11647808709237753
iteration : 1120
train acc:  0.8203125
train loss:  0.3917294144630432
train gradient:  0.18144396546354002
iteration : 1121
train acc:  0.8359375
train loss:  0.32857972383499146
train gradient:  0.19508701335288037
iteration : 1122
train acc:  0.8671875
train loss:  0.3126789331436157
train gradient:  0.14070620227775496
iteration : 1123
train acc:  0.859375
train loss:  0.2985864281654358
train gradient:  0.1459037453890979
iteration : 1124
train acc:  0.8515625
train loss:  0.32933908700942993
train gradient:  0.13129122496764647
iteration : 1125
train acc:  0.8515625
train loss:  0.3599999248981476
train gradient:  0.16958932154574968
iteration : 1126
train acc:  0.875
train loss:  0.3438927233219147
train gradient:  0.13648710006081916
iteration : 1127
train acc:  0.8515625
train loss:  0.31714490056037903
train gradient:  0.12449088829481467
iteration : 1128
train acc:  0.921875
train loss:  0.2348032295703888
train gradient:  0.09002710973059941
iteration : 1129
train acc:  0.859375
train loss:  0.30121758580207825
train gradient:  0.14217142553794004
iteration : 1130
train acc:  0.921875
train loss:  0.21537503600120544
train gradient:  0.06664546560494533
iteration : 1131
train acc:  0.890625
train loss:  0.32036542892456055
train gradient:  0.1292747393261844
iteration : 1132
train acc:  0.8984375
train loss:  0.23923419415950775
train gradient:  0.09056868347975208
iteration : 1133
train acc:  0.8203125
train loss:  0.3637576699256897
train gradient:  0.13037335778003978
iteration : 1134
train acc:  0.8125
train loss:  0.3994136452674866
train gradient:  0.16163642886697757
iteration : 1135
train acc:  0.875
train loss:  0.2867089509963989
train gradient:  0.16426475661027368
iteration : 1136
train acc:  0.8671875
train loss:  0.27450162172317505
train gradient:  0.09380512847188748
iteration : 1137
train acc:  0.890625
train loss:  0.25233858823776245
train gradient:  0.12621819567399778
iteration : 1138
train acc:  0.8125
train loss:  0.3765389323234558
train gradient:  0.2616712838543269
iteration : 1139
train acc:  0.8671875
train loss:  0.26350849866867065
train gradient:  0.11250075125673487
iteration : 1140
train acc:  0.90625
train loss:  0.2421722114086151
train gradient:  0.08305772739720035
iteration : 1141
train acc:  0.890625
train loss:  0.3052455186843872
train gradient:  0.11547395665190784
iteration : 1142
train acc:  0.875
train loss:  0.29915109276771545
train gradient:  0.10051139439668967
iteration : 1143
train acc:  0.875
train loss:  0.36091071367263794
train gradient:  0.16961615892227155
iteration : 1144
train acc:  0.84375
train loss:  0.3172786235809326
train gradient:  0.09744459418693487
iteration : 1145
train acc:  0.890625
train loss:  0.2806212306022644
train gradient:  0.12953471600659616
iteration : 1146
train acc:  0.875
train loss:  0.2856179177761078
train gradient:  0.08882789275807966
iteration : 1147
train acc:  0.875
train loss:  0.3206435739994049
train gradient:  0.14306635668412274
iteration : 1148
train acc:  0.8828125
train loss:  0.2741844058036804
train gradient:  0.08446157866774893
iteration : 1149
train acc:  0.78125
train loss:  0.4060477018356323
train gradient:  0.24069271755788818
iteration : 1150
train acc:  0.84375
train loss:  0.34479981660842896
train gradient:  0.22395010313210467
iteration : 1151
train acc:  0.8671875
train loss:  0.31123220920562744
train gradient:  0.13125118205246644
iteration : 1152
train acc:  0.875
train loss:  0.26741737127304077
train gradient:  0.08481415338986208
iteration : 1153
train acc:  0.875
train loss:  0.37401723861694336
train gradient:  0.12316226880312402
iteration : 1154
train acc:  0.875
train loss:  0.3290100693702698
train gradient:  0.27347487288886635
iteration : 1155
train acc:  0.90625
train loss:  0.3120425343513489
train gradient:  0.10266297837769525
iteration : 1156
train acc:  0.8203125
train loss:  0.3698651194572449
train gradient:  0.16682387390872044
iteration : 1157
train acc:  0.859375
train loss:  0.24103698134422302
train gradient:  0.10468178670742492
iteration : 1158
train acc:  0.8984375
train loss:  0.2525225579738617
train gradient:  0.07567755187052108
iteration : 1159
train acc:  0.859375
train loss:  0.29157084226608276
train gradient:  0.10968522233451203
iteration : 1160
train acc:  0.8828125
train loss:  0.2865701913833618
train gradient:  0.12646057990763165
iteration : 1161
train acc:  0.8984375
train loss:  0.2665925920009613
train gradient:  0.09169402988788204
iteration : 1162
train acc:  0.8828125
train loss:  0.26365143060684204
train gradient:  0.1151232698272759
iteration : 1163
train acc:  0.8359375
train loss:  0.3813320994377136
train gradient:  0.15094709687478275
iteration : 1164
train acc:  0.90625
train loss:  0.221678227186203
train gradient:  0.08658291969649144
iteration : 1165
train acc:  0.890625
train loss:  0.30562424659729004
train gradient:  0.12090530573402326
iteration : 1166
train acc:  0.8828125
train loss:  0.2570395767688751
train gradient:  0.15941883008257168
iteration : 1167
train acc:  0.875
train loss:  0.27501580119132996
train gradient:  0.09942014761580825
iteration : 1168
train acc:  0.875
train loss:  0.2789004445075989
train gradient:  0.10329873245685076
iteration : 1169
train acc:  0.875
train loss:  0.30068808794021606
train gradient:  0.09636627589631916
iteration : 1170
train acc:  0.8359375
train loss:  0.37527942657470703
train gradient:  0.16423346457237836
iteration : 1171
train acc:  0.84375
train loss:  0.36393967270851135
train gradient:  0.12405854716304608
iteration : 1172
train acc:  0.8359375
train loss:  0.337746262550354
train gradient:  0.12300916983629301
iteration : 1173
train acc:  0.9296875
train loss:  0.21820491552352905
train gradient:  0.07472708861990272
iteration : 1174
train acc:  0.8984375
train loss:  0.29560643434524536
train gradient:  0.1294243124812368
iteration : 1175
train acc:  0.8125
train loss:  0.38203150033950806
train gradient:  0.2063726850494656
iteration : 1176
train acc:  0.8984375
train loss:  0.25644612312316895
train gradient:  0.08583654726933217
iteration : 1177
train acc:  0.8515625
train loss:  0.2970643937587738
train gradient:  0.10297492191982181
iteration : 1178
train acc:  0.890625
train loss:  0.274699330329895
train gradient:  0.08027087292798729
iteration : 1179
train acc:  0.8828125
train loss:  0.27833467721939087
train gradient:  0.1043432912115497
iteration : 1180
train acc:  0.8671875
train loss:  0.32834169268608093
train gradient:  0.21068809155255072
iteration : 1181
train acc:  0.875
train loss:  0.34149426221847534
train gradient:  0.11789968382016709
iteration : 1182
train acc:  0.8828125
train loss:  0.25910112261772156
train gradient:  0.1012030739296039
iteration : 1183
train acc:  0.8359375
train loss:  0.36645662784576416
train gradient:  0.1821509124375842
iteration : 1184
train acc:  0.8359375
train loss:  0.31609588861465454
train gradient:  0.12909365034240433
iteration : 1185
train acc:  0.859375
train loss:  0.3025585114955902
train gradient:  0.11518554298200896
iteration : 1186
train acc:  0.84375
train loss:  0.3452892303466797
train gradient:  0.11759550231266351
iteration : 1187
train acc:  0.8515625
train loss:  0.32814329862594604
train gradient:  0.1912291553389175
iteration : 1188
train acc:  0.875
train loss:  0.273611456155777
train gradient:  0.13298598958293678
iteration : 1189
train acc:  0.8671875
train loss:  0.2909644842147827
train gradient:  0.10182723702293796
iteration : 1190
train acc:  0.8671875
train loss:  0.3322625756263733
train gradient:  0.11891249855075338
iteration : 1191
train acc:  0.8515625
train loss:  0.30114734172821045
train gradient:  0.10217464354019287
iteration : 1192
train acc:  0.84375
train loss:  0.3477950096130371
train gradient:  0.11671833345638272
iteration : 1193
train acc:  0.8984375
train loss:  0.31273722648620605
train gradient:  0.1480990042575156
iteration : 1194
train acc:  0.9140625
train loss:  0.23756706714630127
train gradient:  0.058537081378596374
iteration : 1195
train acc:  0.859375
train loss:  0.31433597207069397
train gradient:  0.14199810022068868
iteration : 1196
train acc:  0.8125
train loss:  0.38209453225135803
train gradient:  0.20116658844009772
iteration : 1197
train acc:  0.8515625
train loss:  0.36140793561935425
train gradient:  0.1804389565065241
iteration : 1198
train acc:  0.875
train loss:  0.32702893018722534
train gradient:  0.18595915936651192
iteration : 1199
train acc:  0.8125
train loss:  0.3682580590248108
train gradient:  0.19633232545179302
iteration : 1200
train acc:  0.8515625
train loss:  0.3511757254600525
train gradient:  0.1625511812015657
iteration : 1201
train acc:  0.8515625
train loss:  0.3959776759147644
train gradient:  0.17255863405088342
iteration : 1202
train acc:  0.90625
train loss:  0.25151166319847107
train gradient:  0.09407962810951641
iteration : 1203
train acc:  0.875
train loss:  0.3010507822036743
train gradient:  0.2185560883492905
iteration : 1204
train acc:  0.8828125
train loss:  0.2723239064216614
train gradient:  0.13012262054874896
iteration : 1205
train acc:  0.859375
train loss:  0.3402128517627716
train gradient:  0.16116444892526577
iteration : 1206
train acc:  0.84375
train loss:  0.3235088586807251
train gradient:  0.2113923605582511
iteration : 1207
train acc:  0.875
train loss:  0.27904054522514343
train gradient:  0.13078788725387114
iteration : 1208
train acc:  0.8984375
train loss:  0.27395927906036377
train gradient:  0.10563164169580842
iteration : 1209
train acc:  0.859375
train loss:  0.3565441071987152
train gradient:  0.10087406015550245
iteration : 1210
train acc:  0.859375
train loss:  0.332614004611969
train gradient:  0.12085641870578369
iteration : 1211
train acc:  0.8828125
train loss:  0.28847992420196533
train gradient:  0.08452959920126353
iteration : 1212
train acc:  0.859375
train loss:  0.28557759523391724
train gradient:  0.15021131037178487
iteration : 1213
train acc:  0.8984375
train loss:  0.27364057302474976
train gradient:  0.100590601503884
iteration : 1214
train acc:  0.8203125
train loss:  0.3565736711025238
train gradient:  0.12959770540600268
iteration : 1215
train acc:  0.890625
train loss:  0.27963751554489136
train gradient:  0.11201398845015564
iteration : 1216
train acc:  0.890625
train loss:  0.23761406540870667
train gradient:  0.07680669258287223
iteration : 1217
train acc:  0.859375
train loss:  0.3164197504520416
train gradient:  0.09759098260709628
iteration : 1218
train acc:  0.8671875
train loss:  0.3499397039413452
train gradient:  0.15417210419976712
iteration : 1219
train acc:  0.890625
train loss:  0.26213252544403076
train gradient:  0.07007070301268065
iteration : 1220
train acc:  0.859375
train loss:  0.32296139001846313
train gradient:  0.12156717652428467
iteration : 1221
train acc:  0.84375
train loss:  0.39288079738616943
train gradient:  0.22957861618104225
iteration : 1222
train acc:  0.8359375
train loss:  0.3309774398803711
train gradient:  0.13910716026509173
iteration : 1223
train acc:  0.8359375
train loss:  0.4124206006526947
train gradient:  0.12991719543422828
iteration : 1224
train acc:  0.8515625
train loss:  0.313213974237442
train gradient:  0.10236153147623271
iteration : 1225
train acc:  0.8671875
train loss:  0.3079972267150879
train gradient:  0.13364092227872018
iteration : 1226
train acc:  0.8515625
train loss:  0.3088090121746063
train gradient:  0.11063702085239374
iteration : 1227
train acc:  0.90625
train loss:  0.2516273558139801
train gradient:  0.1394218101852994
iteration : 1228
train acc:  0.8828125
train loss:  0.2815552055835724
train gradient:  0.13005609468620966
iteration : 1229
train acc:  0.84375
train loss:  0.33856961131095886
train gradient:  0.1664792844744161
iteration : 1230
train acc:  0.8671875
train loss:  0.3093271851539612
train gradient:  0.11121587572326176
iteration : 1231
train acc:  0.875
train loss:  0.30848428606987
train gradient:  0.10134519784141208
iteration : 1232
train acc:  0.9609375
train loss:  0.1856294572353363
train gradient:  0.0806330952540793
iteration : 1233
train acc:  0.8984375
train loss:  0.2558223009109497
train gradient:  0.07960082583735505
iteration : 1234
train acc:  0.859375
train loss:  0.30665427446365356
train gradient:  0.12976482636656372
iteration : 1235
train acc:  0.7890625
train loss:  0.43931353092193604
train gradient:  0.203605131042598
iteration : 1236
train acc:  0.84375
train loss:  0.3354402184486389
train gradient:  0.13399209589095032
iteration : 1237
train acc:  0.8359375
train loss:  0.31776735186576843
train gradient:  0.10648971500860697
iteration : 1238
train acc:  0.8828125
train loss:  0.31618034839630127
train gradient:  0.13878249282950988
iteration : 1239
train acc:  0.90625
train loss:  0.25101912021636963
train gradient:  0.07585680284085508
iteration : 1240
train acc:  0.890625
train loss:  0.27380770444869995
train gradient:  0.08380907608973426
iteration : 1241
train acc:  0.8828125
train loss:  0.33378398418426514
train gradient:  0.16549619512361433
iteration : 1242
train acc:  0.8671875
train loss:  0.2781881093978882
train gradient:  0.09340961303059593
iteration : 1243
train acc:  0.890625
train loss:  0.29349079728126526
train gradient:  0.13570991791019926
iteration : 1244
train acc:  0.84375
train loss:  0.3506055176258087
train gradient:  0.21616544953102598
iteration : 1245
train acc:  0.890625
train loss:  0.2803424298763275
train gradient:  0.11739893982877704
iteration : 1246
train acc:  0.828125
train loss:  0.3726155757904053
train gradient:  0.24225612027364912
iteration : 1247
train acc:  0.875
train loss:  0.3265581727027893
train gradient:  0.0907692810691748
iteration : 1248
train acc:  0.8515625
train loss:  0.32722654938697815
train gradient:  0.106606744469452
iteration : 1249
train acc:  0.8671875
train loss:  0.3577384352684021
train gradient:  0.12935832023372534
iteration : 1250
train acc:  0.84375
train loss:  0.3213684558868408
train gradient:  0.14293743455739794
iteration : 1251
train acc:  0.8203125
train loss:  0.367609441280365
train gradient:  0.1944750263224781
iteration : 1252
train acc:  0.9140625
train loss:  0.2406030297279358
train gradient:  0.07481050299441802
iteration : 1253
train acc:  0.828125
train loss:  0.39617565274238586
train gradient:  0.1658217249934474
iteration : 1254
train acc:  0.875
train loss:  0.29704880714416504
train gradient:  0.10523014854468152
iteration : 1255
train acc:  0.8515625
train loss:  0.3204140067100525
train gradient:  0.11101897279336587
iteration : 1256
train acc:  0.828125
train loss:  0.3921528458595276
train gradient:  0.17098237831624982
iteration : 1257
train acc:  0.8828125
train loss:  0.2758745849132538
train gradient:  0.09683302204903234
iteration : 1258
train acc:  0.859375
train loss:  0.3194776177406311
train gradient:  0.1586035167050814
iteration : 1259
train acc:  0.890625
train loss:  0.25071534514427185
train gradient:  0.14068769453131874
iteration : 1260
train acc:  0.8046875
train loss:  0.29011040925979614
train gradient:  0.12106175755307529
iteration : 1261
train acc:  0.875
train loss:  0.3167347311973572
train gradient:  0.1260551310738765
iteration : 1262
train acc:  0.890625
train loss:  0.31547248363494873
train gradient:  0.15683677749545813
iteration : 1263
train acc:  0.8671875
train loss:  0.2456691861152649
train gradient:  0.08655101597999623
iteration : 1264
train acc:  0.8515625
train loss:  0.29240402579307556
train gradient:  0.09391031878542412
iteration : 1265
train acc:  0.8828125
train loss:  0.26801660656929016
train gradient:  0.07832570174078389
iteration : 1266
train acc:  0.8828125
train loss:  0.2835679054260254
train gradient:  0.18875937768446344
iteration : 1267
train acc:  0.9140625
train loss:  0.26741015911102295
train gradient:  0.11478132181551572
iteration : 1268
train acc:  0.796875
train loss:  0.4146415591239929
train gradient:  0.22985799621117475
iteration : 1269
train acc:  0.8515625
train loss:  0.3227783441543579
train gradient:  0.18956094951396918
iteration : 1270
train acc:  0.8671875
train loss:  0.2703046202659607
train gradient:  0.09979684142204026
iteration : 1271
train acc:  0.8515625
train loss:  0.25247660279273987
train gradient:  0.10290562177266707
iteration : 1272
train acc:  0.8828125
train loss:  0.24661606550216675
train gradient:  0.1531705763939148
iteration : 1273
train acc:  0.8671875
train loss:  0.3594132661819458
train gradient:  0.16017968502399874
iteration : 1274
train acc:  0.8828125
train loss:  0.27576708793640137
train gradient:  0.12865719104042933
iteration : 1275
train acc:  0.890625
train loss:  0.2695418894290924
train gradient:  0.11484830961950167
iteration : 1276
train acc:  0.890625
train loss:  0.2573580741882324
train gradient:  0.0740728254979239
iteration : 1277
train acc:  0.8671875
train loss:  0.315426230430603
train gradient:  0.12957361059169828
iteration : 1278
train acc:  0.828125
train loss:  0.3062242865562439
train gradient:  0.0976103137684436
iteration : 1279
train acc:  0.859375
train loss:  0.2793525159358978
train gradient:  0.09408970665832127
iteration : 1280
train acc:  0.796875
train loss:  0.435621440410614
train gradient:  0.13289215465711793
iteration : 1281
train acc:  0.90625
train loss:  0.2373036891222
train gradient:  0.06303993320333084
iteration : 1282
train acc:  0.8515625
train loss:  0.3391914665699005
train gradient:  0.16978955089768472
iteration : 1283
train acc:  0.8828125
train loss:  0.24510402977466583
train gradient:  0.08731208583411468
iteration : 1284
train acc:  0.8984375
train loss:  0.2793331444263458
train gradient:  0.08333021681945033
iteration : 1285
train acc:  0.8359375
train loss:  0.3410535752773285
train gradient:  0.16027514353666966
iteration : 1286
train acc:  0.875
train loss:  0.32376354932785034
train gradient:  0.128208837168628
iteration : 1287
train acc:  0.859375
train loss:  0.31701725721359253
train gradient:  0.12308989817163482
iteration : 1288
train acc:  0.875
train loss:  0.41110244393348694
train gradient:  0.1659688219080613
iteration : 1289
train acc:  0.8828125
train loss:  0.28342413902282715
train gradient:  0.07610105990638924
iteration : 1290
train acc:  0.8828125
train loss:  0.28204989433288574
train gradient:  0.09514748838928724
iteration : 1291
train acc:  0.921875
train loss:  0.19663144648075104
train gradient:  0.0816513770686647
iteration : 1292
train acc:  0.8515625
train loss:  0.37230804562568665
train gradient:  0.15679196451095473
iteration : 1293
train acc:  0.84375
train loss:  0.39216917753219604
train gradient:  0.18790298224339536
iteration : 1294
train acc:  0.8828125
train loss:  0.3024345338344574
train gradient:  0.11641971205132129
iteration : 1295
train acc:  0.859375
train loss:  0.3459104895591736
train gradient:  0.16763648991334856
iteration : 1296
train acc:  0.890625
train loss:  0.23378288745880127
train gradient:  0.06653469360837158
iteration : 1297
train acc:  0.921875
train loss:  0.24520057439804077
train gradient:  0.08904992542191754
iteration : 1298
train acc:  0.8984375
train loss:  0.294000506401062
train gradient:  0.13659939817263583
iteration : 1299
train acc:  0.8984375
train loss:  0.2735021114349365
train gradient:  0.08717174670166436
iteration : 1300
train acc:  0.890625
train loss:  0.2859269976615906
train gradient:  0.09785058521734498
iteration : 1301
train acc:  0.875
train loss:  0.26709604263305664
train gradient:  0.12098583885891066
iteration : 1302
train acc:  0.859375
train loss:  0.2919881343841553
train gradient:  0.11827275284019144
iteration : 1303
train acc:  0.8671875
train loss:  0.3088221549987793
train gradient:  0.13080962645632646
iteration : 1304
train acc:  0.8125
train loss:  0.33051633834838867
train gradient:  0.15918218005003232
iteration : 1305
train acc:  0.8125
train loss:  0.46005985140800476
train gradient:  0.2542257669486372
iteration : 1306
train acc:  0.890625
train loss:  0.2384546846151352
train gradient:  0.10667532274187379
iteration : 1307
train acc:  0.859375
train loss:  0.3232012987136841
train gradient:  0.18346584414073835
iteration : 1308
train acc:  0.859375
train loss:  0.3193301260471344
train gradient:  0.09349630161009924
iteration : 1309
train acc:  0.8828125
train loss:  0.31170007586479187
train gradient:  0.1149757737398466
iteration : 1310
train acc:  0.8828125
train loss:  0.2568342685699463
train gradient:  0.08120630899251922
iteration : 1311
train acc:  0.875
train loss:  0.2535304129123688
train gradient:  0.15722698081798706
iteration : 1312
train acc:  0.8046875
train loss:  0.3591754734516144
train gradient:  0.13729418269307836
iteration : 1313
train acc:  0.8828125
train loss:  0.28328603506088257
train gradient:  0.09065796718131647
iteration : 1314
train acc:  0.8515625
train loss:  0.31632983684539795
train gradient:  0.20032727048493304
iteration : 1315
train acc:  0.8515625
train loss:  0.3537585139274597
train gradient:  0.10637390480980737
iteration : 1316
train acc:  0.875
train loss:  0.26669132709503174
train gradient:  0.08166932019058393
iteration : 1317
train acc:  0.8671875
train loss:  0.2945997714996338
train gradient:  0.09687121086689603
iteration : 1318
train acc:  0.828125
train loss:  0.3517078161239624
train gradient:  0.15700922831681147
iteration : 1319
train acc:  0.859375
train loss:  0.30552345514297485
train gradient:  0.11244217445873872
iteration : 1320
train acc:  0.796875
train loss:  0.4392363131046295
train gradient:  0.1791062732138402
iteration : 1321
train acc:  0.8828125
train loss:  0.2807672917842865
train gradient:  0.11804166317283807
iteration : 1322
train acc:  0.8828125
train loss:  0.2948818802833557
train gradient:  0.11268504125291834
iteration : 1323
train acc:  0.8515625
train loss:  0.3445869982242584
train gradient:  0.1222136592009216
iteration : 1324
train acc:  0.90625
train loss:  0.27041250467300415
train gradient:  0.08607689090655724
iteration : 1325
train acc:  0.90625
train loss:  0.20910832285881042
train gradient:  0.07275086845211265
iteration : 1326
train acc:  0.84375
train loss:  0.33478081226348877
train gradient:  0.11936199715760018
iteration : 1327
train acc:  0.8671875
train loss:  0.3459151089191437
train gradient:  0.21014183957338922
iteration : 1328
train acc:  0.8671875
train loss:  0.31022006273269653
train gradient:  0.1452420684561624
iteration : 1329
train acc:  0.8828125
train loss:  0.27365681529045105
train gradient:  0.1479950097393956
iteration : 1330
train acc:  0.8671875
train loss:  0.2661229372024536
train gradient:  0.07695585537935282
iteration : 1331
train acc:  0.8359375
train loss:  0.32044345140457153
train gradient:  0.12508704172990043
iteration : 1332
train acc:  0.765625
train loss:  0.4469301700592041
train gradient:  0.2554555999085698
iteration : 1333
train acc:  0.9296875
train loss:  0.2386920154094696
train gradient:  0.06880919114651426
iteration : 1334
train acc:  0.875
train loss:  0.2975504696369171
train gradient:  0.1087266313279894
iteration : 1335
train acc:  0.890625
train loss:  0.36147934198379517
train gradient:  0.14379817009794427
iteration : 1336
train acc:  0.84375
train loss:  0.3162550628185272
train gradient:  0.13534487332894582
iteration : 1337
train acc:  0.7890625
train loss:  0.41700083017349243
train gradient:  0.2170980243275273
iteration : 1338
train acc:  0.8984375
train loss:  0.22901886701583862
train gradient:  0.07660969935135144
iteration : 1339
train acc:  0.8359375
train loss:  0.28268182277679443
train gradient:  0.1328046164135726
iteration : 1340
train acc:  0.8203125
train loss:  0.3420940637588501
train gradient:  0.16701965369117297
iteration : 1341
train acc:  0.8515625
train loss:  0.36586129665374756
train gradient:  0.1524881122671261
iteration : 1342
train acc:  0.8515625
train loss:  0.3078376352787018
train gradient:  0.10194915074650643
iteration : 1343
train acc:  0.8046875
train loss:  0.4349232614040375
train gradient:  0.23162393841738302
iteration : 1344
train acc:  0.84375
train loss:  0.31056535243988037
train gradient:  0.14159430468446635
iteration : 1345
train acc:  0.84375
train loss:  0.3541565537452698
train gradient:  0.1658363682925248
iteration : 1346
train acc:  0.8515625
train loss:  0.3602900207042694
train gradient:  0.1792908627334427
iteration : 1347
train acc:  0.875
train loss:  0.2777757942676544
train gradient:  0.07713188592111625
iteration : 1348
train acc:  0.890625
train loss:  0.25934475660324097
train gradient:  0.11729389746241105
iteration : 1349
train acc:  0.859375
train loss:  0.32470712065696716
train gradient:  0.11171194260408764
iteration : 1350
train acc:  0.8203125
train loss:  0.3395755887031555
train gradient:  0.13637418710584812
iteration : 1351
train acc:  0.890625
train loss:  0.24284133315086365
train gradient:  0.2515198272010758
iteration : 1352
train acc:  0.8359375
train loss:  0.3145838677883148
train gradient:  0.16678784044530243
iteration : 1353
train acc:  0.84375
train loss:  0.2731814384460449
train gradient:  0.11969435459162957
iteration : 1354
train acc:  0.8828125
train loss:  0.28562888503074646
train gradient:  0.10823689062498078
iteration : 1355
train acc:  0.8359375
train loss:  0.3342567980289459
train gradient:  0.14590344547921646
iteration : 1356
train acc:  0.8671875
train loss:  0.26892802119255066
train gradient:  0.12380305722060736
iteration : 1357
train acc:  0.8515625
train loss:  0.2944558262825012
train gradient:  0.1292365028200791
iteration : 1358
train acc:  0.890625
train loss:  0.24411837756633759
train gradient:  0.09267806989402363
iteration : 1359
train acc:  0.828125
train loss:  0.41584688425064087
train gradient:  0.1740635510612426
iteration : 1360
train acc:  0.9296875
train loss:  0.1999802589416504
train gradient:  0.06358590881570438
iteration : 1361
train acc:  0.8046875
train loss:  0.407646119594574
train gradient:  0.34107708991468305
iteration : 1362
train acc:  0.7890625
train loss:  0.4262249171733856
train gradient:  0.17157812247768095
iteration : 1363
train acc:  0.8125
train loss:  0.4078008532524109
train gradient:  0.21401352960969755
iteration : 1364
train acc:  0.8515625
train loss:  0.30119043588638306
train gradient:  0.2934331371107346
iteration : 1365
train acc:  0.875
train loss:  0.3212053179740906
train gradient:  0.14299038007047804
iteration : 1366
train acc:  0.90625
train loss:  0.25820496678352356
train gradient:  0.09692270253593088
iteration : 1367
train acc:  0.8671875
train loss:  0.3067830801010132
train gradient:  0.18474258350181824
iteration : 1368
train acc:  0.84375
train loss:  0.30255749821662903
train gradient:  0.09170647395746613
iteration : 1369
train acc:  0.859375
train loss:  0.2908778190612793
train gradient:  0.0820247238748765
iteration : 1370
train acc:  0.84375
train loss:  0.3518422842025757
train gradient:  0.15460799561602961
iteration : 1371
train acc:  0.8515625
train loss:  0.2986963391304016
train gradient:  0.13453862248934603
iteration : 1372
train acc:  0.8984375
train loss:  0.271520733833313
train gradient:  0.08893966731292183
iteration : 1373
train acc:  0.8515625
train loss:  0.3822375535964966
train gradient:  0.22609927375288433
iteration : 1374
train acc:  0.8671875
train loss:  0.3241577744483948
train gradient:  0.13978975740600352
iteration : 1375
train acc:  0.8828125
train loss:  0.2836061716079712
train gradient:  0.12193224778494605
iteration : 1376
train acc:  0.8828125
train loss:  0.2984645962715149
train gradient:  0.09740108659704526
iteration : 1377
train acc:  0.8515625
train loss:  0.30612149834632874
train gradient:  0.17986977611101013
iteration : 1378
train acc:  0.8203125
train loss:  0.4347321391105652
train gradient:  0.22397626784487717
iteration : 1379
train acc:  0.8828125
train loss:  0.27721408009529114
train gradient:  0.1040039271593508
iteration : 1380
train acc:  0.8671875
train loss:  0.32180410623550415
train gradient:  0.11883946860344613
iteration : 1381
train acc:  0.8984375
train loss:  0.289785772562027
train gradient:  0.10575972012163995
iteration : 1382
train acc:  0.90625
train loss:  0.23231132328510284
train gradient:  0.06940963548227569
iteration : 1383
train acc:  0.8359375
train loss:  0.3050450086593628
train gradient:  0.1245332742645744
iteration : 1384
train acc:  0.890625
train loss:  0.2529827058315277
train gradient:  0.06996007363249056
iteration : 1385
train acc:  0.8359375
train loss:  0.289675772190094
train gradient:  0.10134749090548173
iteration : 1386
train acc:  0.8671875
train loss:  0.3345935344696045
train gradient:  0.11524553960483937
iteration : 1387
train acc:  0.859375
train loss:  0.3327953815460205
train gradient:  0.1695182601335058
iteration : 1388
train acc:  0.890625
train loss:  0.3121582567691803
train gradient:  0.23033092739802058
iteration : 1389
train acc:  0.859375
train loss:  0.300747275352478
train gradient:  0.1552550399395925
iteration : 1390
train acc:  0.8984375
train loss:  0.2273482084274292
train gradient:  0.08888696216452054
iteration : 1391
train acc:  0.8359375
train loss:  0.36498862504959106
train gradient:  0.13356754838674118
iteration : 1392
train acc:  0.921875
train loss:  0.23340913653373718
train gradient:  0.08827177807456785
iteration : 1393
train acc:  0.8671875
train loss:  0.34797561168670654
train gradient:  0.20175880176272093
iteration : 1394
train acc:  0.8828125
train loss:  0.2805079221725464
train gradient:  0.11348317522429235
iteration : 1395
train acc:  0.8984375
train loss:  0.21501128375530243
train gradient:  0.08473387671718562
iteration : 1396
train acc:  0.8671875
train loss:  0.29887497425079346
train gradient:  0.08671800318594348
iteration : 1397
train acc:  0.890625
train loss:  0.3027788996696472
train gradient:  0.11675017060687432
iteration : 1398
train acc:  0.8671875
train loss:  0.3268110752105713
train gradient:  0.14083572109285042
iteration : 1399
train acc:  0.8515625
train loss:  0.38546276092529297
train gradient:  0.16389582928879706
iteration : 1400
train acc:  0.90625
train loss:  0.2680202126502991
train gradient:  0.1051096334029668
iteration : 1401
train acc:  0.8203125
train loss:  0.4045361876487732
train gradient:  0.20625796341431848
iteration : 1402
train acc:  0.9296875
train loss:  0.2231004536151886
train gradient:  0.07065981749779701
iteration : 1403
train acc:  0.828125
train loss:  0.33401936292648315
train gradient:  0.12275047958944169
iteration : 1404
train acc:  0.8125
train loss:  0.4271552860736847
train gradient:  0.27514749236213754
iteration : 1405
train acc:  0.8203125
train loss:  0.43179216980934143
train gradient:  0.23828151057043262
iteration : 1406
train acc:  0.859375
train loss:  0.3238612115383148
train gradient:  0.12085436592904125
iteration : 1407
train acc:  0.90625
train loss:  0.20765045285224915
train gradient:  0.0863243122959114
iteration : 1408
train acc:  0.8828125
train loss:  0.2625366449356079
train gradient:  0.08172434242329277
iteration : 1409
train acc:  0.890625
train loss:  0.272550106048584
train gradient:  0.0995908421893014
iteration : 1410
train acc:  0.8515625
train loss:  0.3591105341911316
train gradient:  0.13948250132438114
iteration : 1411
train acc:  0.859375
train loss:  0.26208657026290894
train gradient:  0.09119837781982695
iteration : 1412
train acc:  0.8515625
train loss:  0.3162356913089752
train gradient:  0.10207657388030182
iteration : 1413
train acc:  0.9140625
train loss:  0.2763007879257202
train gradient:  0.09946057306639232
iteration : 1414
train acc:  0.9140625
train loss:  0.2559638023376465
train gradient:  0.09073268485926543
iteration : 1415
train acc:  0.890625
train loss:  0.2777618169784546
train gradient:  0.12861544385805135
iteration : 1416
train acc:  0.8359375
train loss:  0.37151557207107544
train gradient:  0.1615883349274473
iteration : 1417
train acc:  0.8046875
train loss:  0.40614569187164307
train gradient:  0.20411570969042614
iteration : 1418
train acc:  0.90625
train loss:  0.29132699966430664
train gradient:  0.08604983418008079
iteration : 1419
train acc:  0.890625
train loss:  0.30401790142059326
train gradient:  0.08195397480579912
iteration : 1420
train acc:  0.8515625
train loss:  0.312502920627594
train gradient:  0.09848914528587728
iteration : 1421
train acc:  0.84375
train loss:  0.35650748014450073
train gradient:  0.16400921526383422
iteration : 1422
train acc:  0.890625
train loss:  0.30808812379837036
train gradient:  0.1319403345027789
iteration : 1423
train acc:  0.8671875
train loss:  0.2782307267189026
train gradient:  0.09163799751281236
iteration : 1424
train acc:  0.84375
train loss:  0.3305477499961853
train gradient:  0.16732271474279725
iteration : 1425
train acc:  0.875
train loss:  0.33517664670944214
train gradient:  0.13513263580656099
iteration : 1426
train acc:  0.84375
train loss:  0.3255470395088196
train gradient:  0.1353119525058572
iteration : 1427
train acc:  0.859375
train loss:  0.3305785357952118
train gradient:  0.12018481202368379
iteration : 1428
train acc:  0.8984375
train loss:  0.3132827877998352
train gradient:  0.11832005433510664
iteration : 1429
train acc:  0.8515625
train loss:  0.38571059703826904
train gradient:  0.2372930631882479
iteration : 1430
train acc:  0.8515625
train loss:  0.33299022912979126
train gradient:  0.12350281659342245
iteration : 1431
train acc:  0.8515625
train loss:  0.40053343772888184
train gradient:  0.24916654937506905
iteration : 1432
train acc:  0.8671875
train loss:  0.3123827278614044
train gradient:  0.12801069383513045
iteration : 1433
train acc:  0.90625
train loss:  0.2940279543399811
train gradient:  0.1529093547743798
iteration : 1434
train acc:  0.875
train loss:  0.28929412364959717
train gradient:  0.08889770655736998
iteration : 1435
train acc:  0.8671875
train loss:  0.30733951926231384
train gradient:  0.17580814829803498
iteration : 1436
train acc:  0.875
train loss:  0.3499525785446167
train gradient:  0.11462257649232392
iteration : 1437
train acc:  0.859375
train loss:  0.34260356426239014
train gradient:  0.1465403579519045
iteration : 1438
train acc:  0.84375
train loss:  0.30475056171417236
train gradient:  0.14250359932035478
iteration : 1439
train acc:  0.90625
train loss:  0.22272327542304993
train gradient:  0.128872606743389
iteration : 1440
train acc:  0.8515625
train loss:  0.29783761501312256
train gradient:  0.13505064881949233
iteration : 1441
train acc:  0.828125
train loss:  0.3418116271495819
train gradient:  0.12813651474284954
iteration : 1442
train acc:  0.890625
train loss:  0.2785748839378357
train gradient:  0.136217649991014
iteration : 1443
train acc:  0.8828125
train loss:  0.27664464712142944
train gradient:  0.10984835981011247
iteration : 1444
train acc:  0.859375
train loss:  0.31046062707901
train gradient:  0.18160780650404323
iteration : 1445
train acc:  0.90625
train loss:  0.2545871138572693
train gradient:  0.13849321733623496
iteration : 1446
train acc:  0.828125
train loss:  0.34506481885910034
train gradient:  0.12931062996273196
iteration : 1447
train acc:  0.8671875
train loss:  0.2791910171508789
train gradient:  0.09965087145410406
iteration : 1448
train acc:  0.8828125
train loss:  0.3314845860004425
train gradient:  0.1496811964913245
iteration : 1449
train acc:  0.8515625
train loss:  0.3164075016975403
train gradient:  0.13875004807698255
iteration : 1450
train acc:  0.859375
train loss:  0.3615180253982544
train gradient:  0.12721923030086452
iteration : 1451
train acc:  0.8515625
train loss:  0.30655980110168457
train gradient:  0.12182142060756221
iteration : 1452
train acc:  0.859375
train loss:  0.26011818647384644
train gradient:  0.11308554077608564
iteration : 1453
train acc:  0.8515625
train loss:  0.36113041639328003
train gradient:  0.149931627802313
iteration : 1454
train acc:  0.859375
train loss:  0.33808761835098267
train gradient:  0.11799779215397203
iteration : 1455
train acc:  0.875
train loss:  0.3214583694934845
train gradient:  0.09541126884614096
iteration : 1456
train acc:  0.8984375
train loss:  0.2705685496330261
train gradient:  0.12935237167669378
iteration : 1457
train acc:  0.828125
train loss:  0.35063737630844116
train gradient:  0.1700039153014294
iteration : 1458
train acc:  0.9296875
train loss:  0.2046246975660324
train gradient:  0.07592365772548959
iteration : 1459
train acc:  0.8671875
train loss:  0.3401637077331543
train gradient:  0.11478974664365608
iteration : 1460
train acc:  0.859375
train loss:  0.30209577083587646
train gradient:  0.11991029427366058
iteration : 1461
train acc:  0.875
train loss:  0.28351303935050964
train gradient:  0.0988859352350852
iteration : 1462
train acc:  0.8828125
train loss:  0.26431572437286377
train gradient:  0.10891045714389273
iteration : 1463
train acc:  0.84375
train loss:  0.39109504222869873
train gradient:  0.17923788900838025
iteration : 1464
train acc:  0.8828125
train loss:  0.24870410561561584
train gradient:  0.10599676945835831
iteration : 1465
train acc:  0.8671875
train loss:  0.327786922454834
train gradient:  0.1191079048462473
iteration : 1466
train acc:  0.859375
train loss:  0.30150240659713745
train gradient:  0.13323687708989657
iteration : 1467
train acc:  0.859375
train loss:  0.2805755138397217
train gradient:  0.11161734309780218
iteration : 1468
train acc:  0.828125
train loss:  0.32312634587287903
train gradient:  0.13655889598741144
iteration : 1469
train acc:  0.859375
train loss:  0.3177521824836731
train gradient:  0.14004460179282632
iteration : 1470
train acc:  0.8828125
train loss:  0.276624858379364
train gradient:  0.08882779879106718
iteration : 1471
train acc:  0.890625
train loss:  0.2305152714252472
train gradient:  0.06749397031587577
iteration : 1472
train acc:  0.8203125
train loss:  0.29335880279541016
train gradient:  0.1058727509322814
iteration : 1473
train acc:  0.859375
train loss:  0.3430696725845337
train gradient:  0.13301676656850625
iteration : 1474
train acc:  0.8359375
train loss:  0.36130258440971375
train gradient:  0.1711933699361518
iteration : 1475
train acc:  0.8671875
train loss:  0.29093074798583984
train gradient:  0.09182224961345968
iteration : 1476
train acc:  0.8359375
train loss:  0.3041093349456787
train gradient:  0.1620590433755651
iteration : 1477
train acc:  0.8671875
train loss:  0.2839839458465576
train gradient:  0.1411967558616663
iteration : 1478
train acc:  0.8828125
train loss:  0.30703142285346985
train gradient:  0.10965637990717128
iteration : 1479
train acc:  0.875
train loss:  0.2696055471897125
train gradient:  0.1058520999047097
iteration : 1480
train acc:  0.859375
train loss:  0.28498759865760803
train gradient:  0.14011430572381023
iteration : 1481
train acc:  0.890625
train loss:  0.23218153417110443
train gradient:  0.07602390295297831
iteration : 1482
train acc:  0.84375
train loss:  0.341575562953949
train gradient:  0.11955249065556828
iteration : 1483
train acc:  0.859375
train loss:  0.34327834844589233
train gradient:  0.12341216704372988
iteration : 1484
train acc:  0.890625
train loss:  0.26278430223464966
train gradient:  0.09202345629668668
iteration : 1485
train acc:  0.859375
train loss:  0.2681451141834259
train gradient:  0.12025453695845903
iteration : 1486
train acc:  0.8984375
train loss:  0.29073482751846313
train gradient:  0.1673658561336514
iteration : 1487
train acc:  0.9140625
train loss:  0.2737081050872803
train gradient:  0.07803346860055486
iteration : 1488
train acc:  0.84375
train loss:  0.32564812898635864
train gradient:  0.12322940497032363
iteration : 1489
train acc:  0.796875
train loss:  0.38859081268310547
train gradient:  0.1823551861666839
iteration : 1490
train acc:  0.8046875
train loss:  0.4255586564540863
train gradient:  0.19169533918549916
iteration : 1491
train acc:  0.8828125
train loss:  0.29857075214385986
train gradient:  0.13481637960659376
iteration : 1492
train acc:  0.84375
train loss:  0.3063580393791199
train gradient:  0.09919122351231402
iteration : 1493
train acc:  0.8515625
train loss:  0.3132638931274414
train gradient:  0.17156979005518058
iteration : 1494
train acc:  0.875
train loss:  0.2524721026420593
train gradient:  0.0918710406373411
iteration : 1495
train acc:  0.8984375
train loss:  0.27871254086494446
train gradient:  0.06858095527695526
iteration : 1496
train acc:  0.8671875
train loss:  0.26964637637138367
train gradient:  0.10496390118992288
iteration : 1497
train acc:  0.8828125
train loss:  0.2954665422439575
train gradient:  0.13555661041362294
iteration : 1498
train acc:  0.84375
train loss:  0.32804977893829346
train gradient:  0.1390223540590264
iteration : 1499
train acc:  0.859375
train loss:  0.3051920235157013
train gradient:  0.10397449554978563
iteration : 1500
train acc:  0.8984375
train loss:  0.2965981960296631
train gradient:  0.12351991348619243
iteration : 1501
train acc:  0.8046875
train loss:  0.3200727105140686
train gradient:  0.11653113766440207
iteration : 1502
train acc:  0.8671875
train loss:  0.30171456933021545
train gradient:  0.1110098424560604
iteration : 1503
train acc:  0.859375
train loss:  0.2761383056640625
train gradient:  0.13500807780581697
iteration : 1504
train acc:  0.8984375
train loss:  0.27044370770454407
train gradient:  0.12273928428550127
iteration : 1505
train acc:  0.90625
train loss:  0.29207706451416016
train gradient:  0.12441253137907284
iteration : 1506
train acc:  0.8828125
train loss:  0.3310401439666748
train gradient:  0.13713783028088145
iteration : 1507
train acc:  0.8515625
train loss:  0.32426387071609497
train gradient:  0.13458520842107996
iteration : 1508
train acc:  0.875
train loss:  0.3455412983894348
train gradient:  0.16738817439589
iteration : 1509
train acc:  0.8671875
train loss:  0.37141096591949463
train gradient:  0.18332570415443378
iteration : 1510
train acc:  0.828125
train loss:  0.32450035214424133
train gradient:  0.12018473354432682
iteration : 1511
train acc:  0.828125
train loss:  0.35662490129470825
train gradient:  0.1353819583269431
iteration : 1512
train acc:  0.8984375
train loss:  0.2530359625816345
train gradient:  0.09087361527203551
iteration : 1513
train acc:  0.8359375
train loss:  0.3496119976043701
train gradient:  0.1288319342225549
iteration : 1514
train acc:  0.859375
train loss:  0.31025099754333496
train gradient:  0.14083167357831947
iteration : 1515
train acc:  0.8515625
train loss:  0.3288126289844513
train gradient:  0.1349143948213966
iteration : 1516
train acc:  0.8828125
train loss:  0.28713491559028625
train gradient:  0.1504079137029555
iteration : 1517
train acc:  0.8828125
train loss:  0.28376367688179016
train gradient:  0.10673509766960088
iteration : 1518
train acc:  0.8671875
train loss:  0.28421422839164734
train gradient:  0.10769237042872253
iteration : 1519
train acc:  0.90625
train loss:  0.2710716426372528
train gradient:  0.06929259075135365
iteration : 1520
train acc:  0.78125
train loss:  0.4463818073272705
train gradient:  0.2674778357396252
iteration : 1521
train acc:  0.90625
train loss:  0.2225113809108734
train gradient:  0.07562989884836041
iteration : 1522
train acc:  0.859375
train loss:  0.25868356227874756
train gradient:  0.07139017011709665
iteration : 1523
train acc:  0.8671875
train loss:  0.34082409739494324
train gradient:  0.13483675290598723
iteration : 1524
train acc:  0.8671875
train loss:  0.3049667775630951
train gradient:  0.1266842872587677
iteration : 1525
train acc:  0.875
train loss:  0.3150612413883209
train gradient:  0.13700781345276808
iteration : 1526
train acc:  0.859375
train loss:  0.2950955033302307
train gradient:  0.07327515859580319
iteration : 1527
train acc:  0.859375
train loss:  0.34214258193969727
train gradient:  0.13037912287939193
iteration : 1528
train acc:  0.8515625
train loss:  0.2859264016151428
train gradient:  0.10365414443564244
iteration : 1529
train acc:  0.875
train loss:  0.2955557405948639
train gradient:  0.09614846217616979
iteration : 1530
train acc:  0.859375
train loss:  0.3641074299812317
train gradient:  0.13611498302614944
iteration : 1531
train acc:  0.921875
train loss:  0.22272448241710663
train gradient:  0.05886700547564219
iteration : 1532
train acc:  0.7890625
train loss:  0.41259801387786865
train gradient:  0.1615613526486857
iteration : 1533
train acc:  0.828125
train loss:  0.34939637780189514
train gradient:  0.13923221169664038
iteration : 1534
train acc:  0.8046875
train loss:  0.3816266655921936
train gradient:  0.17709251030801673
iteration : 1535
train acc:  0.859375
train loss:  0.3138331174850464
train gradient:  0.13866300514067154
iteration : 1536
train acc:  0.875
train loss:  0.25769323110580444
train gradient:  0.11466159334007507
iteration : 1537
train acc:  0.8359375
train loss:  0.34675800800323486
train gradient:  0.13336200230309564
iteration : 1538
train acc:  0.859375
train loss:  0.30842870473861694
train gradient:  0.14188393683538975
iteration : 1539
train acc:  0.875
train loss:  0.2998373806476593
train gradient:  0.10352986993846512
iteration : 1540
train acc:  0.9453125
train loss:  0.19524580240249634
train gradient:  0.07692574573109251
iteration : 1541
train acc:  0.84375
train loss:  0.3361114263534546
train gradient:  0.2902025354012549
iteration : 1542
train acc:  0.828125
train loss:  0.46588134765625
train gradient:  0.21968383098824296
iteration : 1543
train acc:  0.8828125
train loss:  0.2316674292087555
train gradient:  0.07290642743080795
iteration : 1544
train acc:  0.8359375
train loss:  0.36148911714553833
train gradient:  0.15762066047483553
iteration : 1545
train acc:  0.8828125
train loss:  0.2847557067871094
train gradient:  0.10066890614549653
iteration : 1546
train acc:  0.8671875
train loss:  0.3071533441543579
train gradient:  0.10502267043442469
iteration : 1547
train acc:  0.890625
train loss:  0.2575598955154419
train gradient:  0.09682936930737046
iteration : 1548
train acc:  0.859375
train loss:  0.3349757492542267
train gradient:  0.12563347076406528
iteration : 1549
train acc:  0.8515625
train loss:  0.29656657576560974
train gradient:  0.09449245710725299
iteration : 1550
train acc:  0.8828125
train loss:  0.2992766499519348
train gradient:  0.11908799225160252
iteration : 1551
train acc:  0.84375
train loss:  0.2895525395870209
train gradient:  0.10067024418633917
iteration : 1552
train acc:  0.875
train loss:  0.30744656920433044
train gradient:  0.1661241540648814
iteration : 1553
train acc:  0.8203125
train loss:  0.32764172554016113
train gradient:  0.14511588471802145
iteration : 1554
train acc:  0.828125
train loss:  0.29230546951293945
train gradient:  0.1629483235484871
iteration : 1555
train acc:  0.8125
train loss:  0.39710062742233276
train gradient:  0.23021795146764473
iteration : 1556
train acc:  0.8671875
train loss:  0.31133976578712463
train gradient:  0.10153634700430064
iteration : 1557
train acc:  0.8984375
train loss:  0.25992193818092346
train gradient:  0.0891549127107335
iteration : 1558
train acc:  0.8671875
train loss:  0.3347538709640503
train gradient:  0.1004442131381194
iteration : 1559
train acc:  0.8828125
train loss:  0.3372959792613983
train gradient:  0.13214634202110156
iteration : 1560
train acc:  0.84375
train loss:  0.29065191745758057
train gradient:  0.14464249159324394
iteration : 1561
train acc:  0.890625
train loss:  0.29761773347854614
train gradient:  0.16362651240377796
iteration : 1562
train acc:  0.8359375
train loss:  0.35267752408981323
train gradient:  0.13618768368698328
iteration : 1563
train acc:  0.875
train loss:  0.27994588017463684
train gradient:  0.07908751811913643
iteration : 1564
train acc:  0.84375
train loss:  0.3508276343345642
train gradient:  0.1685810008797818
iteration : 1565
train acc:  0.875
train loss:  0.29021573066711426
train gradient:  0.08892846194325475
iteration : 1566
train acc:  0.7890625
train loss:  0.42986494302749634
train gradient:  0.20049621303566567
iteration : 1567
train acc:  0.8671875
train loss:  0.3062019944190979
train gradient:  0.08540963749153876
iteration : 1568
train acc:  0.8515625
train loss:  0.33626633882522583
train gradient:  0.17015268189303032
iteration : 1569
train acc:  0.828125
train loss:  0.3617280423641205
train gradient:  0.13401803022195893
iteration : 1570
train acc:  0.828125
train loss:  0.32753652334213257
train gradient:  0.1358101514980991
iteration : 1571
train acc:  0.8828125
train loss:  0.28246021270751953
train gradient:  0.0790707844194541
iteration : 1572
train acc:  0.859375
train loss:  0.32039695978164673
train gradient:  0.09816152376699294
iteration : 1573
train acc:  0.8828125
train loss:  0.3098870515823364
train gradient:  0.11217382361634767
iteration : 1574
train acc:  0.890625
train loss:  0.376733660697937
train gradient:  0.1541686631212864
iteration : 1575
train acc:  0.8984375
train loss:  0.24031640589237213
train gradient:  0.09153492949552927
iteration : 1576
train acc:  0.8515625
train loss:  0.3295668959617615
train gradient:  0.16229643046671594
iteration : 1577
train acc:  0.875
train loss:  0.30622225999832153
train gradient:  0.13927522631078376
iteration : 1578
train acc:  0.8828125
train loss:  0.3105543255805969
train gradient:  0.09121061767459807
iteration : 1579
train acc:  0.859375
train loss:  0.34051454067230225
train gradient:  0.18046747305758726
iteration : 1580
train acc:  0.8515625
train loss:  0.33489587903022766
train gradient:  0.16119640824554027
iteration : 1581
train acc:  0.828125
train loss:  0.3965693414211273
train gradient:  0.21894346209648596
iteration : 1582
train acc:  0.8671875
train loss:  0.2766025960445404
train gradient:  0.15233803334377186
iteration : 1583
train acc:  0.796875
train loss:  0.34534475207328796
train gradient:  0.1412579847819443
iteration : 1584
train acc:  0.8828125
train loss:  0.2796253561973572
train gradient:  0.09055757182796492
iteration : 1585
train acc:  0.828125
train loss:  0.43219101428985596
train gradient:  0.23151245925516706
iteration : 1586
train acc:  0.859375
train loss:  0.36055952310562134
train gradient:  0.15743120289741314
iteration : 1587
train acc:  0.8359375
train loss:  0.3221933841705322
train gradient:  0.09688954911229554
iteration : 1588
train acc:  0.859375
train loss:  0.30528414249420166
train gradient:  0.11843442008824864
iteration : 1589
train acc:  0.84375
train loss:  0.3808247447013855
train gradient:  0.17627700341689168
iteration : 1590
train acc:  0.890625
train loss:  0.2128835916519165
train gradient:  0.10130715894048575
iteration : 1591
train acc:  0.8828125
train loss:  0.2971673607826233
train gradient:  0.10834344966539725
iteration : 1592
train acc:  0.875
train loss:  0.3668552041053772
train gradient:  0.1517699005085218
iteration : 1593
train acc:  0.890625
train loss:  0.3475140333175659
train gradient:  0.14016046795690654
iteration : 1594
train acc:  0.875
train loss:  0.2928682267665863
train gradient:  0.10842494887473923
iteration : 1595
train acc:  0.8125
train loss:  0.37727946043014526
train gradient:  0.15290250504175562
iteration : 1596
train acc:  0.8359375
train loss:  0.3594449460506439
train gradient:  0.1404085213417725
iteration : 1597
train acc:  0.828125
train loss:  0.36121538281440735
train gradient:  0.18771884785844645
iteration : 1598
train acc:  0.875
train loss:  0.3111298382282257
train gradient:  0.11074458764007031
iteration : 1599
train acc:  0.890625
train loss:  0.26740923523902893
train gradient:  0.10523067248704802
iteration : 1600
train acc:  0.8671875
train loss:  0.34506604075431824
train gradient:  0.1425265268512353
iteration : 1601
train acc:  0.84375
train loss:  0.36347874999046326
train gradient:  0.1834855872531358
iteration : 1602
train acc:  0.8125
train loss:  0.34173232316970825
train gradient:  0.11352998681530956
iteration : 1603
train acc:  0.8359375
train loss:  0.3149864673614502
train gradient:  0.13263912965476912
iteration : 1604
train acc:  0.9140625
train loss:  0.261343389749527
train gradient:  0.07892960332915454
iteration : 1605
train acc:  0.84375
train loss:  0.32717400789260864
train gradient:  0.1246014201920227
iteration : 1606
train acc:  0.84375
train loss:  0.31029611825942993
train gradient:  0.16976863597406866
iteration : 1607
train acc:  0.8671875
train loss:  0.33418726921081543
train gradient:  0.15391080658561285
iteration : 1608
train acc:  0.8984375
train loss:  0.2770777642726898
train gradient:  0.07647689020117626
iteration : 1609
train acc:  0.890625
train loss:  0.264917254447937
train gradient:  0.10599722603606272
iteration : 1610
train acc:  0.9140625
train loss:  0.28371191024780273
train gradient:  0.10116070401794813
iteration : 1611
train acc:  0.90625
train loss:  0.2650960087776184
train gradient:  0.09273984905856474
iteration : 1612
train acc:  0.8671875
train loss:  0.2903870940208435
train gradient:  0.09958207229533945
iteration : 1613
train acc:  0.8984375
train loss:  0.2593543231487274
train gradient:  0.07698516576132713
iteration : 1614
train acc:  0.8046875
train loss:  0.3314615488052368
train gradient:  0.12278044321088927
iteration : 1615
train acc:  0.8671875
train loss:  0.3118247985839844
train gradient:  0.11376963485868714
iteration : 1616
train acc:  0.875
train loss:  0.31115883588790894
train gradient:  0.1081211995580485
iteration : 1617
train acc:  0.859375
train loss:  0.31540346145629883
train gradient:  0.1363893950370647
iteration : 1618
train acc:  0.8515625
train loss:  0.3220863938331604
train gradient:  0.11300041366138268
iteration : 1619
train acc:  0.9140625
train loss:  0.24991783499717712
train gradient:  0.06167327553929751
iteration : 1620
train acc:  0.859375
train loss:  0.38414978981018066
train gradient:  0.13817223020667768
iteration : 1621
train acc:  0.9140625
train loss:  0.29092955589294434
train gradient:  0.09075916840588469
iteration : 1622
train acc:  0.8359375
train loss:  0.31341663002967834
train gradient:  0.11719435430677481
iteration : 1623
train acc:  0.84375
train loss:  0.35940730571746826
train gradient:  0.18874779544047016
iteration : 1624
train acc:  0.875
train loss:  0.29312199354171753
train gradient:  0.1243684721885883
iteration : 1625
train acc:  0.90625
train loss:  0.2485385537147522
train gradient:  0.08757830736540978
iteration : 1626
train acc:  0.8671875
train loss:  0.2706362009048462
train gradient:  0.11057605848216047
iteration : 1627
train acc:  0.8984375
train loss:  0.2994406223297119
train gradient:  0.1214271980719386
iteration : 1628
train acc:  0.875
train loss:  0.27300313115119934
train gradient:  0.10144667813842433
iteration : 1629
train acc:  0.8515625
train loss:  0.31381309032440186
train gradient:  0.09770372680656475
iteration : 1630
train acc:  0.8359375
train loss:  0.3433806598186493
train gradient:  0.13096434641867594
iteration : 1631
train acc:  0.859375
train loss:  0.31545162200927734
train gradient:  0.1428728278269035
iteration : 1632
train acc:  0.8671875
train loss:  0.303005188703537
train gradient:  0.1001548910812669
iteration : 1633
train acc:  0.828125
train loss:  0.4224435091018677
train gradient:  0.19107872023830824
iteration : 1634
train acc:  0.828125
train loss:  0.331805557012558
train gradient:  0.1329923337482838
iteration : 1635
train acc:  0.8359375
train loss:  0.34932655096054077
train gradient:  0.19547240783371256
iteration : 1636
train acc:  0.8515625
train loss:  0.31631553173065186
train gradient:  0.16593801510284925
iteration : 1637
train acc:  0.8359375
train loss:  0.3812239170074463
train gradient:  0.16383668952208819
iteration : 1638
train acc:  0.8984375
train loss:  0.24665868282318115
train gradient:  0.09638526152490771
iteration : 1639
train acc:  0.890625
train loss:  0.2603045701980591
train gradient:  0.09057621900284037
iteration : 1640
train acc:  0.8515625
train loss:  0.3084849715232849
train gradient:  0.1052402729710105
iteration : 1641
train acc:  0.8515625
train loss:  0.3060818314552307
train gradient:  0.15957753948872672
iteration : 1642
train acc:  0.875
train loss:  0.265743613243103
train gradient:  0.1279590845306313
iteration : 1643
train acc:  0.859375
train loss:  0.2891005575656891
train gradient:  0.11392153618335527
iteration : 1644
train acc:  0.859375
train loss:  0.28897273540496826
train gradient:  0.13411176633491112
iteration : 1645
train acc:  0.8515625
train loss:  0.3143666982650757
train gradient:  0.1438573320466699
iteration : 1646
train acc:  0.8984375
train loss:  0.2118527591228485
train gradient:  0.09196976028337434
iteration : 1647
train acc:  0.8671875
train loss:  0.2900586426258087
train gradient:  0.07979232522699409
iteration : 1648
train acc:  0.890625
train loss:  0.23851153254508972
train gradient:  0.11369282728668513
iteration : 1649
train acc:  0.859375
train loss:  0.34109100699424744
train gradient:  0.3835003213086857
iteration : 1650
train acc:  0.9296875
train loss:  0.26372307538986206
train gradient:  0.10925814128044259
iteration : 1651
train acc:  0.828125
train loss:  0.3528450131416321
train gradient:  0.13485222582717743
iteration : 1652
train acc:  0.8828125
train loss:  0.28382688760757446
train gradient:  0.12021642029957179
iteration : 1653
train acc:  0.8984375
train loss:  0.2800140976905823
train gradient:  0.09320565279081891
iteration : 1654
train acc:  0.859375
train loss:  0.2879835367202759
train gradient:  0.09729818573026039
iteration : 1655
train acc:  0.84375
train loss:  0.3402310609817505
train gradient:  0.14671285368120238
iteration : 1656
train acc:  0.8984375
train loss:  0.35088589787483215
train gradient:  0.14050160634305725
iteration : 1657
train acc:  0.8828125
train loss:  0.29393231868743896
train gradient:  0.1286789415772187
iteration : 1658
train acc:  0.828125
train loss:  0.3177173435688019
train gradient:  0.1266946003893216
iteration : 1659
train acc:  0.8828125
train loss:  0.28806379437446594
train gradient:  0.1074727670963919
iteration : 1660
train acc:  0.875
train loss:  0.2577769458293915
train gradient:  0.1269179292884784
iteration : 1661
train acc:  0.8671875
train loss:  0.34540820121765137
train gradient:  0.1719109704368602
iteration : 1662
train acc:  0.8984375
train loss:  0.29009270668029785
train gradient:  0.16417462862041798
iteration : 1663
train acc:  0.875
train loss:  0.3146226406097412
train gradient:  0.12109139538957742
iteration : 1664
train acc:  0.8515625
train loss:  0.3041548430919647
train gradient:  0.09655252317820695
iteration : 1665
train acc:  0.8203125
train loss:  0.35625848174095154
train gradient:  0.19363190060073426
iteration : 1666
train acc:  0.90625
train loss:  0.2532827854156494
train gradient:  0.08600914321489703
iteration : 1667
train acc:  0.8671875
train loss:  0.3643064498901367
train gradient:  0.15964289617059457
iteration : 1668
train acc:  0.9140625
train loss:  0.29800957441329956
train gradient:  0.14630270457800892
iteration : 1669
train acc:  0.921875
train loss:  0.22191834449768066
train gradient:  0.07334285831992592
iteration : 1670
train acc:  0.875
train loss:  0.3184356093406677
train gradient:  0.14380244002289905
